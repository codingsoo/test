[
  {
    "number": 47740,
    "title": "[Core] does ray support affinity scheduling for tasks or actions? ",
    "created_at": "2024-09-19T03:58:33Z",
    "closed_at": "2024-10-08T11:37:57Z",
    "labels": [
      "question",
      "P1",
      "core"
    ],
    "url": "https://github.com/ray-project/ray/issues/47740",
    "body": "### Description\n\nDoes Ray support affinity scheduling for tasks or actors, like pod affinity for k8s?\r\n\r\nAllows a group of different tasks to run on the same node and share Object storage memory, reducing grpc transfers.\n\n### Use case\n\nIn the ray data scenario, if there are multiple Physical operators of the ActorPoolMapOperator type, By using task affinity, objects generated by the parent Operator task can be directly consumed by tasks of the current Operator, reducing Object transfer across nodes.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/47740/comments",
    "author": "kadisi",
    "comments": [
      {
        "user": "jjyao",
        "created_at": "2024-10-07T21:26:47Z",
        "body": "@kadisi You can use placement group to schedule a group of tasks on the same node."
      }
    ]
  },
  {
    "number": 45815,
    "title": "C++ Python Cross-language invocation",
    "created_at": "2024-06-07T23:48:17Z",
    "closed_at": "2024-06-12T22:49:26Z",
    "labels": [
      "question",
      "triage",
      "core"
    ],
    "url": "https://github.com/ray-project/ray/issues/45815",
    "body": "### Description\n\n_No response_\n\n### Use case\n\nWe have 2 tasks\r\ntask1 (can be thought as reading parquet files into memory)\r\ntask2 (using torch tensors do something)\r\nWe are thinking of having task1 in cpp and task2 in python. \r\nDoes ray support that?\r\nI saw some cross platform invocation documentation but that was mainly around java-cpp.\r\nIf cross platform is not the solution how can we manage this?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/45815/comments",
    "author": "jaystarshot",
    "comments": [
      {
        "user": "jaystarshot",
        "created_at": "2024-06-10T20:59:04Z",
        "body": "cc: @SongGuyang "
      },
      {
        "user": "SongGuyang",
        "created_at": "2024-06-11T02:28:02Z",
        "body": "Actually, we already have a simple cross language implementation between C++ and Python. But this feature is still experimental and we don't have official docs about it. You can learn from the two tests: PythonInvocationTest of cpp/src/ray/test/cluster/cluster_mode_test.cc and cpp/test_python_call_cpp.py."
      }
    ]
  },
  {
    "number": 40397,
    "title": "[RLlib] Saving an environment independent rllib checkpoint isn't possible anymore",
    "created_at": "2023-10-17T08:00:24Z",
    "closed_at": "2023-10-18T11:58:25Z",
    "labels": [
      "question",
      "P2",
      "rllib-checkpointing-or-recovery",
      "rllib-env"
    ],
    "url": "https://github.com/ray-project/ray/issues/40397",
    "body": "### What happened + What you expected to happen\n\nSaving an environment independent rllib checkpoint isn't possible anymore. The learning environment implementation used for training isn't necessarily available on deployment environment, why restoring/unpickling a checkpoint w/o the need of the learning environment makes sense.\r\nWith previous versions a checkpoint could be restored w/o the dependence of the learning environment implementation, setting \"env=None\" in the algorithm config.\n\n### Versions / Dependencies\n\nray 2.6\r\npython 3.10\r\nubuntu 22.04\n\n### Reproduction script\n\n`alg_class = get_trainable_cls(\"DQN\")`\r\n`config['env'] = None`\r\n`config.pop('env_config')`\r\n`algorithm = alg_class(config=config)`\r\n`algorithm.restore(checkpoint_path)`\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/40397/comments",
    "author": "karstenddwx",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2023-10-18T11:57:54Z",
        "body": "Hey @karstenddwx , thanks for raising this issue. I don't actually think that this would have ever worked, not providing an env, b/c of the need for RLlib to know about the spaces for creating the policy/ies in your algorithm.\r\n\r\nThe code below does work fine. Could you confirm that doing this would help you with the problem?\r\n\r\n```\r\nfrom ray.rllib.algorithms.ppo import PPOConfig\r\nimport gymnasium as gym\r\n\r\n\r\nenv = gym.make(\"CartPole-v1\")\r\n\r\nconfig = (\r\n    PPOConfig()\r\n    .environment(\"CartPole-v1\")\r\n)\r\nalgo = config.build()\r\nalgo.train()\r\ndir = algo.save()\r\nalgo.stop()\r\n\r\n# \"Fix\" config: Remove Env, BUT provide spaces information manually, such that RLlib can create the policies correctly.\r\nconfig.environment(\r\n    None,\r\n    observation_space=env.observation_space,\r\n    action_space=env.action_space,\r\n).rollouts(num_rollout_workers=0)\r\nalgo2 = config.build()\r\nalgo2.restore(dir)\r\n\r\n# print some actions\r\nprint(algo2.compute_single_action(env.reset()[0]))  # [0] -> env.reset returns obs and infos\r\nprint(algo2.compute_single_action(env.reset()[0]))\r\nprint(algo2.compute_single_action(env.reset()[0]))\r\n```\r\n\r\n"
      },
      {
        "user": "karstenddwx",
        "created_at": "2023-10-19T09:08:39Z",
        "body": "Hi Sven, thanks for looking into this.\r\nI can confirm that not providing an env has still worked with ray 2.5.1, but not with 2.6.\r\n\r\nI have to do some refactoring to be able to run it the AIR 2.x way (build/train/save). I'll let you know whether it works as you suggested."
      }
    ]
  },
  {
    "number": 37677,
    "title": "[<Ray component: Cluster>] KeyError: 'CPU' error in Linux",
    "created_at": "2023-07-22T10:49:28Z",
    "closed_at": "2023-07-24T21:18:06Z",
    "labels": [
      "question",
      "triage",
      "core"
    ],
    "url": "https://github.com/ray-project/ray/issues/37677",
    "body": "### What happened + What you expected to happen\r\n\r\n**What I will do:**\r\nI tried to get the total number of cpus provided by the cluster;\r\n\r\n**What I got wrong:**\r\nThe specific error information is as follows:\r\n{cluster_resources()['CPU']} CPU resources in total;\r\nKeyError: 'CPU'\r\n\r\n**Update:**\r\n_I seem to have found the reason, when there is no available cpu in the cluster, the 'CPU' key is no longer in the returned dict; This leads to errors;_\r\n\r\n### Versions / Dependencies\r\n\r\nray: 2.3.1\r\nos: debian 11\r\npython: 3.9.2\r\n\r\n### Reproduction script\r\n\r\nfrom ray import init, cluster_resources\r\ninit()\r\nprint(f\"{cluster_resources()['CPU']}\")\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/37677/comments",
    "author": "stevenhubhub",
    "comments": [
      {
        "user": "jjyao",
        "created_at": "2023-07-24T21:18:06Z",
        "body": "Yea, try to do `cluster_resources().get(\"CPU\", 0)`"
      },
      {
        "user": "stevenhubhub",
        "created_at": "2023-07-27T08:45:26Z",
        "body": "> Yea, try to do `cluster_resources().get(\"CPU\", 0)`\r\n\r\nThanks!"
      },
      {
        "user": "davide-russo-tfs",
        "created_at": "2024-09-30T10:51:06Z",
        "body": "Good morning, I have the same issue while trying to use Ray on Databricks cluster (with autoscaling). The runtime used is 15.1ML.\r\nI imported the following libraries:\r\n```\r\nfrom ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\r\nfrom ray.util.multiprocessing import Pool\r\nimport ray\r\n```\r\nThis is how I set up the environment:\r\n```\r\nsetup_ray_cluster(\r\n        num_worker_nodes  = 4,\r\n        num_cpus_per_node = 4,\r\n        autoscale          = True\r\n    )\r\nray.init(ignore_reinit_error = True)\r\n```\r\nthen I decorated a function to be run in parallel by using `@ray.remote` and tried to create a pool of processes this way:\r\n```\r\nwith Pool(processes = 8) as pool:\r\n        pool.starmap(foo, inputs)\r\n```\r\n\r\nHow can I solve this problem? Thank you for your help."
      }
    ]
  },
  {
    "number": 32905,
    "title": "RLlib Local Replay Buffer Setup Potential Bug",
    "created_at": "2023-02-28T19:08:45Z",
    "closed_at": "2023-03-09T21:09:21Z",
    "labels": [
      "question",
      "P2"
    ],
    "url": "https://github.com/ray-project/ray/issues/32905",
    "body": "### What happened + What you expected to happen\n\nI don't have reproducible code for an issue as I'm just reading the source code at this time to understand how the library works but I've come across this section of code that, from my limited understanding, appears to be incorrect.\r\n\r\nI'm at git hash 16b2963f1177b207a64ae38716c694e1f524ea4f\r\n\r\nOn line 2693 of python/ray/rllib/algorithms/algorithms.py in _create_local_replay_buffer_if_necessary() is this if condition:\r\n\r\n```\r\n        if not config.get(\"replay_buffer_config\") or config[\"replay_buffer_config\"].get(\r\n            \"no_local_replay_buffer\" or config.get(\"no_local_replay_buffer\")\r\n        ):  \r\n```\r\n\r\nWithout knowing how this is supposed to work (which is why I'm opening a bug report and not a pull request), that just looks incorrect. I imagine it's supposed to be something like:\r\n\r\n```\r\n        if not config.get(\"replay_buffer_config\") or config[\"replay_buffer_config\"].get(\"no_local_replay_buffer\") or config.get(\"no_local_replay_buffer\"):\r\n```\r\n\n\n### Versions / Dependencies\n\ngit 16b2963f1177b207a64ae38716c694e1f524ea4f\n\n### Reproduction script\n\nedit python/ray/rllib/algorithms/algorithms.py\n\n### Issue Severity\n\nNone",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/32905/comments",
    "author": "mechanyx",
    "comments": [
      {
        "user": "mvindiola1",
        "created_at": "2023-03-05T22:04:53Z",
        "body": "Hi @mechanyx,\r\n\r\nWhat change did you make in your suggestion? The logic conditions look identical to me. I must be missing something. "
      },
      {
        "user": "mechanyx",
        "created_at": "2023-03-05T22:11:15Z",
        "body": "@mvindiola1 \r\n\r\nThe current code has an or with 2 conditions with the 2nd or condition having an additional or condition inside a call to get().\r\n\r\nI think it's supposed to be 3 condition or.\r\n\r\nTo state it another way. Here are side by side reductions:\r\n\r\n`if not c.get(\"a\") or c[\"x\"].get(\"b\" or c.get(\"b\"))`\r\n`if not c.get(\"a\") or c[\"x\"].get(\"b\") or c.get(\"b\")`\r\n\r\nI moved 1 parenthesis."
      },
      {
        "user": "ArturNiederfahrenhorst",
        "created_at": "2023-03-08T19:09:23Z",
        "body": "That's indeed a bug, but the behavior ends up being correct by accident because config[\"no_local_replay_buffer\"] is written to config[\"replay_buffer_config][\"no_local_replay_buffer\"] in the validation of the config. I'm opening a PR to fix this."
      }
    ]
  },
  {
    "number": 31151,
    "title": "ray 2.0.0.dev0[<Ray component: Core|RLlib|etc...>] ",
    "created_at": "2022-12-16T06:43:57Z",
    "closed_at": "2022-12-16T12:58:13Z",
    "labels": [
      "question",
      "docs"
    ],
    "url": "https://github.com/ray-project/ray/issues/31151",
    "body": "### Description\n\nHellow,i'm sorry to bother you.I want to use ray 2.0.0.dev0.But I don't know where I can find it.There is only version ray 3.0.0.dev0 in the documentation.Can you tell me where i can get it,thanks!\n\n### Link\n\n_No response_",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/31151/comments",
    "author": "aa-oo",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2022-12-16T12:56:58Z",
        "body": "Hey @aa-oo , thanks for filing this issue.\r\n* All versions that have the \"dev\" in them are referring to the master branch (at that time). The current master branch version is called \"3.0.0.dev0\".\r\n* To get a stable version of Ray, you can simply try `pip install ray==2.2` (brand new one) or some older versions like `pip install ray==2.1` or `pip install ray==2.0`."
      },
      {
        "user": "aa-oo",
        "created_at": "2022-12-16T13:52:05Z",
        "body": "Ok,thank you!\r\n\r\n\r\n\r\n\r\n------------------&nbsp;原始邮件&nbsp;------------------\r\n发件人: \"Sven ***@***.***&gt;; \r\n发送时间: 2022年12月16日(星期五) 晚上8:57\r\n收件人: ***@***.***&gt;; \r\n抄送: ***@***.***&gt;; ***@***.***&gt;; \r\n主题: Re: [ray-project/ray] ray 2.0.0.dev0[<Ray component: Core|RLlib|etc...&gt;]  (Issue #31151)\r\n\r\n\r\n\r\n\r\n\r\n \r\nHey @aa-oo , thanks for filing this issue.\r\n  \r\nAll versions that have the \"dev\" in them are referring to the master branch (at that time). The current master branch version is called \"3.0.0.dev0\".\r\n \r\nTo get a stable version of Ray, you can simply try pip install ray==2.2 (brand new one) or some older versions like pip install ray==2.1 or pip install ray==2.0.\r\n  \r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;"
      }
    ]
  },
  {
    "number": 30662,
    "title": "[tune] How to use an imported parameter via argparse in trainable function",
    "created_at": "2022-11-25T17:32:05Z",
    "closed_at": "2022-11-29T20:04:15Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/30662",
    "body": "### What happened + What you expected to happen\n\nI have a tuning task using an imported parameter via argparse in trainable function. The task crashes complaining the argument is not provided. It works fine If I use it outside the trainable function. \n\n### Versions / Dependencies\n\nRay 2.1.0\n\n### Reproduction script\n\nThe script being imported called “input_param.py”:\r\n\r\n    import sys, argparse\r\n\r\n    parser = argparse.ArgumentParser(description='')\r\n    parser.add_argument('--ttt', type=int, required=True, help='anything > 1')\r\n    args = parser.parse_args()\r\n\r\n    ttt = args.ttt\r\n\r\nThe tuning task code is named as ‘example.py’:\r\n\r\n    import os\r\n    from ray import tune, air\r\n    from hyperopt import hp\r\n    from ray.tune.search.hyperopt import HyperOptSearch\r\n    import input_param as input_param\r\n\r\n    def trainable(config):\r\n        #print('!! ttt = ', input_param.ttt)\r\n        score = config[\"a\"] ** 2 + config[\"b\"]\r\n        tune.report(SCORE=score)\r\n\r\n\r\n    search_space = {\r\n        \"a\": hp.uniform(\"a\", 0, 1),\r\n        \"b\": hp.uniform(\"b\", 0, 1)\r\n        }\r\n\r\n    raw_log_dir = \"./ray_log\"\r\n    raw_log_name = \"example\"\r\n\r\n    algorithm = HyperOptSearch(search_space, metric=\"SCORE\", mode=\"max\", n_initial_points=1)\r\n\r\n\r\n    tuner = tune.Tuner(trainable,\r\n            tune_config = tune.TuneConfig(\r\n                num_samples = 10,\r\n                search_alg=algorithm,\r\n                ),\r\n            param_space=search_space,\r\n            run_config = air.RunConfig(local_dir = raw_log_dir, name = raw_log_name) #\r\n            )\r\n\r\n    print('!! ttt = ', input_param.ttt)\r\n    results = tuner.fit()\r\n    print(results.get_best_result(metric=\"SCORE\", mode=\"max\").config)\r\n\r\nI run the task via the following command:\r\n\r\n    py example.py --ttt 99\r\n\r\nThe following is part of the error:\r\n\r\n    (pid=19560) default_worker.py: error: the following arguments are required: --ttt\r\n    (pid=19560) 2022-11-23 20:45:01,769     ERROR worker.py:763 -- Worker exits with an exit code 2.\r\n\r\n\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/30662/comments",
    "author": "wxie2013",
    "comments": [
      {
        "user": "justinvyu",
        "created_at": "2022-11-28T17:56:06Z",
        "body": "Is it possible to work around this by passing the arguments into the config? Is there a specific reason why the arguments need to be stored and accessed in the trainable as a separate python module?\r\n\r\n```python\r\nsearch_space = {\r\n    # ...\r\n    \"ttt\": input_param.ttt,\r\n}\r\n```"
      },
      {
        "user": "wxie2013",
        "created_at": "2022-11-28T20:19:02Z",
        "body": "Thanks for the follow-up.  It is possible to implement a walkaround.  It would be nice to understand the reason why above example code doesn't work so that I won't stumble into similar problems in the future. "
      },
      {
        "user": "Yard1",
        "created_at": "2022-11-29T18:45:27Z",
        "body": "Hey @wxie2013, as I mentioned in the discuss thread, this is because the trainable function is ran in a separate process on each Tune worker in parallel. Therefore, argparse will expect arguments that are simply not provided when Ray spawns those processes."
      },
      {
        "user": "wxie2013",
        "created_at": "2022-11-29T20:04:15Z",
        "body": "Hi @Yard1, got it. Thanks for the help"
      }
    ]
  },
  {
    "number": 30525,
    "title": "[RLlib][Windows] error in custom_env.py",
    "created_at": "2022-11-21T12:43:43Z",
    "closed_at": "2022-11-23T12:56:45Z",
    "labels": [
      "question",
      "windows"
    ],
    "url": "https://github.com/ray-project/ray/issues/30525",
    "body": "### What happened + What you expected to happen\n\nwhen i install raylib and run my first rayrl code(example \"custom_env.py\"), i come across an error, \r\n(rllib2) C:\\Users\\Raymond\\Desktop\\rayRL>python custom_env.py --framework torch\r\nRunning with following CLI options: Namespace(as_test=False, framework='torch', local_mode=False, no_tune=False, run='PPO', stop_iters=50, stop_reward=0.1, stop_timesteps=100000)\r\n2022-11-21 20:41:05,424 INFO worker.py:1528 -- Started a local Ray instance.\r\nTraceback (most recent call last):\r\n  File \"custom_env.py\", line 161, in <module>\r\n    get_trainable_cls(args.run)\r\nAttributeError: 'dict' object has no attribute 'environment'\r\nI am very consusing .\n\n### Versions / Dependencies\n\ninstall as instruction\n\n### Reproduction script\n\npython custom_env.py\n\n### Issue Severity\n\n_No response_",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/30525/comments",
    "author": "XA23i",
    "comments": [
      {
        "user": "XA23i",
        "created_at": "2022-11-21T13:22:22Z",
        "body": "seems work when use ray-3.0 Howeveer no instruction tell me that "
      },
      {
        "user": "XA23i",
        "created_at": "2022-11-21T13:35:15Z",
        "body": "btw I am windows"
      },
      {
        "user": "samo133",
        "created_at": "2022-11-21T23:14:31Z",
        "body": "The same error is generated while running the custom model/ custom environment example\r\noperating system: Ubuntu 20.04\r\nRay version:2.1.0"
      },
      {
        "user": "samo133",
        "created_at": "2022-11-22T00:23:34Z",
        "body": "The solution that works for me:\r\nfrom ray.rllib.algorithms.ppo import PPOConfig\r\nconfig = (\r\n        PPOConfig()\r\n        .environment(SimpleCorridor, env_config={\"corridor_length\": 5})\r\n        .framework(args.framework)\r\n        .rollouts(num_rollout_workers=1)\r\n        .training(\r\n            model={\r\n                \"custom_model\": \"my_model\",\r\n                \"vf_share_layers\": True,\r\n            }\r\n        )\r\n        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\r\n        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"1\")))\r\n    )\r\n\r\n"
      },
      {
        "user": "sven1977",
        "created_at": "2022-11-23T12:56:10Z",
        "body": "I can confirm it's working on master (3.0) on Windows as well as 2.1.\r\n\r\nThis script is part of our CI testing suite, so it's constantly checked.\r\n\r\n\r\n```\r\n+--------------------------------+------------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\r\n| Trial name                     | status     | loc             |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\r\n|--------------------------------+------------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\r\n| PPO_SimpleCorridor_fb9cf_00000 | TERMINATED | 127.0.0.1:36436 |      3 |          34.8335 | 12000 | 0.157853 |              1.59751 |             -2.09562 |            9.27972 |\r\n+--------------------------------+------------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\r\n\r\n```"
      }
    ]
  },
  {
    "number": 24818,
    "title": "[Tune] Issue on page /tune/api_docs/search_space.html",
    "created_at": "2022-05-15T16:37:20Z",
    "closed_at": "2022-05-17T13:21:48Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/24818",
    "body": "how can i generate an interval by ray.tune as a hyperparameter for search space?\r\n\r\ni want my hyperparameter be a format as (a, b), which a <= b. so that my program can sample for this interval.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/24818/comments",
    "author": "BabelTower",
    "comments": [
      {
        "user": "matthewdeng",
        "created_at": "2022-05-15T18:47:32Z",
        "body": "Hey @BabelTower, can you elaborate more on the criteria for this interval?\r\n\r\nIn your scenario, are a and b already known and for each Trial you want to generate 1 value in the range (a, b)? Or do you want to generate values for a and b which will be used within the Trial?"
      },
      {
        "user": "BabelTower",
        "created_at": "2022-05-16T13:14:46Z",
        "body": "I want to generate values for a and b which will be used within the Trial.\r\n\r\nI want to achieve the same effect as the following code. But this is an ugly way to write it.\r\n\r\n```\r\nfrom ray import tune\r\nfrom ray.tune import CLIReporter\r\nfrom ray.tune.schedulers import ASHAScheduler\r\nfrom hyperopt import hp\r\nfrom ray.tune.suggest.hyperopt import HyperOptSearch\r\nfrom ray.tune.suggest.bayesopt import BayesOptSearch\r\nimport argparse\r\nfrom functools import partial\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom utils import yaml_config_hook\r\nfrom train import train\r\nfrom cluster import cluster\r\n\r\n\r\ndef train_ray(config, args):\r\n    dic = vars(args)\r\n\r\n    for key, value in config.items():\r\n        keys = key.split(' ')\r\n        if len(keys)==1:\r\n            dic[keys[0]] = value\r\n        elif len(keys)==2:\r\n            dic[keys[0]][keys[1]] = value\r\n        else:\r\n           raise KeyError('Too many keys!')    \r\n\r\n    for key, value in dic.items():\r\n        if isinstance(dic[key], dict) and 'lower' in dic[key]:\r\n            if dic[key][\"lower\"] > dic[key][\"upper\"]:\r\n                dic[key][\"lower\"], dic[key][\"upper\"] = dic[key][\"upper\"], dic[key][\"lower\"]\r\n\r\n    args = argparse.Namespace(**dic)\r\n\r\n    train(args, report=True)\r\n    # cluster(args, report=True)\r\n\r\n\r\ndef main(num_samples=200, max_num_epochs=1000, gpus_per_trial=1, refer_metric='acc'):\r\n    parser = argparse.ArgumentParser()\r\n    config = yaml_config_hook(\"config/cora.yaml\")\r\n    for k, v in config.items():\r\n        parser.add_argument(f\"--{k}\", default=v, type=type(v))\r\n    args = parser.parse_args()\r\n\r\n    config = {\r\n        # \"learning_rate\": tune.choice([3e-4, 5e-4, 1e-4]),\r\n        # \"batch_size\": tune.choice([64, 128]),\r\n        # \"instance_temperature\": tune.quniform(0.8, 1.0, 0.01), \r\n        # \"cluster_temperature\": tune.quniform(0.6, 0.8, 0.01),\r\n        # \"ne_weight\": tune.quniform(13.0, 14.0, 0.01),\r\n        \"add_edge lower\":  tune.quniform(0.0, 0.4, 0.1), \r\n        \"add_edge upper\":  tune.quniform(0.0, 0.4, 0.1), \r\n        \"edge_pert lower\": tune.quniform(0.0, 0.4, 0.1), \r\n        \"edge_pert upper\": tune.quniform(0.0, 0.4, 0.1), \r\n        \"drop_feat lower\": tune.quniform(0.4, 0.8, 0.1), \r\n        \"drop_feat upper\": tune.quniform(0.4, 0.8, 0.1), \r\n        # \"drop_node\": tune.quniform(0.0, 0.1, 0.01),\r\n    }\r\n    bayesopt_search = BayesOptSearch(metric=refer_metric, mode=\"max\")\r\n    hyperopt_search = HyperOptSearch(metric=refer_metric, mode=\"max\")\r\n    asha_scheduler = ASHAScheduler(\r\n      metric=refer_metric, \r\n      mode=\"max\", \r\n      grace_period=200,\r\n      reduction_factor=3,\r\n      max_t=args.epochs)\r\n    result = tune.run(\r\n        partial(train_ray, args=args),\r\n        config = config,\r\n        num_samples = num_samples,\r\n        search_alg=hyperopt_search,\r\n        scheduler=asha_scheduler,\r\n        resources_per_trial = {\"cpu\": 2, \"gpu\": gpus_per_trial},\r\n    )\r\n    \r\n    best_trial = result.get_best_trial(refer_metric, \"max\")\r\n    print(\"Best trial config: {}\".format(best_trial.config))\r\n    print(\"Best trial acc: {}\".format(best_trial.last_result[\"acc\"]))\r\n    print(\"Best trial nmi: {}\".format(best_trial.last_result[\"nmi\"]))\r\n    print(\"Best trial ari: {}\".format(best_trial.last_result[\"ari\"]))\r\n    print(\"Best trial f1 : {}\".format(best_trial.last_result[\"f1\"]))\r\n\r\n    dfs = result.trial_dataframes\r\n    [d.acc.plot() for d in dfs.values()]\r\n    \r\n    \r\nif __name__ == '__main__':\r\n    main()    \r\n    \r\n```"
      }
    ]
  },
  {
    "number": 14276,
    "title": "[rllib] I cannot debug into env step function while using register_env",
    "created_at": "2021-02-23T12:46:11Z",
    "closed_at": "2021-04-24T07:06:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/14276",
    "body": "`config[\"env\"] = MyEnv`\r\nI found it possible to debug into env step function using the code style above.\r\n\r\nBut now I have a complex environment as follows and I don't know how to convert it to the above way because I want to debug into the env step function.\r\n`register_env(\"tictactoe\",lambda config: PettingZooEnv(env_creator()))`\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/14276/comments",
    "author": "itmorn",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2021-02-24T08:37:54Z",
        "body": "Interesting. Did you try setting `local_mode=True` in your call to `ray.init(local_mode=True)`?\r\n"
      },
      {
        "user": "sven1977",
        "created_at": "2021-03-12T14:14:28Z",
        "body": "@itmorn ^^"
      },
      {
        "user": "itmorn",
        "created_at": "2021-04-24T07:08:15Z",
        "body": "thanks"
      }
    ]
  },
  {
    "number": 13721,
    "title": "[RLlib] changing dtype of input_dict['obs'] when using _use_trajectory_view_api",
    "created_at": "2021-01-26T23:04:50Z",
    "closed_at": "2021-03-12T15:48:51Z",
    "labels": [
      "question",
      "P3",
      "cannot-reproduce"
    ],
    "url": "https://github.com/ray-project/ray/issues/13721",
    "body": "When using the trajectory view api, the type of input_dict['obs']  and input_dict['obs_flat'] is normally tf.float32; however, when creating the dynamic_tf_policy in initialize_loss_from_dummy_batch it seems that some how the dummy batch when calculating the last_r = policy._values(**input_dict) input_dict['obs'] changes type.\r\n\r\nray.__version__ == ray2.0.0.dev\r\n\r\nThis was causing issues with an existing custom model when migrating to trajectory view API. out model required the dtype to be fixed at tf.float32. \r\n\r\n### Reproduction (REQUIRED)\r\n# Given\r\ninsert print(input_dict, \"in postprocessing\") between line 124 and 125 of postprocessing.py. \r\ninsert print(input_dict, \"in forward\") between line 120 and 121 of fcnet.py\r\n\r\n# When\r\nthen run a ppo trained agent (probably works with other agents) while using _use_trajectory_view_api is true\r\n\r\n# Then\r\nobserve input_dict[\"obs\"] during forward passes changes from tf.float32 to tf.float64 (but only once)\r\nthis occurrence should line up with the occurrence in postprocessing.py\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13721/comments",
    "author": "raoul-khour-ts",
    "comments": [
      {
        "user": "raoul-khour-ts",
        "created_at": "2021-01-26T23:06:34Z",
        "body": "This was kind of annoying our current solution is to explicitly cast the input_dict[\"obs\"] to a float32 during forward; however, this seems like a bug."
      },
      {
        "user": "sven1977",
        "created_at": "2021-02-10T15:31:54Z",
        "body": "Thanks for filing this @raoul-khour-ts ! Will take a look."
      },
      {
        "user": "sven1977",
        "created_at": "2021-02-10T15:49:37Z",
        "body": "Can't reproduce this with e.g. CartPole-v0. The change you observed does not happen here:\r\ne.g.\r\n\r\n```\r\n{'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>, 'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>, 'prev_actions': <tf.Tensor 'default_policy/prev_action:0' shape=(?,) dtype=int64>, 'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>, 'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>, 'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>, 'infos': <tf.Tensor 'default_policy/infos:0' shape=(?,) dtype=float32>, 'eps_id': <tf.Tensor 'default_policy/eps_id:0' shape=(?,) dtype=float32>, 'unroll_id': <tf.Tensor 'default_policy/unroll_id:0' shape=(?,) dtype=float32>, 'agent_index': <tf.Tensor 'default_policy/agent_index:0' shape=(?,) dtype=float32>, 't': <tf.Tensor 'default_policy/t:0' shape=(?,) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/is_training:0' shape=() dtype=bool>, 'obs_flat': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>} in forward\r\n{'obs': array([[0., 0., 0., 0.]], dtype=float32), 'seq_lens': array([1], dtype=int32)} in postprocessing\r\n{'obs': <tf.Tensor 'default_policy/kwarg_obs:0' shape=(1, 4) dtype=float32>, 'seq_lens': <tf.Tensor 'default_policy/kwarg_seq_lens:0' shape=(1,) dtype=int32>, 'is_training': False, 'obs_flat': <tf.Tensor 'default_policy/kwarg_obs:0' shape=(1, 4) dtype=float32>} in forward\r\n{'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>, 'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>, 'prev_actions': <tf.Tensor 'default_policy/prev_action:0' shape=(?,) dtype=int64>, 'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>, 'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>, 'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>, 'infos': <tf.Tensor 'default_policy/infos:0' shape=(?,) dtype=float32>, 'eps_id': <tf.Tensor 'default_policy/eps_id:0' shape=(?,) dtype=float32>, 'unroll_id': <tf.Tensor 'default_policy/unroll_id:0' shape=(?,) dtype=float32>, 'agent_index': <tf.Tensor 'default_policy/agent_index:0' shape=(?,) dtype=float32>, 't': <tf.Tensor 'default_policy/t:0' shape=(?,) dtype=float32>, 'is_training': True, 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>, 'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>, 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 2) dtype=float32>, 'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>, 'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>, 'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>, 'obs_flat': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>} in forward\r\n{'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>, 'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>, 'prev_actions': <tf.Tensor 'default_policy/prev_action:0' shape=(?,) dtype=int64>, 'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>, 'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>, 'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>, 'infos': <tf.Tensor 'default_policy/infos:0' shape=(?,) dtype=float32>, 'eps_id': <tf.Tensor 'default_policy/eps_id:0' shape=(?,) dtype=float32>, 'unroll_id': <tf.Tensor 'default_policy/unroll_id:0' shape=(?,) dtype=float32>, 'agent_index': <tf.Tensor 'default_policy/agent_index:0' shape=(?,) dtype=float32>, 't': <tf.Tensor 'default_policy/t:0' shape=(?,) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/is_training:0' shape=() dtype=bool>, 'obs_flat': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>} in forward\r\n{'obs': array([[0., 0., 0., 0.]], dtype=float32), 'seq_lens': array([1], dtype=int32)} in postprocessing\r\n{'obs': <tf.Tensor 'default_policy/kwarg_obs:0' shape=(1, 4) dtype=float32>, 'seq_lens': <tf.Tensor 'default_policy/kwarg_seq_lens:0' shape=(1,) dtype=int32>, 'is_training': False, 'obs_flat': <tf.Tensor 'default_policy/kwarg_obs:0' shape=(1, 4) dtype=float32>} in forward\r\n{'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>, 'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>, 'prev_actions': <tf.Tensor 'default_policy/prev_action:0' shape=(?,) dtype=int64>, 'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>, 'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>, 'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>, 'infos': <tf.Tensor 'default_policy/infos:0' shape=(?,) dtype=float32>, 'eps_id': <tf.Tensor 'default_policy/eps_id:0' shape=(?,) dtype=float32>, 'unroll_id': <tf.Tensor 'default_policy/unroll_id:0' shape=(?,) dtype=float32>, 'agent_index': <tf.Tensor 'default_policy/agent_index:0' shape=(?,) dtype=float32>, 't': <tf.Tensor 'default_policy/t:0' shape=(?,) dtype=float32>, 'is_training': True, 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>, 'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>, 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 2) dtype=float32>, 'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>, 'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>, 'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>, 'obs_flat': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>} in forward\r\n{'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>, 'new_obs': <tf.Tensor 'default_policy_1/tower/new_obs:0' shape=(?, 4) dtype=float32>, 'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>, 'prev_actions': <tf.Tensor 'default_policy_1/tower/prev_actions:0' shape=(?,) dtype=int64>, 'rewards': <tf.Tensor 'default_policy_1/tower/rewards:0' shape=(?,) dtype=float32>, 'prev_rewards': <tf.Tensor 'default_policy_1/tower/prev_rewards:0' shape=(?,) dtype=float32>, 'dones': <tf.Tensor 'default_policy_1/tower/dones:0' shape=(?,) dtype=float32>, 'infos': <tf.Tensor 'default_policy_1/tower/infos:0' shape=(?,) dtype=float32>, 'eps_id': <tf.Tensor 'default_policy_1/tower/eps_id:0' shape=(?,) dtype=float32>, 'unroll_id': <tf.Tensor 'default_policy_1/tower/unroll_id:0' shape=(?,) dtype=float32>, 'agent_index': <tf.Tensor 'default_policy_1/tower/agent_index:0' shape=(?,) dtype=float32>, 't': <tf.Tensor 'default_policy_1/tower/t:0' shape=(?,) dtype=float32>, 'is_training': <tf.Tensor 'default_policy_1/tower/is_training:0' shape=() dtype=bool>, 'obs_flat': <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>} in forward\r\nOrderedDict([('is_exploring', <tf.Tensor 'default_policy/is_exploring:0' shape=() dtype=bool>), ('timestep', <tf.Tensor 'default_policy/timestep:0' shape=() dtype=int64>), ('obs', <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>), ('actions', <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>), ('action_logp', <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>), ('action_dist_inputs', <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 2) dtype=float32>), ('vf_preds', <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>), ('value_targets', <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>), ('advantages', <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>), ('is_training', True), ('obs_flat', <tf.Tensor 'default_policy/obs:0' shape=(?, 4) dtype=float32>)]) in forward\r\n{'obs': <tf.Tensor 'default_policy_1/tower_1/Slice:0' shape=(?, 4) dtype=float32>, 'new_obs': <tf.Tensor 'default_policy_1/tower_1/new_obs:0' shape=(?, 4) dtype=float32>, 'actions': <tf.Tensor 'default_policy_1/tower_1/Slice_1:0' shape=(?,) dtype=int64>, 'prev_actions': <tf.Tensor 'default_policy_1/tower_1/prev_actions:0' shape=(?,) dtype=int64>, 'rewards': <tf.Tensor 'default_policy_1/tower_1/rewards:0' shape=(?,) dtype=float32>, 'prev_rewards': <tf.Tensor 'default_policy_1/tower_1/prev_rewards:0' shape=(?,) dtype=float32>, 'dones': <tf.Tensor 'default_policy_1/tower_1/dones:0' shape=(?,) dtype=float32>, 'infos': <tf.Tensor 'default_policy_1/tower_1/infos:0' shape=(?,) dtype=float32>, 'eps_id': <tf.Tensor 'default_policy_1/tower_1/eps_id:0' shape=(?,) dtype=float32>, 'unroll_id': <tf.Tensor 'default_policy_1/tower_1/unroll_id:0' shape=(?,) dtype=float32>, 'agent_index': <tf.Tensor 'default_policy_1/tower_1/agent_index:0' shape=(?,) dtype=float32>, 't': <tf.Tensor 'default_policy_1/tower_1/t:0' shape=(?,) dtype=float32>, 'is_training': <tf.Tensor 'default_policy_1/tower_1/is_training:0' shape=() dtype=bool>, 'obs_flat': <tf.Tensor 'default_policy_1/tower_1/Slice:0' shape=(?, 4) dtype=float32>} in forward\r\nOrderedDict([('is_exploring', <tf.Tensor 'default_policy/is_exploring:0' shape=() dtype=bool>), ('timestep', <tf.Tensor 'default_policy/timestep:0' shape=() dtype=int64>), ('obs', <tf.Tensor 'default_policy_1/tower_1/Slice:0' shape=(?, 4) dtype=float32>), ('actions', <tf.Tensor 'default_policy_1/tower_1/Slice_1:0' shape=(?,) dtype=int64>), ('action_logp', <tf.Tensor 'default_policy_1/tower_1/Slice_2:0' shape=(?,) dtype=float32>), ('action_dist_inputs', <tf.Tensor 'default_policy_1/tower_1/Slice_3:0' shape=(?, 2) dtype=float32>), ('vf_preds', <tf.Tensor 'default_policy_1/tower_1/Slice_4:0' shape=(?,) dtype=float32>), ('value_targets', <tf.Tensor 'default_policy_1/tower_1/Slice_5:0' shape=(?,) dtype=float32>), ('advantages', <tf.Tensor 'default_policy_1/tower_1/Slice_6:0' shape=(?,) dtype=float32>), ('is_training', True), ('obs_flat', <tf.Tensor 'default_policy_1/tower_1/Slice:0' shape=(?, 4) dtype=float32>)]) in forward\r\n{'obs': array([[-0.00422059,  0.21197446, -0.00986806, -0.2807055 ]],\r\n      dtype=float32), 'seq_lens': array([1], dtype=int32)} in postprocessing\r\n{'obs': array([[ 0.06375833, -0.01632409,  0.06279647,  0.3893305 ]],\r\n      dtype=float32), 'seq_lens': array([1], dtype=int32)} in postprocessing\r\n```\r\n\r\nEverything remains float32. Could this be your environment that's causing this, maybe sometimes returning float32, sometimes float64 as observations?"
      },
      {
        "user": "sven1977",
        "created_at": "2021-02-10T15:51:22Z",
        "body": "Did the following and could not reproduce:\r\n1) Added the two prints suggested by @raoul-khour-ts above.\r\n2) Ran the rllib/agents/ppo/tests/test_ppo.py::TestPPO::test_ppo_compilation_and_lr_schedule for only the framework=\"tf\" part of the loop and got the above output, showing that all observation tensors and np.arrays are always float32."
      },
      {
        "user": "sven1977",
        "created_at": "2021-03-12T15:48:51Z",
        "body": "Closing this issue now due to inactivity and non-reproducability.\r\n\r\nFeel free to re-open should this still be an issue."
      }
    ]
  },
  {
    "number": 13632,
    "title": "[rllib] trainer.restore() cannot find GPU for PyTorch",
    "created_at": "2021-01-22T11:54:02Z",
    "closed_at": "2023-03-28T21:53:20Z",
    "labels": [
      "question",
      "rllib"
    ],
    "url": "https://github.com/ray-project/ray/issues/13632",
    "body": "**Environment:**\r\nray  1.0.0\r\ngym 0.17.3\r\ntorch 1.7.1+cu110\r\n\r\nWhen I run my codes:\r\n```\r\nray.init(log_to_driver=args.ray_log)\r\nconfig = ppo.DEFAULT_CONFIG.copy()\r\n    config['lr'] = 0.0001\r\n    config['num_workers'] = args.num_workers\r\n    config['num_gpus'] = args.num_gpus\r\n    config[\"framework\"] = \"torch\"\r\n    config['num_cpus_per_worker'] = 1\r\n    #config[\"callbacks\"] = MyCallback\r\n    config[\"explore\"] = not args.eval\r\n    config[\"model\"].update({\r\n        \"custom_model\": \"my_model\",\r\n        \"fcnet_hiddens\": [256, 256],\r\n        \"fcnet_activation\": \"tanh\"\r\n    })\r\n    trainer = ppo.PPOTrainer(config=config, env=\"my_env\")\r\n    if args.checkpoint:\r\n        trainer.restore(args.checkpoint)\r\n    for _ in range(10000):\r\n            result = trainer.train()\r\n            print(pretty_print(result))\r\n            checkpoint = trainer.save()\r\n            print(\"checkpoint saved at\", checkpoint)\r\n```\r\nI have two gpus in my machine and when I do not run ``` trainer.restore() ```, everything is ok.\r\n\r\nHowever, when I run ``` trainer.restore() ``` to load my model from checkpoint, error occurs as:\r\n```\r\n2021-01-22 19:20:18,030 ERROR worker.py:1018 -- Possible unhandled error from worker: ray::RolloutWorker.restore() (pid=39920, ip=192.168.8.17)                                                                                                                             \r\nFile \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task                                                                 \r\nFile \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor                                               \r\nFile \"/home/yuxuan/Python3/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 916, in restore                   objs = pickle.loads(objs)                                                                                                          \r\nFile \"/home/yuxuan/Python3/lib/python3.6/site-packages/torch/storage.py\", line 141, in _load_from_bytes                                return torch.load(io.BytesIO(b))                                                                                                   \r\nFile \"/home/yuxuan/Python3/lib/python3.6/site-packages/torch/serialization.py\", line 595, in load                                      \r\nreturn _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)                                                  \r\nFile \"/home/yuxuan/Python3/lib/python3.6/site-packages/torch/serialization.py\", line 774, in _legacy_load                              result = unpickler.load()                                                                                                          \r\nFile \"/home/yuxuan/Python3/lib/python3.6/site-packages/torch/serialization.py\", line 730, in persistent_load                           deserialized_objects[root_key] = restore_location(obj, location)                                                                   \r\nFile \"/home/yuxuan/Python3/lib/python3.6/site-packages/torch/serialization.py\", line 175, in default_restore_location                  result = fn(storage, location)                                                                                                     \r\nFile \"/home/yuxuan/Python3/lib/python3.6/site-packages/torch/serialization.py\", line 151, in _cuda_deserialize                         device = validate_cuda_device(location)                                                                                            \r\nFile \"/home/yuxuan/Python3/lib/python3.6/site-packages/torch/serialization.py\", line 135, in validate_cuda_device                      raise RuntimeError('Attempting to deserialize object on a CUDA '                                                                 \r\nRuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.            \r\n```\r\nI have tried to use ray version=1.0.1 but ``` ValueError: ('Observation for a Box/MultiBinary/MultiDiscrete space should be an np.array, not a Python list.', None) ``` occurs when I tried to compute_action with MultiAgentEnv.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13632/comments",
    "author": "tymss",
    "comments": [
      {
        "user": "umbertov",
        "created_at": "2021-03-25T16:57:45Z",
        "body": "i solved this by simply `import torch`. However, it looks like the call to `restore` (in my case it was on an agent, with `agent.restore()`) still fails because the PyTorch state dict is serialized as a GPU tensor, so the de-serializing function of torch needs the `map_location='cpu'` kwarg, however there is no way to pass it to `restore`. I'm looking for a way to circumvent this; the easiest that comes to my mind is to do `torch.load(..., map_location='cpu')` and `torch.save(...)` right afterwards, but `torch.load` complains with `RuntimeError: Invalid magic number; corrupt file?`. \r\n\r\nLet me know if you managed to solve this"
      },
      {
        "user": "millerh1",
        "created_at": "2025-01-15T21:03:11Z",
        "body": "I'm experiencing this issue currently I think. Attempting to restore a failed raytune run leads to `RuntimeError: No CUDA GPUs are available` using a ray cluster deployed on AWS VMs. My version of Ray is `2.40.0`. \n\nI saw this issue was completed. Was wondering if anyone has a solution? Adding torch at the beginning of my script does not fix this for me. "
      },
      {
        "user": "umbertov",
        "created_at": "2025-01-19T08:59:03Z",
        "body": "@millerh1 i think that if you have the same problem, e.g. having a checkpoint with weights mapped to a GPU, you might work around it by converting it into another checkpoint with weights mapped to the CPU, and then load it that way. \nIt would look something like a `torch.load(my_checkpoint_path, map_location=\"cpu\")` followed by a `torch.save(new_checkpoint_path)`.\n\nOr you could directly export the model with CPU weights, by moving it to the CPU `model.to('cpu'` or `model.cpu()` before exporting the checkpoint."
      }
    ]
  },
  {
    "number": 13576,
    "title": "[tune] Errors when using points_to_evaluate argument for a few search algos",
    "created_at": "2021-01-20T05:28:07Z",
    "closed_at": "2021-01-21T02:23:26Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/13576",
    "body": "I am trying out a few search algos (run from scratch) with Ray version 1.1.0, and is running into a few issues when using `points_to_evaluate` argument:\r\n\r\nSearch Space Setup:\r\n```\r\neven_int_model_dim = [x for x in range(1, 12+1) if x % 2 == 0]\r\neven_int_batch_size = [x for x in range(1, 16+1) if x % 2 == 0]\r\nint_sequence = [x for x in range(20, 100+1)]\r\nint_local_context_len = [x for x in range(3, 15+1)]\r\nint_num_heads = [x for x in range(2, 6+1)]\r\n\r\nconfig={'seed': 0, 'train_start_date': 'None', 'valid_start_date': '2017-01-01', 'test_start_date': '2018-01-01',\r\n\t'data_path': 'path/to/data', 'num_epoch': 100, 'loss_fn': loss_fn, 'device': 'cuda:0', \r\n\t'sequence': tune.choice(int_sequence),\r\n\t'local_context_len': tune.choice(int_local_context_len), 'batch_size': tune.choice(even_int_batch_size), 'num_heads': tune.choice(int_num_heads),\r\n\t'model_dim': tune.choice(even_int_model_dim), 'num_layers': tune.choice([1, 2]), 'dropout': tune.uniform(0, 0.7),\r\n\t'allocator': 'numark', 'max_weight': 0.1, 'stochasticity': tune.choice([True, False]),\r\n\t'resample': False, 'n_draws': 100, 'n_portfolios': 5, 'feature_dims': 0, \r\n\t'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, \r\n\t'eps': 0.00000001, 'weight_decay': 0, 'amsgrad': True}\r\n\r\ncurrent_best = [{'sequence': 100, 'local_context_len': 5, 'batch_size': 16, \r\n\t\t 'num_heads': 6, 'model_dim': 12, 'num_layers': 1, 'dropout': 0.01, 'stochasticity': False}]\r\n```\r\n\r\nWhen using `HyperOpt`:\r\n`algo = HyperOptSearch(points_to_evaluate=current_best)`\r\nError:\r\n> File \"/home/user/anaconda3/envs/user/lib/python3.7/site-packages/hyperopt/pyll/base.py\", line 874, in rec_eval\r\n>     rval_var = node.pos_args[int(switch_i) + 1]\r\n> IndexError: list index out of range\r\n\r\nWhen using `Optuna`:\r\n`algo = OptunaSearch(points_to_evaluate=current_best)`\r\nError:\r\n> TypeError: __init__() got an unexpected keyword argument 'points_to_evaluate'\r\n\r\nWhen using `Ax`:\r\n`algo = AxSearch(points_to_evaluate=current_best)`\r\nError:\r\n> TypeError: __init__() got an unexpected keyword argument 'points_to_evaluate'",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13576/comments",
    "author": "turmeric-blend",
    "comments": [
      {
        "user": "krfricke",
        "created_at": "2021-01-20T08:35:22Z",
        "body": "Hi @turmeric-blend, the `points_to_evaluate` arguments for most algorithms are currently only available in the nightly wheels / current master and not in the latest release.\r\nYou can try `ray install-nightly` to install the nightly wheels. "
      },
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-21T02:23:26Z",
        "body": "ok thanks"
      }
    ]
  },
  {
    "number": 13517,
    "title": "[tune] how to enforce even integer number in the search space",
    "created_at": "2021-01-18T08:18:39Z",
    "closed_at": "2021-01-19T09:11:44Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/13517",
    "body": "I require my `batch_size` in the search space to be an **even number**, so I tried `tune.qrandint(4, 64, 2)`. However I am also using `HyperOpt` which gave this warning:\r\n\r\n> HyperOpt does not support quantization for integer values. Reverting back to 'randint'.\r\n\r\nMaking certain trials contain odd `batch_size` which produces error in my model. Is there another way to enforce even integer number in the search space?\r\n\r\nEDIT:\r\n\r\nI also tried `tune.sample_from(lambda spec: np.random.randint(2, 64) * 2)`, and HyperOpt gave error:\r\n\r\n> HyperOpt does not support parameters of type `Function` with samplers of type `NoneType`",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13517/comments",
    "author": "turmeric-blend",
    "comments": [
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-18T08:56:32Z",
        "body": "solved by directly using `np.random.randint(2, 128)*2` in config search space instead of `tune.`"
      },
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-19T00:41:36Z",
        "body": "reopening as it actually just returned a constant instead of random even integer between 2 and 128. Issue remains how to  enforce even integer number in the search space?"
      },
      {
        "user": "richardliaw",
        "created_at": "2021-01-19T04:31:36Z",
        "body": "Can you try doing a tune.randint, and then in your training function, multiply it by 2?"
      },
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-19T06:32:27Z",
        "body": "I assume you mean\r\n\r\n```\r\nconfig={..., 'batch_size': tune.randint(1, 64), ...}\r\n\r\n\r\ndef train_function(config):\r\n      batch_size = config['batch_size']*2\r\n```\r\n\r\nI guess it works but then I have to always be aware that my range is half the max range in `randint`.\r\n\r\nAnyway, I did something like this instead, `tune.choice([x for x in range(1, 17) if x % 2 == 0])` which works well."
      },
      {
        "user": "krfricke",
        "created_at": "2021-01-19T09:11:44Z",
        "body": "Please note that this has been improved upon in the latest master and the ray nightly wheels. The next release will support quantized integers in hyperopt out of the box (e.g. using `tune.qrandint()`).\r\n\r\nPlease re-open if you have any more questions."
      }
    ]
  },
  {
    "number": 13405,
    "title": "[rllib] I can't use QMixTrainer trainer.compute_action to obtain an action.",
    "created_at": "2021-01-13T15:29:18Z",
    "closed_at": "2023-04-08T16:01:42Z",
    "labels": [
      "question",
      "rllib"
    ],
    "url": "https://github.com/ray-project/ray/issues/13405",
    "body": "I took TwoStepGame for an example. My code is as follows. \r\nI have tried different obs from TwoStepGame or TwoStepGame.with_agent_groups. It always obtained different errors.\r\nCan any one tell me how to obtain actions from multi-agent env based on QMixTrainer?\r\n\r\n`trainer = QMixTrainer(env=\"grouped_twostep\", config=config)\r\n    for i in range(1):\r\n        res = trainer.train()\r\n        print('==iter==')\r\n        print(res)\r\n\r\n    # env = TwoStepGame(config[\"env_config\"]).with_agent_groups(grouping, obs_space=obs_space, act_space=act_space)\r\n    env = TwoStepGame(config[\"env_config\"])\r\n    obs = env.reset()\r\n    print('init obs: ', obs)\r\n    for i in range(10):\r\n        # all_obs = obs['group_1']\r\n        act_dict={}\r\n        for j in range(2):\r\n            act_dict[j] = trainer.compute_action(obs)\r\n        obs,rew,done,info = env.step(act_dict)\r\n        print(\"obs  \", obs)\r\n        print(\"rew  \",rew)\r\n`",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13405/comments",
    "author": "imxuemei",
    "comments": [
      {
        "user": "Counterfeiter",
        "created_at": "2021-03-21T14:52:43Z",
        "body": "same problem here...\r\n\r\nAny solutions ?\r\n\r\nMy best guss:\r\n\r\n```python\r\nenv = CustomEnv(level=level, normalize_obs = True, verbose = verbose).with_agent_groups(\r\n                grouping, obs_space=obs_space, act_space=act_space)\r\nobs = env.reset()\r\nagent = QMixTrainer(config=config, env=\"custom_env\")\r\nagent.restore(restore)\r\n```\r\n\r\nbut fails. custom_env is registered before...."
      },
      {
        "user": "imxuemei",
        "created_at": "2021-03-23T02:58:11Z",
        "body": "> same problem here...\r\n> \r\n> Any solutions ?\r\n> \r\n> My best guss:\r\n> \r\n> ```python\r\n> env = CustomEnv(level=level, normalize_obs = True, verbose = verbose).with_agent_groups(\r\n>                 grouping, obs_space=obs_space, act_space=act_space)\r\n> obs = env.reset()\r\n> agent = QMixTrainer(config=config, env=\"custom_env\")\r\n> agent.restore(restore)\r\n> ```\r\n> \r\n> but fails. custom_env is registered before....\r\n\r\nI haven't solved it. Other trainers, such as A3CTrainer, can work, but QMIX can't."
      },
      {
        "user": "jzxycsjzy",
        "created_at": "2022-10-27T02:33:55Z",
        "body": "Same problem. I cannot obtain action from action space when I try to use QMix algorithm. Include train.trainer() and tune.tuner().fit()"
      },
      {
        "user": "joshmyersdean",
        "created_at": "2022-11-29T22:34:25Z",
        "body": "Has there been any progress on this issue?"
      }
    ]
  },
  {
    "number": 13399,
    "title": "[core] error occur while using ray",
    "created_at": "2021-01-13T12:19:25Z",
    "closed_at": "2021-01-13T13:34:54Z",
    "labels": [
      "bug",
      "question",
      "triage"
    ],
    "url": "https://github.com/ray-project/ray/issues/13399",
    "body": "When using ray to run my project, I get the following error while invoke something like:\r\n `result = ray.get([f.remote(...) for _ in range(num_cpus)])`.\r\n\r\nCould you please explain what is happening? Is it caused bacause I invoke `ray.get()` for many times.\r\n\r\n```\r\n  File \"/.../lib/python3.6/site-packages/ray/remote_function.py\", line 101, in _remote_proxy\r\n    return self._remote(args=args, kwargs=kwargs)\r\n  File \"/.../lib/python3.6/site-packages/ray/remote_function.py\", line 275, in _remote\r\n    return invocation(args, kwargs)\r\n  File \"/.../lib/python3.6/site-packages/ray/remote_function.py\", line 263, in invocation\r\n    or dict())\r\n  File \"python/ray/_raylet.pyx\", line 1074, in ray._raylet.CoreWorker.submit_task\r\n  File \"python/ray/_raylet.pyx\", line 1078, in ray._raylet.CoreWorker.submit_task\r\n  File \"python/ray/_raylet.pyx\", line 293, in ray._raylet.prepare_args\r\n  File \"/root/anaconda3/envs/onnx/lib/python3.6/site-packages/ray/serialization.py\", line 406, in serialize\r\n    return self._serialize_to_msgpack(value)\r\n  File \"/.../lib/python3.6/site-packages/ray/serialization.py\", line 386, in _serialize_to_msgpack\r\n    self._serialize_to_pickle5(metadata, python_objects)\r\n  File \"/.../lib/python3.6/site-packages/ray/serialization.py\", line 346, in _serialize_to_pickle5\r\n    raise e\r\n  File \"/.../lib/python3.6/site-packages/ray/serialization.py\", line 343, in _serialize_to_pickle5\r\n    value, protocol=5, buffer_callback=writer.buffer_callback)\r\n  File \"/.../lib/python3.6/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\r\n    cp.dump(obj)\r\n  File \"/.../lib/python3.6/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 563, in dump\r\n    return Pickler.dump(self, obj)\r\n_pickle.PickleError: can't pickle repeated message fields, convert to list first\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13399/comments",
    "author": "chenyuqi990215",
    "comments": [
      {
        "user": "krfricke",
        "created_at": "2021-01-13T12:21:11Z",
        "body": "This looks more like your function cannot be pickled. Can you show us how your function looks like (i.e. `def f`)?"
      },
      {
        "user": "chenyuqi990215",
        "created_at": "2021-01-13T12:48:05Z",
        "body": "```\r\n@ray.remote\r\ndef conv_ray_function(input_new, kernal_weight, output, attribute, output_mask, threhold, iter_idx):\r\n```\r\n\r\nThe types of each params are: \r\n\r\n```\r\n<class 'numpy.ndarray'>\r\n<class 'numpy.ndarray'>\r\n<class 'numpy.ndarray'>\r\n<class 'dict'>\r\n<class 'numpy.ndarray'>\r\n<class 'float'>\r\n<class 'numpy.ndarray'>\r\n```"
      },
      {
        "user": "krfricke",
        "created_at": "2021-01-13T13:27:10Z",
        "body": "Sorry I should have been more specific. Can you give us the full `conv_ray_function` definition?"
      },
      {
        "user": "chenyuqi990215",
        "created_at": "2021-01-13T13:34:54Z",
        "body": "I found the problem, thanks. It is because one of the element in the `dict` is not serializable!"
      },
      {
        "user": "Harrypotterrrr",
        "created_at": "2022-07-13T21:08:32Z",
        "body": "> I found the problem, thanks. It is because one of the element in the `dict` is not serializable!\r\n\r\nThank you, i came across the same problem in different scenario. I initialize my model in apache beam `__init__`, which should be in `setup` function. Thanks for your answer!"
      }
    ]
  },
  {
    "number": 13387,
    "title": "[rllib] Supporting vectorization of MultiAgentEnv within a single process?",
    "created_at": "2021-01-13T01:11:07Z",
    "closed_at": "2021-01-13T04:50:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/13387",
    "body": "According to issue#4938, vectorization of MultiAgentEnv within a single process is not automatically handled, and one needs to create a custom class for that starting from BaseEnv.\r\n\r\nWould you consider adding this support in the future? (So that vectorization is performed `num_envs_per_worker > 1`)\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13387/comments",
    "author": "ToruOwO",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2021-01-13T04:49:31Z",
        "body": "num_envs_per_worker > 1 works just fine with multi agent envs--- I believe that issue is referring to a special use case where the env itself is somehow hosting multiple logical episodes."
      },
      {
        "user": "ToruOwO",
        "created_at": "2021-01-19T02:22:59Z",
        "body": "Hi,\r\n\r\nFor some reason, my `MultiAgentEnv` is never properly initialized whenever `num_envs_per_worker > 1`. This is why I think there might be some issue with vectorization of `MultiAgentEnv`, since the training works fine when the environment subclasses from `BaseEnv`, `VectorEnv` and `RemoteVectorEnv`. I wonder if you have any idea on the cause of this issue and how to fix it?\r\n\r\nThank you!"
      }
    ]
  },
  {
    "number": 12681,
    "title": "[rllib] Observation outside expected value range",
    "created_at": "2020-12-08T19:51:30Z",
    "closed_at": "2020-12-09T12:24:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/12681",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### Describe your question\r\nI am getting `Observation outside expected value range` and I checked the values and all of the values are in those ranges. I searched online and seems like there is not a good answer to this question. \r\n\r\nIs there any pointer how to debug this issue? Appreciate any help.\r\n\r\nray version is 0.8.4\r\n\r\nStack trace:\r\n```\r\nRayTaskError(ValueError): ray::RolloutWorker.sample() (pid=18357, ip=172.17.0.3)\r\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 492, in sample\r\n    batches = [self.input_reader.next()]\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/sampler.py\", line 53, in next\r\n    batches = [self.get_data()]\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/sampler.py\", line 96, in get_data\r\n    item = next(self.rollout_provider)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/sampler.py\", line 346, in _env_runner\r\n    callbacks, soft_horizon, no_done_at_end)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/sampler.py\", line 441, in _process_observations\r\n    policy_id).transform(raw_obs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/models/preprocessors.py\", line 230, in transform\r\n    self.check_shape(observation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/models/preprocessors.py\", line 61, in check_shape\r\n    self._obs_space, observation)\r\nValueError: ('Observation outside expected value range', Dict(....\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/12681/comments",
    "author": "ethem-kinginthenorth",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-12-08T19:52:31Z",
        "body": "99% of the time it really is out of the range, I would try manually checking space.contains(obs) for the obs and space you have."
      },
      {
        "user": "ethem-kinginthenorth",
        "created_at": "2020-12-08T19:58:08Z",
        "body": "It does print out the values and I checked all of them. I see something like below and they are all between -1e20 and +1e20. I am sure I am missing something but kind of puzzled\r\n```\r\nvar1:Box(-1.0000000200408773e+20, 1.0000000200408773e+20, (1,), float32), \r\nvar2:Box(-1e+20, 1e+20, (1,), float64), \r\nvar3:Box(-6250000000000000000, 6250000000000000000, (5,), int64), \r\ntime:Box(-1e+20, 1e+20, (1,), float64), \r\nvar4:Box(-1.0000000200408773e+20, 1.0000000200408773e+20, (1, 5), float32), \r\nvar5:Box(-1.0000000200408773e+20, 1.0000000200408773e+20, (1, 5), float32), \r\nvar6:Box(-1.0000000200408773e+20, 1.0000000200408773e+20, (1,), float32)), \r\n\r\n{'var4': array([[[\r\n\t\t 4.88582578e+04, 7.31112061e+03, 7.25945938e+04, 3.01098730e+04,\r\n         3.94023250e+05, 5.73265312e+04, 3.52349414e+04, 9.64393945e+03,\r\n         6.97031982e+03, 2.14223953e+05, 1.05742898e+05, 1.41523105e+04,\r\n         1.78426172e+04, 1.25530992e+05, 6.69800703e+04, 3.13583145e+04,\r\n         2.90362441e+04, 4.45797383e+04, 4.60930039e+04, 1.34193379e+04,\r\n         5.99366562e+04, 6.79598750e+04, 9.92885625e+04, 5.61552891e+04,\r\n         2.96092031e+04, 6.12517695e+04, 1.06827305e+04, 1.92072031e+04,\r\n         3.07163887e+04, 1.35522500e+04, 8.72297656e+04, 2.08942793e+04,\r\n         1.90838938e+05, 1.04593461e+05, 7.59508105e+03, 1.16540648e+05,\r\n         3.95055234e+04, 4.21355195e+04, 1.27262930e+05, 1.04629004e+04,\r\n         5.13732031e+04, 8.79698730e+03, 6.80711797e+04, 2.89329844e+05,\r\n         3.19741289e+04, 6.23017041e+03, 8.49190703e+04, 7.59360547e+04,\r\n         1.79015801e+04, 5.80420391e+04, 5.77861035e+03],\r\n\t\t \r\n        [1.18575027e+02, 3.43979001e+00, 1.12372108e+02, 5.63302040e+01,\r\n         7.30047119e+02, 1.32382736e+02, 6.96799240e+01, 6.15634270e+01,\r\n         4.60423088e+01, 3.35565704e+02, 2.55701599e+02, 4.89664316e+00,\r\n         2.07302799e+01, 7.12797241e+02, 1.67806961e+02, 1.45109863e+02,\r\n         7.77604980e+01, 5.86119537e+01, 1.56264786e+02, 1.48571339e+01,\r\n         3.01692505e+02, 3.63289154e+02, 7.87328873e+01, 1.49704453e+02,\r\n         8.79278488e+01, 5.25395164e+01, 3.53664255e+00, 1.24862175e+02,\r\n         4.82263031e+01, 2.36730080e+01, 5.56469910e+02, 4.27636414e+01,\r\n         9.88136780e+02, 2.04659500e+02, 2.07328682e+01, 1.59023865e+02,\r\n         3.44469910e+01, 2.82297897e+01, 2.64060852e+02, 7.10659409e+01,\r\n         7.05867844e+01, 4.40806999e+01, 1.88512421e+02, 4.71461914e+02,\r\n         7.66369247e+01, 4.82384586e+00, 3.14151184e+02, 1.13354561e+02,\r\n         1.32134256e+01, 1.29955643e+02, 8.89097881e+00],\r\n\t\t \r\n        [5.50149689e+01, 8.90209794e-01, 8.02079010e+01, 1.18397923e+01,\r\n         3.68902893e+02, 1.28457245e+02, 3.48250092e+02, 3.21365738e+01,\r\n         4.11276932e+01, 2.17834290e+02, 1.75638321e+02, 1.51335669e+00,\r\n         7.29972029e+00, 4.74392731e+02, 1.74303055e+02, 4.72701454e+01,\r\n         1.91395111e+01, 3.83680420e+01, 2.38665207e+02, 7.92286777e+00,\r\n         2.18457474e+02, 6.01870850e+02, 5.01277130e+02, 9.13355103e+01,\r\n         6.43621674e+01, 6.99704895e+01, 1.51335657e+00, 1.20178328e+01,\r\n         3.69437065e+01, 2.11869946e+01, 1.03567004e+03, 3.12463665e+01,\r\n         2.70846338e+03, 8.27004700e+01, 4.80713367e+00, 1.91306061e+02,\r\n         2.97330093e+01, 1.36202097e+01, 4.92909119e+02, 5.96440582e+01,\r\n         4.33532181e+01, 5.51930094e+00, 3.20475540e+01, 1.57478058e+02,\r\n         8.81307697e+00, 4.89615488e+00, 1.21958687e+02, 9.95254440e+01,\r\n         6.67657518e+00, 5.23443260e+01, 8.90209824e-02],\r\n\t\t \r\n        [2.01000000e+02, 8.70000000e+01, 4.50000000e+01, 2.25000000e+02,\r\n         1.74000000e+02, 1.92000000e+02, 2.40000000e+01, 9.00000000e+00,\r\n         3.00000000e+00, 2.01000000e+02, 4.77000000e+02, 1.50000000e+01,\r\n         1.32000000e+02, 3.06000000e+02, 2.76000000e+02, 2.97000000e+02,\r\n         3.15000000e+02, 3.60000000e+02, 1.92000000e+02, 4.80000000e+01,\r\n         7.20000000e+01, 4.20000000e+01, 2.49000000e+02, 2.61000000e+02,\r\n         2.46000000e+02, 3.45000000e+02, 1.68000000e+02, 2.79000000e+02,\r\n         5.10000000e+01, 3.00000000e+01, 6.30000000e+01, 9.90000000e+01,\r\n         1.86000000e+02, 3.00000000e+02, 1.59000000e+02, 2.64000000e+02,\r\n         2.31000000e+02, 1.08000000e+02, 2.01000000e+02, 1.50000000e+01,\r\n         1.38000000e+02, 1.98000000e+02, 2.85000000e+02, 7.62000000e+02,\r\n         8.70000000e+01, 4.20000000e+01, 3.99000000e+02, 1.17000000e+02,\r\n         1.65000000e+02, 2.16000000e+02, 6.90000000e+01],\r\n\t\t \r\n        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32), \r\n\t\t 'var5': array([[\r\n\t\t [1.13988188e+05, 0.00000000e+00, 3.05553312e+05, 6.17835547e+04,\r\n         1.21564547e+05, 7.45126172e+04, 2.67035875e+05, 1.33635656e+05,\r\n         8.42081719e+04, 2.53965918e+04, 1.16304000e+05, 0.00000000e+00,\r\n         6.78658516e+04, 1.31547656e+05, 1.88518406e+05, 5.68450156e+04,\r\n         7.80295312e+04, 9.76387109e+04, 9.91410000e+04, 5.47325234e+04,\r\n         1.85633031e+05, 1.36967094e+05, 1.60025156e+05, 3.86135820e+04,\r\n         8.56951953e+04, 8.19210156e+04, 2.45417363e+04, 6.05617969e+04,\r\n         4.40631688e+05, 1.11127484e+05, 2.57273109e+05, 1.13697508e+05,\r\n         2.13943297e+05, 1.10348430e+05, 9.84424902e+03, 1.42502391e+05,\r\n         1.23500828e+05, 1.12879500e+05, 3.33872875e+05, 1.90306844e+05,\r\n         9.51253984e+04, 2.10685605e+04, 9.12847812e+04, 4.01871914e+04,\r\n         3.29114000e+05, 1.30085805e+05, 1.63006031e+05, 3.89730586e+04,\r\n         1.57686484e+05, 1.85086047e+05, 8.28721328e+04],\r\n\t\t \r\n        [2.66852905e+02, 0.00000000e+00, 6.84133911e+02, 1.39492844e+02,\r\n         1.57386642e+02, 2.10147888e+02, 1.63822168e+03, 5.52221252e+02,\r\n         4.81121552e+02, 7.78194962e+01, 1.70976059e+02, 0.00000000e+00,\r\n         1.12634499e+02, 3.02216583e+02, 7.86791931e+02, 2.19900055e+02,\r\n         1.64349121e+02, 1.71831390e+02, 2.01542648e+02, 3.00014343e+02,\r\n         6.54877869e+02, 1.13219910e+03, 8.37139893e+02, 1.01028458e+02,\r\n         2.74855225e+02, 1.70993073e+02, 2.20124569e+01, 2.14118637e+02,\r\n         8.31200195e+02, 4.79989960e+02, 1.33207727e+03, 2.12968231e+02,\r\n         9.26734924e+02, 1.80603485e+02, 1.59528713e+01, 9.79636993e+01,\r\n         2.65579346e+02, 1.35530212e+02, 1.65390515e+03, 1.19887976e+03,\r\n         2.05736679e+02, 1.11524696e+02, 1.77777374e+02, 8.77156448e+01,\r\n         7.23254333e+02, 4.99206635e+02, 5.15731384e+02, 2.71134968e+01,\r\n         5.20434937e+02, 8.84037537e+02, 1.99430710e+02],\r\n\t\t \r\n        [1.70297058e+02, 0.00000000e+00, 3.77715942e+02, 8.11871109e+01,\r\n         1.16083351e+02, 8.52820892e+01, 3.82095898e+03, 7.02998718e+02,\r\n         2.77478363e+02, 5.72404747e+01, 7.60238724e+01, 0.00000000e+00,\r\n         2.76855278e+01, 2.05193298e+02, 5.64838196e+02, 3.36499290e+01,\r\n         9.26707993e+01, 7.46885986e+01, 9.48073273e+01, 3.88665619e+02,\r\n         6.49052490e+02, 1.15745081e+03, 5.50060730e+02, 2.35015488e+01,\r\n         2.36884766e+02, 6.75669250e+01, 6.58755302e+00, 1.19911224e+02,\r\n         4.07359894e+02, 8.14720154e+02, 3.33775269e+03, 1.13501717e+02,\r\n         1.83445532e+03, 7.85164719e+01, 4.80713320e+00, 5.48636597e+02,\r\n         7.90506287e+01, 1.00949776e+02, 2.54644580e+03, 2.47959058e+03,\r\n         7.78933487e+01, 1.40653162e+01, 6.61425858e+01, 5.23443184e+01,\r\n         4.32285797e+02, 9.21723328e+02, 3.37478424e+02, 1.48665037e+01,\r\n         3.60534698e+02, 5.92612671e+02, 1.20089279e+02],\r\n\t\t \r\n        [4.38000000e+02, 0.00000000e+00, 2.40000000e+02, 4.26000000e+02,\r\n         1.71000000e+02, 4.20000000e+02, 1.65000000e+02, 1.38000000e+02,\r\n         9.90000000e+01, 1.89000000e+02, 3.93000000e+02, 0.00000000e+00,\r\n         3.03000000e+02, 5.13000000e+02, 4.29000000e+02, 4.41000000e+02,\r\n         4.71000000e+02, 4.86000000e+02, 2.91000000e+02, 5.70000000e+01,\r\n         3.48000000e+02, 1.71000000e+02, 2.67000000e+02, 4.14000000e+02,\r\n         3.63000000e+02, 5.67000000e+02, 2.79000000e+02, 4.35000000e+02,\r\n         3.75000000e+02, 1.35000000e+02, 1.80000000e+02, 3.66000000e+02,\r\n         2.73000000e+02, 4.29000000e+02, 2.28000000e+02, 4.20000000e+02,\r\n         5.19000000e+02, 2.97000000e+02, 4.35000000e+02, 1.08000000e+02,\r\n         3.30000000e+02, 4.38000000e+02, 5.82000000e+02, 3.75000000e+02,\r\n         3.57000000e+02, 1.56000000e+02, 4.89000000e+02, 1.47000000e+02,\r\n         4.71000000e+02, 3.72000000e+02, 3.81000000e+02],\r\n\t\t \r\n        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\r\n         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32), \r\n\t\t 'var6': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0.]], dtype=float32), \r\n\t\t'time': array([[0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.],\r\n       [0.]]), \r\n\t   'var1': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0.]], dtype=float32), \r\n\t\t'var2': array([\r\n\t\t[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0.]]), \r\n\t\t'var3': array([[1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0],\r\n       [1, 0, 0, 0, 0]])})\r\n```"
      },
      {
        "user": "sven1977",
        "created_at": "2020-12-09T10:07:25Z",
        "body": "@ethem-kinginthenorth I think it's simply the dtype of the `time` child-space, which is Box(float64), but your obs time is a float32 np.ndarray (it's not printed in your numpy output above, but in this case, the dtype of the array is float32)."
      },
      {
        "user": "sven1977",
        "created_at": "2020-12-09T12:24:04Z",
        "body": "Closing this issue for now. Please feel free to re-open should the above solution not fix your problem.\r\nThanks."
      },
      {
        "user": "jyp0802",
        "created_at": "2022-01-13T02:04:08Z",
        "body": "This is already closed but just leaving a note for people who might come across this same problem.\r\n@sven1977 's solution fixed my problem. My Box was np.float64 but my observations were set to np.float32.\r\n\r\nSo changing the code like this solved it.\r\nfrom: ```gym.spaces.Box(np.float32(low), np.float32(high), dtype=np.float32)```\r\nto: ``` gym.spaces.Box(np.float64(low), np.float64(high), dtype=np.float64) ```\r\n"
      }
    ]
  },
  {
    "number": 12646,
    "title": "[rllib] Training via self-play with AlphaZero",
    "created_at": "2020-12-07T07:35:47Z",
    "closed_at": "2023-04-08T16:01:34Z",
    "labels": [
      "question",
      "rllib"
    ],
    "url": "https://github.com/ray-project/ray/issues/12646",
    "body": "Hello,\r\n\r\nI want to use the AlphaZero agent of rllib on a poker environment  that will learn to play via self-play. I understand that the current agent is designed only for single player games. Is there any way to extend it somehow in order to learn via self-play on Two-player adversarial games like chess and heads up poker? ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/12646/comments",
    "author": "DoxakisCh",
    "comments": [
      {
        "user": "hybug",
        "created_at": "2020-12-08T02:12:58Z",
        "body": "Issue #6669 implement self-play with PPO via multi-agent. But in PokerGame, the opponent-agent compute_action must base on the observation after rl-agent's step. So i don't think multi-agent is the proper way to implement PokerGame's self-play. \r\n\r\nIf you have any progress on PokerGame's selfplay, welcome to communicate with me."
      }
    ]
  },
  {
    "number": 12491,
    "title": "[rllib] episode reward mean",
    "created_at": "2020-11-30T08:58:03Z",
    "closed_at": "2023-03-28T21:47:33Z",
    "labels": [
      "question",
      "rllib"
    ],
    "url": "https://github.com/ray-project/ray/issues/12491",
    "body": "Hello, I try to run my code via rllib. But, I find the rllib auto generated ray results episode reward mean is not equal my code records. I want to know how to calculate the episode reward mean.  By the way, my code is  8 agent.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/12491/comments",
    "author": "zzchuman",
    "comments": [
      {
        "user": "ingambe",
        "created_at": "2020-11-30T21:50:39Z",
        "body": "Have you checked the `metrics_smoothing_episodes` parameter?\r\nIt defines the number of episodes to consider in the computation of these average/min/max metric"
      },
      {
        "user": "zzchuman",
        "created_at": "2020-12-26T13:16:01Z",
        "body": "> metrics_smoothing_episodes\r\n\r\nI am not define the metrics_smoothing_episodes. \r\n\r\n# Wait for metric batches for at most this many seconds. Those that have not returned in time will be collected in the next train iteration.\r\n \"collect_metrics_timeout\": 180,\r\n # Smooth metrics over this many episodes.            \"metrics_smoothing_episodes\": 100,\r\n\r\nSo, it is  the default 100.\r\nAnd, I want to know that  the episode reward mean is the mean value of multi agents?"
      },
      {
        "user": "mingjunwang88",
        "created_at": "2021-08-20T16:16:58Z",
        "body": "@ericl : Does this means the average mean for one episode will need to be divided by 100? I am asking this is because my own code only evaluates the mean reward only on one episode."
      },
      {
        "user": "ericl",
        "created_at": "2021-08-20T16:59:13Z",
        "body": "It will be averaged (smoothed) over the last 100 completed episodes of training overall.\r\n\r\n(so no division is needed)."
      },
      {
        "user": "mingjunwang88",
        "created_at": "2021-08-20T19:09:32Z",
        "body": "make sense. Thanks @ericl "
      }
    ]
  },
  {
    "number": 12348,
    "title": "Parameter Action,hidden invalid action,if all the step action in my valid action space.",
    "created_at": "2020-11-24T12:06:52Z",
    "closed_at": "2023-03-28T21:43:17Z",
    "labels": [
      "question",
      "needs-repro-script"
    ],
    "url": "https://github.com/ray-project/ray/issues/12348",
    "body": "I'm using ray rllib to develop a game bot, when I use the parameter action,I have hidden the invalid action by implements the TFModelV2 forward function, but the step action from the policy not all in the valid action, for example, valid action [1,5,9,15,29,90,100,190....4000], and some step action is 101, and some step in the valid action, can you help me solve this problem, wish your response as soon as possible.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/12348/comments",
    "author": "oasfuyou",
    "comments": [
      {
        "user": "alperyyildiz",
        "created_at": "2020-11-25T09:50:04Z",
        "body": "Is it possible that you are not passing your custom model to the trainer? I faced the same issue and found out that I'm not using my custom model."
      },
      {
        "user": "sven1977",
        "created_at": "2020-11-25T12:31:34Z",
        "body": "Thanks for filing this issue @oasfuyou !\r\nAlso, could you provide a reproduction script? We cannot help you debug issues quickly if we don't have such a script.\r\n"
      }
    ]
  },
  {
    "number": 12251,
    "title": "[RLlib] Restoring GTrXLNet with A3C model requires historic observation",
    "created_at": "2020-11-23T06:43:29Z",
    "closed_at": "2020-11-25T11:36:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/12251",
    "body": "Hey guys,\r\n\r\nI have a custom environment where the observation size is (1,10) and action size is (2). For my environment, I found that A3C with GTrXLNet performs best. For GTrXLNet, I have used following configuration:\r\n\r\n\"model\": {\r\n\t\t\"custom_model\": GTrXLNet,\r\n\t\t\"max_seq_len\": 10,\r\n\t\t\"custom_model_config\": {\r\n\t\t\t\"num_transformer_units\": 1,\r\n\t\t\t\"attn_dim\": 64,\r\n\t\t\t\"num_heads\": 2,\r\n\t\t\t\"memory_tau\": 50,\r\n\t\t\t\"head_dim\": 32,\r\n\t\t\t\"ff_hidden_dim\": 32,\r\n\t\t},\r\n\t},\r\n\r\nIt is mentioned in the GTrXLNet class that it assumes state to be the history of L recent observations. However, I have not changed anything in my environment. My custom environment returns the same sized state from both reset() and step(). I have successfully trained and saved model. Now, when I try to load the model for testing, it asks for multiple inputs. If I understood correctly, it asks me for following tensors:\r\n\r\nTensor(\"default_policy/Reshape_6:0\", shape=(?, ?, 10), dtype=float32) \r\n[<tf.Tensor 'default_policy/Placeholder:0' shape=(?, 10, 10) dtype=float32>, <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 50, 64) dtype=float32>] \r\nTensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)\r\n\r\nAs I returned the same sized state from both reset() and step() during the training, I don't know how to transform state into the required size for testing. Can anyone please help me in this issue?\r\n\r\nThank you in advance to anyone that is able to help me!\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/12251/comments",
    "author": "syed881",
    "comments": [
      {
        "user": "krfricke",
        "created_at": "2020-11-23T10:09:19Z",
        "body": "cc @sven1977 "
      },
      {
        "user": "sven1977",
        "created_at": "2020-11-25T09:58:42Z",
        "body": "Great question @syed881 !\r\nFor testing, you should always pass in the following into your loaded model (I'm assuming this is a \"raw\" tf model that you call from within a session?). Or are you using a restored RLlib Trainer?\r\n\r\nIn case you are using a raw tf model loaded from a tf-export:\r\n- observations: The current observation, but with time rank, so simply: [1=B, 1=T, [your obs dim=10]]\r\n- list of 2 state inputs:\r\n1) The previous 10 observations (all 0s if you are at the beginning of the episode). This way, you should get ([1=B, 10=T, [your obs dim=10]])\r\n2) The previous 10 memory states (your memory space is (64,)). Also here: if at the beginning of the episode, 0-fill this vector at the beginning (left-side). So this should have the shape: [1=B, 10=T, 64]\r\n- the seq_lens array, which should always be just: [1] if your batch size (B) is 1; [1, 1] if your batch size is 2, etc..\r\n\r\nImportant:\r\nNow the 2nd state output (in your config, its always a list of 2 tensors) of the model must be fed in the next model call back in as the new value under point 2) above.\r\n"
      }
    ]
  },
  {
    "number": 11971,
    "title": "[rllib] PPO ICM learning rate",
    "created_at": "2020-11-12T13:05:46Z",
    "closed_at": "2020-11-14T11:21:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/11971",
    "body": "Hello, I know the default ppo learning rate is 5e-5, default curiosity learning rate is 0.001. \r\nI just want to know whether the two learning rate are same?   \r\n\r\nIf I use curiosity in ppotrainer, how do I set it?\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/11971/comments",
    "author": "zzchuman",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-11-13T08:10:26Z",
        "body": "Hey @zzchuman , no they are not the same. The curiosity module has its own optimizer and lr.\r\nYou can set the curiosity lr inside the exploration_config key, the same way as it's done in rllib/utils/explorations/tests/test_curiosity.py:\r\n```\r\n            config[\"exploration_config\"] = {\r\n                \"type\": \"Curiosity\",\r\n                \"eta\": 0.2,\r\n                \"lr\": 0.001,  # <- HERE\r\n                \"feature_dim\": 128,\r\n                \"feature_net_config\": {\r\n                    \"fcnet_hiddens\": [],\r\n                    \"fcnet_activation\": \"relu\",\r\n                },\r\n                \"sub_exploration\": {\r\n                    \"type\": \"StochasticSampling\",\r\n                }\r\n            }\r\n```"
      },
      {
        "user": "zzchuman",
        "created_at": "2020-11-13T08:14:21Z",
        "body": "Thank you! got it! @sven1977 ,  I have a try! Thank you! "
      }
    ]
  },
  {
    "number": 11955,
    "title": "[rllib] Issue with changing name of checkpoint save file",
    "created_at": "2020-11-11T20:40:22Z",
    "closed_at": "2023-04-08T15:57:24Z",
    "labels": [
      "question",
      "rllib"
    ],
    "url": "https://github.com/ray-project/ray/issues/11955",
    "body": "Hello, I am using Rllib for some of our research. While working with RLlib we want to change the name of the file (Checkpoint_{}) where the checkpoint is saved with the .save() function. How can I achieve this?\r\n\r\nThanks \r\nAnshul",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/11955/comments",
    "author": "Pagariya",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-11-11T22:45:51Z",
        "body": "I think this is not possible. What is your use case?"
      },
      {
        "user": "Pagariya",
        "created_at": "2020-11-12T15:17:54Z",
        "body": "We are using it for Robotic Arm Control. We want to change the name of the checkpoint with the name having some metadata of training."
      }
    ]
  },
  {
    "number": 11376,
    "title": "[rllib] Multi-agent with different hyperparameters",
    "created_at": "2020-10-13T21:13:28Z",
    "closed_at": "2020-11-02T09:34:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/11376",
    "body": "\r\nI've been trying to get a simple **hierarchical system** working on some very basic problems and I am assuming I can't get it to work because I cannot set different hyperparameters for the two agents.\r\n\r\nI would like to **train them in the same time**, this means I'll have to settle for the same set of hyperparameters for both agents in any one trial. Is that correct? If so, how would I go about implementing such a feature, where I can set different hyperparameters for agents? Having fixed hyperparams for one of the agents would also work for me I think, but this is not possible either, or is it?\r\n\r\nI'd appreciate any help.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/11376/comments",
    "author": "m3t4n01a",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-11-02T08:26:07Z",
        "body": "There is a hierarchical_training.py example in the examples folder, where this is done with a single trainer and two policies (\"multi-agent\" setup). You can then specify config-overrides within the \"policies\" config key:\r\n```\r\nconfig:\r\n    multiagent:\r\n        policies:\r\n            pol1: {None, [obsspace], [actionspace], {extra config overrides for pol1}}\r\n            pol2: {None, [obsspace], [actionspace], {extra config overrides for pol2}}\r\n```"
      },
      {
        "user": "m3t4n01a",
        "created_at": "2020-11-02T09:34:47Z",
        "body": "Thanks @sven1977 ! \r\nI completely missed the policy config overriding.\r\n"
      }
    ]
  },
  {
    "number": 10416,
    "title": "Always getting .nan reward while training with PPO or DQN",
    "created_at": "2020-08-29T04:38:03Z",
    "closed_at": "2020-08-30T02:54:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10416",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### Can anyone please give me hints why I am always getting the following while training with PPO or DQN?\r\nepisode_len_mean: .nan\r\nepisode_reward_max: .nan\r\nepisode_reward_mean: .nan\r\nepisode_reward_min: .nan\r\nepisodes_this_iter: 0\r\nepisodes_total: 0\r\n\r\nRay Version: 0.8.7\r\nOS: macOS\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10416/comments",
    "author": "ashutosh1906",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-08-29T07:15:57Z",
        "body": "`episodes_total: 0`. This is the reason. Until an episode has finished, we can't calculate any rewards. Does your env eventually return done=True at some point?"
      },
      {
        "user": "ashutosh1906",
        "created_at": "2020-08-30T02:54:07Z",
        "body": "Thank you. After putting \"done = True\" at some points, episodes_total becomes non-zero and does not return any .nan."
      }
    ]
  },
  {
    "number": 10350,
    "title": "[ray] failed to find entry in mmap_records for fd 0",
    "created_at": "2020-08-27T08:09:11Z",
    "closed_at": "2020-09-07T10:45:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10350",
    "body": "### What is the problem?\r\n\r\n```\r\n[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m F0827 15:46:04.819053  1144  1156 malloc.cc:56] failed to find entry in mmap_records for fd 0\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m *** Check failure stack trace: ***\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142bb84d1d  google::LogMessage::Fail()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142bb85e7c  google::LogMessage::SendToLog()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142bb849f9  google::LogMessage::Flush()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142bb84c11  google::LogMessage::~LogMessage()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142bb700d9  ray::RayLog::~RayLog()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142b914236  plasma::GetMmapSize()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142b8f6774  plasma::PlasmaStore::ReturnFromGet()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142b8f6141  boost::asio::detail::wait_handler<>::do_complete()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142be6b01f  boost::asio::detail::scheduler::do_run_one()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142be6c521  boost::asio::detail::scheduler::run()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142be6d552  boost::asio::io_context::run()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x56142b8fb9be  plasma::PlasmaStoreRunner::Start()\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x7f17459dee50  (unknown)\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x7f1745786fa3  start_thread\r\n\u001b[2m\u001b[33m(pid=raylet, ip=10.148.55.22)\u001b[0m     @     0x7f17456914cf  clone\r\n```\r\nI get this error from one machine. I monitor the shared memory and its memory. There is no OOM issue. The total memory is 367G and only used 118G. The shared memory is 150G and only used 40G. After the raylet show this error and die, the shared memory continues to grow.\r\n\r\nIs this issue related to out of shared memory? I don't really understand this error.\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nray 0.8.7\r\npython 3.8.5\r\nos Debian\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10350/comments",
    "author": "Seraphli",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-08-28T06:43:46Z",
        "body": "Doesn'nt it happen upon starting a raylet? It seems like it happens when raylet just starts (by this stacktrace `plasma::PlasmaStoreRunner::Start()`)"
      },
      {
        "user": "Seraphli",
        "created_at": "2020-08-28T06:46:31Z",
        "body": "It happens when the experiment has run for 1.5 hours."
      },
      {
        "user": "rkooo567",
        "created_at": "2020-08-28T07:01:51Z",
        "body": "The error indicates that it gets an object that is mapped onto a mmap of fd 0 (which doesn't make sense because fd 0 is stdin). So, I guess there's some memory corruption, but it is extremely hard to know what's the issue here. "
      }
    ]
  },
  {
    "number": 10312,
    "title": "[ray] How to startup workers more than number of cores",
    "created_at": "2020-08-25T14:55:28Z",
    "closed_at": "2020-08-26T05:16:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10312",
    "body": "How to set ray startup arguments to let 150 workers running on a 96 cores machine? I notice ray will auto-scale on the local machine, but how to set while running a cluster?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10312/comments",
    "author": "Seraphli",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-08-26T01:24:12Z",
        "body": "Just set --num-cpus=150! "
      },
      {
        "user": "Seraphli",
        "created_at": "2020-08-26T05:16:19Z",
        "body": "I tried this and it works. Thank you."
      }
    ]
  },
  {
    "number": 10286,
    "title": "Configure host for ray dashboard",
    "created_at": "2020-08-24T15:20:12Z",
    "closed_at": "2020-08-25T14:21:08Z",
    "labels": [
      "question",
      "docs"
    ],
    "url": "https://github.com/ray-project/ray/issues/10286",
    "body": "I have the latest version of Ray and can configure the port but I also need to configure the host since right now it's set to 127.0.0.1 and I need it set to 0.0.0.0\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10286/comments",
    "author": "tatianafrank",
    "comments": [
      {
        "user": "simon-mo",
        "created_at": "2020-08-24T18:36:20Z",
        "body": "You can call `ray.init(dashboard_host=\"0.0.0.0\")` or `ray start --head --dashboard-host \"0.0.0.0\"`. Please let us know if this can solves your use case."
      },
      {
        "user": "wakandan",
        "created_at": "2021-10-05T09:58:22Z",
        "body": "how do I specify the host in command line? Right now I'm using `ray dashboard ...`"
      }
    ]
  },
  {
    "number": 10276,
    "title": "[ray] Ray show an exception about object may have been evicted",
    "created_at": "2020-08-23T13:28:04Z",
    "closed_at": "2020-08-27T08:18:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10276",
    "body": "I'm getting these two errors while running. What does it mean? The program seems to continue without any fatal error.\r\n```\r\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0823 17:27:37.349381  1162  1162 object_manager.cc:229] The object manager with ID 2918fec1712fa18a6774c2fff07829d6624c58ab is trying to pull object e62bf4e53cc5e0a0b1a939fb010000807e080000 but the object table suggests that this object manager already has the object. The object may have been evicted.\r\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0823 17:27:37.625520  1162  1162 object_manager.cc:247] The object manager with ID 2918fec1712fa18a6774c2fff07829d6624c58ab is trying to pull object 020477c17e8b5615185a5999010000808f080000 but the object table suggests that this object manager already has the object.\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10276/comments",
    "author": "Seraphli",
    "comments": [
      {
        "user": "Seraphli",
        "created_at": "2020-08-23T13:29:22Z",
        "body": "Another error is this.\r\n```\r\n\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0823 17:23:05.706449  1162  1162 object_manager.cc:169] 957151fb5d566031691aceab0100008015070000 attempted to pull an object that's already local.\r\n```"
      },
      {
        "user": "rkooo567",
        "created_at": "2020-08-23T17:26:32Z",
        "body": "This means GCS says the object is in the local machine, but it doesn't actually have that object, so object manager tries to pull objects from other nodes. \r\n\r\nDid you turn on `LRU_evict=True`? cc @stephanie-wang "
      },
      {
        "user": "stephanie-wang",
        "created_at": "2020-08-23T21:38:40Z",
        "body": "This can happen because the object directory is updated asynchronously, so an object manager may have received an object, evicted it (due to memory pressure), and requested it again before the eviction was reflected in the object directory. It should not be fatal but could indicate that there is memory pressure in the cluster."
      },
      {
        "user": "Seraphli",
        "created_at": "2020-08-24T03:56:06Z",
        "body": "If I call an actor method and pass some data to this actor, then this actor use ray.put to put this data into object store, will ray transfer this data to the local machine of this actor?\r\nBTW, we didn't turn on `LRU_evict`."
      },
      {
        "user": "Seraphli",
        "created_at": "2020-08-27T08:18:22Z",
        "body": "I have tried to create a new connection to bypass this problem."
      }
    ]
  },
  {
    "number": 10235,
    "title": "[Tune] Have some trials running on GPU and some trials on CPUs",
    "created_at": "2020-08-21T07:47:31Z",
    "closed_at": "2020-09-16T03:55:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10235",
    "body": "\r\nI only have one GPU and 32 vCPUs on my machine.\r\n\r\nWhen I use `resources_per_trial={'gpu': 1)`, I only have one trial executed at the time, as it waits for the GPU to be released to launch a new trial.\r\n\r\nIs it possible to have other trials running in parallel on the available CPUs while one trial uses the GPU, and when the first trial on GPU finishes, it launches a new trial on GPU?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10235/comments",
    "author": "abelien101",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-09-04T22:58:18Z",
        "body": "Unfortunately this isn't possible. You can do this yourself by checking for gpu utilitization in your training function and acquiring a lock on the gpu via `filelock`. When a training function finishes, it can release the filelock and the next trial that checks can acquire it and use the GPU."
      }
    ]
  },
  {
    "number": 10058,
    "title": "Rllib cannot run on gpu",
    "created_at": "2020-08-12T02:24:26Z",
    "closed_at": "2023-04-08T15:55:37Z",
    "labels": [
      "question",
      "rllib"
    ],
    "url": "https://github.com/ray-project/ray/issues/10058",
    "body": "I install ray[rllib] and tensorflow using the following commands\r\n\r\n```python\r\npip install ray[rllib]==0.8.6\r\npip install tensorflow-gpu==2.3.0 # I've also tried tensorflow==2.3.0 and previous tensorflow versions, similar errors were reported\r\n```\r\nI run rllib's PPO and receive the following error message\r\n\r\n>RuntimeError: GPUs were assigned to this worker by Ray, but TensorFlow reports GPU acceleration is disabled. This could be due to a bad CUDA or TF installation.\r\n\r\nI've tested the environment using my own code and the GPU is used as expected. How can I fix it?\r\n\r\nMy system information:\r\n```\r\nUbuntu: 18.04\r\ncuda: 10.2\r\ncudnn: 7.6.5\r\npython: 3.8.3\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10058/comments",
    "author": "xlnwel",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-08-14T02:36:53Z",
        "body": "cc @sven1977 Is it the known issue? "
      },
      {
        "user": "ericl",
        "created_at": "2020-08-14T20:44:59Z",
        "body": "This is almost certainly due to a bad CUDA installation. Have you checked if tensorflow can use GPUs without Ray?"
      },
      {
        "user": "xlnwel",
        "created_at": "2020-08-15T11:53:35Z",
        "body": "I don't think so. I've run several other programs written in TF and ray, they all used the GPU correctly. The error happens only when I use rllib."
      },
      {
        "user": "xlnwel",
        "created_at": "2020-08-15T23:38:40Z",
        "body": "I also found that if I build TF from source, the error will disappear. "
      }
    ]
  },
  {
    "number": 10048,
    "title": "[RLLib] RLLib Customized Env do we have to put one action_space in environment?",
    "created_at": "2020-08-11T18:48:19Z",
    "closed_at": "2020-08-17T23:11:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10048",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\nI have a customized environment, where per state we have different numbers of actions. The idea is to create a dictionary where for each state, it has different set of number of actions. I know we can use parametric action space to handle this scenario. But we want to use a different model to handle it and customized the action distribution function. In that case, do we have to pass action_space in BaseEnv?  Or we can just put a dummy action_space there and customized the model and action distribution function the way we want. \r\n\r\nThanks.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10048/comments",
    "author": "AprilXiaoyanLiu",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-08-11T22:39:11Z",
        "body": "You must always define the action space as a `gym.Space`, perhaps you can use a Tuple of different action spaces, or a big action space that is the union of all possible child spaces."
      },
      {
        "user": "AprilXiaoyanLiu",
        "created_at": "2020-08-17T23:11:42Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 9999,
    "title": "[core] Ray on slurm cluster hangs after running a while.",
    "created_at": "2020-08-08T10:27:24Z",
    "closed_at": "2021-01-16T06:17:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9999",
    "body": "### What is your question?\r\nI'm running ray on a slurm cluster with 5 nodes. One node is used as the head node without any computing work. Each worker node launch 8 Tasks, each requires 1 gpu and 4 cpus cores to do some heavy computing. However, some Tasks hang after running a while and other Tasks work properly. I've tested the code on a local machine with 2 Tasks and it works with no problem. What might be the cause of this problem? Besides this problem, I find that the log within each Task is repeatedly logged on all worker nodes, but I think that each Task should only be run on one node, why I receive repeated logs?\r\n\r\n**Code to launch the tasks:**\r\n```python\r\nray_worker_ids = []\r\nfor w_idx in range(args.n_task):\r\n    ray_worker_ids.append(process_with_single_worker.remote(work_loads[w_idx], args, w_idx))\r\n```\r\n\r\n**Decorator of the task:**\r\n```python\r\n@ray.remote(num_cpus=4, num_gpus=1, resources={\"worker_node\": 1})\r\ndef process_with_single_worker(work_loads, args, worker_id):\r\n    # some heavy works...\r\n```\r\n**Slurm script:**\r\n```shell\r\n#!/bin/bash\r\n\r\n#SBATCH --mem=128\r\n#SBATCH --gres=gpu:8\r\n#SBATCH --cpus-per-task=64\r\n#SBATCH --nodes=5\r\n#SBATCH --tasks-per-node 1\r\n#SBATCH --exclusive\r\n\r\nworker_num=4 \r\nnum_gpus=8  \r\nnum_cpus=32  # num cpus per node\r\n\r\nnodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # Getting the node names\r\nnodes_array=( $nodes )\r\n\r\nnode1=${nodes_array[0]}\r\n\r\nip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address) # Making address\r\nsuffix=':6379'\r\nip_head=$ip_prefix$suffix\r\nredis_password=$(uuidgen)\r\n\r\nexport ip_head # Exporting for latter access by trainer.py\r\n\r\nsrun --nodes=1 --ntasks=1 -w $node1 ray start --block --head --redis-port=6379 --redis-password=$redis_password --num-gpus=$num_gpus --num-cpus=2 \\\r\n    --internal-config='{\"initial_reconstruction_timeout_milliseconds\": 100000}' \\\r\n    --num-redis-shards 8 & # Starting the head\r\nsleep 30\r\n# Make sure the head successfully starts before any worker does, otherwise\r\n# the worker will not be able to connect to redis. In case of longer delay,\r\n# adjust the sleeptime above to ensure proper order.\r\n\r\nfor ((  i=1; i<=$worker_num; i++ ))\r\ndo\r\n  node2=${nodes_array[$i]}\r\n  srun --nodes=1 --ntasks=1 -w $node2 ray start --block --address=$ip_head --redis-password=$redis_password --num-gpus=$num_gpus --num-cpus=$num_cpus \\\r\n      --memory=$((128 * 1024 * 1024 * 1024)) \\\r\n      --resources='{\"worker_node\": 100}' \\\r\n      --internal-config='{\"initial_reconstruction_timeout_milliseconds\": 100000}' & # Starting the workers\r\n  # Flag --block will keep ray process alive on each compute node.\r\n  sleep 30\r\ndone\r\n\r\npython -u ./script_name.py --redis_password=$redis_password --n_task=32\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9999/comments",
    "author": "angshine",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-08-10T16:17:22Z",
        "body": "> I find that the log within each Task is repeatedly logged on all worker nodes, but I think that each Task should only be run on one node, why I receive repeated logs?\r\n\r\n@wuisawesome Do you have any idea why?\r\n\r\nWhat version of Ray are you using? Also, can you show us logs from /tmp/ray/session_latest/logs/gcs_server.err? "
      },
      {
        "user": "amogkam",
        "created_at": "2020-10-29T23:25:09Z",
        "body": "Is this still an issue @angshine? Could you also send over the console output you are seeing on all the nodes as well as your python script?"
      }
    ]
  },
  {
    "number": 9925,
    "title": "how to get start from scratch as a DEVELOPER of Ray?",
    "created_at": "2020-08-05T08:51:39Z",
    "closed_at": "2020-08-14T03:11:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9925",
    "body": "specially, any details about the modules like plasma, arrow etc.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9925/comments",
    "author": "SeekPoint",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-08-14T02:45:46Z",
        "body": "Hi, @loveJasmine Thanks for your interest in becoming a Ray contributor!! I'd like to suggest a couple ways to start! Learning about core components like Plasma store or other parts of systems are a little tricky now, but we will have more resources online out there sooner or later, so stay tuned :). \r\n\r\n- Use Ray! Think about what's the problem and propose fixes! Joining our slack is also good idea.\r\n- Start from fixing small issues. There are bunch of issues tagged with `good-first-issue`, which are created for beginner contributors! "
      }
    ]
  },
  {
    "number": 9901,
    "title": "[sgd] What's the purpose of LocalRunner in TorchTrainer",
    "created_at": "2020-08-04T08:05:24Z",
    "closed_at": "2020-12-07T08:27:19Z",
    "labels": [
      "question",
      "sgd"
    ],
    "url": "https://github.com/ray-project/ray/issues/9901",
    "body": "Want to ask what's the purpose of always keeping a runner on the local driver node instead of making all runners actors?\r\nAs I go through code, is it because it is easier to get the trained model without pickling from remote nodes? If so, does it mean it is difficult to serialize/deserialize models in ray?\r\n\r\nThanks so much! @richardliaw ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9901/comments",
    "author": "hkvision",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-08-04T08:10:57Z",
        "body": "We should get rid of this because it causes quite a bit of problems :) The reason was previously for interactive workloads + debugging, but it seems like the costs outweigh benefits."
      },
      {
        "user": "hkvision",
        "created_at": "2020-08-04T08:45:10Z",
        "body": "Hi @richardliaw Thanks for your quick reply!\r\n\r\nSo you mean you are going to remove local runner and all use actors in the future?\r\nThen in this case if I want to get the trained model, I need to choose one remote worker and get the model to driver? In the current TorchRunner, seems there is no get_model method, models is just a property of the runner. How to retrieve the attributes of actors in Ray actually?"
      },
      {
        "user": "hkvision",
        "created_at": "2020-12-07T08:27:19Z",
        "body": "I saw this issue has been fixed by adding a `use_local` flag. Closing this issue. Thanks so much!"
      }
    ]
  },
  {
    "number": 9863,
    "title": "How to init Ray with a specified GPU id to run all trials of Tune?",
    "created_at": "2020-08-02T12:46:26Z",
    "closed_at": "2020-08-03T02:01:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9863",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\nSay, I have 4 GPUs with ids=[0, 1, 2, 3] and I only want to run all trials for Tune on id=2 and id=3 only. That means I can only maximize the use of the third and fourth GPU without touching the first two GPUs. How can I achieve this? \r\n\r\n```ray.init(num_cpus=num_cpus, num_gpus=num_gpus, temp_dir=ray_log)```\r\n\r\nThe attribute ```num_gpus``` is the number of GPUs ray can use. When setting ```num_gpus=1```, all the trials run on the first device (GPU id=0).  When increasing ```num_gpus```, all the trials will ordinally use GPUs from id=0 to id=3... I want to know how to specify the exact GPU ids, e.g., all trials run on id=2 and id=3.\r\n\r\nI've tried specifying GPU id in the training functions, but raised ```RuntimeError: CUDA error: invalid device ordinal```. \r\n\r\nI'm still new to this great project. Appreciate your warm help!\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nOS: Linux\r\nPython: 3.7.4\r\nRay: 0.8.6",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9863/comments",
    "author": "guoxuxu",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-08-02T17:24:52Z",
        "body": "Try setting the CUDA_VISIBLE_DEVICES before running the ray script?"
      },
      {
        "user": "guoxuxu",
        "created_at": "2020-08-03T02:01:24Z",
        "body": "Soga. It works now. Thanks very much!"
      },
      {
        "user": "ndvbd",
        "created_at": "2023-05-16T18:22:39Z",
        "body": "But is there a smarter way, to automatically choose the free gpus from the cluster?"
      }
    ]
  },
  {
    "number": 9723,
    "title": "[tune] AWS Spot Instance Automation",
    "created_at": "2020-07-27T10:49:15Z",
    "closed_at": "2020-08-07T12:43:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9723",
    "body": "I'm using the following python tune script:\r\n\r\n**hpo.py**\r\n\r\n```python\r\n# Install and import libraries\r\nimport time\r\nfrom lightgbm import LGBMClassifier\r\nimport pandas as pd\r\nimport ray\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.metrics import f1_score\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tune_sklearn import TuneSearchCV\r\n\r\n# Start timer\r\nstart = time.time()\r\n\r\n# Initialize Ray\r\nray.init(address='auto')\r\n\r\n# Load breast cancer dataset\r\ncancer = load_breast_cancer()\r\nX = cancer.data\r\ny = cancer.target\r\n\r\n# Split into train and test\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\n# Initialize estimator\r\nmodel = LGBMClassifier()\r\n\r\n# Initialize parameter distributions\r\nparam_dists = {\r\n    'boosting_type': ['gbdt'],\r\n    'colsample_bytree': (0.8, 0.9, 'log-uniform'),\r\n    'reg_alpha': (1.1, 1.3),\r\n    'reg_lambda': (1.1, 1.3),\r\n    'min_split_gain': (0.3, 0.4),\r\n    'subsample': (0.7, 0.9),\r\n    'subsample_freq': (20, 21)\r\n}\r\n\r\n# Initialize tuner\r\ntuner = TuneSearchCV(\r\n    model,\r\n    param_dists,\r\n    n_iter=20,\r\n    scoring='f1_weighted',\r\n    n_jobs=-1,\r\n    verbose=2,\r\n    max_iters=10,\r\n    search_optimization='bayesian',\r\n    use_gpu=True,\r\n)\r\n\r\n# Tune hyperparameters\r\ntuner.fit(X_train, y_train)\r\nprint('Best Parameters :', tuner.best_params_)\r\n\r\n# Get cross-validated results\r\ndf_cv = pd.DataFrame(tuner.cv_results_)\r\n\r\n# Predict using best hyperparameters\r\ny_pred = tuner.predict(X_test)\r\nprint('F1 Score:', f1_score(y_test, y_pred, average='weighted'))\r\n\r\n# Get elapsed time\r\nend = time.time()\r\nprint('Elapsed Time :', (end - start))\r\n\r\n# Shutdown Ray\r\nray.shutdown()\r\n```\r\n\r\nand the following configuration file:\r\n\r\n**tune-default-hpo.yaml**\r\n\r\n```yaml\r\ncluster_name: tune-default\r\nprovider: {type: aws, region: us-east-2}\r\nauth: {ssh_user: ubuntu}\r\nmin_workers: 3\r\nmax_workers: 3\r\n\r\nhead_node:\r\n    InstanceType: c5.xlarge\r\n    ImageId: ami-08bf49c7b3a0c761e\r\n\r\n    # Run workers on spot by default. Comment this out to use on-demand.\r\n    InstanceMarketOptions:\r\n        MarketType: spot\r\n        SpotOptions:\r\n            MaxPrice: \"1\"  # Max Hourly Price\r\n\r\n# Provider-specific config for worker nodes, e.g. instance type.\r\nworker_nodes:\r\n    InstanceType: m5.large\r\n    ImageId: ami-08bf49c7b3a0c761e\r\n\r\n    # Run workers on spot by default. Comment this out to use on-demand.\r\n    InstanceMarketOptions:\r\n        MarketType: spot\r\n        SpotOptions:\r\n            MaxPrice: \"1\"  # Max Hourly Price\r\n\r\nsetup_commands: # Set up each node.\r\n    - pip install lightgbm ray scikit-optimize torch torchvision tabulate tensorboard tune_sklearn\r\n```\r\n\r\n**Command**: `ray submit tune-default-hpo.yaml hpo.py --start --stop`\r\n\r\nQuestions:\r\n1. Ray is not terminating the spot instances if the connection is lost. Is there a way to configure ray to terminate them automatically?\r\n2. Also is there a python API to deploy the instances instead of running the command on the terminal?\r\n3. How do I configure Ray to deploy on a single spot instance and not a cluster?\r\n4. How can I set the storage size for the spot instances?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9723/comments",
    "author": "rohan-gt",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-07-29T02:12:27Z",
        "body": "Thanks for making this issue @rohan-gt! \r\n\r\n> Ray is not terminating the spot instances if the connection is lost. Is there a way to configure ray to terminate them automatically?\r\n\r\nWhat do you mean connection is lost? Maybe `ray submit --stop`?\r\n\r\n> Also is there a python API to deploy the instances instead of running the command on the terminal?\r\n\r\nUnfortunately, it's not public-facing.\r\n\r\n> How do I configure Ray to deploy on a single spot instance and not a cluster?\r\n\r\nmin_workers: 0\r\nmax_workers: 0\r\n\r\n> How can I set the storage size for the spot instances?\r\n\r\n```\r\nworker_nodes:\r\n    InstanceType: m4.16xlarge\r\n    ImageId: ami-0def3275  # Default Ubuntu 16.04 AMI.\r\n\r\n    # Set primary volume to 250 GiB\r\n    BlockDeviceMappings:\r\n        - DeviceName: /dev/sda1\r\n          Ebs:\r\n              VolumeSize: 250\r\n```"
      },
      {
        "user": "rohan-gt",
        "created_at": "2020-07-29T05:08:29Z",
        "body": "@richardliaw I tried deploying the cluster from my local PC using the following command:\r\n`ray submit tune-default-hpo.yaml hpo.py --start --stop`\r\n\r\nBut if either the submitted Python script has an error or if I close the terminal mid execution or if my internet connection is lost, the spot instance cluster is not shutdown\r\n\r\nI later tried passing `ray submit --stop` which gives the error `Error: Missing argument 'CLUSTER_CONFIG_FILE'.` while `ray submit tune-default-hpo.yaml hpo.py --stop` errors out with `Command 'ray' not found, did you mean:` because I closed the terminal before Ray was installed on the head node. \r\n\r\nIs there a way to:\r\n1. Automatically reconnect and finish execution?\r\n2. Teardown the cluster if the connection was closed mid-execution and the head node is idle beyond a set time?"
      },
      {
        "user": "richardliaw",
        "created_at": "2020-08-02T20:38:35Z",
        "body": "Sorry for the slow reply:\r\n\r\n```\r\nray submit tune-default-hpo.yaml hpo.py --start --tmux --stop\r\n```\r\nallows you to teardown the cluster automatically after the job is finished AND does not require the internet connection to be kept on your laptop.\r\n\r\nDoes that help?"
      },
      {
        "user": "rohan-gt",
        "created_at": "2020-08-03T22:38:33Z",
        "body": "Hmm this still requires the ray package to be present on the head node before it can shutdown right? Because when I tried closing the terminal mid pip installation of the ray package it didn't auto shutdown. But sure if this is how it is that's good enough"
      },
      {
        "user": "richardliaw",
        "created_at": "2020-08-03T23:11:10Z",
        "body": "Great point; yeah, to clarify, this requires Ray to have started on the head-node (so `ray up` and `ray start` needs to have finished)."
      }
    ]
  },
  {
    "number": 9641,
    "title": "RuntimeError: Synchronized objects should only be shared between processes through inheritance",
    "created_at": "2020-07-22T18:52:24Z",
    "closed_at": "2020-07-28T11:35:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9641",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nRuntimeError: Synchronized objects should only be shared between processes through inheritance\r\n\r\nI'm trying to build an Ape-X Rl myself. The main codes are as follows\r\n\r\n    cfg=Strategist_config()\r\n    cfg.worker_NN_update_period = 200\r\n    cfg.worker_send_samples_period = 100\r\n    cfg.worker_send_batch_size = 2048\r\n    cfg.learner_batch_size = 64\r\n    cfg.learner_start_min_mem = 30000\r\n    cfg.learner_save_NN_period = 1000\r\n    cfg.learner_Shared_NN_update_period = 5\r\n    global_mem=CustomPrioritizedReplayBuffer.remote(1000000, 0.6)\r\n    N_workers=4\r\n\r\n    DDPG_leaner=Learner.remote(cfg,global_mem)\r\n    DDPG_workers = [Worker.remote(i, cfg, global_mem) for i in range(N_workers)]\r\n    DDPG_director=Director.remote(cfg)\r\n\r\n    processes=[]\r\n    for worker in DDPG_workers:\r\n        processes.append(worker)\r\n    processes.append(DDPG_leaner)\r\n    processes.append(DDPG_director)\r\n\r\n    processes=[proc.run.remote() for proc in processes]\r\n    ray.wait(processes)\r\n    ray.timeline()\r\n\r\nAnd I got the error of \r\nFile \"C:/Users/s267565/OneDrive - Cranfield University/Desktop/Works/To_learn/PER_regNN_reg_sharemem_hypertune_noclip_enable_Tamb_maxPrio_raymem/test.py\", line 22, in <module>\r\n    DDPG_leaner=Learner.remote(cfg,global_mem)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ray\\actor.py\", line 379, in remote\r\n    return self._remote(args=args, kwargs=kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ray\\actor.py\", line 572, in _remote\r\n    extension_data=str(actor_method_cpu))\r\n  File \"python\\ray\\_raylet.pyx\", line 911, in ray._raylet.CoreWorker.create_actor\r\n  File \"python\\ray\\_raylet.pyx\", line 916, in ray._raylet.CoreWorker.create_actor\r\n  File \"python\\ray\\_raylet.pyx\", line 276, in ray._raylet.prepare_args\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ray\\serialization.py\", line 396, in serialize\r\n    return self._serialize_to_msgpack(metadata, value)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ray\\serialization.py\", line 368, in _serialize_to_msgpack\r\n    self._serialize_to_pickle5(metadata, python_objects)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ray\\serialization.py\", line 348, in _serialize_to_pickle5\r\n    raise e\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ray\\serialization.py\", line 345, in _serialize_to_pickle5\r\n    value, protocol=5, buffer_callback=writer.buffer_callback)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle_fast.py\", line 70, in dumps\r\n    cp.dump(obj)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle_fast.py\", line 656, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\sharedctypes.py\", line 199, in __reduce__\r\n    assert_spawning(self)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 356, in assert_spawning\r\n    ' through inheritance' % type(obj).__name__\r\nRuntimeError: Synchronized objects should only be shared between processes through inheritance\r\n\r\nSo what am I doing wrong?\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9641/comments",
    "author": "Lewisracing",
    "comments": [
      {
        "user": "davzaman",
        "created_at": "2021-03-07T03:33:12Z",
        "body": "Why was this closed? I am getting the same error.\r\n"
      },
      {
        "user": "ds21194",
        "created_at": "2021-07-12T13:38:32Z",
        "body": "How did you solve this? @Lewisracing "
      }
    ]
  },
  {
    "number": 9637,
    "title": "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host",
    "created_at": "2020-07-22T15:39:55Z",
    "closed_at": "2020-07-23T02:56:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9637",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\nHi I am new to Ray and TensorFlow. I am trying to parallelize a simple neural network problem. When the training data is small (300 MB), I had no problem running, although I still get the following warning: \r\n\r\n> 2020-07-22 10:18:09,186\tWARNING worker.py:1047 -- Warning: The remote function __main__.nn1 has size 749505929 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\r\n\r\nWhen I use a larger training data (~2GB), the program stopped with the following error message: \r\n\r\n> 2020-07-22 10:18:09,186\tWARNING worker.py:1047 -- Warning: The remote function __main__.nn1 has size 749505929 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\r\n> Traceback (most recent call last):\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\redis\\connection.py\", line 700, in send_packed_command\r\n>     sendall(self._sock, item)\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\redis\\_compat.py\", line 8, in sendall\r\n>     return sock.sendall(*args, **kwargs)\r\n> \r\n> ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\r\n> \r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n> \r\n>   File \"<ipython-input-38-71ac0ae4d4d4>\", line 3, in <module>\r\n>     ids = [nn1.remote(Xtrain_id, ytrain_id,  Xtest_id, lbd_list_id,lr_list_id, ne, nl, nlr) for ne in range(1)]\r\n> \r\n>   File \"<ipython-input-38-71ac0ae4d4d4>\", line 3, in <listcomp>\r\n>     ids = [nn1.remote(Xtrain_id, ytrain_id,  Xtest_id, lbd_list_id,lr_list_id, ne, nl, nlr) for ne in range(1)]\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ray\\remote_function.py\", line 95, in _remote_proxy\r\n>     return self._remote(args=args, kwargs=kwargs)\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ray\\remote_function.py\", line 176, in _remote\r\n>     worker.function_actor_manager.export(self)\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ray\\function_manager.py\", line 152, in export\r\n>     \"max_calls\": remote_function._max_calls\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\redis\\client.py\", line 3023, in hmset\r\n>     return self.execute_command('HMSET', name, *items)\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\redis\\client.py\", line 877, in execute_command\r\n>     conn.send_command(*args)\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\redis\\connection.py\", line 721, in send_command\r\n>     check_health=kwargs.get('check_health', True))\r\n> \r\n>   File \"C:\\Users\\jxl128031\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\redis\\connection.py\", line 713, in send_packed_command\r\n>     (errno, errmsg))\r\n> \r\n> ConnectionError: Error 10054 while writing to socket. An existing connection was forcibly closed by the remote host.\r\n\r\nCan someone please help how I can resolve this issue? Thanks. \r\n\r\nMy code is below: \r\n\r\n```\r\nray.shutdown()\r\nray.init(ignore_reinit_error=True, num_cpus=10, memory=28000 * 1024 * 1024, object_store_memory=14000*1024*1024)\r\n\r\n@ray.remote\r\ndef nn1(Xtrain, ytrain, ne):\r\n    import tensorflow as tf\r\n    from tensorflow import keras\r\n    from tensorflow.keras.optimizers import Adam #maybe put this at the top of your file\r\n    import os\r\n\r\n    #tf.random.set_seed(ne)\r\n    tf.random.set_random_seed(ne)\r\n    lbd = 0.01\r\n    lr  = 0.01\r\n    print(lbd, lr)\r\n    NEP =1\r\n    batch_size= 10000\r\n    ptc = 5  \r\n    early_stopping_cb = keras.callbacks.EarlyStopping(patience=ptc, restore_best_weights=True)\r\n  \r\n    model = keras.models.Sequential([  \r\n    keras.layers.Dense(32,activation='relu', input_shape=Xtrain.shape[1:], kernel_initializer='he_normal',\r\n                       kernel_regularizer = keras.regularizers.l1(lbd)),\r\n    keras.layers.BatchNormalization(),\r\n    keras.layers.Dense(1, activation='linear',  kernel_initializer='he_normal',kernel_regularizer = keras.regularizers.l1(lbd))\r\n    ])\r\n\r\n    opt = Adam(lr=lr)  \r\n    model.compile(optimizer=opt, loss='mse')\r\n\r\n    history = model.fit(Xtrain, ytrain, epochs=NEP, validation_data=(Xvalid, yvalid),\r\n                        callbacks=[early_stopping_cb], batch_size = batch_size)\r\n\r\n    return os.getpid()\r\n\r\nXtrain_id = ray.put(Xtrain)\r\nytrain_id = ray.put(ytrain)\r\n\r\nids = [nn1.remote(Xtrain_id, ytrain_id, ne) for ne in range(10)]\r\n```\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n\r\nPython version: 3.7.7\r\nTensorFlow version: 1.14.0\r\nOS: windows 10 \r\nRAM: 76 GB\r\nLogical processors: 24\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9637/comments",
    "author": "semiter1",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-07-22T16:38:18Z",
        "body": "cc @mehrdadn it looks like somehow data that is supposed to go to Plasma is ending up in Redis"
      },
      {
        "user": "semiter1",
        "created_at": "2020-07-22T16:40:43Z",
        "body": "Thanks, richardliaw. Is there anything I can do about it? "
      },
      {
        "user": "richardliaw",
        "created_at": "2020-07-22T16:46:38Z",
        "body": "@mehrdadn can help you, he's the windows expert :) "
      },
      {
        "user": "semiter1",
        "created_at": "2020-07-22T16:48:25Z",
        "body": "> )\r\n\r\nGot it. Thanks! "
      },
      {
        "user": "mehrdadn",
        "created_at": "2020-07-22T21:20:20Z",
        "body": "Thanks for reporting! I'll try to see if I can repro. Just to confirm: how much free RAM do you have? And how big of a pagefile (swap)?\r\n\r\nAlso, if you have Linux running (WSL?) and can occur the same code doesn't trigger that behavior on there, that would be great. But if not then no worries."
      },
      {
        "user": "semiter1",
        "created_at": "2020-07-22T21:34:40Z",
        "body": "Thanks, mehrdadn. I have 76 GB RAM in total, currently 63.6 GB available, and the total page file size for all drive is 77824 MB.\r\n\r\nI do not have Linux, but I do have a mac pro. The issue with the mac is that its RAM size is only 16 GB, and there seems to be other problems associated with that."
      },
      {
        "user": "mehrdadn",
        "created_at": "2020-07-22T22:13:55Z",
        "body": "Okay thanks, I'll take a look. (Btw I deleted your email reply from your answer; it had your personal email address.)"
      },
      {
        "user": "mehrdadn",
        "created_at": "2020-07-22T22:43:12Z",
        "body": "@semiter1 Your repro doesn't run unfortunately. You haven't imported `ray` (which I did myself) but I'm not sure what to set `Xtrain` and `ytrain` to in order to reproduce the issue. If you could post a self-contained repro script that would be great. Thanks!\r\n```\r\nTraceback (most recent call last):\r\n  File \"temp.py\", line 36, in <module>\r\n    Xtrain_id = ray.put(Xtrain)\r\nNameError: name 'Xtrain' is not defined\r\n```"
      },
      {
        "user": "semiter1",
        "created_at": "2020-07-23T02:54:45Z",
        "body": "I figured out the issue. I used Xvalid and yvalid in the validation_date for early stopping, but I never convert them using ray.put, so ray is creating a copy of these two variables for each process. However, it is not intuitive that why it creates an error \"[WinError 10054] An existing connection was forcibly closed by the remote host. \"\r\n\r\nMany thanks for your help, @mehrdadn! "
      },
      {
        "user": "mehrdadn",
        "created_at": "2020-07-23T02:56:07Z",
        "body": "Ah thanks for sharing the info! Glad you solved it."
      }
    ]
  },
  {
    "number": 9569,
    "title": "is_training key with rllib",
    "created_at": "2020-07-19T16:03:11Z",
    "closed_at": "2020-07-20T01:27:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9569",
    "body": "ray 0.8.6, python 3.7, pytorch 1.5.1, using rllib PPO algorithm \r\n\r\nDuring, training, the output of input_dict[\"is_training\"] in ```  def forward(self, input_dict,  ... ``` is starting returning False,False, False then becomes True, True True for each iteration. Aren't it suppose to be True all the time?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9569/comments",
    "author": "cometta",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-07-19T18:10:29Z",
        "body": "It should be False when the Model call happens from out of the compute_actions method of the Policy (sampling/inference) and True when the Model is called in some loss function using a train_batch."
      }
    ]
  },
  {
    "number": 9507,
    "title": "Handling thousands of jobs using ray",
    "created_at": "2020-07-15T19:47:07Z",
    "closed_at": "2020-10-26T08:31:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9507",
    "body": "[autoscaler]\r\n\r\n### If thousands of jobs are to be executed, would the recommendation be\r\n\r\n- To have a single ray autoscale cluster and submit all thousand jobs to it. With my given understanding of ray, for this to work in kubernetes, launch a autoscale ray cluster using `ray up cluster.yaml` command and submit the jobs to this ray cluster using `kubectl create -f job.yaml`. Couple of questions with this approach\r\n  - Is this how Ray was envisioned to be used?\r\n  - With the single Ray Head node, would the execution of thousands of jobs become a bottleneck?\r\n  - How does the RAY_HEAD_NODE_IP or RAY_HEAD_SERVICE_HOST discovery work for a submitted job?\r\n- To launch a ray cluster for each distributable job and delete it, on completion of the job? Is there any restrictions w.r.t this approach?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9507/comments",
    "author": "darthveda-ai",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-07-16T20:38:49Z",
        "body": "The best way to do this is to have different jobs correspond to different Ray clusters (for now)."
      }
    ]
  },
  {
    "number": 9471,
    "title": "[rllib] marwil",
    "created_at": "2020-07-14T15:57:30Z",
    "closed_at": "2020-07-27T17:45:35Z",
    "labels": [
      "question",
      "P3"
    ],
    "url": "https://github.com/ray-project/ray/issues/9471",
    "body": "hello， I want to use the marwil on rllib. \r\n\r\nFirst, I try to run the rllib test_marwil.py. But, it reminds me that no module of from ray.rllib.utils.test_utils import framework_iterator,check_compute_single_action . I dont kon why, Can you teach me?\r\n\r\nAnd, your docs is simple! Can you wirite a tips or example teach me how to set the offline datasets to meet the MARWIL.\r\n\r\nAnd I test it  MARWIL. But, MARWIL rewards is NAN.\r\n\r\nThanks you!\r\nHope you well!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9471/comments",
    "author": "zzchuman",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-07-17T21:26:14Z",
        "body": "cc @sven1977 "
      },
      {
        "user": "09wakharet",
        "created_at": "2020-07-27T17:45:35Z",
        "body": "I can run test_marwil.py as is, and it train properly. Make sure that your ray installation is up to date (sudo pip install ray[rllib] --upgrade). Make sure to specifically install ray[rllib]. To train on new datasets, you'll have to update the data_file variable with your own dataset."
      }
    ]
  },
  {
    "number": 9444,
    "title": "[tune] progress.csv missing header when training interrupted",
    "created_at": "2020-07-13T16:26:45Z",
    "closed_at": "2020-07-16T21:19:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9444",
    "body": "Hi, is there a way to ensure that headers are always generated in progress.csv. \r\n\r\nIn some cases, training is interrupted (i.e. stop/resume) before the csv header can be generated. We use progress.csv to plot training progress in our application so the headers are necessary.\r\n\r\n**Versions**\r\nRay 0.8.5\r\nTensorflow 1.15.0\r\nPython 3.7.3\r\nDebian OS",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9444/comments",
    "author": "ejunprung",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-07-16T21:15:08Z",
        "body": "Headers should be generated whenever the trial starts running. If a trial has not started running, we try to not to write to csv file (it's not possible to know the results ahead of time). \r\n\r\nIf you want to change behavior, you may way to either provide a custom logger or in your application check for file contents?"
      },
      {
        "user": "ejunprung",
        "created_at": "2020-07-16T21:19:34Z",
        "body": "We ended up doing exactly that but it's good to hear that you have the same recommendation. The problem seems to be fixed now!"
      }
    ]
  },
  {
    "number": 9309,
    "title": "[rllib] Cannot detect pybullet environments.",
    "created_at": "2020-07-06T00:56:25Z",
    "closed_at": "2020-07-06T03:43:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9309",
    "body": "### Pybullet Environments Cannot Be Detected By Ray/rllib\r\n\r\nHi, I'm trying to use rllib to train pybullet games. My environment is ray 0.8.4, ubuntu 16.04, Pytorch  1.2.0. It seems that ray cannot detect these games and said the game was not registered. But I can make the gym environment outside ray within the same script. I attached a simple code to show what's wrong. Could someone help with this? Thanks!!\r\n\r\n```\r\nimport ray\r\nfrom ray.rllib.agents.ppo import PPOTrainer\r\nfrom ray.tune.registry import register_env\r\nimport gym\r\nimport pybullet_envs\r\n\r\nenv = gym.make('HumanoidBulletEnv-v0')\r\nprint(\"Made Successfully\")\r\n\r\nclass MyEnv(gym.Env):\r\n    def __init__(self, env_config):\r\n        self.env = gym.make('HumanoidBulletEnv-v0')\r\n        self.action_space = self.env.action_space\r\n        self.observation_space = self.env.observation_space\r\n\r\n    def reset(self):\r\n        obs = self.env.reset()\r\n        return obs\r\n\r\n    def step(self, action):\r\n        action = self.action_space.high * action\r\n        obs, reward, done, info = self.env.step(action)\r\n        return obs, reward, done, info\r\n\r\nregister_env(\"myenv\", lambda config: MyEnv(config))\r\n\r\n\r\ndef main():\r\n    ray.init()    \r\n    trainer = PPOTrainer(env=\"myenv\", config={\r\n        \"use_pytorch\": True,\r\n        })\r\n\r\n    for i in range(100):\r\n        trainer.train()\r\n\r\n    trainer.stop()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n\r\nWhen I run the code, the environment outside ray could be made successfully and 'Made Successfully' was printed. But then I get the error that \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"pybullet_train.py\", line 44, in <module>\r\n    main()\r\n  File \"pybullet_train.py\", line 39, in main\r\n    trainer.train()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 502, in train\r\n    raise e\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 491, in train\r\n    result = Trainable.train(self)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\r\n    result = self._train()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 150, in _train\r\n    fetches = self.optimizer.step()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/optimizers/sync_samples_optimizer.py\", line 59, in step\r\n    for e in self.workers.remote_workers()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/utils/memory.py\", line 29, in ray_get_and_free\r\n    result = ray.get(object_ids)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1513, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(UnregisteredEnv): ray::RolloutWorker.__init__() (pid=8430, ip=192.168.1.8)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 118, in spec\r\n    return self.env_specs[id]\r\nKeyError: 'HumanoidBulletEnv-v0'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray::RolloutWorker.__init__() (pid=8430, ip=192.168.1.8)\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 287, in __init__\r\n    self.env = _validate_env(env_creator(env_context))\r\n  File \"pybullet_train.py\", line 27, in <lambda>\r\n    register_env(\"myenv\", lambda config: MyEnv(config))\r\n  File \"pybullet_train.py\", line 14, in __init__\r\n    self.env = gym.make('HumanoidBulletEnv-v0')\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 142, in make\r\n    return registry.make(id, **kwargs)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 86, in make\r\n    spec = self.spec(path)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 128, in spec\r\n    raise error.UnregisteredEnv('No registered env with id: {}'.format(id))\r\ngym.error.UnregisteredEnv: No registered env with id: HumanoidBulletEnv-v0\r\n```\r\n\r\nI know 'import pybullet_envs' will register the environments in gym. It looked like rollout workers didn't detect these environments. Could someone tell me how to solve this? Thank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9309/comments",
    "author": "KarlXing",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-07-06T02:59:30Z",
        "body": "Can you try moving the import into the constructor for your class? The problem is the import only applies locally and not on the Ray workers."
      },
      {
        "user": "KarlXing",
        "created_at": "2020-07-06T03:43:13Z",
        "body": "Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply."
      },
      {
        "user": "Glaucus-2G",
        "created_at": "2020-07-14T08:27:50Z",
        "body": "> Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply.\r\n\r\n\r\nCould you show me your codes? I am learning how to use it! Thank you!"
      },
      {
        "user": "Glaucus-2G",
        "created_at": "2020-07-15T02:25:16Z",
        "body": "> > Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply.\r\n\r\n> Could you show me your codes? I am learning how to use it! Thank you!\r\n\r\nI think I have got it.  Thank you!"
      }
    ]
  },
  {
    "number": 9295,
    "title": "[RLlib] Mutiagent learning: can't combine replay lockstep and multiple agents controlled by the same policy.",
    "created_at": "2020-07-03T15:07:18Z",
    "closed_at": "2020-07-20T06:03:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9295",
    "body": "Hello,\r\n\r\nWhile implementing MADDPG for PyTorch I noticed that it is not possible to combine the lockstep replay mode configuration and having multiple agents controlled by the same policy. This is due to the implementation of MultiAgentBatch as a dict of `PolicyID -> SampleBatch`. In the contrib TensorFlow implementation this issue was circumvented using parameter sharing. Another solution which is framework agnostic would be to group the agents. \r\n\r\nI think that this combination should be supported without needing to group the agents. However, I suspect that it would require significant code change. Maybe another solution exists?\r\n\r\nAlso, the documentation should be updated alongside with the code that checks that the config is valid.\r\n\r\nThanks \r\n(Amazing work btw)",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9295/comments",
    "author": "raphaelavalos",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-07-03T18:20:01Z",
        "body": "Hmm, I think this should be ok. The batch will indeed be keyed by policy id, but you can look into the \"agent_id\" field of the batch to see which experiences correspond with which original agent."
      },
      {
        "user": "raphaelavalos",
        "created_at": "2020-07-06T06:53:02Z",
        "body": "Okk I will look into that next week and let you know. Thx"
      },
      {
        "user": "raphaelavalos",
        "created_at": "2020-07-14T15:23:44Z",
        "body": "I will update the docs tomorrow and make a PR."
      }
    ]
  },
  {
    "number": 9288,
    "title": "[autoscaler]  how to set `ssh_user` if `worker_ips` usernames are all different each other?",
    "created_at": "2020-07-03T07:01:53Z",
    "closed_at": "2020-07-06T01:16:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9288",
    "body": "In local configuration, It looks like `ssh_user`  in `configuration.yaml` should be set as the user of `head_node`, right?\r\n\r\nBut what if the `worker_node`'s username is different with `head_node`, say head_node's user name is `user1` and `worker_node`'s is `user2`? How does `head_node` find out worker's username when connecting via ssh?\r\n\r\nWhen the nodes are connected within the same networks(192.168.1.x) and I submitted some jobs to the head node, only head node worked hard and nothing happened in workers.\r\n\r\nIn this situation, should I run ray manually in each worker?  like `ray start --address=<address>` ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9288/comments",
    "author": "rightx2",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-07-04T17:21:37Z",
        "body": "Yep! In this situation, starting ray manually is the way to go."
      },
      {
        "user": "rightx2",
        "created_at": "2020-07-06T01:16:12Z",
        "body": "@richardliaw  Thanks. I've solved the problem.\r\n"
      }
    ]
  },
  {
    "number": 9250,
    "title": "Relation between training_batch_size/ rollout_fragment_length and number of workers for assessing the scalability of RL algorithms",
    "created_at": "2020-07-01T16:11:54Z",
    "closed_at": "2020-07-06T20:39:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9250",
    "body": "[rllib]\r\n\r\nI want to assess the scalability of my training with APPO algorithm on 32,64,128 cores/CPUs. As per my understanding, in distributed reinforcement learning, the environment is run in parallel in a distributed manner, and then then the batches of training samples are collected. The neural network is then trained using this data. The documentation says \"Sample batches of this size (`rollout_fragment_length`) are collected from rollout workers and combined into a larger batch of `train_batch_size` for learning.\"\r\n\r\nMy question is whether the `rollout_fragment_length` affects the training significantly or not? For example, if `rollout_fragment_length` is 20 and I am using 32 workers, then 640 (32*20) samples will be collected for training the neural network. With the same `rollout_fragment_length` and 128 workers, it will be 2560 (128*20) samples. Shall I use the same sample batch size (`rollout_fragment_length`) irrespective of the number of workers, or do I need to change the sample batch size as I increase the number of workers?\r\n\r\nThank you. \r\n\r\n*Ray 0.7.6, Python 3.6.8, Tensorflow 1.14.0, Numpy 1.16.1*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9250/comments",
    "author": "surajp92",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-07-02T02:56:21Z",
        "body": "It shouldn't matter too much here, basically you want to set it small enough so that the train batch isn't bigger than expected due to multiple workers, but not too small since VF bootstrapping is done at rollout fragment boundaries. Also setting it too small adds system overhead."
      },
      {
        "user": "surajp92",
        "created_at": "2020-07-06T20:39:44Z",
        "body": "Thank you. "
      }
    ]
  },
  {
    "number": 8872,
    "title": "AWS Secure Key ",
    "created_at": "2020-06-10T00:29:53Z",
    "closed_at": "2020-10-26T08:33:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8872",
    "body": "For using Tune and AWS:\r\nIt is more secure to pull AWS access keys from the environment variables instead of putting directly in YAML. One way is to pull the keys from the local machine environment variables on \"ray submit ...\". Not sure if there are other ways.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8872/comments",
    "author": "richardrl",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-06-10T04:46:45Z",
        "body": "You can put your keys in the boto config, no need to hardcode them in the yaml. Actually any method supported by boto works."
      },
      {
        "user": "richardrl",
        "created_at": "2020-06-10T20:49:20Z",
        "body": "@ericl You mean, set them in the environment variables? \r\n\r\nThere must be a secure way of transferring the AWS_ACCESS_SECRET_ID and AWS_ACCESS_SECRET_KEY from the local machine (running \"ray submit ... \") to the AWS EC2 instance. It can be done, but I'm not sure there's a clean way to do this with the current code functionalities."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-10-26T08:33:42Z",
        "body": "(closing because stale)"
      }
    ]
  },
  {
    "number": 8861,
    "title": "Timesteps in one iteration changes [rllib]",
    "created_at": "2020-06-09T15:54:24Z",
    "closed_at": "2020-06-10T15:14:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8861",
    "body": "I am using the appo algorithm to train my custom environment. For my problem, number of time steps in one episode is fixed at 4000 and I set the the done flag to True after 4000 time steps. \r\n\r\nI am also fixing the length of one episode in the config file. Below are my appo algorithm parameters:\r\n```\r\n    config = appo.DEFAULT_CONFIG.copy()\r\n    config[\"log_level\"] = \"WARN\"\r\n    config[\"num_gpus\"] = 0\r\n    config[\"num_workers\"] = 1 # serial training\r\n    config[\"lr\"] = 2.5e-4\r\n    config[\"horizon\"] = 4000\r\n    config[\"train_batch_size\"] = 4000\r\n    config[\"num_sgd_iter\"] = 4          # Number of SGD epochs to execute per train batch\r\n\r\n    trainer = appo.APPOTrainer(config=config, env=\"myenv\")\r\n    for i in range(count):\r\n         result = trainer.train()\r\n```\r\nHere is the progress history of training. I have observed that the episodes in one iteration and total timesteps in one iteration change as the training progresses. Is this normal or I am doing some mistake? From the documentation, I thought that I can control the number of time steps in one iteration using the train_batch_size parameter. \r\n  \r\nepisode_len_mean | episodes_this_iter | timesteps_this_iter | done | timesteps_total | episodes_total | training_iteration\r\n-- | -- | -- | -- | -- | -- | --\r\n4000 | 1 | 4000 | False | 4000 | 1 | 1\r\n4000 | 3 | 12000 | False | 16000 | 4 | 2\r\n4000 | 3 | 12000 | False | 28000 | 7 | 3\r\n3815 | 3 | 8000 | False | 36000 | 10 | 4\r\n3662.15384615385 | 3 | 12000 | False | 48000 | 13 | 5\r\n3725.5 | 3 | 12000 | False | 60000 | 16 | 6\r\n3768.84210526316 | 3 | 12000 | False | 72000 | 19 | 7\r\n3790.85714285714 | 2 | 8000 | False | 80000 | 21 | 8\r\n3817 | 3 | 12000 | False | 92000 | 24 | 9\r\n3837.33333333333 | 3 | 8000 | False | 100000 | 27 | 10\r\n\r\n*Ray version: 0.7.6 Python version:3.6.8, TensorFlow version: 1.14.0:*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8861/comments",
    "author": "surajp92",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-06-10T04:48:25Z",
        "body": "You actually want timesteps_per_iter. There is also min_iter_time_s which takes precedence."
      },
      {
        "user": "surajp92",
        "created_at": "2020-06-10T15:14:47Z",
        "body": "Thank you. "
      },
      {
        "user": "surajp92",
        "created_at": "2020-06-11T15:20:44Z",
        "body": "I also have a question related to episode_len_mean. I am fixing the length of one episode with config[\"horizon\"]=4000. When I use asynchronous algorithms like appo or a2c, the mean length of episode changes? Should the episode length not be constant with training iteration? Thank you. "
      }
    ]
  },
  {
    "number": 8811,
    "title": "Exclude file_mounts?",
    "created_at": "2020-06-06T01:23:45Z",
    "closed_at": "2020-10-26T08:34:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8811",
    "body": "Is there a way to exclude certain subdirectories/filetypes within the AWS YAML?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8811/comments",
    "author": "richardrl",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-06-06T10:01:56Z",
        "body": "cc @allenyin55 "
      },
      {
        "user": "richardliaw",
        "created_at": "2020-10-26T08:34:27Z",
        "body": "Implemented!"
      }
    ]
  },
  {
    "number": 8762,
    "title": "How to run my code in head node without having the yaml file?",
    "created_at": "2020-06-03T12:44:55Z",
    "closed_at": "2020-06-03T19:56:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8762",
    "body": "Dear Experts,\r\nI have access to a cluster head node via ssh.\r\nI don't have the yaml file to up or down it.\r\nNow, I uploaded my code into the head node server. Its ip is 10.0.0.109.\r\nI checked that ray is running by doing ```ps aux | grep ray``` \r\nIn my code, I tried all of the following without any success:\r\n```\r\nray.init(address='auto')\r\n```\r\n```\r\nray.init(address='10.0.0.109:6379')\r\n```\r\n```\r\nray.init(address='publicIpOfServer:6379')\r\n```\r\n\r\nI always got this:\r\n```\r\n2020-06-03 12:43:40,903 WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\r\n2020-06-03 12:43:40,905 WARNING services.py:217 -- Some processes that the driver needs to connect to have not registered with Redis, so retrying. Have you run 'ray start' on this node?\r\n2020-06-03 12:43:41,907 WARNING services.py:217 -- Some processes that the driver needs to connect to have not registered with Redis, so retrying. Have you run 'ray start' on this node?\r\n2020-06-03 12:43:42,909 WARNING services.py:217 -- Some processes that the driver needs to connect to have not registered with Redis, so retrying. Have you run 'ray start' on this node?\r\n2020-06-03 12:43:43,910 WARNING services.py:217 -- Some processes that the driver needs to connect to have not registered with Redis, so retrying. Have you run 'ray start' on this node?\r\n2020-06-03 12:43:44,912 WARNING services.py:217 -- Some processes that the driver needs to connect to have not registered with Redis, so retrying. Have you run 'ray start' on this node?\r\nRedis has started but no raylets have registered yet.\r\n  File \"loadTestScenarioTotalPackets.py\", line 94, in main\r\n    ray.init(address='auto')\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py\", line 827, in init\r\n    connect_only=True)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/node.py\", line 139, in __init__\r\n    redis_password=self.redis_password)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/services.py\", line 210, in get_address_info_from_redis\r\n    redis_address, node_ip_address, redis_password=redis_password)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/services.py\", line 181, in get_address_info_from_redis_helper\r\n    \"Redis has started but no raylets have registered yet.\")\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8762/comments",
    "author": "aefn",
    "comments": [
      {
        "user": "virtualluke",
        "created_at": "2020-06-03T14:59:53Z",
        "body": "What does` ps aux | grep ray` look like, do you have a set of idle workers running?"
      },
      {
        "user": "rkooo567",
        "created_at": "2020-06-03T17:07:48Z",
        "body": "cc @ijrsvt \r\nIt also could be related to those Docker issues. \r\n\r\n "
      },
      {
        "user": "aefn",
        "created_at": "2020-06-03T19:46:08Z",
        "body": "> What does` ps aux | grep ray` look like, do you have a set of idle workers running?\r\n\r\n```\r\nubuntu@ip-10-0-0-109:~$ ps aux | grep ray\r\nubuntu   24863  0.2  0.2  67480  9496 ?        Sl   12:20   1:10 /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/core/src/ray/thirdparty/redis/src/redis-server *:6379\r\nubuntu   24868  0.0  0.2  64276  9392 ?        Sl   12:20   0:24 /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/core/src/ray/thirdparty/redis/src/redis-server *:57513\r\nubuntu   24872  0.8  0.4 412328 19388 ?        Sl   12:20   3:38 /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/core/src/ray/gcs/gcs_server --redis_address=10.0.0.109 --redis_port=6379 --config_list= --redis_password=5241590000000000\r\nubuntu   24873 10.7  2.4 1012124 96640 ?       Sl   12:20  47:22 /home/ubuntu/anaconda3/envs/tensorflow_p36/bin/python -u /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py --redis-address=10.0.0.109:6379 --autoscaling-config=~/ray_bootstrap_config.yaml --redis-password=5241590000000000\r\nubuntu   24879  0.2  2.9 1555948 116272 ?      Sl   12:20   1:12 /home/ubuntu/anaconda3/envs/tensorflow_p36/bin/python -u /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/dashboard/dashboard.py --host=localhost --port=8265 --redis-address=10.0.0.109:6379 --temp-dir=/tmp/ray --redis-password 5241590000000000\r\nubuntu   24880  0.0  0.1 1135532 4088 ?        S    12:20   0:00 /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/core/src/plasma/plasma_store_server -s /tmp/ray/session_2020-06-03_12-20-35_465518_24860/sockets/plasma_store -m 1145515622 -d /dev/shm\r\nubuntu   24881  0.4  0.5 962452 22284 ?        Sl   12:20   2:00 /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2020-06-03_12-20-35_465518_24860/sockets/raylet --store_socket_name=/tmp/ray/session_2020-06-03_12-20-35_465518_24860/sockets/plasma_store --object_manager_port=8076 --min_worker_port=10000 --max_worker_port=10999 --node_manager_port=60029 --node_ip_address=10.0.0.109 --redis_address=10.0.0.109 --redis_port=6379 --num_initial_workers=2 --maximum_startup_concurrency=2 --static_resource_list=node:10.0.0.109,1.0,CPU,2,memory,43,object_store_memory,15 --config_list= --python_worker_command=/home/ubuntu/anaconda3/envs/tensorflow_p36/bin/python /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/workers/default_worker.py --node-ip-address=10.0.0.109 --node-manager-port=60029 --object-store-name=/tmp/ray/session_2020-06-03_12-20-35_465518_24860/sockets/plasma_store --raylet-name=/tmp/ray/session_2020-06-03_12-20-35_465518_24860/sockets/raylet --redis-address=10.0.0.109:6379 --config-list= --temp-dir=/tmp/ray --redis-password=5241590000000000 --java_worker_command= --redis_password=5241590000000000 --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2020-06-03_12-20-35_465518_24860\r\nubuntu   24882  0.5  1.9 549104 76068 ?        Sl   12:20   2:17 /home/ubuntu/anaconda3/envs/tensorflow_p36/bin/python -u /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/reporter.py --redis-address=10.0.0.109:6379 --redis-password 5241590000000000\r\nubuntu   24883  1.2  1.8 321028 72796 ?        S    12:20   5:27 /home/ubuntu/anaconda3/envs/tensorflow_p36/bin/python -u /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/log_monitor.py --redis-address=10.0.0.109:6379 --logs-dir=/tmp/ray/session_2020-06-03_12-20-35_465518_24860/logs --redis-password 5241590000000000\r\nubuntu   28074  1.0  1.9 2257464 79444 ?       Sl   14:57   3:00 ray::IDLE\r\nubuntu   28625  0.0  0.0  14860  1124 pts/0    S+   19:41   0:00 grep --color=auto ray\r\n```"
      },
      {
        "user": "aefn",
        "created_at": "2020-06-03T19:56:40Z",
        "body": "I found the problem but actually I don't know why it was the problem.\r\n```\r\nubuntu@ip-10-0-0-109:~$ which python\r\n/home/ubuntu/anaconda3/bin//python\r\n```\r\nI was trying to run my code with that python.\r\nthen I saw in the ```ps aux``` output that my cluster is running by\r\n```\r\n/home/ubuntu/anaconda3/envs/tensorflow_p36/bin/python\r\n```\r\nthen I tried to run my code by that python\r\n```\r\n/home/ubuntu/anaconda3/envs/tensorflow_p36/bin/python path/to/my/code\r\n```\r\nand it worked"
      }
    ]
  },
  {
    "number": 8695,
    "title": "[raysgd][tfrunner] Getting stuck after running the task ",
    "created_at": "2020-06-01T05:54:04Z",
    "closed_at": "2020-06-23T11:26:28Z",
    "labels": [
      "question",
      "sgd"
    ],
    "url": "https://github.com/ray-project/ray/issues/8695",
    "body": "[raysgd][tfrunner]\r\nI am using tf runner example code and created a cluster with 2 different machines(192.168.140.44, 49)\r\nAfter running \"$ python sample_code.py --address 192.168.140.44:6379 -n 2\"\r\nI am getting the following log and in the end it stuck there for infinite time.\r\n```\r\n- 2020-06-01 11:43:48,179 WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\r\n- (pid=30968) 2020-06-01 11:43:50.643620: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n\r\n- (pid=35845, ip=192.168.140.44) 2020-06-01 11:43:54.718155: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n- (pid=35845, ip=192.168.140.44) 2020-06-01 11:43:54.730408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2597005000 Hz\r\n- (pid=35845, ip=192.168.140.44) 2020-06-01 11:43:54.738331: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b96458ee90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n- (pid=35845, ip=192.168.140.44) 2020-06-01 11:43:54.738393: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n- (pid=35845, ip=192.168.140.44) 2020-06-01 11:43:54.738530: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 56. Tune using inter_op_parallelism_threads for best performance.\r\n- (pid=35845, ip=192.168.140.44) E0601 11:43:54.747657173   35845 socket_utils_common_posix.cc:222] check for SO_REUSEPORT: {\"created\":\"@1590992034.747646644\",\"description\":\"SO_REUSEPORT unavailable on compiling system\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":190}\r\n- (pid=30968) 2020-06-01 11:43:50.655770: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2597035000 Hz\r\n- (pid=30968) 2020-06-01 11:43:50.659451: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562568aa1470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n- (pid=30968) 2020-06-01 11:43:50.659495: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n- (pid=30968) 2020-06-01 11:43:50.659704: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 56. Tune using inter_op_parallelism_threads for best performance.\r\n- (pid=30968) E0601 11:43:50.668888712   30968 socket_utils_common_posix.cc:222] check for SO_REUSEPORT: {\"created\":\"@1590992030.668871022\",\"description\":\"SO_REUSEPORT unavailable on compiling system\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":190}\r\n- (pid=30968) 2020-06-01 11:43:50.675626: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:14724, 1 -> 192.168.140.44:54153}\r\n- (pid=30968) 2020-06-01 11:43:50.679194: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:14724\r\n- (pid=35845, ip=192.168.140.44) 2020-06-01 11:43:54.762549: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.140.49:14724, 1 -> localhost:54153}\r\n- (pid=35845, ip=192.168.140.44) 2020-06-01 11:43:54.769828: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:54153\r\n```\r\n\r\nRay version : 0.8.5\r\npython version : 3.6.10\r\ntensorflow version : 2.2.0\r\nOS : centos 7 \r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8695/comments",
    "author": "panditamey1",
    "comments": [
      {
        "user": "panditamey1",
        "created_at": "2020-06-01T06:05:14Z",
        "body": " Code is as follows\r\n\r\n`\r\n\r\n\timport argparse\r\n\timport tensorflow as tf\r\n\tfrom tensorflow.keras.models import Sequential\r\n\tfrom tensorflow.keras.layers import Dense\r\n\timport numpy as np\r\n\timport json\r\n\timport ray\r\n\t#from ray import tune\r\n\tfrom ray.util.sgd.tf.tf_trainer import TFTrainer, TFTrainable\r\n\timport os\r\n\timport time\r\n\t#NUM_TRAIN_SAMPLES = 100000000\r\n\tNUM_TRAIN_SAMPLES = 1000000\r\n\tNUM_TEST_SAMPLES = 4000\r\n\r\n\r\n\tdef create_config(batch_size):\r\n\t\treturn {\r\n\t\t\t# todo: batch size needs to scale with # of workers\r\n\t\t\t\"batch_size\": batch_size,\r\n\t\t\t\"fit_config\": {\r\n\t\t\t\t\"steps_per_epoch\": NUM_TRAIN_SAMPLES // batch_size\r\n\t\t\t},\r\n\t\t\t\"evaluate_config\": {\r\n\t\t\t\t\"steps\": NUM_TEST_SAMPLES // batch_size,\r\n\t\t\t}\r\n\t\t}\r\n\r\n\tdef linear_dataset(a=2, size=1000):\r\n\t\tx = np.random.rand(size)\r\n\t\ty = x / 2\r\n\r\n\t\tx = x.reshape((-1, 1))\r\n\t\ty = y.reshape((-1, 1))\r\n\r\n\t\treturn x, y\r\n\r\n\t def simple_dataset(config):\r\n\t\tbatch_size = config[\"batch_size\"]\r\n\t\tx_train, y_train = linear_dataset(size=NUM_TRAIN_SAMPLES)\r\n\t\tx_test, y_test = linear_dataset(size=NUM_TEST_SAMPLES)\r\n\r\n\t\ttrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n\t\ttest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\n\t\ttrain_dataset = train_dataset.shuffle(NUM_TRAIN_SAMPLES).repeat().batch(\r\n\t\t\tbatch_size)\r\n\t\ttest_dataset = test_dataset.repeat().batch(batch_size)\r\n\r\n\t\treturn train_dataset, test_dataset\r\n\r\n\r\n\r\n\t def simple_model(config):\r\n\t\tmodel = Sequential([Dense(10, input_shape=(1, )), Dense(1)])\r\n\r\n\t\tmodel.compile(\r\n\t\t\toptimizer=\"sgd\",\r\n\t\t\tloss=\"mean_squared_error\",\r\n\t\t\tmetrics=[\"mean_squared_error\"])\r\n\r\n\t\treturn model\r\n\r\n\tdef train_example(num_replicas=1, batch_size=128, use_gpu=False):\r\n\t\ttrainer = TFTrainer(\r\n\t\t\tmodel_creator=simple_model,\r\n\t\t\tdata_creator=simple_dataset,\r\n\t\t\tnum_replicas=num_replicas,\r\n\t\t\tuse_gpu=use_gpu,\r\n\t\t\tverbose=True,\r\n\t\t\tconfig=create_config(batch_size))\r\n\r\n\t\t# model baseline performance\r\n\t\tstart_stats = trainer.validate()\r\n\t\tprint(start_stats)\r\n\t\tnum_epochs = 10\r\n\t\tstart = time.time()\r\n\t\t# train for 2 epochs\r\n\t\tfor e in range(num_epochs):\r\n\t\t\ttrainer.train()\r\n\t\t#trainer.train()\r\n\t\tstop = time.time()\r\n\r\n\t\t# model performance after training (should improve)\r\n\t\tend_stats = trainer.validate()\r\n\t\tprint(end_stats)\r\n\r\n\t\t# sanity check that training worked\r\n\t\tdloss = end_stats[\"validation_loss\"] - start_stats[\"validation_loss\"]\r\n\t\tdmse = (end_stats[\"validation_mean_squared_error\"] -\r\n\t\t\t\tstart_stats[\"validation_mean_squared_error\"])\r\n\t\tprint(f\"dLoss: {dloss}, dMSE: {dmse}\")\r\n\t\tprint(\"total time ray tf: \",stop - start)\r\n\t\tray_time = stop - start\r\n\t\tsample_config = create_config(batch_size)\r\n\r\n\t\ttf_default_model = simple_model(sample_config)\r\n\t\ttrain_dataset, test_dataset = simple_dataset(sample_config)\r\n\t\tstart = time.time()\r\n\r\n\r\n\t\ttf_default_model.fit(train_dataset, epochs=num_epochs, steps_per_epoch = sample_config[\"fit_config\"][\"steps_per_epoch\"])\r\n\t\tprint(\"total time tf default : \", time.time() - start)\r\n\t\tprint(tf_default_model.evaluate(test_dataset,steps = sample_config[\"evaluate_config\"][\"steps\"]))\r\n\t\tprint(\"total time ray :\",ray_time)\r\n\r\n\t\tif dloss > 0 or dmse > 0:\r\n\t\t\tprint(\"training sanity check failed. loss increased!\")\r\n\t\telse:\r\n\t\t\tprint(\"success!\")\r\n\r\n\r\n\tdef tune_example(num_replicas=1, use_gpu=False):\r\n\t\tconfig = {\r\n\t\t\t\"model_creator\": tune.function(simple_model),\r\n\t\t\t\"data_creator\": tune.function(simple_dataset),\r\n\t\t\t\"num_replicas\": num_replicas,\r\n\t\t\t\"use_gpu\": use_gpu,\r\n\t\t\t\"trainer_config\": create_config(batch_size=128)\r\n\t\t}\r\n\r\n\t\tanalysis = tune.run(\r\n\t\t\tTFTrainable,\r\n\t\t\tnum_samples=2,\r\n\t\t\tconfig=config,\r\n\t\t\tstop={\"training_iteration\": 2},\r\n\t\t\tverbose=1)\r\n\r\n\t\treturn analysis.get_best_config(metric=\"validation_loss\", mode=\"min\")\r\n\r\n\r\n\tif __name__ == \"__main__\":\r\n\t\tparser = argparse.ArgumentParser()\r\n\t\tparser.add_argument(\r\n\t\t\t\"--address\",\r\n\t\t\trequired=False,\r\n\t\t\ttype=str,\r\n\t\t\thelp=\"the address to use for Ray\")\r\n\t\tparser.add_argument(\r\n\t\t\t\"--num-replicas\",\r\n\t\t\t\"-n\",\r\n\t\t\ttype=int,\r\n\t\t\tdefault=1,\r\n\t\t\thelp=\"Sets number of replicas for training.\")\r\n\t\tparser.add_argument(\r\n\t\t\t\"--use-gpu\",\r\n\t\t\taction=\"store_true\",\r\n\t\t\tdefault=False,\r\n\t\t\thelp=\"Enables GPU training\")\r\n\t\tparser.add_argument(\r\n\t\t\t\"--tune\", action=\"store_true\", default=False, help=\"Tune training\")\r\n\t\tparser.add_argument(\r\n\t\t\t\"--num_cpu\",\r\n\t\t\t\"-ncpus\",\r\n\t\t\ttype=int,\r\n\t\t\tdefault=1,\r\n\t\t\thelp=\"Sets number of cpus for training.\")\r\n\t\tparser.add_argument(\r\n\t\t\t\"--batch_size\",\r\n\t\t\t\"-batch_size\",\r\n\t\t\ttype=int,\r\n\t\t\tdefault=128,\r\n\t\t\thelp=\"Sets number of cpus for training.\")\r\n\r\n\r\n\t\targs, _ = parser.parse_known_args()\r\n\r\n\t\tray.init(address=args.address)#num_cpus=args.num_cpu)\r\n               train_example(num_replicas=args.num_replicas, use_gpu=args.use_gpu, \r\n                  batch_size=args.batch_size)\r\n\r\n\r\n\r\n`"
      },
      {
        "user": "richardliaw",
        "created_at": "2020-06-21T22:17:58Z",
        "body": "Hm, is it possible that you're requesting more resources than you have on the cluster?"
      },
      {
        "user": "panditamey1",
        "created_at": "2020-06-23T11:26:28Z",
        "body": "> Hm, is it possible that you're requesting more resources than you have on the cluster?\r\n\r\nIf I am requesting more resources then ray gives different warning and stops there. I don't have screenshot of that right now.\r\n\r\nI guess this issue was something to do with the firewall. I have cleared this issue.\r\nThanks"
      }
    ]
  },
  {
    "number": 8680,
    "title": "How to check if rllib using tensorflow gpu?",
    "created_at": "2020-05-30T01:51:01Z",
    "closed_at": "2020-06-02T03:56:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8680",
    "body": "Hi, Is it possible to check whether rllib using tensorflow gpu or not? I try to run ray tune with PPO Algorithm and num_gpus = 1, but It only using 1GB /16GB of my GPU. Is this because the rllib using tensorflow CPU instead of GPU?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8680/comments",
    "author": "Nicholaz99",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-05-30T02:13:53Z",
        "body": "cc @mfitton "
      },
      {
        "user": "ericl",
        "created_at": "2020-05-30T07:55:25Z",
        "body": "RLlib will raise an error if GPU support isn't working actually. The reason it's only using 1GB is since your model/batch size is small. For RL, it's very uncommon to use lots of GPU memory since most models are quite small."
      }
    ]
  },
  {
    "number": 8669,
    "title": "[rllib] Cooperative Multi-Agent env - best way to train based on summed rewards across all agents? ",
    "created_at": "2020-05-29T00:37:14Z",
    "closed_at": "2020-06-18T11:57:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8669",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n1. I'm considering converting my centralized model which governs multiple moving entities into a multi-agent model where the model learned based on the summed rewards of all agents. While looking through the various examples on the repo, I encountered an example for summing rewards per agent across timesteps using a specified \"postprocessing_fn\", as well as a centralized critic model that trained over opponent actions in combination with self rewards. However, I did not come across an example of a system that trained over summed rewards across multiple agents - could you point me to an example that does this or show me how I could accomplish this using a centralized value function or centralized loss function? \r\n\r\n2. A little broader of a question - My application informs the movements of agents to accomplish a certain condition within their environment; it aims to be resilient to the case of agents becoming unresponsive, still fulfilling the aforementioned condition using the reduced agent count. Is there a means of training my model on a multi-agent environment without specifying a specific amount of participating agents? Or is the best way to produce several trained models, each corresponding to specific agent counts? \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8669/comments",
    "author": "rbala19",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-05-29T02:44:06Z",
        "body": "> trained over summed rewards across multiple agents\r\n\r\nI think the best way is to do this in the environment, by doing the summing explicitly and then giving the same summed reward to each agent.\r\n\r\n> specifying a specific amount of participating agents\r\n\r\nAbsolutely, in RLlib the number of agents can vary over time and agents can act at different timescales. The only requirement is the number of policies is fixed. If an agent isn't present in the obs dict at a certain time step, no action will be computed for it.\r\n\r\nHope this helps!"
      }
    ]
  },
  {
    "number": 8652,
    "title": "How to access true_obs_shape in custom model?",
    "created_at": "2020-05-28T07:56:59Z",
    "closed_at": "2020-05-28T19:31:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8652",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8652/comments",
    "author": "forhonourlx",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-05-28T19:31:35Z",
        "body": "Duplicate"
      }
    ]
  },
  {
    "number": 8622,
    "title": "Restoring from checkpoint on different machine",
    "created_at": "2020-05-26T17:23:53Z",
    "closed_at": "2020-05-26T19:35:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8622",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI trained a model on one machine and I am able to load and restore properly from a saved checkpoint on that machine. When I copied over the checkpoint and try to restore and execute on a different machine, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_rllib_model.py\", line 77, in <module>\r\n    test_agent.restore(checkpoint_path)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/tune/trainable.py\", line 417, in restore\r\n    self._restore(checkpoint_path)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 639, in _restore\r\n    self.__setstate__(extra_data)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 192, in __setstate__\r\n    Trainer.__setstate__(self, state)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 1070, in __setstate__\r\n    self.optimizer.restore(state[\"optimizer\"])\r\nAttributeError: 'NoneType' object has no attribute 'restore'\r\n```\r\nAm I missing some files or something? I've copied over all the files in the checkpoint directory and also the `params.pkl` and `params.json` file. \r\nI'm creating a trainer instance and restoring from the checkpoint. \r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8622/comments",
    "author": "jangkj09",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-05-26T18:14:29Z",
        "body": "Maybe the ray versions are different on the two machines? Could you post your specs?"
      },
      {
        "user": "jangkj09",
        "created_at": "2020-05-26T19:35:30Z",
        "body": "Yes, that solved the problem. On one machine I had the stable 0.8.5 and on the other I had 0.9.0.dev0\r\nRe-installing with 0.8.5 resolved the problem. \r\n\r\nI don't know if this exists somewhere in the docs, but it would be helpful to indicate this difference more explicitly. I spent over an hour trying to debug what was going on. Thanks!"
      }
    ]
  },
  {
    "number": 8557,
    "title": "Use an external redis server for ray",
    "created_at": "2020-05-22T17:07:25Z",
    "closed_at": "2020-05-23T09:55:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8557",
    "body": "Hi,\r\nI'm trying to use an external redis server that's accessible by all machines in my cluster. So, I'd expect, something like \r\n`$ ray start --head --address='redis.internal.net:6379'` to work, but instead I get,\r\n\r\n`Exception: If --head is passed in, a Redis server will be started, so a Redis address should not be provided.`\r\n\r\nAny way I can make this work? \r\nThanks",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8557/comments",
    "author": "avinayak",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T21:20:26Z",
        "body": "The master shard of Redis should be started in the head node for now unfortunately. \r\n\r\nHowever, we are currently in active progress supporting standalone and pluggable GCS (Redis is GCS in your case), so that you can use standalone Redis or other storages like SQL as GCS."
      }
    ]
  },
  {
    "number": 8546,
    "title": "[Ray]libgomp: Thread creation failed: Resource temporarily unavailable",
    "created_at": "2020-05-22T06:21:07Z",
    "closed_at": "2020-06-05T08:33:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8546",
    "body": "ray==0.8.4 \r\n\r\n\r\nwhen I run ray to train, it always appeared the issue of:\r\n\r\nlibgomp: Thread creation failed: Resource temporarily unavailable\r\n(pid=116340)\r\n(pid=116340) libgomp: Thread creation failed: Resource temporarily unavailable\r\nE0522 14:15:23.659037 116590 task_manager.cc:236] 3 retries left for task ffffffffffffffff7dec85640100, attempting to resubmit.\r\n2020-05-22 14:15:23,659 WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffff7dec85640100.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8546/comments",
    "author": "csdingbin",
    "comments": [
      {
        "user": "SunLightFor6",
        "created_at": "2020-08-12T13:23:34Z",
        "body": "I noticed the pid here is very large, so I suggest you check whether the cpu core can support such a high load or not. Maybe `ulimit -u` can help."
      }
    ]
  },
  {
    "number": 8545,
    "title": "[ray] Is it bad practice to use sockets (pyzmq) to communicate between ray remote functions?",
    "created_at": "2020-05-22T06:17:38Z",
    "closed_at": "2020-05-27T15:03:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8545",
    "body": "I have a `send()` function that generates random numpy arrays at every time step, and a `recv()` function that receives and prints those generated arrays. I am using `zmq` for sending/receiving the numpy arrays across the processes, and `pyarrow` to serialize and deserialize arrays. I wasn't able to find any examples using ray and zmq together, so I would like to know whether this is bad practice. If so, is there a recommended way to have the distributed-ly running processes communicate with each other using ray?\r\n\r\nThank you so much! \r\n\r\nPasted below is minimal working code (on Ubuntu 18.0.4, python=3.6.9, pyzmq=19.0.1, ray=0.8.5, pyarrow=0.17.1):\r\n\r\n```python\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport ray\r\nimport zmq\r\nray.init()\r\n\r\n\r\n@ray.remote\r\ndef send():\r\n    port = 5556\r\n    context = zmq.Context()\r\n    send_socket = context.socket(zmq.PUSH)\r\n    send_socket.bind(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        msg = np.random.rand(1, 3) # this could be larger, e.g. numpy-ed torch neural network weights\r\n        object_id = pa.serialize(msg).to_buffer()\r\n        send_socket.send(object_id)\r\n\r\n@ray.remote\r\ndef recv():        \r\n    port = 5556\r\n    context = zmq.Context()\r\n    recv_socket = context.socket(zmq.PULL)\r\n    recv_socket.connect(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        object_id = recv_socket.recv()\r\n        msg = pa.deserialize(object_id)\r\n        print(msg)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ray.wait([send.remote(), recv.remote()])\r\n```\r\n## Note:\r\nI had to use pyarrow for serialization since ray object id's (obtained via `ray.put()`) could not be passed through zmq sockets; doing so gives the error below:  \r\n```\r\nObjectID(45b95b1c8bd3a9c4ffffffff0100008801000000) does not provide a buffer interface.\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8545/comments",
    "author": "cyoon1729",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T06:33:20Z",
        "body": "Ray already handles inter-process communication as well as serialization using apache arrow. You can just do.\r\n\r\n```python3\r\nimport ray\r\nray.init()\r\n\r\n@ray.remote\r\nclass ReceiveServer:\r\n    def recv(self, msg):\r\n        print(msg)\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\nrecv_server_handle = ReceiveServer.remote()\r\nray.wait(send.remote(recv_server_handle))\r\n```\r\nThis should do the same thing."
      },
      {
        "user": "cyoon1729",
        "created_at": "2020-05-22T07:35:22Z",
        "body": "@rkooo567 Thank you so much for your response and the example above. I would like to ask another question:\r\n \r\nSay, for instance, I have the `ReceiveServer` above to store the `msg` in an internal storage `self.storage (deque)` when `recv()` is called in `send()`, while continuously (as in a `while: True` loop) sampling data from `self.storage` and processing it in another member function `process()`.\r\n\r\nIf I were to run `process.remote()` asynchronously with respect to `send()`, would a mutual exclusion of `ReceiveSercer.storage` be enforced? Is this legal? \r\n\r\nThe code below implements what I tried to describe, but does not print anything:\r\n```python\r\nimport ray\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\n\r\n@ray.remote \r\nclass ReceiveServer:\r\n    def __init__(self):\r\n        self.storage = deque(maxlen=2000)\r\n\r\n    def recv(self, data):\r\n        self.storage.append(data)\r\n        \r\n    def process(self):\r\n        while True:\r\n            if len(self.storage) > 0:\r\n                data = random.sample(self.buffer, 1)\r\n                \r\n                # do something to data\r\n                # ...\r\n\r\n                print(data)  # does not print anything \r\n\r\nreceive_server = ReceiveServer.remote()\r\nray.wait([send.remote(receive_server), receive_server.process.remote()])\r\n```\r\nIf it is indeed acceptable to use ray, pyarrow, and zmq together as in the first example, I would like to proceed with that. Are there any glaring issues with doing so? In particular, ray will be used purely as an alternative to python multiprocessing. \r\n\r\nThank you so much again for your time.\r\n"
      },
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T21:17:35Z",
        "body": "It doesn't print anything because Actor (class with @ray.remote) is running in a single process, and `recv` will never run because `process` is occupying the process (because it is running a while loop). \r\n\r\nmutual exclusion of ReceiveSercer.storage be enforced? Is this legal?: Yes. Ray handles this issue and you never need to worry about locking. \r\n\r\nThere's nothing wrong with using zmq and pyarrow if you have the right reason. It is just not efficient because what you try to achieve using zmq and pyarrow is what Ray exists for. Ray is a distributed computing framework that abstracts inter-process communication problems (and many others).    \r\n\r\nYou can make this work in this way.  \r\n```python3\r\nimport ray\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\nimport asyncio\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\n\r\n@ray.remote \r\nclass ReceiveServer:\r\n    def __init__(self):\r\n        self.storage = deque(maxlen=2000)\r\n\r\n    async def recv(self, data):\r\n        self.storage.append(data)\r\n        \r\n    async def process(self):\r\n        while True:\r\n            await asyncio.sleep(0.0)\r\n            if len(self.storage) > 0:\r\n                data = random.sample(self.buffer, 1)\r\n                \r\n                # do something to data\r\n                # ...\r\n\r\n                print(data)  # does not print anything \r\n\r\nreceive_server = ReceiveServer.remote()\r\nray.wait([send.remote(receive_server), receive_server.process.remote()])\r\n```"
      },
      {
        "user": "cyoon1729",
        "created_at": "2020-05-27T15:03:37Z",
        "body": "Thanks @rkooo567! This was very helpful. "
      },
      {
        "user": "uchiiii",
        "created_at": "2023-08-07T15:14:51Z",
        "body": "I am very new to ray-project and have a question regarding this.\r\n\r\nRay supports inter-process communication as suggested above. What kind of protocol is used under the hood, `zmq` or anything else? Or it shares data using object storage like Plasma? \r\n\r\nThank you for you reply in advance! "
      }
    ]
  },
  {
    "number": 8482,
    "title": "import error",
    "created_at": "2020-05-18T11:41:30Z",
    "closed_at": "2020-05-22T01:21:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8482",
    "body": "ray 0.8.5,when I try to use ray， it occurs  Ray must be imported before pickle5 because Ray requires a specific version of pickle5 (which is packaged along with Ray.\r\n\r\nI want to know it must import  pickle5 before import ray,  Right?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8482/comments",
    "author": "zzchuman",
    "comments": [
      {
        "user": "suquark",
        "created_at": "2020-05-18T21:00:49Z",
        "body": "yes, we are using an internal version of pickle5, because the current pickle5 has memory leak issues. so we have to import ray before importing the pickle5 library.\r\n\r\nI have created a PR in the upstream (pickle5-backport), it would address the problem once merged."
      }
    ]
  },
  {
    "number": 8458,
    "title": "[rllib] [tune] Iteration vs. Episode explanation",
    "created_at": "2020-05-15T15:10:01Z",
    "closed_at": "2020-05-18T21:09:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8458",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI have a simple question, the answer for which that I could not find in the documentation. I am not familiar with using iterations in reinforcement learning, as I've previously seen that the train step ends when the episode is done. In the rllib case, it looks like the train step continues until the iteration is done? I am not clear on how iterations are used. \r\n\r\nAlso, when is the network updated; after an episode or after an iteration?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8458/comments",
    "author": "Leonolovich",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-05-17T21:11:19Z",
        "body": "Iterations are just for metrics reporting. They have no connection with episodes; many episodes are executing concurrently and may span one or more iterations. Similarly, there is no connection with SGD updates, those are executed over batches of `train_batch_size` depending on the algorithm.\r\n\r\nYou can check `num_steps_sampled`, `episodes_total`, `num_steps_trained` metrics to see what is happening."
      },
      {
        "user": "armando-fandango",
        "created_at": "2021-04-30T14:22:42Z",
        "body": "I tried running only for 30 episodes by using this \"stop={\"episodes_total\": 30}\" but it ran for 40 episodes. I tried changing it to 10 and it ran for 20 episodes. How to control for how many episodes the tne.run() should execute? How to set number of iterastions?"
      },
      {
        "user": "Morphlng",
        "created_at": "2023-11-19T02:16:40Z",
        "body": "> I tried running only for 30 episodes by using this \"stop={\"episodes_total\": 30}\" but it ran for 40 episodes. I tried changing it to 10 and it ran for 20 episodes. How to control for how many episodes the tne.run() should execute? How to set number of iterastions?\r\n\r\nThe extra episodes may due to evaluation? You can try setting `evaluation_interval=0` to disable it."
      }
    ]
  },
  {
    "number": 8440,
    "title": "[rllib] Set log_dir to something other than ~/ray_results?",
    "created_at": "2020-05-14T04:32:44Z",
    "closed_at": "2020-05-14T16:46:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8440",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nThis is a simple question, but somehow I am unable to find a straightforward answer, is there a way to change the default log_dir from ~/ray_results to something else?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8440/comments",
    "author": "jangkj09",
    "comments": [
      {
        "user": "stefanbschneider",
        "created_at": "2020-06-19T09:34:45Z",
        "body": "What was the answer?"
      },
      {
        "user": "mimoralea",
        "created_at": "2021-06-27T13:32:25Z",
        "body": "I realize this is a pretty old question, but for those looking for the answer `local_dir` is what you want:\r\n\r\n```\r\ntune.run(trainable, num_samples=2, local_dir=\"./results\", name=\"test_experiment\")\r\n```"
      }
    ]
  },
  {
    "number": 8413,
    "title": "[sgd] Can TorchTrainer print out something every one or several iterations?",
    "created_at": "2020-05-12T08:36:16Z",
    "closed_at": "2020-06-11T20:23:53Z",
    "labels": [
      "question",
      "sgd"
    ],
    "url": "https://github.com/ray-project/ray/issues/8413",
    "body": "Seems by default TorchTrainer only returns stats after train() finishes? During the training, is there a way I get some information (for example loss values, or just something to indicate the training is happening in the background?) for each iteration or every several iterations?\r\nOtherwise if one epoch training takes a lot of time, then I probably don't know what's going on. I may doubt whether the program crashes indeed or the training is just long.\r\n\r\n@richardliaw \r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8413/comments",
    "author": "hkvision",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-05-14T08:36:26Z",
        "body": "You can specify `num_steps`, which allows you to short-circuit the training. Also, `use_tqdm=True` is usually what I use."
      },
      {
        "user": "hkvision",
        "created_at": "2020-06-09T12:35:33Z",
        "body": "> You can specify `num_steps`, which allows you to short-circuit the training. Also, `use_tqdm=True` is usually what I use.\r\n\r\nThank you so much! @richardliaw Sorry for the late reply. `use_tqdm` works great!\r\nIf I specify `num_steps`, then every call for `train` only trains several batches, and would it be the case that some data won't get trained?"
      },
      {
        "user": "richardliaw",
        "created_at": "2020-06-11T20:23:53Z",
        "body": "Ah yes; there's a workaround but we should push this. I'll make a new PR."
      }
    ]
  },
  {
    "number": 8383,
    "title": "Tune error for PyTorch model",
    "created_at": "2020-05-09T09:38:14Z",
    "closed_at": "2020-06-11T20:23:00Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/8383",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### Error trying to use tube for hyperparameter tuning for a PyTorch model?\r\n\r\n*Python version: 3.7.0, Torch version: 1.1.0.post2, Ray version 0.8.5, MacOS :*\r\n \r\nHi all,\r\n\r\nIt's the first time I am trying to use Tune for my PyTorch model. My model itself is running OK however when I tried to use Tune I got the following: \r\n\r\n`    def train_model(N_EPOCHS):\r\n    \r\n\r\n    for epoch in range(N_EPOCHS):\r\n\r\n        start_time = time.time()\r\n        optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"]) \r\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\r\n        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n    \r\n        end_time = time.time()\r\n\r\n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n        tune.track.log(mean_accuracy=train_acc)\r\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\r\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')`\r\n\r\n`analysis = tune.run(\r\n    train_model,\r\n    config={\"lr\": tune.grid_search([0.0005, 0.01])})`\r\n\r\n The Error I got\r\n\r\n\r\n`2020-05-09 11:48:33,921\tERROR logger.py:196 -- pip install 'ray[tune]' to see TensorBoard files.\r\n2020-05-09 11:48:33,924\tWARNING logger.py:328 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\r\n2020-05-09 11:48:34,395\tWARNING worker.py:1090 -- Warning: The actor WrappedTrackFunc has size 17993759 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\r\n2020-05-09 11:48:35,946\tERROR trial_runner.py:519 -- Trial train_model_00001: Error processing event.\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 467, in _process_trial\r\n    result = self.trial_executor.fetch_result(trial)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 431, in fetch_result\r\n    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/worker.py\", line 1515, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(TuneError): ray::WrappedTrackFunc.train() (pid=1591, ip=192.168.1.5)\r\n  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 417, in ray._raylet.execute_task.function_executor\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\r\n    result = self._train()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 216, in _train\r\n    self._report_thread_runner_error(block=True)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 259, in _report_thread_runner_error\r\n    .format(err_tb_str)))\r\nray.tune.error.TuneError: Trial raised an exception. Traceback:\r\nray::WrappedTrackFunc.train() (pid=1591, ip=192.168.1.5)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 110, in run\r\n    self._entrypoint()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 162, in entrypoint\r\n    return self._trainable_func(config, self._status_reporter)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 288, in _trainable_func\r\n    output = train_func(config)\r\n  File \"<ipython-input-56-b9344418465c>\", line 4, in train_model\r\nTypeError: 'dict' object cannot be interpreted as an integer\r\n2020-05-09 11:48:35,959\tERROR trial_runner.py:519 -- Trial train_model_00000: Error processing event.\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 467, in _process_trial\r\n    result = self.trial_executor.fetch_result(trial)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 431, in fetch_result\r\n    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/worker.py\", line 1515, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(TuneError): ray::WrappedTrackFunc.train() (pid=1593, ip=192.168.1.5)\r\n  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 417, in ray._raylet.execute_task.function_executor\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\r\n    result = self._train()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 216, in _train\r\n    self._report_thread_runner_error(block=True)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 259, in _report_thread_runner_error\r\n    .format(err_tb_str)))\r\nray.tune.error.TuneError: Trial raised an exception. Traceback:\r\nray::WrappedTrackFunc.train() (pid=1593, ip=192.168.1.5)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 110, in run\r\n    self._entrypoint()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 162, in entrypoint\r\n    return self._trainable_func(config, self._status_reporter)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 288, in _trainable_func\r\n    output = train_func(config)\r\n  File \"<ipython-input-56-b9344418465c>\", line 4, in train_model\r\nTypeError: 'dict' object cannot be interpreted as an integer`",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8383/comments",
    "author": "papilloncode",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-05-09T18:39:40Z",
        "body": "I think the typo is in the first line of the code:\r\n\r\n```\r\n###################\r\ndef train_model(config):  # Change this to config from N_EPOCHS.\r\n###################\r\n\tfor epoch in range(N_EPOCHS):\r\n\t\r\n\t    start_time = time.time()\r\n\t    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"]) \r\n\t    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\r\n\t    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n\t\r\n\t    end_time = time.time()\r\n\t\r\n\t    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n\t    tune.track.log(mean_accuracy=train_acc)\r\n\t    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\r\n\t    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n\t    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')`"
      }
    ]
  },
  {
    "number": 8091,
    "title": "[rllib] What's the default optimizer for PPO?",
    "created_at": "2020-04-19T23:28:22Z",
    "closed_at": "2020-04-20T06:02:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8091",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What's the default optimizer for PPO and How to change it?\r\nHi,  I'm new to Ray/rllib. I'm trying to run PPO algorithms, but didn't find the way to set optimizer type such as adam or rmsprop after checking the common config and default ppo config. Could someone please help to point out where it is set and how to change it?\r\n\r\nI'm now using Ray 0.8.4. Thanks. \r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8091/comments",
    "author": "KarlXing",
    "comments": [
      {
        "user": "skkuai",
        "created_at": "2022-03-04T08:45:49Z",
        "body": "I think I found it. The default optimizer is Adam.\r\n\r\nray > rllib > policy > torch_policy.py  line 786~\r\n\r\n```python\r\n    @DeveloperAPI\r\n    def optimizer(\r\n            self\r\n    ) -> Union[List[\"torch.optim.Optimizer\"], \"torch.optim.Optimizer\"]:\r\n        \"\"\"Custom the local PyTorch optimizer(s) to use.\r\n\r\n        Returns:\r\n            The local PyTorch optimizer(s) to use for this Policy.\r\n        \"\"\"\r\n        if hasattr(self, \"config\"):\r\n            optimizers = [\r\n                torch.optim.Adam(\r\n                    self.model.parameters(), lr=self.config[\"lr\"])\r\n            ]\r\n        else:\r\n            optimizers = [torch.optim.Adam(self.model.parameters())]\r\n        if getattr(self, \"exploration\", None):\r\n            optimizers = self.exploration.get_exploration_optimizer(optimizers)\r\n        return optimizers\r\n```"
      }
    ]
  },
  {
    "number": 8042,
    "title": "Ray on Windows",
    "created_at": "2020-04-16T08:59:20Z",
    "closed_at": "2020-04-16T23:18:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8042",
    "body": "Hi there,\r\n\r\nI am struggling to install Ray on Windows with python 3.6.7. Pip does not seem to be able to find a version that fits. And I can't locate any unofficial wheels. \r\n\r\nSo my question is - will it be possible to support Windows in a future release?\r\n\r\nThanks in advance\r\n\r\nFig ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8042/comments",
    "author": "lefig",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-04-16T23:16:51Z",
        "body": "cc @mehrdadn "
      },
      {
        "user": "mehrdadn",
        "created_at": "2020-04-16T23:18:00Z",
        "body": "Windows support is still under development. Feel free to follow #631 for updates!"
      },
      {
        "user": "mehrdadn",
        "created_at": "2020-06-24T20:18:20Z",
        "body": "Please see [#631](/ray-project/ray/issues/631#issuecomment-648980254) for recent updates on Windows support!"
      }
    ]
  },
  {
    "number": 7933,
    "title": "Ray in Slurm deployment error",
    "created_at": "2020-04-08T10:25:18Z",
    "closed_at": "2020-06-19T04:52:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7933",
    "body": "Hi guys, \r\n\r\nI've been trying to run Rllib in slurm with multiple nodes, as shown below, I can init the head and\r\nthe workers. \r\n\r\nHowever the program seems to fail immediately after that, caused by this error:\r\n\r\n*File \"PROGRAM/lib/python3.6/socket.py\", line 745, in getaddrinfo\r\nfor res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nsocket.gaierror: [Errno -2] Name or service not known*\r\n\r\nI've tried as the suggestions in #6573 but haven't got luck to work it out.\r\n\r\nI'm using  ray 0.7.3 and the slurm nodes are running on Centos 7.7.\r\n\r\nAny advice would be appreciated.\r\nThanks.\r\n\r\n<pre>\r\nINFO scripts.py:289 -- Using IP address 128.214.XX.XX for this node.\r\nINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-06_09-25-40_049064_8595/logs.\r\nINFO services.py:409 -- Waiting for redis server at 128.214.XX.XX:6379 to respond...\r\nINFO services.py:409 -- Waiting for redis server at 127.0.0.1:6379 to respond...\r\nINFO services.py:409 -- Waiting for redis server at 127.0.0.1:48948 to respond...\r\nINFO services.py:809 -- Starting Redis shard with 10.0 GB max memory.\r\nINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-06_09-25-40_049064_8595/logs.\r\n2020-04-06 09:25:40,964\tWARNING services.py:1301 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\r\nINFO services.py:1476 -- Starting the Plasma object store with 54.04 GB memory using /dev/shm.\r\nINFO scripts.py:319 -- \r\nStarted Ray on this node. You can add additional nodes to the cluster by calling\r\n\r\nray start --redis-address 128.214.XX.XX:6379 --redis-password XXXXXXXXXXXXXXXXXXXXXXXx\r\n\r\nfrom the node you wish to add. You can connect a driver to the cluster from Python by running\r\n\r\nimport ray\r\nray.init(redis_address=\"128.214.XX.XX:6379\", redis_password=\"XXXXXXXXXXXXXXXXXXXXXXXx\")\r\n\r\nIf you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run\r\n\r\nray stop\r\nINFO services.py:414 -- Failed to connect to the redis server, retrying.\r\nINFO services.py:409 -- Waiting for redis server at 128.214.XX.XX:6379 to respond...\r\nINFO scripts.py:363 -- Using IP address 128.214.XX.199 for this node.\r\nINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-06_09-25-40_049064_8595/logs.\r\n2020-04-06 09:25:41,123\tWARNING services.py:1301 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\r\nINFO services.py:1476 -- Starting the Plasma object store with 54.04 GB memory using /dev/shm.\r\nINFO scripts.py:371 -- \r\nStarted Ray on this node. If you wish to terminate the processes that have been started, run\r\n\r\nray stop\r\nINFO services.py:409 -- Waiting for redis server at 128.214.XX.XX:6379 to respond...\r\nINFO scripts.py:363 -- Using IP address 128.214.XX.200 for this node.\r\nINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-06_09-25-40_049064_8595/logs.\r\n2020-04-06 09:25:42,363\tWARNING services.py:1301 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\r\nINFO services.py:1476 -- Starting the Plasma object store with 54.04 GB memory using /dev/shm.\r\nINFO scripts.py:371 -- \r\nStarted Ray on this node. If you wish to terminate the processes that have been started, run\r\n\r\nray stop\r\n\r\n\r\nskip...\r\n\r\nINFO services.py:409 -- Waiting for redis server at 128.214.XX.XX:6379 to respond...\r\nINFO scripts.py:363 -- Using IP address 128.214.XX.221 for this node.\r\nINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-06_09-25-40_049064_8595/logs.\r\n2020-04-06 09:27:07,525\tWARNING services.py:1301 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\r\nINFO services.py:1476 -- Starting the Plasma object store with 54.04 GB memory using /dev/shm.\r\nINFO scripts.py:371 -- \r\nStarted Ray on this node. If you wish to terminate the processes that have been started, run\r\n\r\nray stop\r\nINFO services.py:409 -- Waiting for redis server at 128.214.XX.XX:6379 to respond...\r\nINFO scripts.py:363 -- Using IP address 128.214.XX.222 for this node.\r\nINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-06_09-25-40_049064_8595/logs.\r\n2020-04-06 09:27:13,269\tWARNING services.py:1301 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\r\nINFO services.py:1476 -- Starting the Plasma object store with 54.04 GB memory using /dev/shm.\r\nINFO scripts.py:371 -- \r\nStarted Ray on this node. If you wish to terminate the processes that have been started, run\r\n\r\nray stop\r\nPROGRAM/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nPROGRAM/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nPROGRAM/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nPROGRAM/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nPROGRAM/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n_np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nPROGRAM/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\nnp_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float3[0m\r\nwarnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\r\nINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-06_09-28-02_453911_9098/logs.\r\nINFO services.py:409 -- Waiting for redis server at 127.0.0.1:57253 to respond...\r\nTraceback (most recent call last):\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 484, in connect\r\nsock = self._connect()\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 511, in _connect\r\nsocket.SOCK_STREAM):\r\n<b>File \"PROGRAM/lib/python3.6/socket.py\", line 745, in getaddrinfo\r\nfor res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nsocket.gaierror: [Errno -2] Name or service not known</b>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/client.py\", line 667, in execute_command\r\nconnection.send_command(*args)\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 610, in send_command\r\nself.send_packed_command(self.pack_command(*args))\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 585, in send_packed_command\r\nself.connect()\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 489, in connect\r\nraise ConnectionError(self._error_message(e))\r\nredis.exceptions.ConnectionError: Error -2 connecting to 128.214.XX.XX:6379:57253. Name or service not known.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 484, in connect\r\nsock = self._connect()\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 511, in _connect\r\nsocket.SOCK_STREAM):\r\nFile \"PROGRAM/lib/python3.6/socket.py\", line 745, in getaddrinfo\r\nfor res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nsocket.gaierror: [Errno -2] Name or service not known\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"train.py\", line 282, in <module>\r\nray.init(num_cpus= num_cpus, node_ip_address=os.environ[\"ip_head\"], object_store_memory=int(9.5**11))\r\nFile \"PROGRAM/lib/python3.6/site-packages/ray/worker.py\", line 1429, in init\r\nhead=True, shutdown_at_exit=False, ray_params=ray_params)\r\nFile \"PROGRAM/lib/python3.6/site-packages/ray/node.py\", line 143, in __init__\r\nself.start_head_processes()\r\nFile \"PROGRAM/lib/python3.6/site-packages/ray/node.py\", line 501, in start_head_processes\r\nself.start_redis()\r\nFile \"PROGRAM/lib/python3.6/site-packages/ray/node.py\", line 354, in start_redis\r\nredis_max_memory=self._ray_params.redis_max_memory)\r\nFile \"PROGRAM/lib/python3.6/site-packages/ray/services.py\", line 607, in start_redis\r\nprimary_redis_client.set(\"NumRedisShards\", str(num_redis_shards))\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/client.py\", line 1171, in set\r\nreturn self.execute_command('SET', *pieces)\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/client.py\", line 673, in execute_command\r\nconnection.send_command(*args)\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 610, in send_command\r\nself.send_packed_command(self.pack_command(*args))\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 585, in send_packed_command\r\nself.connect()\r\nFile \"PROGRAM/lib/python3.6/site-packages/redis/connection.py\", line 489, in connect\r\nraise ConnectionError(self._error_message(e))\r\nredis.exceptions.ConnectionError: Error -2 connecting to 128.214.XX.XX:6379:57253. Name or service not known.\r\n</pre>",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7933/comments",
    "author": "pengyuan-zhou",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-04-09T00:42:55Z",
        "body": "Can you show us logs in a directory, `/tmp/ray/session_2020-04-06_09-28-02_453911_9098/logs/`? It'll be great if you can show us all of them except workers."
      },
      {
        "user": "pengyuan-zhou",
        "created_at": "2020-04-09T08:51:13Z",
        "body": "> Can you show us logs in a directory, `/tmp/ray/session_2020-04-06_09-28-02_453911_9098/logs/`? It'll be great if you can show us all of them except workers.\r\n\r\nHi, unfortunately those files don't exist anymore, could be auto-removed by the system, I'm not sure. \r\n\r\nIs there a way to manually config & change the tmp path for logs? \r\nThanks."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-04-09T19:45:35Z",
        "body": "Yeah, there should be a `--temp-dir` or `temp_dir` argument you can set."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-04-09T19:45:56Z",
        "body": "Can you try upgrading Ray to the latest version?"
      },
      {
        "user": "pengyuan-zhou",
        "created_at": "2020-04-11T16:00:50Z",
        "body": "> Can you try upgrading Ray to the latest version?\r\n\r\nUnfortunately that seems not doable recently, since the program I'm running is dependent on that version of ray"
      },
      {
        "user": "pengyuan-zhou",
        "created_at": "2020-04-14T20:38:09Z",
        "body": "> Can you show us logs in a directory, `/tmp/ray/session_2020-04-06_09-28-02_453911_9098/logs/`? It'll be great if you can show us all of them except workers.\r\n\r\nHi, I've changed the log dir as recommended by @richardliaw , the redis.err is iteratively repeating the following error:\r\n```\r\n*** FATAL CONFIG FILE ERROR ***\r\nReading the configuration file, at line 2\r\n>>> 'requirepass'\r\nBad directive or wrong number of arguments\r\n```\r\nOther files are empty."
      }
    ]
  },
  {
    "number": 7912,
    "title": "Details about the hyperparameter in PPO Algorithm?",
    "created_at": "2020-04-06T14:27:08Z",
    "closed_at": "2020-04-06T14:37:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7912",
    "body": "Hi, so I want to tune my hyperparameter for the PPO Algorithm but I've found difficulties when reading the docs about the configs, so I guess I want to ask you guys in here about:\r\n1. What is the value of `lr_schedule` in the PPO Algorithm? Suppose that my starting learning_rate is `'lr': 1e-4` and I want to decay its value to 0 when I train.\r\n2. Is it possible to set the hidden layer size in the PPO algorithm? If yes, what is the corresponding config as I didn't find it in the documentation (I found this kind of config in the SAC algorithm documentation but not in PPO).\r\n\r\nThank you very much guys! I really appreciate your help 😄 ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7912/comments",
    "author": "Nicholaz99",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-04-06T14:36:29Z",
        "body": "Yeah, sorry, it's not clearly documented. Here are the answers. We'll add this to the docs.\r\n1) You are basically configuring a PiecewiseSchedule.\r\nSo lr_schedule: [[0, 0.01], [1000, 0.0005]] means that you decay from ts=0 (lr=0.01) linearly to ts=1000 (lr=0.0005). After 1000ts your learning rate will stay at 0.0005. The config key \"lr\" is ignored in this setting.\r\n2) You can do e.g. config[\"model\"][\"fcnet_hiddens\"] = [16, 32, 64]. Change the activation by using config[\"model\"][\"fcnet_activation\"] (\"tanh\", \"relu\", or \"linear\")."
      },
      {
        "user": "Nicholaz99",
        "created_at": "2020-04-06T14:47:52Z",
        "body": "Thank you so\r\n\r\n> Yeah, sorry, it's not clearly documented. Here are the answers. We'll add this to the docs.\r\n> \r\n> 1. You are basically configuring a PiecewiseSchedule.\r\n>    So lr_schedule: [[0, 0.01], [1000, 0.0005]] means that you decay from ts=0 (lr=0.01) linearly to ts=1000 (lr=0.0005). After 1000ts your learning rate will stay at 0.0005. The config key \"lr\" is ignored in this setting.\r\n> 2. You can do e.g. config[\"model\"][\"fcnet_hiddens\"] = [16, 32, 64]. Change the activation by using config[\"model\"][\"fcnet_activation\"] (\"tanh\", \"relu\", or \"linear\").\r\n\r\nThank you so much for your help!!! It helps me a lot for my project 😄 "
      }
    ]
  },
  {
    "number": 7849,
    "title": "[rllib] Unable to configure exploration parameters in PPO: Unknown config parameter `explore`",
    "created_at": "2020-04-01T09:25:26Z",
    "closed_at": "2020-05-01T08:08:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7849",
    "body": "Hello,\r\n\r\nI am unable to configure exploration parameters when trying to create a PPO trainer. Dictionary entries \"explore\" and \"exploration_config\" is said to be unknown. Below are the relevant trainer definition and the traceback.\r\n\r\n`trainer = PPOTrainer(\r\n                      env=env_title,\r\n                      config={\r\n                          \r\n                        \"explore\": True,\r\n                        \"exploration_config\": {\r\n                          \"type\": \"EpsilonGreedy\",\r\n                          # Parameters for the Exploration class' constructor:\r\n                          # \"initial_epsilon\"=1.0,  # default is 1.0\r\n                          # \"final_epsilon\"=0.05,  # default is 0.05\r\n                          \"epsilon_timesteps\": max_steps,  # Timesteps over which to anneal epsilon, defult is int(1e5).\r\n                        },\r\n\r\n\r\n                        \"num_workers\": 5,\r\n                        \"num_gpus\": 2,\r\n                        \"model\": nw_model,\r\n                        \"multiagent\": {\r\n                          \"policy_graphs\": policy_graphs,\r\n                          \"policy_mapping_fn\": policy_mapping_fn,\r\n                          \"policies_to_train\": [\"ppo_policy{}\".format(i) for i in range(n_agents)],\r\n                        },\r\n                        \"callbacks\": {\r\n                          \"on_episode_start\": tune.function(on_episode_start),\r\n                          \"on_episode_step\": tune.function(on_episode_step),\r\n                          \"on_episode_end\": tune.function(on_episode_end),\r\n                        },\r\n                        \"log_level\": \"ERROR\",\r\n                      })`\r\n\r\n\r\nFull traceback:\r\n\r\n`Exception                                 Traceback (most recent call last)\r\n<ipython-input-9-252111d46b85> in <module>()\r\n    121                           \"on_episode_end\": tune.function(on_episode_end),\r\n    122                         },\r\n--> 123                         \"log_level\": \"ERROR\",\r\n    124                       })\r\n    125 \r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer_template.py in __init__(self, config, env, logger_creator)\r\n     88 \r\n     89         def __init__(self, config=None, env=None, logger_creator=None):\r\n---> 90             Trainer.__init__(self, config, env, logger_creator)\r\n     91 \r\n     92         def _init(self, config, env_creator):\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py in __init__(self, config, env, logger_creator)\r\n    370             logger_creator = default_logger_creator\r\n    371 \r\n--> 372         Trainable.__init__(self, config, logger_creator)\r\n    373 \r\n    374     @classmethod\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py in __init__(self, config, logger_creator)\r\n     94         self._restored = False\r\n     95         start_time = time.time()\r\n---> 96         self._setup(copy.deepcopy(self.config))\r\n     97         setup_time = time.time() - start_time\r\n     98         if setup_time > SETUP_TIME_THRESHOLD:\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py in _setup(self, config)\r\n    476         merged_config = deep_update(merged_config, config,\r\n    477                                     self._allow_unknown_configs,\r\n--> 478                                     self._allow_unknown_subkeys)\r\n    479         self.raw_user_config = config\r\n    480         self.config = merged_config\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/util.py in deep_update(original, new_dict, new_keys_allowed, whitelist)\r\n    158         if k not in original:\r\n    159             if not new_keys_allowed:\r\n--> 160                 raise Exception(\"Unknown config parameter `{}` \".format(k))\r\n    161         if isinstance(original.get(k), dict):\r\n    162             if k in whitelist:\r\n\r\nException: Unknown config parameter `explore` `\r\n\r\n\r\n\r\nI am using Google Colab and Tensorflow 2.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7849/comments",
    "author": "ZekiDorukErden",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-04-01T09:39:56Z",
        "body": "Hi, you are probably on an older version of ray? What's your version number?\r\nFor now, try to remove these two keys (`exploration_config `and `explore`) altogether. You probably should not run PPO with EpsilonGreedy anyways."
      },
      {
        "user": "ZekiDorukErden",
        "created_at": "2020-04-01T09:52:13Z",
        "body": "Thanks for the reply! Apparently I am using version 0.8.0.dev5 (I copied the code block for dependencies in Ray with Google Colab tutorial without changing)."
      },
      {
        "user": "sven1977",
        "created_at": "2020-04-01T09:59:33Z",
        "body": "Ok, cool. So it's working now?"
      },
      {
        "user": "ZekiDorukErden",
        "created_at": "2020-04-01T10:03:00Z",
        "body": "Yes, when I run without the exploration settings, it worked. Thanks for the help!"
      }
    ]
  },
  {
    "number": 7821,
    "title": "Is it possible to use the policy server/client structure with APEX-DQN in [rllib]?",
    "created_at": "2020-03-30T23:27:45Z",
    "closed_at": "2020-05-12T03:24:44Z",
    "labels": [
      "question",
      "P3"
    ],
    "url": "https://github.com/ray-project/ray/issues/7821",
    "body": "I am trying to run the cartpole_server.py example with APEX-DQN. \r\n\r\n`\r\n\r\n    env = \"CartPole-v0\"\r\n    connector_config = {\r\n        # Use the connector server to generate experiences.\r\n        \"input\": (\r\n            lambda ioctx: PolicyServerInput( \\\r\n                ioctx, SERVER_ADDRESS, SERVER_PORT)\r\n        ),\r\n        # Use a single worker process to run the server.\r\n        \"num_workers\": 0,\r\n        # Disable OPE, since the rollouts are coming from online clients.\r\n        \"input_evaluation\": [],\r\n    }\r\n\r\n    if args.run == \"DQN\":\r\n       ...\r\n    elif args.run == \"PPO\":\r\n        ...\r\n\r\n    elif args.run == \"APEX\":\r\n        trainer = ApexTrainer(\r\n            env=env,\r\n            config=dict(\r\n                connector_config\r\n            ))\r\n\r\n    else:\r\n        raise ValueError(\"--run must be DQN or PPO\")\r\n\r\n    checkpoint_path = CHECKPOINT_FILE.format(args.run)\r\n\r\n    # Attempt to restore from checkpoint if possible.\r\n    if os.path.exists(checkpoint_path):\r\n        checkpoint_path = open(checkpoint_path).read()\r\n        print(\"Restoring from checkpoint path\", checkpoint_path)\r\n        trainer.restore(checkpoint_path)\r\n\r\n    # Serving and training loop\r\n    while True:\r\n        print(pretty_print(trainer.train()))\r\n        checkpoint = trainer.save()\r\n        print(\"Last checkpoint\", checkpoint)\r\n        with open(checkpoint_path, \"w\") as f:\r\n            f.write(checkpoint)\r\n`\r\nwhich gives me an assertion error since num_workers is 0. \r\n\r\n`\r\nTraceback (most recent call last):\r\n  File \"cartpole_server.py\", line 88, in <module>\r\n    print(pretty_print(trainer.train()))\r\n  File \"/home/enbilgin/ws/envs/ray083/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 505, in train\r\n    raise e\r\n  File \"/home/enbilgin/ws/envs/ray083/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 491, in train\r\n    result = Trainable.train(self)\r\n  File \"/home/enbilgin/ws/envs/ray083/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\r\n    result = self._train()\r\n  File \"/home/enbilgin/ws/envs/ray083/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 154, in _train\r\n    fetches = self.optimizer.step()\r\n  File \"/home/enbilgin/ws/envs/ray083/lib/python3.7/site-packages/ray/rllib/optimizers/async_replay_optimizer.py\", line 141, in step\r\n    assert len(self.workers.remote_workers()) > 0\r\nAssertionError\r\n`\r\n\r\nIf I increase it, I run into port allocation issues. How can I use APEX-DQN with the policy server structure?\r\n\r\n- RLlib 0.8.3\r\n- Ubuntu 18.04\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7821/comments",
    "author": "ebilgin",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-04-02T18:27:23Z",
        "body": "This is a bug; Ape-X is asserting that it requires at least one worker, which is reasonable when collecting experiences, but not when using the server input source.\r\n\r\nShould be easy to fix, though not trivial since we still need the distributed replay actors. I would consider using normal DQN with changed hyperparams for now."
      },
      {
        "user": "ebilgin",
        "created_at": "2020-04-02T18:33:15Z",
        "body": "> This is a bug; Ape-X is asserting that it requires at least one worker, which is reasonable when collecting experiences, but not when using the server input source.\r\n> \r\n> Should be easy to fix, though not trivial since we still need the distributed replay actors. I would consider using normal DQN with changed hyperparams for now.\r\n\r\nThank you!"
      }
    ]
  },
  {
    "number": 7737,
    "title": "Why actor methods cannot be called directly?",
    "created_at": "2020-03-25T03:57:27Z",
    "closed_at": "2020-03-31T15:54:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7737",
    "body": "When calling a actor method, that is, call the `__call__` method of an `ActorMethod` object. And this method is implemented as raise an `Exception` directly\r\n```\r\nException: Actor methods cannot be called directly. Instead of running 'object.get()', try 'object.get.remote()'\r\n```\r\n\r\nBut why is it necessary? Why it can't be\r\n\r\n```python\r\nclass ActorMethod:\r\n    ...\r\n    def __call__(self, *args, **kwargs):\r\n        return ray.get(self._remote(args, kwargs))\r\n    ...\r\n```\r\n\r\nThen in some case, If do the following:\r\n```python\r\nclass Foo(object):\r\n    def foo(self):\r\n        return \"foo\"\r\n\r\nclass Bar(object):\r\n    def bar(self, foo_obj):\r\n        return foo_obj.foo()\r\n \r\nRayFoo = ray.remote(Foo)\r\nRayBar = ray.remote(Bar)\r\n\r\nif __name__ == \"__main__\":\r\n    f = Foo()\r\n    b = Bar()\r\n    print(b.bar(f))\r\n\r\n    ray.init(log_to_driver=False)\r\n    rf = RayFoo.remote()\r\n    rb = RayBar.remote()\r\n    print(rb.bar(rf))\r\n```\r\nwith the original `__call__` implementation, this is not possible, but with the proposed one, this works perfectly.\r\n\r\nIs there any design consideration?\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7737/comments",
    "author": "cloudhan",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-25T05:33:16Z",
        "body": "This is a design decision we made a couple years ago. The reason is to remain consistent across the API - tasks, methods, and class invocations.\r\n\r\nThe high level goal is to safeguard against user errors. I should note that commonly, new users often complain about the verbosity of this decision :) "
      },
      {
        "user": "cloudhan",
        "created_at": "2020-03-25T06:03:29Z",
        "body": "Are there any design pattern to walkaround the issue I mentioned, that is, what if I want to support both local and Ray decorated types. How to avoid implementing those types twice?"
      },
      {
        "user": "ericl",
        "created_at": "2020-03-25T07:59:50Z",
        "body": "You can do that with a wrapper class that automatically invokes .remote() under the hood, e.g., `h = Wrapper(handle)`."
      },
      {
        "user": "cloudhan",
        "created_at": "2020-03-25T11:16:55Z",
        "body": "Tried to hack a new decorator that replace the object constructor with a wrapper and then which replace the actor_method_obj.__call__ method with a new wrapper that return ray.get(actor_method_obj.<method_name>.remote()), too convoluted, will use @ericl 's wrapper.\r\n\r\nBTW, it is viable to add an option to allow this type of behavior, e.g.\r\n```python\r\n@ray.remote(allow_non_remote_calls=True)\r\nclass Foo(object): ...\r\n``` "
      }
    ]
  },
  {
    "number": 7652,
    "title": "[Question][rllib] How can I save my customised metrics to TensorBoard after building customised model and policy?",
    "created_at": "2020-03-19T04:24:48Z",
    "closed_at": "2022-04-09T23:07:57Z",
    "labels": [
      "question",
      "P3",
      "tune",
      "rllib"
    ],
    "url": "https://github.com/ray-project/ray/issues/7652",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nRay: 0.8.2\r\nPython: 3.6\r\nTF: 2.1\r\nOS: macOS \r\n\r\n\r\nBy running `rllib train --run=PG --env=CartPole-v0`, we can get some data saved in Tensoborad format and visualize it via Tensorboard, however, after customizing my own model and policy, there is no Tensorboard data after running my code. How can I set my loss, my customized metrics (accuracy, probability distributions, histogram, etc) and save them to TensorBoard?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7652/comments",
    "author": "GoingMyWay",
    "comments": [
      {
        "user": "gjoliver",
        "created_at": "2022-04-09T23:07:57Z",
        "body": "closing due to lack of updates.\r\nif this is still a problem for you, please provide a simple reproduce script so we can help make sure the metrics you need shows up in tensorboard.\r\nthanks."
      }
    ]
  },
  {
    "number": 7559,
    "title": "[rllib] How to decode weight matrix?",
    "created_at": "2020-03-11T13:52:41Z",
    "closed_at": "2023-06-30T22:31:14Z",
    "labels": [
      "question",
      "P3",
      "rllib"
    ],
    "url": "https://github.com/ray-project/ray/issues/7559",
    "body": "Hello,\r\n\r\nWhen try to look at the weights in a trained (PPO) policy by using Trainer.get_weights(), the return dictionary has one-dimensional arrays as weights for multi-layer networks. Is there any way to know how to map these 1-D arrays to weights (and biases) to specific layers, e.g. which indices are the weights connecting first layer to second, which are biases of first layer, etc.\r\n\r\nI work on Google Colab.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7559/comments",
    "author": "ZekiDorukErden",
    "comments": [
      {
        "user": "Rohan138",
        "created_at": "2023-06-30T22:31:14Z",
        "body": "Closing; `get_weights` now returns a dictionary with the required information."
      }
    ]
  },
  {
    "number": 7492,
    "title": "ray.tune.Trainable does not learn when i outsource tensorflow models to a separate class",
    "created_at": "2020-03-06T18:18:14Z",
    "closed_at": "2020-03-07T17:35:34Z",
    "labels": [
      "question",
      "needs-repro-script",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7492",
    "body": "I wrote a framework with tensorflow 1.14 in which i will prepare the environment and run the optimizers depend on a config file. it works perfectly when i run it using direct function call.\r\n\r\n```\r\n    for module in tf_modules:\r\n        config_copy = copy.deepcopy(config)\r\n        ....\r\n        framework = load_module(config_copy['launcher_config']['structure']['framework'])(config_copy)\r\n        for i in range(epochs):\r\n            print('\\r' + framework._train().__str__())\r\n            framework._iteration = i\r\n            print('\\r' + name + ' ' + ''.join(['.'] * ((i % 3) + 1)), end='')\r\n        framework._stop()\r\n        del framework\r\n        print('\\r {} [Done]'.format(name))\r\n```\r\n\r\n```\r\nclass TfTrainable(tune.Trainable):\r\n    ...\r\n\r\n    def _setup(self, config):\r\n        dataset_loader = load_module(config['dataset']['loader'])\r\n        self.dataset = dataset_loader().load(config)\r\n        self.variables = self._prepare_model(config, self.dataset)\r\n        self.sess = tf.Session()\r\n        self.sess.run(tf.global_variables_initializer())\r\n\r\n    def _initialize_variables(self, config, dataset):\r\n        tf_a_index = tf.placeholder(dtype=tf.int32, shape=[None], name='a_index')\r\n\r\n        regularizer = tf.contrib.layers.l2_regularizer(scale=config['regularization'])\r\n        initializer = tf.initializers.random_normal(\r\n            mean=config['tensorflow']['random_mean'],\r\n            stddev=config['tensorflow']['random_stddev'],\r\n            seed=config['random_seed'])\r\n\r\n        a = tf.get_variable( ... )\r\n\r\n        return {\r\n            'tf_a_index': tf_a_index,\r\n            'a': a,\r\n            'a_embedding': tf.nn.embedding_lookup(a, tf_a_index),\r\n             ...\r\n        }\r\n\r\n    def _prepare_model(self, config, dataset: dict):\r\n        variables = self._initialize_variables(config, dataset)\r\n\r\n        self.modules = []\r\n        for index, module_path in enumerate(config['modules']):\r\n            module = load_module(module_path)()\r\n            self.modules.append(module)\r\n            module.generate_loss(config, variables, dataset)\r\n\r\n        return variables\r\n\r\n    ...\r\n\r\n    def _train(self):\r\n        for module in self.modules:\r\n            module.optimize(self.iteration, {**self.config, 'sess': self.sess},\r\n                            self.variables, self.dataset)\r\n\r\n        validation_error = self.compute_errors(self.dataset['validation_groups'], self.variables, self.dataset)\r\n        self.validation_error = self.validation_error.append(validation_error, ignore_index=True)\r\n\r\n        return {\r\n            \"epoch\": self.iteration,\r\n            **validation_error\r\n        }\r\n```\r\n\r\n```\r\nclass TfSquared(Loss):\r\n    def optimize(self, epoch, config: dict, variables: dict, dataset: dict):\r\n        for batch in slicer(dataset['train'], config['batch_size']):\r\n            config['sess'].run(self.optimizer, feed_dict={\r\n                variables['tf_a_index']: batch[...],\r\n                ...\r\n            })\r\n\r\n    def generate_loss(self, config: dict, variables: dict, dataset):\r\n        dot_product = tf.multiply(variables['a'], variables['b'])\r\n        dot_product = tf.reshape(tf.reduce_sum(dot_product, 1, keep_dims=True), (-1,))\r\n        prediction = variables['c'] + dot_product\r\n\r\n        loss_function = tf.sqrt(tf.losses.mean_squared_error(variables['tf_true_value'], prediction))\r\n\r\n        var_list = [variables['a'], variables['b'], variables['c']]\r\n        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=config['learning_rate'])\r\n        self.optimizer = self.optimizer.minimize(loss_function, var_list=var_list)\r\n\r\n```\r\n\r\nHere is the result:\r\n\r\n```\r\n'MAP': 0.05866221681687942\r\n'MAP': 0.06426142363467666\r\n'MAP': 0.07129013705117584\r\n...\r\n```\r\n\r\nwhich means it works\r\n\r\nbut when i run it using `ray.tune` it does not learn anything!\r\nhere is the results:\r\n\r\n```\r\ntune.run(\r\n            framework,\r\n            name=str(index),\r\n            local_dir=save_dir,\r\n            **config['launcher_config']['tune'],\r\n            scheduler=AsyncHyperBandScheduler(\r\n                **config['launcher_config']['scheduler'],\r\n                **config['launcher_config']['experiment_metric']),\r\n            search_alg=HyperOptSearch(\r\n                {\r\n                    **parameters,\r\n                    **param,\r\n                    **Hyperopt(config['hyperparameters']).space\r\n                },\r\n                max_concurrent=5,\r\n                **config['launcher_config']['experiment_metric']))\r\n```\r\n\r\n\r\n```\r\n'MAP': 0.05866221681687942\r\n'MAP': 0.05866221681687942\r\n'MAP': 0.05866221681687942\r\n...\r\n```\r\n\r\nBut it can be fixed if i copy and paste the code inside the loss modules inside the TfTrainable class, but it will break the goal of the written framework.\r\n\r\nCan anyone help me fix this problem ?\r\n\r\nThanks a lot\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7492/comments",
    "author": "MohammadJamali",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-07T07:32:43Z",
        "body": "Can you please provide a reproducible script? Thanks!"
      },
      {
        "user": "MohammadJamali",
        "created_at": "2020-03-07T17:35:34Z",
        "body": "After 5 hours of debugging i found out that this is just a simple mistake with random number generator of each thread that i supposed to be same but they weren't (i set them in a way to be the same, i mean). i set that random numbers manually and the problem gone,  sorry for bothering.\r\nVery Very thank you for your time"
      }
    ]
  },
  {
    "number": 7490,
    "title": "[rllib] why the RolloutWorker uses the default config  everytime",
    "created_at": "2020-03-06T17:57:35Z",
    "closed_at": "2020-03-07T03:58:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7490",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nRay: 0.8.2\r\nPython: 3.6\r\nTF: 2.0\r\nOS: macOS Catalina\r\n\r\nI  create some RolloutWorker instances in our customized training flow, you can run the code.\r\n\r\n```python\r\nimport argparse\r\n\r\nimport ray\r\nimport gym\r\nimport copy\r\nimport random\r\nimport numpy as np\r\n\r\nfrom ray import tune\r\nfrom ray.rllib.utils import try_import_tf\r\n\r\nfrom ray.rllib.models import ModelCatalog\r\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\r\n\r\nfrom ray.rllib.models.tf.tf_action_dist import Categorical\r\nfrom ray.rllib.agents.pg.pg import PGTFPolicy\r\n\r\nfrom ray.rllib.evaluation import RolloutWorker\r\nfrom ray.rllib.evaluation.metrics import collect_metrics\r\nfrom ray.rllib.policy.sample_batch import SampleBatch\r\nfrom ray.rllib.policy.tests.test_policy import TestPolicy\r\nfrom ray.rllib.policy.tf_policy import TFPolicy\r\n\r\nfrom ray.rllib.offline import NoopOutput, IOContext, OutputWriter, InputReader\r\n\r\nfrom ray.rllib.agents.trainer import with_common_config\r\n\r\nfrom ray.rllib.evaluation.postprocessing import Postprocessing, compute_advantages\r\nfrom ray.rllib.policy.tf_policy_template import build_tf_policy\r\n\r\n\r\nfrom ray.rllib.models.tf.misc import normc_initializer, get_activation_fn\r\n\r\n\r\ntf = try_import_tf()\r\n\r\n\r\nclass CustomCategorical(Categorical):\r\n    def __init__(self, inputs, model=None, temperature=1.0):\r\n        \"\"\" The inputs are action logits \"\"\"\r\n        super().__init__(inputs, model, temperature)\r\n        self.softmax = tf.nn.softmax(inputs)\r\n\r\n    def prob(self, actions):\r\n        _prob_given_action = tf.one_hot(actions, depth=self.softmax.get_shape().as_list()[-1]) * self.softmax\r\n        return tf.reduce_sum(_prob_given_action, axis=-1)\r\n\r\n\r\nclass DemoModel(TFModelV2):\r\n    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\r\n        super(DemoModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\r\n\r\n        self.loss_inputs = [\r\n            ('taken_actions', tf.placeholder(tf.int32, (None,))),\r\n            ('returns', tf.placeholder(tf.float32, (None,)))\r\n        ]\r\n        self.ph_obs_input = tf.placeholder(tf.float32, (None,)+obs_space.shape)\r\n\r\n        self.ph_all_inputs = {k: v for _, (k, v) in enumerate(self.loss_inputs + [(\"obs\", self.ph_obs_input)])}\r\n        self.ph_all_inputs['prev_actions'] = tf.placeholder(tf.int32, (None,))\r\n        self.ph_all_inputs['prev_rewards'] = tf.placeholder(tf.float32, (None,))\r\n\r\n        inputs = tf.keras.layers.Input(\r\n            shape=(np.product(obs_space.shape), ))\r\n\r\n        _layer_out = tf.keras.layers.Dense(\r\n            128,\r\n            activation=tf.nn.tanh,\r\n            kernel_initializer=normc_initializer(1.0))(inputs)\r\n\r\n        self._layer_out = tf.keras.layers.Dense(\r\n            num_outputs,\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(1.0))(_layer_out)\r\n\r\n        self._value_out = tf.keras.layers.Dense(\r\n            1,\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(0.01))(_layer_out)\r\n\r\n        self.action_dist = CustomCategorical(self._layer_out, None)\r\n        self.predicted_actions = self.action_dist.sample()\r\n\r\n        # self.model and self.base_model are different\r\n        self.model = tf.keras.Model(inputs, [self._layer_out, self._value_out])\r\n        self.register_variables(self.model.variables)\r\n\r\n    def forward(self, input_dict, state, seq_lens):\r\n        model_out, self._value_out = self.model(input_dict[\"obs_flat\"])\r\n        return model_out, state\r\n\r\n    def custom_loss(self, policy_loss, loss_inputs):\r\n        return policy_loss\r\n\r\n    def from_batch(self, train_batch, is_training=True):\r\n        \"\"\"Convenience function that calls this model with a tensor batch.\r\n\r\n        All this does is unpack the tensor batch to call this model with the\r\n        right input dict, state, and seq len arguments.\r\n        \"\"\"\r\n\r\n        input_dict = {\r\n            \"obs\": train_batch[SampleBatch.CUR_OBS],\r\n            \"is_training\": is_training,\r\n        }\r\n        if SampleBatch.PREV_ACTIONS in train_batch:\r\n            input_dict[\"prev_actions\"] = train_batch[SampleBatch.PREV_ACTIONS]\r\n        if SampleBatch.PREV_REWARDS in train_batch:\r\n            input_dict[\"prev_rewards\"] = train_batch[SampleBatch.PREV_REWARDS]\r\n        states = []\r\n        i = 0\r\n        while \"state_in_{}\".format(i) in train_batch:\r\n            states.append(train_batch[\"state_in_{}\".format(i)])\r\n            i += 1\r\n        return self.__call__(input_dict, states, train_batch.get(\"seq_lens\"))\r\n\r\n    def value_function(self):\r\n        return tf.reshape(self._value_out, [-1])\r\n\r\n    def metrics(self):\r\n        \"\"\"Override to return custom metrics from your model.\r\n\r\n        The stats will be reported as part of the learner stats, i.e.,\r\n            info:\r\n                learner:\r\n                    model:\r\n                        key1: metric1\r\n                        key2: metric2\r\n\r\n        Returns:\r\n            Dict of string keys to scalar tensors.\r\n        \"\"\"\r\n        return {}\r\n\r\n\r\ndef pg_tf_loss(policy, model, dist_class, train_batch):\r\n    \"\"\"The basic policy gradients loss.\"\"\"\r\n    logits, _ = model.from_batch(train_batch)\r\n    action_dist = dist_class(logits, model)\r\n    return -tf.reduce_mean(\r\n        action_dist.logp(train_batch[SampleBatch.ACTIONS]) * train_batch[SampleBatch.REWARDS])\r\n\r\n\r\n# def post_process_advantages(policy,\r\n#                             sample_batch,\r\n#                             other_agent_batches=None,\r\n#                             episode=None):\r\n#     \"\"\"This adds the \"advantages\" column to the sample train_batch.\"\"\"\r\n#     return compute_advantages(\r\n#         sample_batch,\r\n#         0.0,\r\n#         policy.config[\"gamma\"],\r\n#         use_gae=False,\r\n#         use_critic=False)\r\n\r\n\r\nDEFAULT_CONFIG = with_common_config({\r\n    # # No remote workers by default.\r\n    # \"num_workers\": 0,\r\n    # # Learning rate.\r\n    # \"lr\": 0.0004,\r\n})\r\n\r\n\r\nCustomPolicy = build_tf_policy(\r\n    name=\"CustomPolicy\",\r\n    get_default_config=lambda: DEFAULT_CONFIG,\r\n    # postprocess_fn=post_process_advantages,\r\n    loss_fn=pg_tf_loss)\r\n\r\n\r\ndef set_variables(policy: CustomPolicy):\r\n    policy._variables = ray.experimental.tf_utils.TensorFlowVariables(\r\n        [], policy._sess, policy.variables())\r\n\r\n\r\ndef training_workflow(config, reporter):\r\n    sess = tf.Session()\r\n    env = gym.make(\"CartPole-v0\")\r\n\r\n    # policy = CustomPolicy(observation_space=env.observation_space, action_space=env.action_space, config=config,\r\n    #                       sess=sess, model=model, loss_inputs=model.loss_inputs, loss='Not None',\r\n    #                       action_sampler=model.predicted_actions, obs_input=model.ph_obs_input)\r\n\r\n    conf = {'config': config, 'sess': sess, 'model': model, 'loss_inputs': model.loss_inputs, 'loss': 'Not None',\r\n            'action_sampler': model.predicted_actions, 'obs_input': model.ph_obs_input}\r\n\r\n    policy = CustomPolicy(env.observation_space, env.action_space,\r\n                          config=config)  # , existing_inputs=model.ph_all_inputs)#, existing_model=model)\r\n    set_variables(policy)\r\n    workers = [\r\n        RolloutWorker.as_remote().remote(env_creator=lambda c: gym.make(\"CartPole-v0\"),\r\n                                         policy=CustomPolicy\r\n                                         )\r\n        for _ in range(config[\"num_workers\"])\r\n    ]\r\n\r\n    for i in range(config[\"num_iters\"]):\r\n        # Broadcast weights to the policy evaluation workers\r\n        weights = ray.put({\"default_policy\": policy.get_weights()})\r\n        for w in workers:\r\n            w.set_weights.remote(weights)\r\n\r\n        # Gather a batch of samples\r\n        T1 = SampleBatch.concat_samples(\r\n            ray.get([w.sample.remote() for w in workers]))\r\n\r\n        # Update the remote policy replicas and gather another batch of samples\r\n        # new_value = policy.get_weights()\r\n        # for w in workers:\r\n        #     w.for_policy.remote(lambda p: p.update_some_value(new_value))\r\n\r\n        # Gather another batch of samples\r\n        T2 = SampleBatch.concat_samples(\r\n            ray.get([w.sample.remote() for w in workers]))\r\n\r\n        # Improve the policy using the T1 batch\r\n        policy.learn_on_batch(T1)\r\n\r\n        # Do some arbitrary updates based on the T2 batch\r\n        # print(f'iter: {i}, sum_rewards: {(sum(T2[\"rewards\"])):.2f}')\r\n\r\n        reporter(**collect_metrics(remote_workers=workers))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--gpu\", action=\"store_true\")\r\n    parser.add_argument(\"--num-iters\", type=int, default=3)\r\n    parser.add_argument(\"--num-workers\", type=int, default=1)\r\n    parser.add_argument(\"--num-cpus\", type=int, default=0)\r\n\r\n    args = parser.parse_args()\r\n    ray.init(num_cpus=args.num_cpus or None)\r\n    ModelCatalog.register_custom_model(\"demo_model\", DemoModel)\r\n\r\n    tune.run(\r\n        training_workflow,\r\n        # resources_per_trial={\r\n        #     \"gpu\": 1 if args.gpu else 0,\r\n        #     \"cpu\": 1,\r\n        #     \"extra_cpu\": args.num_workers,\r\n        # },\r\n        config={\r\n            \"num_workers\": args.num_workers,\r\n            \"num_iters\": args.num_iters,\r\n            \"lr\": 1e-3,\r\n            \"model\": {\r\n                \"custom_model\": \"demo_model\",\r\n                \"max_seq_len\": 20,\r\n                \"custom_options\": {\r\n                    \"activation\": tf.nn.tanh,\r\n                }\r\n            },\r\n        },\r\n    )\r\n```\r\n\r\nIn\r\n```\r\n    workers = [\r\n        RolloutWorker.as_remote().remote(env_creator=lambda c: gym.make(\"CartPole-v0\"),\r\n                                         policy=CustomPolicy\r\n                                         )\r\n        for _ in range(config[\"num_workers\"])\r\n    ]\r\n```\r\nThe RolloutWorker creates a instance by using the default config not the config passed into the workflow. \r\n\r\nAre there any methods to fix it?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7490/comments",
    "author": "GoingMyWay",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-03-06T21:56:48Z",
        "body": "You need to explicitly pass it via RolloutWorker.remote(policy_config=config), or use `WorkerSet(trainer_config=config)` to create the rollout workers."
      },
      {
        "user": "GoingMyWay",
        "created_at": "2020-03-07T02:34:41Z",
        "body": "> You need to explicitly pass it via RolloutWorker.remote(policy_config=config), or use `WorkerSet(trainer_config=config)` to create the rollout workers.\r\n\r\nExactly. Thanks."
      }
    ]
  },
  {
    "number": 7486,
    "title": "[ray,tune] How to set different gpu resources in different gpu card",
    "created_at": "2020-03-06T13:39:03Z",
    "closed_at": "2020-03-07T07:29:37Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7486",
    "body": "I have 2 gpu cards with different memory: \r\n\r\n- **cuda:0** has **8GB** memory and \r\n- **cuda:1** has **16GB** memory. \r\n\r\nRunning a single job requires max. **4GB** memory. \r\n\r\nIf I set the gpu resources per trial in <code>tune.run</code> to **0.25** on **cuda:1**, I can run 4 jobs in parallel. But the **0.25** gpu resources setting cannot be applied to **cuda:0**. Can I specify the different gpu split for different cuda devices in ray? Thanks.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7486/comments",
    "author": "jamestszhim",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-07T07:28:06Z",
        "body": "If on the same node, you may want to start ray twice (`CUDA_VISIBLE_DEVICES=0 ray start --head` and then `CUDA_VISIBLE_DEVICES=1 ray start --address`) and assign them custom resources of 8GB and 16GB memory. See the docs for more information (search \"custom resources\").\r\n\r\nMore generally, this isn't actually a Tune problem but a Ray problem (of not being able to elegantly handle heterogeneous resources on a single node)."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-03-07T07:29:37Z",
        "body": "I'm going to pre-emptively close this issue for now, but please reopen if you run into any problems/have any questions!"
      }
    ]
  },
  {
    "number": 7467,
    "title": "[tune][rllib] _InactiveRpcError Deadline Exceeded",
    "created_at": "2020-03-05T16:32:58Z",
    "closed_at": "2020-04-22T17:22:40Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7467",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI have VirtualBox running on Centos 7 and I am having trouble initializing Ray. After I run ray.init(), I get an _InactiveRpcError due to a Deadline Exceeded exception. What info should I provide in order to troubleshoot this error?\r\n\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nray 0.8.2\r\nredis 3.4.1\r\nPython 3.6\r\nCentos 7 on VirtualBox",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7467/comments",
    "author": "Leonolovich",
    "comments": [
      {
        "user": "Leonolovich",
        "created_at": "2020-03-05T20:46:30Z",
        "body": "Running ray.init(local_mode=True) allows me to continue without errors to my tune.run() step, but I havent been able to resolve the Inactive Rcp Error. This is not ideal as I can only get one worker to perform training when using local_mode=True."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-04-22T02:36:49Z",
        "body": "Can you please try again on the latest Ray version?"
      },
      {
        "user": "Leonolovich",
        "created_at": "2020-04-22T17:22:39Z",
        "body": "That appears to have made the issue go away. For documentation, I was having the issue on version 0.8.2 or Ray and I no longer have the issue on version 0.8.4.\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 7466,
    "title": "[rllib] Relationship between iterations and number of episodes played",
    "created_at": "2020-03-05T15:34:19Z",
    "closed_at": "2020-03-12T00:55:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7466",
    "body": "I am training two agents in a multi-agent  environment (with different trainers, one using DQN an one using PPO), for both agents I am using the default configurations. I noticed that the number of episodes executed per iteration is different between these two algorithms. I am getting 4 episodes per iteration for DQN and 1 episode per iteration for PPO.    \r\n\r\nMy question is, what determines exactly how many episodes get 'played' per iteration? is this algorithm specific?  and is there anyway to control it? \r\n\r\nNote: by iterations here I mean:\r\n\r\n```\r\n  for i in range(ITERATIONS):\r\n        # improve first policy\r\n        trainer0.train()\r\n        # improve second policy\r\n        trainer1.train()  \r\n        .... \r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7466/comments",
    "author": "Degiorgio",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-03-05T16:37:08Z",
        "body": "It's the Trainer's default config settings. You probably won't be able to force the exact number of episodes per train iteration, if your env produces different episode lengths:\r\n\r\nPPO's default:\r\nsample_batch_size=200, batch_mode=truncate_episodes, training_batch_size=4000 -> workers collect batches of 200 ts (per worker), then the policy network gets updated using the 4000 last ts (in n minibatch-chunks of 128 (`sgd_minibatch_size`)).\r\n\r\nDQN's default:\r\ntrain_batch_size=32, sample_batch_size=4, timesteps_per_iteration=1000 -> workers collect chunks of 4 ts and add these to the replay buffer (of size `buffer_size` ts), then at each train call, at least 1000 ts are pulled altogether from the buffer (in batches of 32) to update the network.\r\n\r\n"
      }
    ]
  },
  {
    "number": 7465,
    "title": "[tune] Population based training for Mask RCNN - Actor died and unable to restore",
    "created_at": "2020-03-05T14:51:01Z",
    "closed_at": "2020-08-04T00:40:47Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7465",
    "body": "Python version - 3.6.9\r\nRay version - 0.7.6\r\nTensorflow version - 1.10.0\r\n\r\n**Background**: I am doing population based training (PBT) for the Mask RCNN with two configs (learning rate and gradient clipping norm. I have four trials (single set of hyperparameter trainings) running in parallel. Here is the goal is minimize the loss, when it see a good trial with a minimum loss it makes the checkpoints (ray objects). When there is trial with more loss then it perturbs from the good one. This is all happening in correct way. Say fitModel_0_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.0001 started well and the training went over 34 iterations (log below showing 34 iteration)\r\n\r\n```\r\n== Status ==\r\nPopulationBasedTraining: 7 checkpoints, 2 perturbs\r\nResources requested: 32/36 CPUs, 8/8 GPUs, 0.0/625.0 GiB heap, 0.0/43.12 GiB objects\r\nMemory usage on this node: 67.6/503.8 GiB\r\nResult logdir: /scratch1/lnatu/training_project/Mask_RCNN/logs/Tuneexp_randomseed0/allupd_pbt_experiment_8gpu_25cpu\r\nNumber of trials: 4 ({'RUNNING': 4})\r\nRUNNING trials:\r\n - **fitModel_0_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.0001@perturbed[GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.01048]:                RUNNING, [8 CPUs, 2 GPUs], [pid=24192], 23996 s, 34 iter**\r\n - fitModel_1_GRADIENT_CLIP_NORM=3.0,LEARNING_RATE=0.0001@perturbed[GRADIENT_CLIP_NORM=3.0,LEARNING_RATE=0.008384]:                RUNNING, [8 CPUs, 2 GPUs], [pid=22606], 24083 s, 35 iter\r\n - fitModel_2_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.01:             RUNNING, [8 CPUs, 2 GPUs], [pid=22580], 12872 s, 18 iter\r\n - fitModel_3_GRADIENT_CLIP_NORM=3.0,LEARNING_RATE=0.01:             RUNNING, [8 CPUs, 2 GPUs], [pid=22583], 14962 s, 21 iter\r\n```\r\n \r\nThen there was some issue and I see the following that ray couldn’t restore the trial from the checkpoint and the actor died.\r\n \r\n```\r\n_2020-03-04 22:39:31,069              ERROR ray_trial_executor.py:213 -- Error starting runner for Trial fitModel_0_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.0001@perturbed[GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.01048]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\", line 556, in _process_trial\r\n    trial, force=result.get(SHOULD_CHECKPOINT, False))\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\", line 595, in _checkpoint_trial_if_needed\r\n    self.trial_executor.save(trial, storage=Checkpoint.DISK)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 559, in save\r\n    trial.runner.save.remote())\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/worker.py\", line 2123, in get\r\n    raise value\r\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task._\r\n \r\n_During handling of the above exception, another exception occurred:_\r\n \r\n_Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 211, in start_trial\r\n    self._start_trial(trial, checkpoint)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 148, in _start_trial\r\n    str(trial)))\r\n**RuntimeError: Restore from checkpoint failed for Trial fitModel_0_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.0001@perturbed[GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.01048].\r\n2020-03-04 22:39:33,248              INFO ray_trial_executor.py:224 -- Trying to start runner for Trial fitModel_0_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.0001@perturbed[GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.01048] without checkpoint.**\r\n```\r\n \r\nThen the trial was again starting from first iteration. If there is an error, ray is supposed to restore the trial from previous checkpoint, if not the training is starting from first iteration and just looping instead of going forward.\r\n \r\n```\r\n== Status ==\r\nPopulationBasedTraining: 8 checkpoints, 2 perturbs\r\nResources requested: 32/36 CPUs, 8/8 GPUs, 0.0/625.0 GiB heap, 0.0/43.12 GiB objects\r\nMemory usage on this node: 65.8/503.8 GiB\r\nResult logdir: /scratch1/lnatu/training_project/Mask_RCNN/logs/Tuneexp_randomseed0/allupd_pbt_experiment_8gpu_25cpu\r\nNumber of trials: 4 ({'RUNNING': 4})\r\nRUNNING trials:\r\n - **fitModel_0_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.0001@perturbed[GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.01048]:                RUNNING, 2 failures: /scratch1/lnatu/training_project/Mask_RCNN/logs/Tuneexp_randomseed0/allupd_pbt_experiment_8gpu_25cpu/fitModel_0_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.0001_2020-03-04_14-41-313mp1sljg/error_2020-03-04_22-39-33.txt, [8 CPUs, 2 GPUs], [pid=22602], 748 s, 1 iter**\r\n - fitModel_1_GRADIENT_CLIP_NORM=3.0,LEARNING_RATE=0.0001@perturbed[GRADIENT_CLIP_NORM=3.0,LEARNING_RATE=0.008384]:                RUNNING, [8 CPUs, 2 GPUs], [pid=22606], 27020 s, 39 iter\r\n - fitModel_2_GRADIENT_CLIP_NORM=2.0,LEARNING_RATE=0.01:             RUNNING, [8 CPUs, 2 GPUs], [pid=22580], 15071 s, 21 iter\r\n - fitModel_3_GRADIENT_CLIP_NORM=3.0,LEARNING_RATE=0.01:             RUNNING, [8 CPUs, 2 GPUs], [pid=22583], 15736 s, 22 iter\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7465/comments",
    "author": "NS-Lakshmi",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-25T03:47:22Z",
        "body": "Can you try upgrading to the latest Ray version?"
      },
      {
        "user": "utke1",
        "created_at": "2020-03-27T15:21:08Z",
        "body": "The immediate problem is that the network implementation doesn't play with the newer numpy version required by the latest Ray.  We have not yet observed the problem outside of this particular training scenario. We will update this, once we see it again."
      },
      {
        "user": "amogkam",
        "created_at": "2020-08-04T00:40:46Z",
        "body": "@utke1 were you able to fix this problem? I'm going to close this for now, but feel free to open it again if this is still an issue."
      }
    ]
  },
  {
    "number": 7461,
    "title": "[tune] How to use local_mode=False?",
    "created_at": "2020-03-05T09:39:06Z",
    "closed_at": "2020-09-16T16:37:18Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7461",
    "body": "*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nray==0.8.2\r\nPython 3.7.3\r\nUbuntu 18.04\r\n\r\nI have no issue using tune with `local_mode=True`, however, if I disable local mode, then the last line is:\r\n```\r\n2020-03-05 09:30:28,589 INFO services.py:1078 -- View the Ray dashboard at localhost:8265\r\n```\r\nI do see 100% CPU usage on the main Python process and 20% on the redis server. The workers are idle. This has been running over 10min without a change.\r\n\r\nIs there anything I'm missing or don't understand? Should it just work?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7461/comments",
    "author": "letmaik",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-05T20:38:50Z",
        "body": "Did you set `num_samples` for `tune.run`?"
      },
      {
        "user": "letmaik",
        "created_at": "2020-03-05T21:31:24Z",
        "body": "@richardliaw  Yes, I set that to 1."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-03-05T21:36:42Z",
        "body": "Ah - can you set it to 8?"
      },
      {
        "user": "letmaik",
        "created_at": "2020-03-05T21:39:56Z",
        "body": "What does that do? I don't have any sample_from in my search space and with local mode it already works. Could you explain a bit more about this?"
      },
      {
        "user": "richardliaw",
        "created_at": "2020-03-06T01:30:50Z",
        "body": "It'll just run the same function 8 times, to test that Ray's parallelism is working. \r\n\r\nYou might see lots of workers idle because ray starts worker processes by default. Since you've set number of samples to 1, you'll only be running 1 job (no parallelism)."
      },
      {
        "user": "letmaik",
        "created_at": "2020-03-06T08:16:38Z",
        "body": "Changing to 8 doesn't make a difference, it's still not starting any jobs. Very odd! What's the right way to debug is? Is there some option to enable super-verbose logging?"
      },
      {
        "user": "richardliaw",
        "created_at": "2020-03-07T07:39:16Z",
        "body": "Yeah; one way to debug this is to use the Ray Dashboard. \r\n\r\nCan you also post a stacktrace/logs from the output?"
      },
      {
        "user": "Halopend",
        "created_at": "2020-04-07T20:26:36Z",
        "body": "Mine is getting a broken pipe error. not sure why\r\nraceback (most recent call last):\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/redis/connection.py\", line 700, in send_packed_command\r\n    sendall(self._sock, item)\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/redis/_compat.py\", line 8, in sendall\r\n    return sock.sendall(*args, **kwargs)\r\nBrokenPipeError: [Errno 32] Broken pipe\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/halopend/.vscode/extensions/ms-python.python-2020.3.71659/pythonFiles/lib/python/debugpy/wheels/debugpy/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"/Users/halopend/.vscode/extensions/ms-python.python-2020.3.71659/pythonFiles/lib/python/debugpy/wheels/debugpy/../debugpy/server/cli.py\", line 429, in main\r\n    run()\r\n  File \"/Users/halopend/.vscode/extensions/ms-python.python-2020.3.71659/pythonFiles/lib/python/debugpy/wheels/debugpy/../debugpy/server/cli.py\", line 266, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/halopend/Documents/School/Neural.tmp/Assignment 4/a4_cnn_tf2.py\", line 383, in <module>\r\n    testRunner(config)\r\n  File \"/Users/halopend/Documents/School/Neural.tmp/Assignment 4/a4_cnn_tf2.py\", line 357, in testRunner\r\n    config=config\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/ray/tune/tune.py\", line 229, in run\r\n    run_identifier = Experiment.register_if_needed(exp)\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/ray/tune/experiment.py\", line 212, in register_if_needed\r\n    register_trainable(name, run_object)\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/ray/tune/registry.py\", line 67, in register_trainable\r\n    _global_registry.register(TRAINABLE_CLASS, name, trainable)\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/ray/tune/registry.py\", line 108, in register\r\n    self.flush_values()\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/ray/tune/registry.py\", line 130, in flush_values\r\n    _internal_kv_put(_make_key(category, key), value, overwrite=True)\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/ray/experimental/internal_kv.py\", line 38, in _internal_kv_put\r\n    updated = worker.redis_client.hset(key, \"value\", value)\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/redis/client.py\", line 3004, in hset\r\n    return self.execute_command('HSET', name, key, value)\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/redis/client.py\", line 877, in execute_command\r\n    conn.send_command(*args)\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/redis/connection.py\", line 721, in send_command\r\n    check_health=kwargs.get('check_health', True))\r\n  File \"/Users/halopend/.venv/lib/python3.7/site-packages/redis/connection.py\", line 713, in send_packed_command\r\n    (errno, errmsg))\r\nredis.exceptions.ConnectionError: Error 32 while writing to socket. Broken pipe."
      },
      {
        "user": "krfricke",
        "created_at": "2020-09-16T16:37:18Z",
        "body": "Since this issue is a bit dated, I'm closing it. If you still have problems running Tune, please open a new issue and provide a reproducible code example - that way we can better see if this is a problem on your side or in Tune. We're happy to help in either case."
      }
    ]
  },
  {
    "number": 7456,
    "title": "[rllib, tune] Passing tunable hyper parameter into custom model",
    "created_at": "2020-03-04T21:39:28Z",
    "closed_at": "2020-03-05T09:55:42Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7456",
    "body": "I am trying to set up the dropout as a tunable hyperparameter with PBT using the default PPO agent with a custom model.\r\n\r\nI was able to set up the custom model based on the custom Keras model example, however I am having difficulties accessing the current config from within the model. Based on the examples with dropout and Tune, they uses self.config.get(\"dropout\") to obtain the current config value for dropout hyperparameter. However the config is accessible to the trainable but is not passed to the model.\r\n\r\nIs there an easy way to access the config dropout value from within the custom model? ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7456/comments",
    "author": "pengmun",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-04T23:22:41Z",
        "body": "I think you need to specify it as part of `model_config`.\r\n\r\ncc @sven1977 "
      },
      {
        "user": "pengmun",
        "created_at": "2020-03-05T09:55:42Z",
        "body": "Thanks for the reply, I think I have a rough idea on how to get it working. \r\n\r\nCurrently, I am specifying dropout as a `custom_options` under `model_config` and using `tune.sample_from` to initialize the `dropout_factor`.\r\n\r\nBy doing this, I was able to access the `dropout_factor` within my model and the current `dropout_factor` for each agent shows up in the training status table. However, the `dropout_factor` does not mutate with training, the dropout for the top performing agent is copied over directly to the other agents. The `dropout_factor` is treated as a static config setting.\r\n\r\nThe next thing that I need to do is code up a `custom_explore_fn` to mutate my `dropout_factor`. I tried specifying the sampling distribution for dropout within `hyperparam_mutations` of PopulationBasedTraining for the PBT to mutate my `dropout_factor`. However, the `hyperparam_mutations` expects a dict key followed by the sampling distribution, but the dropout_factor is contained within a nested dictionary. "
      }
    ]
  },
  {
    "number": 7424,
    "title": "Actor method arguments",
    "created_at": "2020-03-03T19:27:19Z",
    "closed_at": "2020-03-03T21:06:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7424",
    "body": "Why do actor methods do not support passing arguments? There is an assertion that fails if the actor method function arguments are larger than 0.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7424/comments",
    "author": "commanderka",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-03T19:31:13Z",
        "body": "Can you please provide more context? i.e., a script and stack trace for reproducing this issue?"
      },
      {
        "user": "commanderka",
        "created_at": "2020-03-03T20:20:46Z",
        "body": "I can only provide a code snippet, the problem is that there are too many dependencies. But I think its a more conceptual thing anyway.\r\n\r\n```\r\n@ray.remote(num_gpus=1)\r\nclass PreprocessorActor(object):\r\n    def __init__(self):\r\n        self.detector = bla\r\n        self.landmarkDetector = bla\r\n        self.transformer = AffineTransformer((112, 112), TransformType.Improved)\r\n        self.preprocessor = IPCustomPreprocessor.IPCustomPreprocessor(self.detector, self.landmarkDetector, self.transformer)\r\n    @ray.method\r\n    def preprocess(self,imagePath):\r\n        return self.preprocessor.preprocess_from_path(imagePath)\r\n        \r\nif __name__=='__main__':\r\n    ray.init(address=\"sss-digits-1:6379\")\r\n    #create several actors\r\n    actorList = []\r\n    for nActor in range(5):\r\n        actor = PreprocessorActor.remote()\r\n        actorList.append(actor)\r\n    \r\n    imagePathToPreprocess = \"/media/sss_data/FaceDatabases/CaltecFaces/25/image_0409.jpg\"\r\n    preprocIds = []\r\n    for currentActor in actorList:\r\n        preprocId = currentActor.preprocess.remote(currentActor,imagePathToPreprocess)\r\n        preprocIds.append(preprocId)\r\n    results = ray.get(preprocIds)\r\n    for preprocResultIndex,preprocResult in enumerate(results):\r\n        if preprocResult is not None and preprocResult.error_message is None:\r\n            preprocessedImage = preprocResult.preprocessed_image\r\n            cv2.imwrite(\"/media/sss_data_3/preprocImages/{0}.jpg\".format(preprocResultIndex),preprocessedImage)\r\n```\r\ngives me \r\n\r\n> File \"testActors.py\", line 10, in <module>\r\n>     class PreprocessorActor(object):\r\n>   File \"testActors.py\", line 18, in PreprocessorActor\r\n>     @ray.method\r\n>   File \"/usr/local/lib/python3.6/dist-packages/ray/actor.py\", line 40, in method\r\n>     assert len(args) == 0\r\n> AssertionError\r\n\r\nThe idea is to have remote workers that are constantly fed with images to preprocess and to collect the preprocessed images. The problem is that the initialization of the preprocessing takes time, so I used the concept of Actors. Perhaps I have some conceptually wrong understanding, I dont know.\r\nRay version is 0.8.2"
      },
      {
        "user": "simon-mo",
        "created_at": "2020-03-03T20:26:05Z",
        "body": "`@ray.method` decorator is only there if you want to pass special parameters for a remote method, for example, [specifying the number of return values](@ray.method(num_return_vals=2)). By default, all methods for a `@ray.remote` actor can be called. \r\n\r\nYou can just remote the `@ray.method` decorator. "
      },
      {
        "user": "commanderka",
        "created_at": "2020-03-03T21:06:27Z",
        "body": "Works like this now. I think the error was just misleading. I will close the issue. Nevertheless I would encourage to update the doku with some practical samples, perhaps also concerning the ActorPool class.\r\n\r\n```\r\n@ray.remote\r\nclass PreprocessorActor(object):\r\n    def __init__(self):\r\n        self.detector = something\r\n        self.landmarkDetector = something\r\n        self.transformer = AffineTransformer((112, 112), TransformType.Relative)\r\n        self.preprocessor = IPCustomPreprocessor.IPCustomPreprocessor(self.detector, self.landmarkDetector, self.transformer)\r\n    def preprocess(self,imagePath):\r\n        return self.preprocessor.preprocess_from_path(imagePath)\r\n        \r\n\r\n\r\nif __name__=='__main__':\r\n    ray.init(address=\"sss-digits-1:6379\")\r\n    #create several actors\r\n    actorList = []\r\n    for nActor in range(5):\r\n        actor = PreprocessorActor.remote()\r\n        actorList.append(actor)\r\n\r\n    \r\n    imagePathToPreprocess = \"/media/sss_data/FaceDatabases/CaltecFaces/20/image_0308.jpg\"\r\n    preprocIds = []\r\n    for currentActor in actorList:\r\n        preprocId = currentActor.preprocess.remote(imagePathToPreprocess)\r\n        preprocIds.append(preprocId)\r\n    results = ray.get(preprocIds)\r\n    for preprocResultIndex,preprocResult in enumerate(results):\r\n        if preprocResult is not None and preprocResult.error_message is None:\r\n            preprocessedImage = preprocResult.preprocessed_image\r\n            cv2.imwrite(\"/media/sss_data_3/preprocImages/{0}.jpg\".format(preprocResultIndex),preprocessedImage)\r\n```\r\n"
      }
    ]
  },
  {
    "number": 7394,
    "title": "Does DQN \"rollout.py\" have exploration turned off?",
    "created_at": "2020-03-02T03:57:53Z",
    "closed_at": "2020-03-02T10:34:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7394",
    "body": "When I call \"rollout.py\" I am not sure if exploration is turned off or not. I've looked over the file and can't seem to find `explore=False` anywhere.\r\n\r\nSo, when we evaluate trained policy (e.g. DQN) with rollout script - does it actually turn off random actions or not?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7394/comments",
    "author": "drozzy",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-03-02T04:08:14Z",
        "body": "I don't think it's actually turned off by default right now."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:01:53Z",
        "body": "The default config for DQN for evaluation is `exploration=False` (greedy action selection)."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:03:53Z",
        "body": "However, in rollout.py, we do not use the evaluation_config, which is something, we should probably change."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:09:22Z",
        "body": "Then again, rollout.py picks up an already trained DQN, so its timesteps should already be past the epsilon exploration period, which then means it's (almost) not exploring anymore (if `final_epsilon` is 0.0, it won't explore at all). So for your specific DQN case, it should be fine (as in: not picking random actions anymore). What's your `exploration_config`?"
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T10:34:15Z",
        "body": "The above PR makes sure that rollout.py always uses the evaluation_config (which for DQN, is explore=False).\r\nIn the meantime, you can add `--config '{\"explore\": false}'` to your rollout.py command line to make sure, your algo picks only greedy acitons."
      },
      {
        "user": "drozzy",
        "created_at": "2020-03-02T13:07:44Z",
        "body": "Awesome."
      }
    ]
  },
  {
    "number": 7389,
    "title": "[rlib] Changing Results directory",
    "created_at": "2020-03-01T20:17:17Z",
    "closed_at": "2020-03-02T15:53:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7389",
    "body": "By default, when training an agent ( ray.rllib.agents) results are saved to   ~/ray_results/RUN,  where the format of RUN seems to depend on the agent used. Is there  way to change this to a custom directory and change the aforementioned format ?\r\n\r\nI skimmed  through the documentation and couldn't find any option for this. \r\n\r\nNote: I am not using tune.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7389/comments",
    "author": "Degiorgio",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-01T22:27:15Z",
        "body": "You can pass in a custom logger_creator - see `trainer.py:Trainer.__init__`:\r\n\r\n```\r\n\r\n# Create a default logger creator if no logger_creator is specified\r\nif logger_creator is None:\r\n    timestr = datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\r\n    logdir_prefix = \"{}_{}_{}\".format(self._name, self._env_id,\r\n                                      timestr)\r\n\r\n    def default_logger_creator(config):\r\n        \"\"\"Creates a Unified logger with a default logdir prefix\r\n        containing the agent name and the env id\r\n        \"\"\"\r\n        if not os.path.exists(DEFAULT_RESULTS_DIR):\r\n            os.makedirs(DEFAULT_RESULTS_DIR)\r\n        logdir = tempfile.mkdtemp(\r\n            prefix=logdir_prefix, dir=DEFAULT_RESULTS_DIR)\r\n        return UnifiedLogger(config, logdir, loggers=None)\r\n\r\n    logger_creator = default_logger_creator\r\n```"
      },
      {
        "user": "Degiorgio",
        "created_at": "2020-03-02T15:53:51Z",
        "body": "@richardliaw  Thanks  this worked!"
      }
    ]
  },
  {
    "number": 7375,
    "title": "[rllib] Thread lock for Environment",
    "created_at": "2020-02-28T16:11:40Z",
    "closed_at": "2020-03-05T17:56:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7375",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI have a simulator that reads a .dat file, performs the simulation based on the .dat file parameters, and returns an accuracy rating. I have currently got rllib training on this task by using the gym environment to interact with the simulator. The step() function takes an action, modifies the .dat file, runs the external simulator.exe, reads the output file from this simulator and uses the results to calculate a reward. \r\n\r\nThis process appears to be working correctly, but the simulator.exe takes up to 30 seconds to run each iteration, making each episode quite long. I would like to speed this up using multiple workers, however this presents the issue of the read&write to the .dat file (multiple workers accessing the file at the same time). Is there a way to create a worker lock within the Environment class so that subsequent workers wait until the lock is released before attempting to read & write from the file?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nPython 3.6\r\nCentos 7\r\nTensorFlow 1.14",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7375/comments",
    "author": "Leonolovich",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-02-28T20:03:44Z",
        "body": "Yeah; maybe try like - \r\n```\r\nfrom filelock import FileLock \r\n\r\nclass CustomEnv():\r\n... \r\n    def step(self, ...):\r\n        with FileLock(os.path.expanduser(\"~/.sim.lock\")):\r\n            ...\r\n```"
      }
    ]
  },
  {
    "number": 7350,
    "title": "[tune] Pattern for picking best results from successive tune runs?",
    "created_at": "2020-02-27T14:00:07Z",
    "closed_at": "2020-03-01T13:31:34Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7350",
    "body": "I'm building an image classification pipeline that will maintain itself over time.  Schematically it is something like:\r\n\r\n- images stream in and are normalized/stored over time\r\n- every so often, a new tune hyperparameter training run is triggered, resulting in new model params\r\n- a governor process inspects the available hyperparameter results (possibly from most recent tune run, or maybe from a previous tune run), finds the best configuration, and if that is not currently deployed it trains/deploys that config\r\n\r\nAre there any best practice patterns for doing this with tune?\r\n\r\nMy current thoughts were:\r\n\r\n- store persistent analysis results from each tune.run centrally (eg: set local_dir or remote_dir to some global location), then have the governor search that location for best results by instantiating an ExperimentAnalysis for each experiment_state.json  \r\n- store persistent analysis results from each tune.run somewhere (not necessarily in a single ray_results dir structure), and keep a DB knowing where.  Governor can then query the DB for path to experiment_state.json\r\n  - At this point, I could also just store results and param json to the DB and ditch the tune analysis results entirely\r\n\r\nAre there better patterns available?  This feels fine for a single tune analysis, but I'll have many for competing models, etc., that I'll want to search across.  I'll also want to be able to deprecate these selectively if I update the training data, etc.  ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7350/comments",
    "author": "ca-scribner",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-02-28T23:00:15Z",
        "body": "I would probably just ditch the analysis object for now and build something specialized for my DB. \r\n\r\nI would also implement a specific logger class for that said DB, so that I don't need to wrangle csv or json files."
      },
      {
        "user": "ca-scribner",
        "created_at": "2020-03-01T13:31:34Z",
        "body": "Thanks for the suggestions, that makes sense.\r\n\r\nFor a logger class, you mean to implement something inheriting the Tune Logger class and specified to Tune by tune.run(loggers=MyLoggers)?  \r\n\r\nThanks to your suggestion I also learned some new things from the docs.  Hadn't noticed before there's an MLFlow logger available.  Might try using both that and custom and just see which feels best."
      }
    ]
  },
  {
    "number": 7295,
    "title": "If the game(atari) is over, will the worker reset the environment into the initial state (such as call env.reset()) ?",
    "created_at": "2020-02-24T15:57:33Z",
    "closed_at": "2020-02-27T16:23:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7295",
    "body": "",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7295/comments",
    "author": "dxu23nc",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-24T20:55:00Z",
        "body": "Yes"
      }
    ]
  },
  {
    "number": 7251,
    "title": "[rllib] In multiagent environment, is timesteps_total the total timesteps per agent or over all agents?",
    "created_at": "2020-02-20T21:16:05Z",
    "closed_at": "2020-02-20T22:05:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7251",
    "body": "### In multiagent environment, is timesteps_total the total timesteps per agent or across all agents?\r\n\r\nFor example, I have 4 policies in my multiagent policy configuration, and after the first training iteration the timesteps_total is 4000.\r\n\r\nIs that number per agent or overall? I.e.:\r\n\r\n1. Per agent - each agent has run 4000 timesteps, so the total number of timesteps is 16000\r\n2. Overall - each agent has run 1000 timesteps, so the total number of timesteps is 4000\r\n\r\nWhich one is it?\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7251/comments",
    "author": "coreylowman",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-20T22:01:51Z",
        "body": "It's the number of times step has been called on the env (so probably it means each agent has run 4000 timesteps, assuming each agent participates in every step)."
      },
      {
        "user": "coreylowman",
        "created_at": "2020-02-20T22:05:44Z",
        "body": "Thanks, makes sense!"
      },
      {
        "user": "drozzy",
        "created_at": "2020-02-23T06:15:10Z",
        "body": "> it means each agent has run 4000 timesteps\r\n\r\nWouldn't his `timesteps_total` be 16,000 then?"
      }
    ]
  },
  {
    "number": 7195,
    "title": "Question about nested actors resource allocation",
    "created_at": "2020-02-17T14:35:43Z",
    "closed_at": "2021-07-02T02:13:40Z",
    "labels": [
      "question",
      "P3"
    ],
    "url": "https://github.com/ray-project/ray/issues/7195",
    "body": "### What is your question?\r\n\r\n[updated]\r\n\r\nHi, \r\n\r\nI'm under the following scenario to use ray for parallelism:\r\nI have created several ray actors, say each actor is on one node and occupies some resources on this node. In each actor, I am creating several nested actors (or remote functions) to perform tasks. The outer actor would collect results returned by inner actors followed by its own tasks.\r\nWant to ask the following questions:\r\n- From my experiments, inner actors are placed randomly on any node in the cluster. In the case where there are enough remaining resources on every node, would it have better performance if inner actors are placed on the same node with the outer actor that creates them to avoid data transfer across different nodes?\r\n- Seems inner and outer actors don't share resources. Am I doing wrong or is there a better way to achieve that the inner actors use the resources of the outer actor if the outer actor is doing nothing else at that time?\r\nThanks so much for the help in advance!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7195/comments",
    "author": "hkvision",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2021-07-02T02:13:40Z",
        "body": "You should use something like placement groups for this (colocation, and having a concept of \"grouped resources\")."
      }
    ]
  },
  {
    "number": 7194,
    "title": "[rllib]PPO with action branching?",
    "created_at": "2020-02-17T13:42:11Z",
    "closed_at": "2020-02-18T05:29:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7194",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### PPO with action branching?\r\nRay version: 0.8.0\r\nTensorflow Version: 1.14.0\r\nOS: Ubuntu 18.04\r\n\r\nI'm currently working on training action branching agents with PPO. What else do I need to do besides set the action space to something like `gym.spaces.Tuple([gym.spaces.Discrete(3), gym.spaces.Discrete(5)])`, or I need to write a custom loss function? I was wondering if the gradients would be correct. ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7194/comments",
    "author": "jinbo-huang",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-17T18:57:55Z",
        "body": "Yeah that's all you need for PPO. The action will be automatically computed for the space."
      },
      {
        "user": "jinbo-huang",
        "created_at": "2020-02-18T05:29:02Z",
        "body": "Thank you for your answer. It helped a lot."
      }
    ]
  },
  {
    "number": 7190,
    "title": "How to customize the log_dir in the .yaml file? Thanks in advance.",
    "created_at": "2020-02-16T23:27:29Z",
    "closed_at": "2020-03-05T22:57:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7190",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7190/comments",
    "author": "dxu23nc",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-05T22:57:31Z",
        "body": "You should be able to set `local_dir` in the rllib yaml."
      },
      {
        "user": "stefanbschneider",
        "created_at": "2020-06-19T09:37:15Z",
        "body": "Can I configure it via the `config` somehow ? Ie that's passed to `ppo.PPOTrainer(config=config, env=MyEnv)` for example."
      }
    ]
  },
  {
    "number": 7173,
    "title": "[rllib]: SAC fails with MultiAgentEnv and \"normalize_actions\" set to True (default)?",
    "created_at": "2020-02-14T19:39:21Z",
    "closed_at": "2020-02-17T18:26:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7173",
    "body": "Hello,\r\n\r\nI am having the following issue when letting the \"normalize_actions\" of SAC to True and using a subclass of MultiAgentEnv:\r\n\r\n> \r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-11-b623dc3951e1> in <module>\r\n>       1 # rllib trainer:\r\n> ----> 2 trainer = agents.sac.SACTrainer(env=\"my_multiagent_env\", config=config)\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py in __init__(self, config, env, logger_creator)\r\n>      81 \r\n>      82         def __init__(self, config=None, env=None, logger_creator=None):\r\n> ---> 83             Trainer.__init__(self, config, env, logger_creator)\r\n>      84 \r\n>      85         def _init(self, config, env_creator):\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py in __init__(self, config, env, logger_creator)\r\n>     395             logger_creator = default_logger_creator\r\n>     396 \r\n> --> 397         Trainable.__init__(self, config, logger_creator)\r\n>     398 \r\n>     399     @classmethod\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py in __init__(self, config, logger_creator)\r\n>     170 \r\n>     171         start_time = time.time()\r\n> --> 172         self._setup(copy.deepcopy(self.config))\r\n>     173         setup_time = time.time() - start_time\r\n>     174         if setup_time > SETUP_TIME_THRESHOLD:\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py in _setup(self, config)\r\n>     526 \r\n>     527         with get_scope():\r\n> --> 528             self._init(self.config, self.env_creator)\r\n>     529 \r\n>     530             # Evaluation related\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py in _init(self, config, env_creator)\r\n>     100             else:\r\n>     101                 self.workers = self._make_workers(env_creator, policy, config,\r\n> --> 102                                                   self.config[\"num_workers\"])\r\n>     103             if make_policy_optimizer:\r\n>     104                 self.optimizer = make_policy_optimizer(self.workers, config)\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py in _make_workers(self, env_creator, policy, config, num_workers)\r\n>     571             config,\r\n>     572             num_workers=num_workers,\r\n> --> 573             logdir=self.logdir)\r\n>     574 \r\n>     575     @DeveloperAPI\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py in __init__(self, env_creator, policy, trainer_config, num_workers, logdir, _setup)\r\n>      58             # Always create a local worker\r\n>      59             self._local_worker = self._make_worker(\r\n> ---> 60                 RolloutWorker, env_creator, policy, 0, self._local_config)\r\n>      61 \r\n>      62             # Create a number of remote workers\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py in _make_worker(self, cls, env_creator, policy, worker_index, config)\r\n>     214             seed=(config[\"seed\"] + worker_index)\r\n>     215             if config[\"seed\"] is not None else None,\r\n> --> 216             _fake_sampler=config.get(\"_fake_sampler\", False))\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py in __init__(self, env_creator, policy, policy_mapping_fn, policies_to_train, tf_session_creator, batch_steps, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_filter, clip_rewards, clip_actions, env_config, model_config, policy_config, worker_index, monitor_path, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, _fake_sampler)\r\n>     366                         self.policy_map) +\r\n>     367                     \"{} is not a subclass of BaseEnv, MultiAgentEnv or \"\r\n> --> 368                     \"ExternalMultiAgentEnv?\".format(self.env))\r\n>     369 \r\n>     370         self.filters = {\r\n> \r\n> ValueError: Have multiple policies {'policy1': <ray.rllib.policy.tf_policy_template.SACTFPolicy object at 0x7f5104319320>}, but the env <NormalizeActionWrapper<environments.rllib_multiagent_compatibility_wrapper.RllibMultiAgentEnv object at 0x7f51042f0fd0>> is not a subclass of BaseEnv, MultiAgentEnv or ExternalMultiAgentEnv?\r\n> \r\n\r\nWhen I set \"normalize_actions\" to False, this issue disappears.\r\nI am not sure whether this is a bug or if I am doing something wrong?\r\n\r\nRay version: 0.8.1\r\n\r\nRegards,\r\nYann\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7173/comments",
    "author": "yannbouteiller",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-15T01:19:29Z",
        "body": "It looks like NormalizeActions does not support multi-agent (this needs a better error message)."
      },
      {
        "user": "aiguoth",
        "created_at": "2020-06-16T07:24:08Z",
        "body": "hey, I can't use NormalizeAction in the MultiAgentEnv,too. I tried to add it to the MultiAgentEnv, but failed.  have you fixed it? @yannbouteiller "
      },
      {
        "user": "yannbouteiller",
        "created_at": "2020-06-16T14:35:26Z",
        "body": "> hey, I can't use NormalizeAction in the MultiAgentEnv,too. I tried to add it to the MultiAgentEnv, but failed. have you fixed it? @yannbouteiller\r\n\r\nNo sorry, I didn't retry since."
      },
      {
        "user": "mimoralea",
        "created_at": "2021-05-20T22:13:13Z",
        "body": "Same issue. Solved it by overriding the `__class__` method to return the rllib `MultiAgentEnv` class, like so:\r\n\r\n```\r\nimport gym\r\nfrom ray.rllib.env.multi_agent_env import MultiAgentEnv\r\n\r\n\r\nclass MyActionWrapper(gym.ActionWrapper):\r\n\r\n    def __init__(self, env):\r\n        super().__init__(env)\r\n\r\n    def action(self, act):\r\n        return act\r\n\r\n    @property\r\n    def __class__(self):\r\n        return MultiAgentEnv\r\n```"
      }
    ]
  },
  {
    "number": 7168,
    "title": "[tune] When would explore function be called at pbt?",
    "created_at": "2020-02-14T09:08:39Z",
    "closed_at": "2020-09-16T16:34:10Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7168",
    "body": "Hi, I'm using pbt for hyperparameter searching. I'm wondering when would the explore function be called. Does it only be called after the exploit function(after cloning the weight  and hyperparameters from good model to poor model)?  \r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7168/comments",
    "author": "sherylwang",
    "comments": [
      {
        "user": "waldroje",
        "created_at": "2020-02-14T20:16:21Z",
        "body": "One simple situation is to control interaction between hyperparameters, example... . ensure that \"train_batch_size\" >= \"sample_batch_size\", if those two parameters where being updated.... so it could look something like this.\r\n\r\n```\r\ndef explore(config):\r\n    if \"train_batch_size\" in config:\r\n        config[\"train_batch_size\"] = max(config[\"train_batch_size\"], config[\"sample_batch_size\"])\r\n```"
      },
      {
        "user": "sherylwang",
        "created_at": "2020-02-15T01:01:19Z",
        "body": "I haven't found the meaning of sample_batch_size in tune. Is this a original parameter of tune? @waldroje   \r\n"
      },
      {
        "user": "waldroje",
        "created_at": "2020-02-15T15:24:16Z",
        "body": " rllib.Trainer inherits fromtune.Trainable, and rllib.Trainer has default config, which you can view in the code... has a pretty good explanation for most of the default configuration, of which includes both sample_batch_size and train_batch_size."
      },
      {
        "user": "krfricke",
        "created_at": "2020-09-16T16:34:10Z",
        "body": "Hi @sherylwang, this is an old issue but I'll still answer it. Yes, the explore function is only called after exploiting. This makes sense because otherwise you would train two trials with the same parameters from the same checkpoint, and should get very similar results. Exploring after exploiting ensures that you end up with a different configuration to continue your training.\r\n\r\nThe rest of the parameters seem to be relate to rllib.\r\n\r\nFor constraints in the parameters you can use a custom explore function that handles these cases. E.g. the function defined by @waldroje.\r\n\r\nPlease feel free to re-open or open a new issue if you still have questions."
      }
    ]
  },
  {
    "number": 7161,
    "title": "[rllib] A way to add more info to the input_dict?",
    "created_at": "2020-02-14T00:03:10Z",
    "closed_at": "2020-02-14T05:10:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7161",
    "body": "What would be the easiest way to add `prev_dones` (as like `prev_rewards` and `prev_actions`) to the `input_dict`?\r\n\r\nThis is for the case where `lstm_use_prev_action_reward = True`.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7161/comments",
    "author": "alversafa",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-14T01:38:58Z",
        "body": "Hmm wouldn't you never see a prev_done since the episode would end?\r\n\r\nIf it's some special thing from the env you can put it in the observation space (use a Dict or Tuple space)."
      },
      {
        "user": "alversafa",
        "created_at": "2020-02-14T02:45:02Z",
        "body": "I was thinking for a setting where you chain multiple episodes to form a trial.\r\n\r\nHow would we use a dict? I wrapped the environment so that it returns an observation dict of the form `{'obs': ..., 'done': ...}`, however, I don't seem to be getting it on the `forward()` method of the model?"
      },
      {
        "user": "alversafa",
        "created_at": "2020-02-14T05:10:22Z",
        "body": "Made it possible by putting it in the observation vector."
      }
    ]
  },
  {
    "number": 7135,
    "title": "latest_dlami only in tree cc43c9c1a2434d377d232cb23a2abc425af06875",
    "created_at": "2020-02-12T09:58:53Z",
    "closed_at": "2020-03-05T22:58:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7135",
    "body": "Hi there, \r\njust reporting that the latest_dlami imageId is only implemented in cc43c9c1a2434d377d232cb23a2abc425af06875, not in 0.8.1 yet.\r\n\r\nRegards",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7135/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-05T22:58:14Z",
        "body": "Pip install `ray==0.8.2` should address your issue"
      }
    ]
  },
  {
    "number": 7083,
    "title": "[tune] Error handling checkpoint when using checkpoint_score_attr and keep_checkpoints_num",
    "created_at": "2020-02-07T09:39:07Z",
    "closed_at": "2020-02-07T10:41:52Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7083",
    "body": "Version info :\r\n- Ray 0.8.1\r\n- Python 3.7.3\r\n\r\nHi,\r\nI'm using tune with the Trainable interface. \r\nI'm trying to keep only the best checkpoint of each trial, thus this I've configured tune.run to do so :\r\n```\r\nanalysis = tune.run(\r\n        trainable,\r\n        scheduler=sched,\r\n        stop={\r\n            \"r2_mean\": 0.5,\r\n            \"training_iteration\": 3 if args.smoke_test else 30,\r\n        },\r\n        resources_per_trial={\r\n            \"cpu\": 6,\r\n            \"gpu\": int(args.use_gpu)\r\n        },\r\n        num_samples=1 if args.smoke_test else 70,\r\n        checkpoint_at_end=True,\r\n        checkpoint_freq=2,\r\n        checkpoint_score_attr=\"r2_mean\",\r\n        keep_checkpoints_num=1,\r\n        config=search_space,\r\n        local_dir=\"./ray_results_roberta\")\r\n```\r\nThe problem appears when the second checkpoint of a trial is about to be saved and the checkpoint manager attempt to remove the old checkpoint, I have the following error : \r\n```\r\n2020-02-07 09:07:00,011 ERROR ray_trial_executor.py:585 -- Trial FullTrainable_9f6d5e76: Error handling checkpoint /home/stageIA/datadrive/Projects/Kaggle/Google_qa/ray_results_roberta/FullTrainable/FullTrainable_9f6d5e76_2020-02-07_08-34-14q43cnxqu/checkpoint_3/model.pt\r\nh                                                                                                                                                        \r\nTraceback (most recent call last):                                                                                                                       \r\n  File \"/home/stageIA/datadrive/anaconda3/envs/transformers/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 582, in save               \r\n    trial.on_checkpoint(checkpoint)                                     \r\n  File \"/home/stageIA/datadrive/anaconda3/envs/transformers/lib/python3.7/site-packages/ray/tune/trial.py\", line 399, in on_checkpoint\r\n    self.checkpoint_manager.on_checkpoint(checkpoint)                   \r\n  File \"/home/stageIA/datadrive/anaconda3/envs/transformers/lib/python3.7/site-packages/ray/tune/checkpoint_manager.py\", line 114, in on_checkpoint\r\n    self.delete(worst)                                                  \r\n  File \"/home/stageIA/datadrive/anaconda3/envs/transformers/lib/python3.7/site-packages/ray/tune/trial.py\", line 91, in delete\r\n    shutil.rmtree(checkpoint_path)                                                             \r\n  File \"/home/stageIA/datadrive/anaconda3/envs/transformers/lib/python3.7/shutil.py\", line 491, in rmtree\r\n    _rmtree_safe_fd(fd, path, onerror)                                  \r\n  File \"/home/stageIA/datadrive/anaconda3/envs/transformers/lib/python3.7/shutil.py\", line 410, in _rmtree_safe_fd\r\n    onerror(os.scandir, path, sys.exc_info())                    \r\n  File \"/home/stageIA/datadrive/anaconda3/envs/transformers/lib/python3.7/shutil.py\", line 406, in _rmtree_safe_fd\r\n    with os.scandir(topfd) as scandir_it:                        \r\nNotADirectoryError: [Errno 20] Not a directory: '/home/stageIA/datadrive/Projects/Kaggle/Google_qa/ray_results_roberta/FullTrainable/FullTrainable_9f6d5e76_2020-02-07_08-34-14q43cnxqu/checkpoint_2/model.pth'\r\n```\r\nIndeed, the path points to a file and not a directory. \r\nIf i do'nt use checkpoint_score_attr and keep_checkpoints_num (only checkpoint_freq), there is no problem and all checkpoints are correctly saved.\r\nIs this a bug or did I miss something ?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7083/comments",
    "author": "Lawiss",
    "comments": [
      {
        "user": "ujvl",
        "created_at": "2020-02-07T10:19:02Z",
        "body": "Yeah this was fixed by #6758 so I would recommend just installing the latest wheels. "
      },
      {
        "user": "Lawiss",
        "created_at": "2020-02-07T10:41:52Z",
        "body": "Thanks, i'll try it!"
      }
    ]
  },
  {
    "number": 6986,
    "title": "[Question][rllib] Stochastic Game tensorboard separate rewards",
    "created_at": "2020-01-31T07:44:54Z",
    "closed_at": "2020-02-06T16:32:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/6986",
    "body": "### What is your question?\r\n\r\nI am designing a simple stochastic game wherein I have two agents. The first agent (the good guy) is rewarded according to some task. The second agent (the adversary) is rewarded negative proportional to the first. This is to encourage the adversary to screw up the good guy.\r\n\r\nAs a first pass, I just set the reward of the adversary equal to negative the reward of the good guy. This seems to cause some issue with tensorboard, however, because it looks like the rewards are summed together, which results is a reward of 0 for each iteration.\r\n\r\nIt would be nice to be able to visualize the rewards of each agent individually. I imagine that this would be very useful for other MARL scenarios, not just SG. Is this something that is possible?\r\n\r\nThank you!\r\n\r\npython3.7\r\ntensorflow2.1\r\nray0.8.1\r\nmac10.14\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6986/comments",
    "author": "rusu24edward",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-01T01:43:53Z",
        "body": "Are you using separate policies for each agent? You can view the individual policy scores under the `policy_X_reward_mean` etc keys."
      },
      {
        "user": "rusu24edward",
        "created_at": "2020-02-03T21:58:22Z",
        "body": "I am using separate policies for each agent. I'm not sure what you mean by `policy_x_reward_mean` key. Is that something in the tensorboard interface?"
      },
      {
        "user": "ericl",
        "created_at": "2020-02-03T22:00:29Z",
        "body": "Yep, you should be able to find those in tensorboard, `result.json`, or printed to stdout if you use the `-v` flag."
      },
      {
        "user": "rusu24edward",
        "created_at": "2020-02-06T16:32:01Z",
        "body": "Nice! I found them. For me, they are a few pages in stored as `policy_reward_mean/<policy_name>`\r\n\r\nThank you!"
      }
    ]
  },
  {
    "number": 6917,
    "title": "[rllib] How to compute custom metrics with episodes from evaluation stage?",
    "created_at": "2020-01-24T15:49:38Z",
    "closed_at": "2020-01-27T19:18:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/6917",
    "body": "### What is your question?\r\n\r\nWhen I say evaluation I mean the evaluation that is run based on the `evaluation_interval` configuration:\r\n\r\n```\r\n    # === Evaluation Settings ===\r\n    # Evaluate with every `evaluation_interval` training iterations.\r\n    # The evaluation stats will be reported under the \"evaluation\" metric key.\r\n    # Note that evaluation is currently not parallelized, and that for Ape-X\r\n    # metrics are already only reported for the lowest epsilon workers.\r\n    \"evaluation_interval\": None,\r\n    # Number of episodes to run per evaluation period.\r\n    \"evaluation_num_episodes\": 10,\r\n    # Extra arguments to pass to evaluation workers.\r\n    # Typical usage is to pass extra args to evaluation env creator\r\n    # and to disable exploration by computing deterministic actions\r\n    # TODO(kismuz): implement determ. actions and include relevant keys hints\r\n    \"evaluation_config\": {},\r\n```\r\n\r\nI'm running a MultiAgentEnvironment and want to access the episodes from the evaluation step to compute a custom metric score that will appear in tensorboard.\r\n\r\nSpecifically, I have N policies that are trained in a 2 player MultiAgentEnv. I want to run evaluation episodes between all the policies, and then based on the results of ALL the episodes from the evaluation, compute an ELO score for each of the policies using bayesian elo. I'd like this metric to be reported in tensorboard.\r\n\r\nI already have code to compute ELO given the results of an episode, so I just need to figure out how to get access to the episodes and then how to put the metric in tensorboard. How do I go about this?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nRay version: 0.8.0\r\nPython version: 3.6.0",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6917/comments",
    "author": "coreylowman",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-01-24T21:14:50Z",
        "body": "The only way to do this currently would be overriding the _evaluate() method of the Trainer class (you can check out the current impl to see what it does)."
      }
    ]
  },
  {
    "number": 6887,
    "title": "[Tune] Does resources_per_trial={'gpu':4} in tune equal to  tf.distribute.MirroredStrategy(\"4 GPUs\")",
    "created_at": "2020-01-22T08:52:05Z",
    "closed_at": "2020-01-22T18:55:45Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/6887",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nRay: 0.8.0dev\r\npython: 3.5\r\nTensorflow 2.1.0\r\nUbuntu 16.04\r\n\r\nI wonder if resources_per_trial={\"gpu\":4} with num_samples = 1 using tune equal to tf.distribute.MirroredStrategy(\"4 GPUs\") with a custom training loop without tune?  I am trying to count the time cost under varied GPUs number but get different results from them.\r\n\r\nCustom training loop without tune: more GPUs, less time/epoch cost\r\ntune API: more GPUs, almost same time cost (even increating a little more time/epoch)\r\n\r\nWhy??",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6887/comments",
    "author": "ProNoobLi",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-01-22T18:55:45Z",
        "body": "The GPUs are just assigned via CUDA_VISIBLE_DEVICES, it doesn't actually do anything unless your code is using them (for example, using tf.distributed.MirroredStrategy)."
      },
      {
        "user": "ProNoobLi",
        "created_at": "2020-01-23T03:06:04Z",
        "body": "> for example, using tf.distributed.MirroredStrategy\r\n\r\nCan we combine MirroredStrategy with Tune?"
      }
    ]
  },
  {
    "number": 6799,
    "title": "[tune] how to deal with data that is not compatible with pickle?",
    "created_at": "2020-01-15T15:12:18Z",
    "closed_at": "2020-03-31T11:52:04Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/6799",
    "body": "When I call `ray.tune.run`, I get this:\r\n\r\n<details>\r\n    <summary>The traceback</summary>\r\n\r\n    ```python\r\n    ~/repositories/tab/lib/pipelines/tune.py in run_tuning(pipeline_name, config)\r\n         62     analysis = ray.tune.run(\r\n         63         trial, search_alg=optimization_algorithm,\r\n    ---> 64         **config['ray_tune_run_parameters']\r\n         65     )\r\n         66     return analysis\r\n\r\n    ~/miniconda3/envs/tab/lib/python3.7/site-packages/ray/tune/tune.py in run(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\r\n        231     for i, exp in enumerate(experiments):\r\n        232         if not isinstance(exp, Experiment):\r\n    --> 233             run_identifier = Experiment.register_if_needed(exp)\r\n        234             experiments[i] = Experiment(\r\n        235                 name=name,\r\n\r\n    ~/miniconda3/envs/tab/lib/python3.7/site-packages/ray/tune/experiment.py in register_if_needed(cls, run_object)\r\n        191                 logger.warning(\r\n        192                     \"No name detected on trainable. Using {}.\".format(name))\r\n    --> 193             register_trainable(name, run_object)\r\n        194             return name\r\n        195         else:\r\n\r\n    ~/miniconda3/envs/tab/lib/python3.7/site-packages/ray/tune/registry.py in register_trainable(name, trainable)\r\n         69         raise TypeError(\"Second argument must be convertable to Trainable\",\r\n         70                         trainable)\r\n    ---> 71     _global_registry.register(TRAINABLE_CLASS, name, trainable)\r\n         72 \r\n         73 \r\n\r\n    ~/miniconda3/envs/tab/lib/python3.7/site-packages/ray/tune/registry.py in register(self, category, key, value)\r\n        108             raise TuneError(\"Unknown category {} not among {}\".format(\r\n        109                 category, KNOWN_CATEGORIES))\r\n    --> 110         self._to_flush[(category, key)] = pickle.dumps(value)\r\n        111         if _internal_kv_initialized():\r\n        112             self.flush_values()\r\n\r\n    ~/miniconda3/envs/tab/lib/python3.7/site-packages/ray/cloudpickle/cloudpickle_fast.py in dumps(obj, protocol, buffer_callback)\r\n         66     with io.BytesIO() as file:\r\n         67         cp = CloudPickler(file, protocol=protocol, buffer_callback=buffer_callback)\r\n    ---> 68         cp.dump(obj)\r\n         69         return file.getvalue()\r\n         70 \r\n\r\n    ~/miniconda3/envs/tab/lib/python3.7/site-packages/ray/cloudpickle/cloudpickle_fast.py in dump(self, obj)\r\n        555     def dump(self, obj):\r\n        556         try:\r\n    --> 557             return Pickler.dump(self, obj)\r\n        558         except RuntimeError as e:\r\n        559             if \"recursion\" in e.args[0]:\r\n\r\n    ~/miniconda3/envs/tab/lib/python3.7/site-packages/catboost/./_catboost.so in _catboost._PoolBase.__reduce_cython__()\r\n\r\n    TypeError: no default __reduce__ due to non-trivial __cinit__\r\n    ```\r\n</details>\r\n\r\nWhat workarounds are available for such cases? Can I force ray to run in the main thread, if I don't need parallelism?\r\n\r\nPython: 3.7.5\r\nRay: 0.8.0\r\nOS: Ubuntu 18.04.3",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6799/comments",
    "author": "Yura52",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-01-15T19:03:59Z",
        "body": "Can you post a script for reproduction? It seems like you’re instantiating a model before it is fed into Tune."
      },
      {
        "user": "Yura52",
        "created_at": "2020-01-16T07:59:34Z",
        "body": "> Can you post a script for reproduction? It seems like you’re instantiating a model before it is fed into Tune.\r\n\r\nYes, I create this non-picklable object before calling `tune.run`. I think, #3373 might be relevant, because my issue is almost the same and it is caused by a type from the same library.\r\nMy script is big and messy. If my comments are not sufficient, let me know, I will try to minimize my code and publish it."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-03-27T23:31:31Z",
        "body": "Sorry for the slow reply. You can try using `ray.init(local_mode=True)` to force Tune to run on the main thread."
      }
    ]
  },
  {
    "number": 6794,
    "title": "Redis can not establish connection to another node",
    "created_at": "2020-01-14T23:17:44Z",
    "closed_at": "2020-03-19T21:42:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/6794",
    "body": "When I run the code: \r\n```python\r\nimport time\r\n\r\n@ray.remote\r\ndef f():\r\n    time.sleep(0.01)\r\n    return ray.services.get_node_ip_address()\r\n\r\n\r\nset(ray.get([f.remote() for _ in range(1000)]))\r\n```\r\n It can print the ip of the node that execte this code.  But I have start ray on another node, and it seems good. But why it didn't the ip of another node? And when I run `ray.init(redis_address=\"head node ip: port\")` , it output like the follow:\r\n```\r\n(pid=raylet, ip=152.14.93.144) F0114 18:12:17.869905 324126 redis_context.cc:215] Could not establish connection to redis head ip:39902 (context.err = 1)\r\n(pid=raylet, ip=152.14.93.144) *** Check failure stack trace: ***\r\n(pid=raylet, ip=152.14.93.144)     @           0x8403fa  google::LogMessage::Fail()\r\n(pid=raylet, ip=152.14.93.144)     @           0x8417f3  google::LogMessage::SendToLog()\r\n(pid=raylet, ip=152.14.93.144)     @           0x840122  google::LogMessage::Flush()\r\n(pid=raylet, ip=152.14.93.144)     @           0x840311  google::LogMessage::~LogMessage()\r\n(pid=raylet, ip=152.14.93.144)     @           0x5639a2  ray::RayLog::~RayLog()\r\n(pid=raylet, ip=152.14.93.144)     @           0x4f5fb8  ray::gcs::ConnectWithRetries<>()\r\n(pid=raylet, ip=152.14.93.144)     @           0x4f658e  ray::gcs::RedisContext::Connect()\r\n(pid=raylet, ip=152.14.93.144)     @           0x4bc4ef  ray::gcs::RedisGcsClient::Connect()\r\n(pid=raylet, ip=152.14.93.144)     @           0x410a0c  main\r\n(pid=raylet, ip=152.14.93.144)     @     0x7fa050025830  __libc_start_main\r\n(pid=raylet, ip=152.14.93.144)     @           0x421d81  (unknown) \r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6794/comments",
    "author": "dxu23nc",
    "comments": [
      {
        "user": "edoakes",
        "created_at": "2020-01-15T23:54:04Z",
        "body": "What environment are you running in? Is redis accessible from the machine you're trying to connect from? You can try testing this using `redis-cli`."
      },
      {
        "user": "dxu23nc",
        "created_at": "2020-01-16T14:05:11Z",
        "body": "It is ubuntu 16.04. I run redis-cli. It reported \"not found  this command\" . But I can start the redis-server by run ray start....  ."
      }
    ]
  },
  {
    "number": 6779,
    "title": "Using local mode with the Ray Object store",
    "created_at": "2020-01-13T09:33:28Z",
    "closed_at": "2020-10-30T17:47:55Z",
    "labels": [
      "question",
      "P3"
    ],
    "url": "https://github.com/ray-project/ray/issues/6779",
    "body": "I am unsure what the right way to use local mode is when making use of the Ray object store. \r\n\r\nIn my program, I have a custom Tune `Trainable` class and I use the object store to pass the training data around to each of the workers (I put the training data into the object store before calling `tune.run`, and then pass the returned reference as a `config` parameter to the `Trainable`'s). \r\n\r\nI've put a modification of Tune's BOHB example below to show what I am doing. Now, when I use local mode it looks like Ray generates a dummy Object ID. Calling `get_pinned_object` returns a KeyError along the lines of \r\n```\r\nKeyError: 'Value for LocalModeObjectID(...) not found'\r\n```\r\n\r\nIdeally, I was hoping that I'd still get the serialised object back in local mode so that I can test my entire application. \r\n\r\nIs the above the intended effect of using local mode? If so, is there any way in which I can run Ray/Tune code in a serial way so that my entire application that uses the object store would still work? \r\n\r\n## Example\r\n\r\n```python\r\nimport json\r\nimport os\r\n\r\nimport numpy as np\r\nimport ConfigSpace as CS\r\nimport ray\r\nfrom ray.tune import Trainable, run\r\nfrom ray.tune.schedulers.hb_bohb import HyperBandForBOHB\r\nfrom ray.tune.suggest.bohb import TuneBOHB\r\nfrom ray.tune.util import pin_in_object_store, get_pinned_object\r\n\r\nclass MyTrainableClass(Trainable):\r\n    \"\"\"Example agent whose learning curve is a random sigmoid.\r\n\r\n    The dummy hyperparameters \"width\" and \"height\" determine the slope and\r\n    maximum reward value reached.\r\n    \"\"\"\r\n\r\n    def _setup(self, config):\r\n        self.timestep = 0\r\n\r\n        # Below works when local_mode = False, but \r\n        # returns a KeyError when local_mode = True\r\n        self.X = get_pinned_object(config[\"X\"])\r\n\r\n    def _train(self):\r\n        self.timestep += 1\r\n        v = np.tanh(float(self.timestep) / self.config.get(\"width\", 1))\r\n        v *= self.config.get(\"height\", 1)\r\n\r\n        # Here we use `episode_reward_mean`, but you can also report other\r\n        # objectives such as loss or accuracy.\r\n        return {\"episode_reward_mean\": v}\r\n\r\n    def _save(self, checkpoint_dir):\r\n        path = os.path.join(checkpoint_dir, \"checkpoint\")\r\n        with open(path, \"w\") as f:\r\n            f.write(json.dumps({\"timestep\": self.timestep}))\r\n        return path\r\n\r\n    def _restore(self, checkpoint_path):\r\n        with open(checkpoint_path) as f:\r\n            self.timestep = json.loads(f.read())[\"timestep\"]\r\n\r\nif __name__ == \"__main__\":\r\n    ray.init(local_mode=True)\r\n\r\n    # pin a dummy object into the object store\r\n    X = pin_in_object_store(10)\r\n\r\n    # BOHB uses ConfigSpace for their hyperparameter search space\r\n    config_space = CS.ConfigurationSpace()\r\n    config_space.add_hyperparameter(\r\n        CS.UniformFloatHyperparameter(\"height\", lower=10, upper=100))\r\n    config_space.add_hyperparameter(\r\n        CS.UniformFloatHyperparameter(\"width\", lower=0, upper=100))\r\n\r\n    experiment_metrics = dict(metric=\"episode_reward_mean\", mode=\"max\")\r\n    bohb_hyperband = HyperBandForBOHB(\r\n        time_attr=\"training_iteration\",\r\n        max_t=5,\r\n        reduction_factor=4,\r\n        **experiment_metrics)\r\n    bohb_search = TuneBOHB(\r\n        config_space, max_concurrent=4, **experiment_metrics)\r\n\r\n    run(MyTrainableClass,\r\n        name=\"bohb_test\",\r\n        scheduler=bohb_hyperband,\r\n        search_alg=bohb_search,\r\n        num_samples=32,\r\n        stop={\"training_iteration\": 200},\r\n        config={\"X\": X})\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6779/comments",
    "author": "thuijskens",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-01-13T21:02:43Z",
        "body": "Ah; maybe try setting `ray.init(num_cpus=1)` for something close to a serial-mode application."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-10-30T17:47:55Z",
        "body": "BTW I think this should work now. "
      }
    ]
  },
  {
    "number": 6719,
    "title": "what does ray use under the hood",
    "created_at": "2020-01-06T19:33:20Z",
    "closed_at": "2020-01-15T23:55:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/6719",
    "body": "What does ray use under the hood for communication? Is it grpc?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6719/comments",
    "author": "sebastiangonsal",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-01-07T04:32:24Z",
        "body": "Yep, gRPC for most things."
      }
    ]
  },
  {
    "number": 6707,
    "title": "Exception: Version mismatch. anybody help me?",
    "created_at": "2020-01-06T02:38:45Z",
    "closed_at": "2020-03-05T23:11:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/6707",
    "body": "when I using cond env,the error occurs like the following:\r\n(keras-py36) ...@gnode005:/.$ python\r\nPython 3.6.7 | packaged by conda-forge | (default, Feb 28 2019, 09:07:38) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import ray\r\n>>> ray.init(\"168.168.2.131:6379\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/ghpccgio/home/xiaoyouqing/.local/lib/python3.6/site-packages/ray/worker.py\", line 801, in init\r\n    if _internal_config else {})\r\n  File \"/ghpccgio/home/xiaoyouqing/.local/lib/python3.6/site-packages/ray/worker.py\", line 1158, in connect\r\n    raise e\r\n  File \"/ghpccgio/home/xiaoyouqing/.local/lib/python3.6/site-packages/ray/worker.py\", line 1155, in connect\r\n    ray.services.check_version_info(worker.redis_client)\r\n  File \"/ghpccgio/home/xiaoyouqing/.local/lib/python3.6/site-packages/ray/services.py\", line 574, in check_version_info\r\n    raise Exception(error_message)\r\nException: Version mismatch: The cluster was started with:\r\n    Ray: 0.8.0\r\n    Python: 3.6.5\r\n    Pyarrow: 0.14.0.RAY\r\nThis process on node 168.168.2.131 was started with:\r\n    Ray: 0.8.0\r\n    Python: 3.6.7\r\n    Pyarrow: 0.14.0.RAY\r\n\r\n\r\nsystem env is:\r\npci@pci:~$ python\r\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> exit()\r\npci@pci:~$ which python\r\n/home/pci/anaconda3/bin/python\r\n\r\n\r\ncould anybody help me??\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6707/comments",
    "author": "gogo03",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-01-06T18:08:45Z",
        "body": "It looks like you didn't start Ray in the conda environment?"
      },
      {
        "user": "ijrsvt",
        "created_at": "2020-03-05T23:11:34Z",
        "body": "Please reopen if you are still having issues."
      },
      {
        "user": "chuckhope",
        "created_at": "2021-11-25T16:34:26Z",
        "body": "> It looks like you didn't start Ray in the conda environment?\r\n\r\nHi there, I use ray commit to run the .py file. I have changed the env to my conda but still, the command  \" ray commit\" uses default python to connect to the cluster which causes the mismatch error. Do you have any ideas? Thanks a lot. I think it may be related to the \"ray.init()\" in my .py file."
      }
    ]
  },
  {
    "number": 6609,
    "title": "autoscalar, webui, python2, python3 confusion!",
    "created_at": "2019-12-26T16:23:10Z",
    "closed_at": "2020-03-21T23:22:48Z",
    "labels": [
      "question",
      "P3"
    ],
    "url": "https://github.com/ray-project/ray/issues/6609",
    "body": "using python 3.7.3, ubuntu 18.04.3 \r\n\r\n### What is your question?\r\nHow do I get the webui to work using the local autoscaler yaml file?\r\n\r\nIt appears that the webui requires python3 however, the local autoscaler starts a python2 process.\r\n\r\nI have tried to add --include-webui to the local example-full.yaml  on both the head and worker sections for start up.\r\nIf I start a process from the command line and then open a REPL and try to join the cluster using\r\nray.init( redis_address =\"\", include_webui=True)  the webui part is ignored.\r\n\r\nthanks john\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6609/comments",
    "author": "johncleveland",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-21T23:22:48Z",
        "body": "Sorry for the late response.\r\n\r\nWe've deprecated Python2. To use the Webui with the autoscaler, you should be able to do `ray dashboard [yaml]`.\r\n\r\nThe web UI should now start automatically.\r\n\r\nFeel free to reopen this issue if you have any more questions!"
      }
    ]
  },
  {
    "number": 5717,
    "title": "[rllib] Is it possible to add extra inputs to the model?",
    "created_at": "2019-09-17T02:56:42Z",
    "closed_at": "2020-02-11T20:48:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/5717",
    "body": "The current implementation uses the `observation` as the input to the RL agent's policy's model, which seems to be hardly modified.\r\n\r\nI am trying to add extra input to the model (for example some extra information as the second stream of inputs), but find the codes is so sophisticated that I don't know how to do.\r\n\r\nIt's easy to build the model with multiple input placeholders. But the question is how to let the `policy.compute_actions` or `agent.compute_action` to input such data.\r\n\r\nThanks a lot for your reply!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/5717/comments",
    "author": "pengzhenghao",
    "comments": [
      {
        "user": "federicofontana",
        "created_at": "2019-09-17T08:42:25Z",
        "body": "At least partially related: it is not clear from where to retrieve the required (recurrent) `state` when using `policy.compute_actions` or `policy.compute_single_action` in combination with recurrent networks."
      },
      {
        "user": "ericl",
        "created_at": "2019-09-17T17:19:34Z",
        "body": "What are you trying to do? Usually, you can put this information in the observation. Observations can be arbitrarily complex with Dict spaces. For states, the new states are returned by the compute action call."
      },
      {
        "user": "LucCADORET",
        "created_at": "2020-02-11T13:53:24Z",
        "body": "Same as @FedericoFontana, I have a trained agent using lstm and I want to load one of its checkpoint to run it against a test env. When calling the `model.compute_action` method, I get the error `ValueError: Must pass in RNN state batches for placeholders [<tf.Tensor 'default_policy/Placeholder:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 256) dtype=float32>], got []`\r\n\r\nHere's my code:\r\n\r\n```\r\n    model.restore(\r\n        '/home/ray_results/PPO/PPO_CustomEnv_ab41bcf8_0_2020-02-11_14-09-00h8cckrwu/checkpoint_50/checkpoint-50')\r\n\r\n    env = CustomEnv(\r\n        data=df_test, flatten_obs=True, lag_window=1)\r\n    obs = env.reset()\r\n    for _ in range(1000):\r\n        action = model.compute_action(obs, prev_action=0, prev_reward=0) # This line throws the error\r\n        obs, reward, done, info = env.step(action)\r\n        env.render()\r\n```\r\nWhich hidden state is supposed to be put here, since it's the first run ?\r\n\r\nI'm using TF 1.15."
      },
      {
        "user": "ericl",
        "created_at": "2020-02-11T20:48:32Z",
        "body": "You need to specify the state argument of compute actions. The policy has a method to get the initial state, see the rollout.py script."
      }
    ]
  },
  {
    "number": 5384,
    "title": "Is the performance of impala in ray consistent of the original paper? ",
    "created_at": "2019-08-06T08:21:10Z",
    "closed_at": "2020-03-19T21:57:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/5384",
    "body": "\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/5384/comments",
    "author": "sufeidechabei",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-08-07T03:28:54Z",
        "body": "We never evaluate with a deep resnet model, so it's hard to know for sure. But it is faster at solving Atari than any other algorithm and the sampling throughput is about on par or faster."
      },
      {
        "user": "edoakes",
        "created_at": "2020-03-19T21:57:25Z",
        "body": "Automatically closing stale issue. Please re-open if still relevant."
      }
    ]
  },
  {
    "number": 5359,
    "title": "Cannot rollout APEX (dqn) Unknown config parameter `local_evaluator_tf_session_args` ",
    "created_at": "2019-08-02T20:54:10Z",
    "closed_at": "2019-08-07T13:39:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/5359",
    "body": "I can't see to play the checkpoint made via APEX (dqn).\r\n\r\nHere is my script:\r\n```\r\nrllib rollout apex/checkpoint_51/checkpoint-51 --run APEX --env BreakoutNoFrameskip-v4 --steps 10000\r\n```\r\nand here is the error:\r\n\r\n```\r\n2019-08-02 16:52:27,807\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\r\nTraceback (most recent call last):\r\n  File \"/Users/andriy/miniconda3/bin/rllib\", line 10, in <module>\r\n    sys.exit(cli())\r\n  File \"/Users/andriy/miniconda3/lib/python3.7/site-packages/ray/rllib/scripts.py\", line 40, in cli\r\n    rollout.run(options, rollout_parser)\r\n  File \"/Users/andriy/miniconda3/lib/python3.7/site-packages/ray/rllib/rollout.py\", line 103, in run\r\n    agent = cls(env=args.env, config=config)\r\n  File \"/Users/andriy/miniconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 87, in __init__\r\n    Trainer.__init__(self, config, env, logger_creator)\r\n  File \"/Users/andriy/miniconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 328, in __init__\r\n    Trainable.__init__(self, config, logger_creator)\r\n  File \"/Users/andriy/miniconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 99, in __init__\r\n    self._setup(copy.deepcopy(self.config))\r\n  File \"/Users/andriy/miniconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 429, in _setup\r\n    self._allow_unknown_subkeys)\r\n  File \"/Users/andriy/miniconda3/lib/python3.7/site-packages/ray/tune/util.py\", line 172, in deep_update\r\n    raise Exception(\"Unknown config parameter `{}` \".format(k))\r\nException: Unknown config parameter `local_evaluator_tf_session_args` \r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/5359/comments",
    "author": "drozzy",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-08-05T02:46:52Z",
        "body": "I think this is because you're using a checkpoint saved from an older version of rllib. We don't guarantee compatibility there."
      },
      {
        "user": "drozzy",
        "created_at": "2019-08-07T13:39:44Z",
        "body": "YES! You were right.\r\nThank you so much!"
      }
    ]
  },
  {
    "number": 5194,
    "title": "[rllib] how does Ray seed different actors",
    "created_at": "2019-07-14T14:36:47Z",
    "closed_at": "2019-07-18T07:33:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/5194",
    "body": "### Describe the problem\r\n<!-- Describe the problem clearly here. -->\r\nI noticed that `worker_index` is used to seed environment of each actor. However, I cannot find any invocation of `seed()` method (defined in utils/seed.py). I also try to grep `tf.set_random_seed()` and got only utils/seed.py. So how does Ray seed each actor? Without the different seeds, their noise (for exploration) would be identical and lose the advantage of actor(s)-learner architecture.\r\n\r\n### Source code / logs\r\n<!-- Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. -->\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/5194/comments",
    "author": "joneswong",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-07-14T21:22:23Z",
        "body": "There is actually no seed, so it is completely random by default."
      },
      {
        "user": "joneswong",
        "created_at": "2019-07-15T02:08:08Z",
        "body": "> There is actually no seed, so it is completely random by default.\r\n\r\nWhen there are many actors, some of them that are instantiated at the same time might behave in the same way due to the identical initial random seed (i.e., time). Although network parameters are synced with the learner periodically, the OU/Gaussian noise samples might be identical which waste these actors. I think we need a mechanism to seed the actors."
      },
      {
        "user": "ericl",
        "created_at": "2019-07-15T02:37:45Z",
        "body": "That makes sense... we could potentially seed by worker_index plus some random seed generated at the trainer."
      }
    ]
  },
  {
    "number": 5059,
    "title": "PBT  update config in custom explore function",
    "created_at": "2019-06-28T09:24:47Z",
    "closed_at": "2019-06-29T01:50:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/5059",
    "body": "I am new to ray and tune. I want to use PBT with a dynamic config. For example,\r\n\r\n```\r\ndef explore(config):\r\n    # here config['varA'] = None not something as in TrainMNIST._setup\r\n    return config\r\n\r\nclass TrainMNIST(Trainable):\r\n    def _setup(self, config):\r\n        config['varA'] = something\r\n        self.config.update(config)\r\n\r\npbt = PopulationBasedTraining(\r\n        ...,        \r\n        custom_explore_fn=explore)\r\n\r\ntune.run(\r\n        TrainMNIST,\r\n        name=\"exp\",\r\n        scheduler=pbt,\r\n        stop={\r\n            \"test_acc\": 0.99,\r\n            \"training_iteration\": 100,\r\n        },\r\n        resources_per_trial={\r\n            \"cpu\": 2,\r\n            \"gpu\": 0.25,\r\n        },\r\n        **{\"config\":\r\n            { \"args\": vars(args),\r\n              \"varA\": None,\r\n              }\r\n        }\r\n```\r\n\r\nHow can I explore with config['varA'] = something?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/5059/comments",
    "author": "heurainbow",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-06-28T18:36:35Z",
        "body": "Yes, you can do anything to the config in the explore function. I don't think changing it in setup makes sense though."
      },
      {
        "user": "heurainbow",
        "created_at": "2019-06-29T00:48:22Z",
        "body": "> Yes, you can do anything to the config in the explore function. I don't think changing it in setup makes sense though.\r\n\r\nSorry for the misleading. I meant that config['varA']=None when passing into tune.run. Then, I change config['varA']=1 in TrainMNIST._setup. I want to explore with config['varA']=1. However, in explore I get \r\nconfig['varA']=None.\r\n\r\n```\r\nMNIST_ROOT = path/to/mnist\r\nLOCAL_DIR = path/to/results\r\nperturbation_interval = 1\r\n\r\n\r\ndef explore(config):\r\n    \"\"\"Custom explore function.\r\n\r\n    Args:\r\n      config: dictionary containing ray config params.\r\n\r\n    Returns:\r\n      Copy of config with modified augmentation policy.\r\n    \"\"\"\r\n    print(config)\r\n    return config\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\r\n        self.fc = nn.Linear(192, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 3))\r\n        x = x.view(-1, 192)\r\n        x = self.fc(x)\r\n        return F.log_softmax(x, dim=1)\r\n\r\n\r\nclass TrainMNIST(Trainable):\r\n    def _setup(self, config):\r\n        args = config.pop(\"args\")\r\n        args.update(config)\r\n\r\n        self.is_cuda = torch.cuda.is_available()\r\n        torch.manual_seed(args['seed'])\r\n        if self.is_cuda:\r\n            torch.cuda.manual_seed(args['seed'])\r\n\r\n        self.kwargs = {\"num_workers\": 4, \"pin_memory\": True} if self.is_cuda else {}\r\n        mnist_transforms = transforms.Compose(\r\n            [transforms.ToTensor(),\r\n             transforms.Normalize((0.1307,), (0.3081,))])\r\n\r\n        self.trainset = datasets.MNIST(MNIST_ROOT, train=True, download=False, transform=mnist_transforms)\r\n        self.testset = datasets.MNIST(MNIST_ROOT, train=False, download=False, transform=mnist_transforms)\r\n        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=256, shuffle=True, **self.kwargs)\r\n        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=256, shuffle=True, **self.kwargs)\r\n\r\n        self.model = Net()\r\n        if self.is_cuda:\r\n            self.model.cuda()\r\n\r\n        self.optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\r\n        self.scheduler = MultiStepLR(self.optimizer, milestones=[30, 60, 90], gamma=0.2)\r\n        self.args = args\r\n\r\n        if config['a'] is None:\r\n            config['a'] = 1111111\r\n            self.config.update(config)\r\n\r\n    def _train_iteration(self):\r\n        self.model.train()\r\n        for batch_idx, (data, target) in enumerate(self.train_loader):\r\n            if self.is_cuda:\r\n                data, target = data.cuda(), target.cuda()\r\n            self.optimizer.zero_grad()\r\n            output = self.model(data)\r\n            loss = F.nll_loss(output, target)\r\n            loss.backward()\r\n            self.optimizer.step()\r\n\r\n    def _test(self):\r\n        self.model.eval()\r\n        correct = 0\r\n        with torch.no_grad():\r\n            for data, target in self.test_loader:\r\n                if self.is_cuda:\r\n                    data, target = data.cuda(), target.cuda()\r\n                output = self.model(data)\r\n                # get the index of the max log-probability\r\n                pred = output.argmax(dim=1, keepdim=True)\r\n                correct += pred.eq(\r\n                    target.data.view_as(pred)).long().cpu().sum()\r\n\r\n        accuracy = correct.item() / len(self.test_loader.dataset)\r\n        return {\"test_acc\": accuracy}\r\n\r\n    def _train(self):\r\n        self._train_iteration()\r\n        self.scheduler.step(epoch=self._iteration)\r\n        return self._test()\r\n\r\n    def _save(self, checkpoint_dir):\r\n        checkpoint_path = os.path.join(checkpoint_dir, \"model.pth\")\r\n        torch.save(self.model.state_dict(), checkpoint_path)\r\n        return checkpoint_path\r\n\r\n    def _restore(self, checkpoint_path):\r\n        self.model.load_state_dict(torch.load(checkpoint_path))\r\n\r\n    def reset_config(self, new_config):\r\n        self.config.update(new_config)\r\n        return True\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\r\n    parser.add_argument(\r\n        \"--ray-redis-address\",\r\n        help=\"Address of Ray cluster for seamless distributed execution.\")\r\n    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\")\r\n\r\n    args = parser.parse_args()\r\n    if args.ray_redis_address:\r\n        ray.init(redis_address=args.ray_redis_address)\r\n\r\n    pbt = PopulationBasedTraining(\r\n        time_attr=\"training_iteration\",\r\n        reward_attr=\"test_acc\",\r\n        perturbation_interval=perturbation_interval,\r\n        custom_explore_fn=explore,\r\n        log_config=True)\r\n\r\n    tune.run(\r\n        TrainMNIST,\r\n        name=\"exp\",\r\n        scheduler=pbt,\r\n        stop={\r\n            \"test_acc\": 0.95,\r\n            \"training_iteration\": 100,\r\n        },\r\n        resources_per_trial={\r\n            \"cpu\": 2,\r\n            \"gpu\": 0.25,\r\n        },\r\n        reuse_actors =True,\r\n        num_samples = 2,\r\n        checkpoint_freq = 0,\r\n        local_dir = LOCAL_DIR,\r\n        **{\"config\":\r\n            { \"args\": vars(args),\r\n              \"a\": None,\r\n              }\r\n        }\r\n    )\r\n```"
      },
      {
        "user": "ericl",
        "created_at": "2019-06-29T01:35:15Z",
        "body": "Hm, I think this is to be expected given how PBT internally tracks the config. The copy of the config it gives your trial is not connected to the original since the trial may be running on a different machine. Why not set a: 1 in the experiment config instead?"
      },
      {
        "user": "heurainbow",
        "created_at": "2019-06-29T01:45:49Z",
        "body": "I found a way to work aound this by adding trial.config.update(result['config']) in on_trial_result. I use this to make the explore function adapt to each model's performance. Say, for better performance I use small step for hyperparams.\r\n\r\nStill, I think synchronize config is essential for robustness.  \r\n```\r\n    def on_trial_result(self, trial_runner, trial, result):\r\n        trial.config.update(result['config'])\r\n        ...\r\n```"
      },
      {
        "user": "heurainbow",
        "created_at": "2019-06-29T01:50:44Z",
        "body": "update config when getting results"
      }
    ]
  },
  {
    "number": 5017,
    "title": "Confusion about action embedding in parametric_action_cartpole.py",
    "created_at": "2019-06-22T03:52:31Z",
    "closed_at": "2019-06-26T08:53:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/5017",
    "body": "### Describe the problem\r\n<!-- Describe the problem clearly here. -->\r\n\r\nDear all,\r\n  I want to ask about hot to **make the action embedding trainable,** instead of making them initialized randomly. Since my valid action space is large, should I put them into the model to learn them,  instead of the env?  If they are put in the env, when I set **num_workers** > 0, there would be  multiple envs. Under this situation, are the **action embeddings are different across the different envs**? They should be the same, right?\r\n\r\nThanks in advance.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/5017/comments",
    "author": "yangysc",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-06-26T03:47:47Z",
        "body": "I think one solution here is to make the embedding network part of the model itself. That way, synchronization is taken care of automatically.\r\n\r\nOne way this could work is if the action are represented in the environment as just a number. Then, in the model there can be an embedding layer that transforms this into the action embedding."
      },
      {
        "user": "yangysc",
        "created_at": "2019-06-26T08:53:01Z",
        "body": "Thanks for your help.  I think that is a nice solution."
      }
    ]
  },
  {
    "number": 4886,
    "title": "No GPU usage on training in Google Colab",
    "created_at": "2019-05-29T12:19:59Z",
    "closed_at": "2019-10-24T20:13:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/4886",
    "body": "I try to launch simple reinforcement learning example on google colab  (GPU mode)\r\n```\r\n!pip install ray\r\n!pip install ray[debug]\r\n!pip install lz4\r\n!pip install setproctitle\r\n!pip install tensorflow-gpu\r\n!pip install psutil\r\n!pip install gputil\r\n```\r\nand script\r\n```python\r\nimport ray\r\nimport ray.rllib.agents.a3c as a3c\r\nfrom ray.rllib.models import ModelCatalog, Model\r\nfrom ray.tune.registry import register_env\r\nimport gym\r\nfrom gym import RewardWrapper\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, Sequential\r\nimport psutil\r\nimport GPUtil\r\n\r\nclass ScaleReward(RewardWrapper):\r\n    def reward(self, reward):\r\n        return reward/100\r\nray.shutdown()      \r\nray.init(ignore_reinit_error=True)\r\n\r\nconfig = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \r\nsess = tf.Session(config=config) \r\ntf.keras.backend.set_session(sess)\r\n\r\n\r\nclass CartpoleModel(Model):\r\n    def _build_layers_v2(self, input_dict, num_outputs, options):\r\n        self.model = Sequential()\r\n        self.model.add(layers.InputLayer(input_tensor=input_dict[\"obs\"], input_shape=(4,)))\r\n        self.model.add(layers.Dense(4, name='l1', activation='relu'))\r\n        self.model.add(layers.Dense(10, name='l2', activation='relu'))\r\n        self.model.add(layers.Dense(10, name='l3', activation='relu'))\r\n        self.model.add(layers.Dense(10, name='l4', activation='relu'))\r\n        self.model.add(layers.Dense(2, name='l5', activation='relu'))\r\n        return self.model.get_layer(\"l5\").output, self.model.get_layer(\"l4\").output\r\n\r\nModelCatalog.register_custom_model(\"CartpoleModel\", CartpoleModel)\r\nCartpoleEnv = gym.make('CartPole-v0')\r\nCartpoleEnv=ScaleReward(CartpoleEnv)\r\nregister_env(\"CP\", lambda _:CartpoleEnv)\r\n\r\ntrainer = a3c.A3CTrainer(env=\"CP\",config={\r\n    \"model\": {\"custom_model\": \"CartpoleModel\"},'num_gpus':4,\"sample_batch_size\": 100})\r\n\r\ntry:\r\n  while True:\r\n    rest=trainer.train()\r\n    print(rest)\r\n    print(\"CPU usage: {}\".format(psutil.cpu_percent()))\r\n    GPUtil.showUtilization()\r\nexcept KeyboardInterrupt:\r\n  ray.shutdown()\r\n  print(\"Catch it\")\r\n```\r\nhowever, GPU usage is 0%\r\n```\r\nCPU usage: 98.2\r\n| ID | GPU | MEM |\r\n------------------\r\n|  0 |  0% |  5% |\r\n```\r\n\r\nThe only optimizer I got 38% of GPU was PPO. Why model is not trained using GPU?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/4886/comments",
    "author": "yarik1988",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-05-29T19:22:40Z",
        "body": "Ah, A3C doesn't use GPU (this could have a better error message). However, you can use A2C which does."
      },
      {
        "user": "yarik1988",
        "created_at": "2019-05-29T19:37:50Z",
        "body": "Just checked with A2C\r\n```\r\nCPU usage: 95.2\r\n| ID | GPU | MEM |\r\n------------------\r\n|  0 |  1% |  2% |\r\n```\r\n1% of GPU is worth nothing. With pure tensorflow image classification task GPU gives 2-3 times boost in usual.\r\nPart of each Reinforcement learning algorithm is model optimizer, 'adam' or anything else. I suspect even in A2C model weights are optimized using CPU power"
      },
      {
        "user": "ericl",
        "created_at": "2019-05-29T19:41:10Z",
        "body": "Note that your network is extremely small. With small batch sizes, it might not be surprising that the bottleneck is something else. I would try running the tuned Atari a2c examples, which definitely use GPU "
      }
    ]
  },
  {
    "number": 4708,
    "title": "Freeing up memory does not work",
    "created_at": "2019-04-27T16:51:28Z",
    "closed_at": "2020-03-05T23:21:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/4708",
    "body": "Python 3.6.7, ray 0.6.6\r\n\r\nIm using custom memory freeing (because I iterate over datasets throughout the process)\r\n\r\n``` \r\ntask_dataset_object_id = pin_in_object_store(dataset)\r\nobject_id = get_object_id_from_transformed_object_id(task_dataset_object_id)\r\n\r\ndef get_object_id_from_transformed_object_id(transformed_object_id: str):\r\n    \"\"\"\r\n    This function casts the sring representation of ray ObjectID to native ObjectID instance that can be passed to\r\n    the `ray.internal.free` function.\r\n\r\n    Parameters\r\n    ----------\r\n    transformed_object_id: str\r\n        Object id in format returned by `ray.tune.util.pin_in_object_store` function.\r\n\r\n    Returns\r\n    -------\r\n    object_id: ray.ObjectID\r\n        Original `ray.ObjectID` instance.\r\n    \"\"\"\r\n    from ray import ObjectID\r\n    from ray.tune.util import PINNED_OBJECT_PREFIX\r\n    without_prefix = transformed_object_id[len(PINNED_OBJECT_PREFIX):]\r\n    object_id = ObjectID(base64.b64decode(without_prefix))\r\n    return object_id\r\n\r\n\"\"\" fitting process\"\"\"\r\n\r\nray.internal.free([object_id])\r\n\r\n```\r\nand then other datasets in the loop\r\n\r\nHowever, I still get memory error after a number of datasets being pinned and free'd\r\n\r\n``` Traceback (most recent call last):\r\nFile \"/home/yhgtemp/PycharmProjects/reportingandanalysis/autoreporting/PredictionTasks/ShortTermVolFXO/FXOProductionFit.py\", line 275, in <module>\r\nobj.train_all_models(\"192.168.14.240:6379\", local_mode=False)\r\nFile \"/home/yhgtemp/PycharmProjects/reportingandanalysis/autoreporting/PredictionTasks/ShortTermVolFXO/FXOProductionFit.py\", line 139, in train_all_models\r\ntask_dataset_object_id = pin_in_object_store(with_currency_pair_vars_dataset)\r\nFile \"/home/yhgtemp/anaconda3/envs/py3_prod/lib/python3.6/site-packages/ray/tune/util.py\", line 27, in pin_in_object_store\r\nobj_id = ray.put(_to_pinnable(obj))\r\nFile \"/home/yhgtemp/anaconda3/envs/py3_prod/lib/python3.6/site-packages/ray/worker.py\", line 2216, in put\r\nworker.put_object(object_id, value)\r\nFile \"/home/yhgtemp/anaconda3/envs/py3_prod/lib/python3.6/site-packages/ray/worker.py\", line 375, in put_object\r\nself.store_and_register(object_id, value)\r\nFile \"/home/yhgtemp/anaconda3/envs/py3_prod/lib/python3.6/site-packages/ray/worker.py\", line 309, in store_and_register\r\nself.task_driver_id))\r\nFile \"/home/yhgtemp/anaconda3/envs/py3_prod/lib/python3.6/site-packages/ray/utils.py\", line 475, in _wrapper\r\nreturn orig_attr(*args, **kwargs)\r\nFile \"pyarrow/_plasma.pyx\", line 497, in pyarrow._plasma.PlasmaClient.put\r\nFile \"pyarrow/_plasma.pyx\", line 327, in pyarrow._plasma.PlasmaClient.create\r\nFile \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.PlasmaStoreFull: object does not fit in the plasma store ```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/4708/comments",
    "author": "hlbkin",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-04-27T23:02:53Z",
        "body": "This is probably since the `_pinned_objects` field in tune/util.py still has a python reference to the object. The object is queued for deletion but can't actually be freed until the python reference count drops to zero."
      },
      {
        "user": "robertnishihara",
        "created_at": "2019-04-27T23:02:54Z",
        "body": "What is the object store size? And how much memory should be required to run your application?\r\n\r\nCan you share a minimal example that reproduces the issue?"
      },
      {
        "user": "edoakes",
        "created_at": "2020-03-05T23:21:16Z",
        "body": "Stale - please open new issue if still relevant"
      }
    ]
  },
  {
    "number": 4505,
    "title": "Config Does Not Accept Custom Parameters",
    "created_at": "2019-03-29T02:31:36Z",
    "closed_at": "2019-03-29T08:21:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/4505",
    "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **Ray installed from (source or binary)**: Source\r\n- **Ray version**: 0.6.5\r\n- **Python version**: 3.7\r\n- **Exact command to reproduce**: \r\n`run_experiments({\r\n        \"test\": {\r\n            \"run\": my_trainable_func,\r\n            \"env\": multienv_name,\r\n            \"config\": {\r\n                \"multiagent\": {\r\n                    \"policy_graphs\": policy_graphs,\r\n                    \"policy_mapping_fn\": tune.function(lambda agent_id: f'agent_{agent_id}'),\r\n                },\r\n                \"num_iters\": 5\r\n            },\r\n            \"resources_per_trial\": {\r\n                \"cpu\": 2,\r\n                \"gpu\": 1,\r\n            },\r\n        }\r\n    })`\r\n\r\n### Describe the problem\r\nI am trying to include custom config parameters which my_trainable_func uses, but seem unable to add anything because I get an unknown config parameter error. As per Issue #3160, @ericl has mentioned that many config parameters have been deprecated, but I'm curious to know what the intended way of adding algorithm-specific hyperparameters into the config is.\r\n\r\n### Source code / logs\r\n`Traceback (most recent call last):\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 90, in run\r\n(pid=426)     self._entrypoint()\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 141, in entrypoint\r\n(pid=426)     return self._trainable_func(config, self._status_reporter)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 249, in _trainable_func\r\n(pid=426)     output = train_func(config, reporter)\r\n(pid=426)   File \"<ipython-input-13-0b4744367540>\", line 103, in fed_train\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py\", line 276, in __init__\r\n(pid=426)     Trainable.__init__(self, config, logger_creator)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py\", line 88, in __init__\r\n(pid=426)     self._setup(copy.deepcopy(self.config))\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py\", line 364, in _setup\r\n(pid=426)     self._allow_unknown_subkeys)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/util.py\", line 89, in deep_update\r\n(pid=426)     raise Exception(\"Unknown config parameter `{}` \".format(k))\r\n(pid=426) Exception: Unknown config parameter `num_iters` `\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/4505/comments",
    "author": "kiddyboots216",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-03-29T05:29:00Z",
        "body": "This is probably since you are subclassing agent? Agent checks that no unknown config keys are present, to avoid typos in your experiment config.\r\n\r\nIf you want to add a custom key, you can add it to the default config of your custom agent. You can check out any of the existing agent classes for an example of the config."
      },
      {
        "user": "kiddyboots216",
        "created_at": "2019-03-29T08:04:41Z",
        "body": "Ah, to be clear, I was wondering whether there was a way to do this without subclassing Agent ('my_trainable_func' I am trying to Tune a function and not a class). "
      },
      {
        "user": "ericl",
        "created_at": "2019-03-29T08:18:40Z",
        "body": "The exception you posted is originating from agent, so you must be calling agent code somehow."
      },
      {
        "user": "kiddyboots216",
        "created_at": "2019-03-29T08:21:01Z",
        "body": "Thanks Eric! Resolved; I was passing in the config (with custom parameters that were specific to my training function) into the Agent without removing the parameters beforehand. Now I know that I need to remove any parameters the Agent can't identify."
      }
    ]
  },
  {
    "number": 3785,
    "title": "Tune doesn't work with multi agent env",
    "created_at": "2019-01-15T19:50:26Z",
    "closed_at": "2019-01-16T17:55:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3785",
    "body": "<!--\r\nGeneral questions should be asked on the mailing list ray-dev@googlegroups.com.\r\n\r\nBefore submitting an issue, please fill out the following form.\r\n-->\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.1\r\n- **Ray installed from (source or binary)**: Source\r\n- **Ray version**: 0.6.1\r\n- **Python version**: 3.6.7\r\n- **Exact command to reproduce**:\r\n\r\n\r\n<!--\r\nYou can obtain the Ray version with\r\n\r\npython -c \"import ray; print(ray.__version__)\"\r\n-->\r\n\r\n### Describe the problem\r\nI am trying to use Tune in combination with RLlib to train with a custom multi agent environment. It works, when I am just using RLlib. But when I try to train using Tune i get `RecursionError: maximum recursion depth exceeded`. Does Tune currently support multi agent environments? Please find my code and the full stack trace here:\r\n\r\n### Source code / logs\r\n```python3\r\nimport ray\r\nimport ray.rllib.agents.ppo as ppo\r\nimport ray.tune as tune\r\nimport ray.tune.schedulers\r\nfrom ray.tune.logger import pretty_print\r\nfrom ray.tune.registry import register_env\r\nimport beer_distribution_game\r\n\r\ndef env_creator(env_config):\r\n    import gym\r\n    import beer_distribution_game\r\n    return beer_distribution_game.BeerDistributionGameV0()\r\n\r\ndef policy_mapper(agent_id):\r\n    return agent_id\r\n\r\nray.init(redis_address='localhost:6379')\r\nregister_env(\"SimpleBeerGame\", env_creator)\r\n\r\nspace_env = beer_distribution_game.BeerDistributionGameV0()\r\nspaces = space_env.get_spaces()\r\nsingle_config = {\r\n            'model' : {\r\n                'conv_filters' : None,\r\n                'fcnet_activation' : 'relu',\r\n                'fcnet_hiddens': [50, 100, 100]\r\n            },\r\n    'gamma': 0.7\r\n}\r\n\r\nregister_env(\"SimpleBeerGame\", env_creator)\r\n\r\nconfig = {\r\n    'beer-game-tune': {\r\n        'run': 'PPO',\r\n        'env': 'SimpleBeerGame',\r\n        'stop': {'episode_reward_mean' : -2000},\r\n        'config': {\r\n            'multiagent': {\r\n                'policy_mapping_fn': policy_mapper,\r\n                'policy_graphs': {\r\n                    'manufactorer':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['manufactorer']['observation_space'], spaces['manufactorer']['action_space'], single_config),\r\n                    'distributor':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['distributor']['observation_space'], spaces['distributor']['action_space'], single_config),\r\n                    'supplier':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['supplier']['observation_space'], spaces['supplier']['action_space'], single_config),\r\n                    'retailer':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['retailer']['observation_space'], spaces['retailer']['action_space'], single_config)\r\n                    },\r\n                'policies_to_train': [\r\n                    'manufactorer', 'distributor', 'supplier', 'retailer']\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nscheduler = ray.tune.schedulers.AsyncHyperBandScheduler(time_attr='training_iteration', reward_attr='episode_reward_mean', max_t=100)\r\n\r\ntrials = tune.run_experiments(experiments=config,scheduler=scheduler)\r\n```\r\n\r\n\r\nThis is the stack trace:\r\n```\r\n== Status ==\r\nUsing AsyncHyperBand: num_stopped=0\r\nBracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None\r\nBracket: Iter 90.000: None | Iter 30.000: None\r\nBracket: Iter 90.000: None\r\nResources requested: 0/16 CPUs, 0/0 GPUs\r\nMemory usage on this node: 1.7/16.3 GB\r\n\r\nDeprecation warning: Function values are ambiguous in Tune configuations. Either wrap the function with `tune.function(func)` to specify a function literal, or `tune.sample_from(func)` to tell Tune to sample values from the function during variant generation: <function policy_mapper at 0x7fa09014a8c8>\r\n---------------------------------------------------------------------------\r\nRecursionError                            Traceback (most recent call last)\r\n<ipython-input-6-6efdb03a13a1> in <module>\r\n----> 1 trials = tune.run_experiments(experiments=config,scheduler=scheduler)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py in run_experiments(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\r\n    106     last_debug = 0\r\n    107     while not runner.is_finished():\r\n--> 108         runner.step()\r\n    109         if time.time() - last_debug > DEBUG_PRINT_INTERVAL:\r\n    110             logger.info(runner.debug_string())\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in step(self)\r\n    112             raise TuneError(\"Called step when all trials finished?\")\r\n    113         self.trial_executor.on_step_begin()\r\n--> 114         next_trial = self._get_next_trial()\r\n    115         if next_trial is not None:\r\n    116             self.trial_executor.start_trial(next_trial)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in _get_next_trial(self)\r\n    252         trials_done = all(trial.is_finished() for trial in self._trials)\r\n    253         wait_for_trial = trials_done and not self._search_alg.is_finished()\r\n--> 254         self._update_trial_queue(blocking=wait_for_trial)\r\n    255         trial = self._scheduler_alg.choose_trial_to_run(self)\r\n    256         return trial\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in _update_trial_queue(self, blocking, timeout)\r\n    362             timeout (int): Seconds before blocking times out.\r\n    363         \"\"\"\r\n--> 364         trials = self._search_alg.next_trials()\r\n    365         if blocking and not trials:\r\n    366             start = time.time()\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/basic_variant.py in next_trials(self)\r\n     48             trials (list): Returns a list of trials.\r\n     49         \"\"\"\r\n---> 50         trials = list(self._trial_generator)\r\n     51         self._finished = True\r\n     52         return trials\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/basic_variant.py in _generate_trials(self, unresolved_spec, output_path)\r\n     67             raise TuneError(\"Must specify `run` in {}\".format(unresolved_spec))\r\n     68         for _ in range(unresolved_spec.get(\"num_samples\", 1)):\r\n---> 69             for resolved_vars, spec in generate_variants(unresolved_spec):\r\n     70                 experiment_tag = str(self._counter)\r\n     71                 if resolved_vars:\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in generate_variants(unresolved_spec)\r\n     40         \"cpu\": {\"eval\": \"spec.config.num_workers\"}\r\n     41     \"\"\"\r\n---> 42     for resolved_vars, spec in _generate_variants(unresolved_spec):\r\n     43         assert not _unresolved_values(spec)\r\n     44         yield format_vars(resolved_vars), spec\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _generate_variants(spec)\r\n    138     for resolved_spec in grid_search:\r\n    139         resolved_vars = _resolve_lambda_vars(resolved_spec, lambda_vars)\r\n--> 140         for resolved, spec in _generate_variants(resolved_spec):\r\n    141             for path, value in grid_vars:\r\n    142                 resolved_vars[path] = _get_value(spec, path)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _generate_variants(spec)\r\n    121 def _generate_variants(spec):\r\n    122     spec = copy.deepcopy(spec)\r\n--> 123     unresolved = _unresolved_values(spec)\r\n    124     if not unresolved:\r\n    125         yield {}, spec\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _unresolved_values(spec)\r\n    258         elif isinstance(v, dict):\r\n    259             # Recurse into a dict\r\n--> 260             for (path, value) in _unresolved_values(v).items():\r\n    261                 found[(k, ) + path] = value\r\n    262         elif isinstance(v, list):\r\n\r\n... last 1 frames repeated, from the frame below ...\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _unresolved_values(spec)\r\n    258         elif isinstance(v, dict):\r\n    259             # Recurse into a dict\r\n--> 260             for (path, value) in _unresolved_values(v).items():\r\n    261                 found[(k, ) + path] = value\r\n    262         elif isinstance(v, list):\r\n\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\nThis is the whole config:\r\n```python\r\n{'beer-game-tune': {'config': {'multiagent': {'policies_to_train': ['manufactorer',\r\n                                                                    'distributor',\r\n                                                                    'supplier',\r\n                                                                    'retailer'],\r\n                                              'policy_graphs': {'distributor': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                                Box(4,),\r\n                                                                                Discrete(20),\r\n                                                                                {'gamma': 0.7,\r\n                                                                                 'model': {'conv_filters': None,\r\n                                                                                           'fcnet_activation': 'relu',\r\n                                                                                           'fcnet_hiddens': [50,\r\n                                                                                                             100,\r\n                                                                                                             100]}}),\r\n                                                                'manufactorer': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                                 Box(4,),\r\n                                                                                 Discrete(20),\r\n                                                                                 {'gamma': 0.7,\r\n                                                                                  'model': {'conv_filters': None,\r\n                                                                                            'fcnet_activation': 'relu',\r\n                                                                                            'fcnet_hiddens': [50,\r\n                                                                                                              100,\r\n                                                                                                              100]}}),\r\n                                                                'retailer': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                             Box(4,),\r\n                                                                             Discrete(20),\r\n                                                                             {'gamma': 0.7,\r\n                                                                              'model': {'conv_filters': None,\r\n                                                                                        'fcnet_activation': 'relu',\r\n                                                                                        'fcnet_hiddens': [50,\r\n                                                                                                          100,\r\n                                                                                                          100]}}),\r\n                                                                'supplier': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                             Box(4,),\r\n                                                                             Discrete(20),\r\n                                                                             {'gamma': 0.7,\r\n                                                                              'model': {'conv_filters': None,\r\n                                                                                        'fcnet_activation': 'relu',\r\n                                                                                        'fcnet_hiddens': [50,\r\n                                                                                                          100,\r\n                                                                                                          100]}})},\r\n                                              'policy_mapping_fn': <function policy_mapper at 0x7f804434a1e0>}},\r\n                    'env': 'SimpleBeerGame',\r\n                    'run': 'PPO',\r\n                    'stop': {'episode_reward_mean': -2000}}}\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3785/comments",
    "author": "MariusDanner",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-01-16T02:06:59Z",
        "body": "The issue is that tune is trying to expand lambda functions to generate trial variants. To fix that, you can 'escape' the policy mapper function with tune.function(func).\r\n\r\nThis is an unfortunate gotcha of the tune API, we should eventually raise an error on raw functions passed in the config."
      },
      {
        "user": "MariusDanner",
        "created_at": "2019-01-16T17:55:14Z",
        "body": "Thank you! Now it works"
      }
    ]
  },
  {
    "number": 3660,
    "title": "extract the feature of conv2 or fc1",
    "created_at": "2018-12-29T04:11:59Z",
    "closed_at": "2019-01-04T07:20:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3660",
    "body": "Hi, based on the visionnet model in rllib, the structure of DNN is: input->conv1->conv2->fc1->fc2.\r\n\r\nMy question is that how to modify the codes in Rllib to extract the features on conv2 layer or fc1 layer? I have tried several ways but cannot pass the compiler. Thanks.\r\n\r\nFor ES codes, only the feature on fc2 can be extracted by the following two parts:\r\n1) From rllib/models/visionnet.py\r\nclass VisionNetwork(Model):\r\n\r\n    def _build_layers_v2(self, input_dict, num_outputs, options):\r\n        inputs = input_dict[\"obs\"]\r\n        filters = options.get(\"conv_filters\")\r\n        if not filters:\r\n            filters = get_filter_config(options)\r\n\r\n        activation = get_activation_fn(options.get(\"conv_activation\", \"relu\"))\r\n\r\n        with tf.name_scope(\"vision_net\"):\r\n            for i, (out_size, kernel, stride) in enumerate(filters[:-1], 1):\r\n                inputs = slim.conv2d(\r\n                    inputs,\r\n                    out_size,\r\n                    kernel,\r\n                    stride,\r\n                    activation_fn=activation, \r\n                    scope=\"conv{}\".format(i))\r\n\r\n            out_size, kernel, stride = filters[-1]\r\n           #out_size = filters[-1,0]\r\n            inputs = slim.flatten (inputs, scope=\"embed\")\r\n            fc1 = slim.fully_connected(inputs, out_size, activation_fn=activation, scope=\"fc1\")\r\n            fc2 = slim.fully_connected(fc1, num_outputs, activation_fn=None, normalizer_fn=None,  scope=\"fc2\")\r\n\r\n            return fc2, fc1\r\n\r\n2) From rllib/agents/es/policies.py:\r\nclass GenericPolicy(object):\r\n    def __init__(self, sess, env,env2,action_space, obs_space, preprocessor,\r\n                 observation_filter, model_options, action_noise_std):\r\n        self.sess = sess\r\n        self.action_space = action_space\r\n        self.action_noise_std = action_noise_std\r\n        self.preprocessor = preprocessor\r\n        self.observation_filter = get_filter(observation_filter,\r\n                                             self.preprocessor.shape)\r\n        self.inputs = tf.placeholder(tf.float32,\r\n                                     [None] + list(self.preprocessor.shape))\r\n\r\n        # Policy network.\r\n        dist_class, dist_dim = ModelCatalog.get_action_dist(\r\n            self.action_space, model_options, dist_type=\"deterministic\")\r\n\r\n        self.model = ModelCatalog.get_model({\r\n            \"obs\": self.inputs}, obs_space, dist_dim, model_options)\r\n\r\n        self.dist = dist_class(self.model.outputs)\r\n        self.sampler = self.dist.sample()\r\n        self.entro=self.dist.entropy()\r\n        self.prob=self.dist.softmax()\r\n\r\n        self.variables = ray.experimental.TensorFlowVariables(\r\n            self.model.outputs, self.sess)\r\n\r\n        self.num_params = sum(\r\n            np.prod(variable.shape.as_list())\r\n            for _, variable in self.variables.variables.items())\r\n\r\nHow to modify part 1) or part 2) to extract the feature layer of conv2 or fc1 especially for ES?\r\n\r\nThanks a lot.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3660/comments",
    "author": "stellaxu",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-12-29T05:10:54Z",
        "body": "If you use a custom model, then you can return whatever tensor you want as the feature layer and access it via model.last_layer (or define it as an extra attribute on the model)."
      },
      {
        "user": "stellaxu",
        "created_at": "2018-12-30T05:45:45Z",
        "body": "Thanks Eric, I got it~~"
      }
    ]
  },
  {
    "number": 3173,
    "title": "Issue running train.py can not locate lz4 or GPU",
    "created_at": "2018-10-31T19:32:10Z",
    "closed_at": "2018-11-01T06:06:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3173",
    "body": "When running the following (it does run succesfully but I get errors in setup on the gpu and lz4)\r\nRay does not find lz4 or my gpu\r\n.\r\n\r\n```\r\n===============================================================================================================================\r\n**********  stats  ************************\r\n\r\n\r\nUbuntu 16.04\r\n\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\n\r\n\r\n>>import ray; print(ray.__version__)\r\n0.5.3\r\n\r\n\r\n>>pip install lz4\r\nRequirement already satisfied: lz4 in /usr/lib/python2.7/dist-packages (0.7.0)\r\n\r\n\r\n>>lrwxrwxrwx  1 root root    10 Oct  1 22:50 cuda -> ./cuda-9.2\r\n\r\n>>nvidia-smi\r\n\r\nWed Oct 31 15:13:34 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1060    Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   57C    P0    28W /  N/A |    365MiB /  6069MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1056      G   /usr/lib/xorg/Xorg                           262MiB |\r\n|    0      2508      G   compiz                                         7MiB |\r\n|    0      6924      G   ...uest-channel-token=14148272352303891226    92MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n=============================================================================================================\r\n\r\n\r\n\r\nsudo python /home/rjn/.local/lib/python2.7/site-packages/ray/rllib/train.py --env=Pong-ram-v4 --run=IMPALA \r\n\r\n\r\nProcess STDOUT and STDERR is being redirected to /tmp/raylogs/.\r\nWaiting for redis server at 127.0.0.1:27991 to respond...\r\nWaiting for redis server at 127.0.0.1:19620 to respond...\r\nStarting the Plasma object store with 6.00 GB memory.\r\nStarting local scheduler with the following resources: {'GPU': 1, 'CPU': 8}.\r\nFailed to start the UI, you may need to run 'pip install jupyter'.\r\n== Status ==\r\nUsing FIFO scheduling algorithm.\r\n\r\n\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nCreated LogSyncer for /home/rjn/ray_results/default/IMPALA_Pong-ram-v4_0_2018-10-31_15-27-354nWfrf -> \r\n== Status ==\r\nUsing FIFO scheduling algorithm.\r\nResources requested: 3/8 CPUs, 1/1 GPUs\r\nResult logdir: /home/rjn/ray_results/default\r\nRUNNING trials:\r\n - IMPALA_Pong-ram-v4_0:\tRUNNING\r\n\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\n2018-10-31 15:27:36.476907: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:36.551185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-31 15:27:36.551570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.733\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.50GiB\r\n2018-10-31 15:27:36.551584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-10-31 15:27:36.750449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-31 15:27:36.750495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-10-31 15:27:36.750501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-10-31 15:27:36.750697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5263 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\n2018-10-31 15:27:38.726462: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:38.728976: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-10-31 15:27:38.729023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.729030: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.729057: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.44.0\r\n2018-10-31 15:27:38.729080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.44.0\r\n2018-10-31 15:27:38.729086: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.44.0\r\n2018-10-31 15:27:38.747833: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:38.750367: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-10-31 15:27:38.750429: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.750454: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.750518: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.44.0\r\n2018-10-31 15:27:38.750584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.44.0\r\n2018-10-31 15:27:38.750590: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.44.0\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sample_batch.SampleBatch'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sample_batch.SampleBatch'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sampler.RolloutMetrics'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nResult for IMPALA_Pong-ram-v4_0:\r\n  date: 2018-10-31_15-27-47\r\n  done: false\r\n  episode_len_mean: 1290.9\r\n  episode_reward_max: -19.0\r\n  episode_reward_mean: -20.2\r\n  episode_reward_min: -21.0\r\n  episodes: 10\r\n  experiment_id: 16a7956cbdbf4bc78ef6f280ddc93142\r\n  hostname: rjn-Oryx-Pro\r\n  info:\r\n    learner:\r\n      cur_lr: 0.0005000000237487257\r\n      entropy: 862.072021484375\r\n      grad_gnorm: 40.0\r\n      policy_loss: -53.67061996459961\r\n      var_gnorm: 22.6600399017334\r\n      vf_explained_var: 0.07421219348907471\r\n      vf_loss: 30.673662185668945\r\n    num_steps_sampled: 14750\r\n    num_steps_trained: 14500\r\n    num_weight_syncs: 295\r\n    sample_throughput: 2048.296\r\n    train_throughput: 4096.592\r\n  iterations_since_restore: 1\r\n  node_ip: 192.168.1.100\r\n  pid: 7915\r\n  policy_reward_mean: {}\r\n  time_since_restore: 10.099857091903687\r\n  time_this_iter_s: 10.099857091903687\r\n  time_total_s: 10.099857091903687\r\n  timestamp: 1541014067\r\n  timesteps_since_restore: 14750\r\n  timesteps_this_iter: 14750\r\n  timesteps_total: 14750\r\n  training_iteration: 1\r\n\r\n\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3173/comments",
    "author": "rnunziata",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-11-01T00:09:15Z",
        "body": "> sudo python\r\n\r\nThis is almost certainly going to cause a different python environment to be used. Why not drop the sudo?"
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T01:03:36Z",
        "body": "removing the sudo did make a difference....same errors. Any thoughts on this. Maybe I will try to compile form source."
      },
      {
        "user": "ericl",
        "created_at": "2018-11-01T03:52:28Z",
        "body": "I can't think of a reason why lz4 can't be found if you can load it from a python interpreter.\r\n\r\nThough, not having that is probably fine on a single machine since no network transfers are happening."
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T04:38:08Z",
        "body": "ok...so you are saying I can at least ignore it.\r\nwhat about  it not findng cuda device.  Its a nivda GTX1060  and seem to be ok. Or am I not reading those errors correctly since it does run if I use Pong-ram-v0. "
      },
      {
        "user": "ericl",
        "created_at": "2018-11-01T05:05:06Z",
        "body": "Hm, the error might be expected on the workers since we force those to use CPUs. If you see GPU utilization then that should be fine."
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T05:50:44Z",
        "body": "I think that is it...there are two workers here and only two errors  I can not tell by the error line what task they belong to but I think it is probably good guess. I think the learner grabs the entire GPU memory since I  do not set any kind of growth or percentage variable. Thank you for your help."
      },
      {
        "user": "ericl",
        "created_at": "2018-11-01T06:06:29Z",
        "body": "Great!"
      }
    ]
  },
  {
    "number": 3166,
    "title": "Cannot copy a copy of an actor",
    "created_at": "2018-10-30T14:20:33Z",
    "closed_at": "2018-10-31T16:22:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3166",
    "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Ray installed from (source or binary)**: binary\r\n- **Ray version**: latest a221f55b0d611de29324e3b1aa79eec1c10458ba\r\n- **Python version**: 3.6.5\r\n- **Exact command to reproduce**:\r\n<pre>\r\nimport copy\r\nimport ray\r\nfrom ray.rllib.utils import merge_dicts\r\nray.init()\r\n@ray.remote\r\nclass Foo(object):\r\n    def __init__(self):\r\n        pass\r\n\r\nfoo = {\"dummy\": Foo.remote()}\r\nfoo_1 = merge_dicts(foo, {})\r\nfoo_2 = merge_dicts(foo_1, {})    # TypeError: a bytes-like object is required, not 'NoneType'\r\n\"\"\"\r\nor\r\n\"\"\"\r\nfoo = Foo.remote()\r\nfoo_1 = copy.deepcopy(foo)\r\nfoo_2 = copy.deepcopy(foo_1)    # TypeError: a bytes-like object is required, not 'NoneType'\r\n</pre>\r\n\r\nIs this expected?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3166/comments",
    "author": "llan-ml",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-10-30T17:30:20Z",
        "body": "I don't think actor handles are copyable via copy. You should be able to pickle and unpickle them to achieve the same thing though."
      }
    ]
  },
  {
    "number": 3075,
    "title": "Multi-period observation input ",
    "created_at": "2018-10-17T16:25:39Z",
    "closed_at": "2018-10-17T19:55:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3075",
    "body": "Hi, \r\n\r\nI was wondering is there any implementation to pass last m observation as input into either of rlLib algorithms? I checked to see if I can write a custom preprocessor function, but in that case I need a custom memory to keep previous observation and handling it with replay memory in off-policy algorithms would be easy. Since, the original DQN paper used such trick, I though probably it is developed and I just need to ask how to use it. \r\nI appreciate your comments.\r\n\r\nThanks,",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3075/comments",
    "author": "oroojlooy",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-10-17T16:40:43Z",
        "body": "Hey @oroojlooy, you can do this with a gym environment wrapper (basically a stateful preprocessor). We already do this for Atari envs, and you can find the code for this in atari_wrappers.py and use it to create a custom env of your own."
      },
      {
        "user": "oroojlooy",
        "created_at": "2018-10-17T19:55:31Z",
        "body": "Thanks @ericl, I see that `FrameStack(env,k)` manages this operation. "
      }
    ]
  },
  {
    "number": 3043,
    "title": "[rllib] ValueError: setting an array element with a sequence",
    "created_at": "2018-10-10T15:24:44Z",
    "closed_at": "2018-10-11T14:09:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3043",
    "body": "<!--\r\nGeneral questions should be asked on the mailing list ray-dev@googlegroups.com.\r\n\r\nBefore submitting an issue, please fill out the following form.\r\n-->\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9.5\r\n- **Ray installed from (source or binary)**: binary\r\n- **Ray version**: 0.5.3\r\n- **Python version**: 3.5.2\r\n- **Exact command to reproduce**: `agent.train()`\r\n\r\n<!--\r\nYou can obtain the Ray version with\r\n\r\npython -c \"import ray; print(ray.__version__)\"\r\n-->\r\n\r\n### Describe the problem\r\n<!-- Describe the problem clearly here. -->\r\nI'm trying to develop a custom recurrent network similar to LSTMs for Ray. Sticking to the PPO example, I use the following code to produce the error below:\r\n\r\n```python3\r\nimport ray\r\nimport ray.rllib.agents.ppo as ppo\r\nfrom ray.tune.logger import pretty_print\r\n\r\nray.init()\r\nconfig = ppo.DEFAULT_CONFIG.copy()\r\nconfig[\"num_gpus\"] = 0\r\nconfig[\"num_workers\"] = 1\r\nconfig[\"model\"][\"use_rnn\"] = True\r\nagent = ppo.PPOAgent(config=config, env=\"CartPole-v0\")\r\n\r\nfor i in range(100):\r\n    result = agent.train() # <-- here it dies\r\n    print(pretty_print(result))\r\n```\r\n\r\nThis produces the following error that I'm unable to interpret. What I try to do is to simulate the behaviour of the built-in LSTM by appropriately duplicating / expanding all occurances of `'use_lstm'` with `'use_rnn'`. The thing is, that my internal state for my RNN has different shapes than the LSTM state, could this be a problem?\r\n\r\nI know that when you try to do something like \r\n```python3\r\nnp.asarray([[1, 2], [2, 3, 4]], dtype=np.float)\r\n```\r\nyou will get the same error. However, since I'm new to Ray I'm unfamiliar with how to debug such issues deep in the framework. Do you have any suggestions?\r\n\r\n### Source code / logs\r\n<!-- Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. -->\r\n\r\n```\r\nError fetching: [<tf.Tensor 'default/Squeeze:0' shape=(?,) dtype=int64>, <tf.Tensor 'default/rnn/while/Exit_4:0' shape=(128, 256) dtype=float32>, <tf.Tensor 'default/rnn/while/Exit_5:0' shape=(128, 256) dtype=float32>, <tf.Tensor 'default/rnn/while/Exit_6:0' shape=(128, 256) dtype=float32>, <tf.Tensor 'default/rnn/while/Exit_7:0' shape=(128, 256, 1) dtype=float32>, <tf.Tensor 'default/rnn/while/Exit_8:0' shape=(128, 256, 3) dtype=float32>, {'vf_preds': <tf.Tensor 'default/value_function/Reshape_2:0' shape=(?,) dtype=float32>, 'logits': <tf.Tensor 'default/add:0' shape=(?, 2) dtype=float32>}], feed_dict={<tf.Tensor 'default/PlaceholderWithDefault:0' shape=() dtype=bool>: True, <tf.Tensor 'default/TupleStateHolder/b:0' shape=(128, 256) dtype=float32>: [<tf.Tensor 'default/zeros_2:0' shape=(128, 256) dtype=float32>], <tf.Tensor 'default/TupleStateHolder/z_buffer:0' shape=(128, 256, 3) dtype=float32>: [<tf.Tensor 'default/zeros_4:0' shape=(128, 256, 3) dtype=float32>], <tf.Tensor 'default/obs:0' shape=(?, 4) dtype=float32>: [array([0., 0., 0., 0.])], <tf.Tensor 'default/TupleStateHolder/z:0' shape=(128, 256) dtype=float32>: [<tf.Tensor 'default/zeros_1:0' shape=(128, 256) dtype=float32>], <tf.Tensor 'default/seq_lens_1:0' shape=(?,) dtype=int32>: array([1.]), <tf.Tensor 'default/TupleStateHolder/v:0' shape=(128, 256) dtype=float32>: [<tf.Tensor 'default/zeros:0' shape=(128, 256) dtype=float32>], <tf.Tensor 'default/TupleStateHolder/i_future_buffer:0' shape=(128, 256, 1) dtype=float32>: [<tf.Tensor 'default/zeros_3:0' shape=(128, 256, 1) dtype=float32>]}\r\nRemote function sample failed with:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/ray/worker.py\", line 945, in _process_task\r\n    *arguments)\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/ray/actor.py\", line 261, in actor_method_executor\r\n    method_returns = method(actor, *args)\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/ray/rllib/evaluation/policy_evaluator.py\", line 303, in sample\r\n    batches = [self.sampler.get_data()]\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 58, in get_data\r\n    item = next(self.rollout_provider)\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/ray/rllib/evaluation/sampler.py\", line 362, in _env_runner\r\n    eval_results[k] = builder.get(v)\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/ray/rllib/utils/tf_run_builder.py\", line 47, in get\r\n    raise e\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/ray/rllib/utils/tf_run_builder.py\", line 43, in get\r\n    self.feed_dict, os.environ.get(\"TF_TIMELINE_DIR\"))\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/ray/rllib/utils/tf_run_builder.py\", line 82, in run_timeline\r\n    fetches = sess.run(ops, feed_dict=feed_dict)\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1109, in _run\r\n    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\r\n  File \"/home/pethor/Documents/project/venv/lib/python3.5/site-packages/numpy/core/numeric.py\", line 501, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nValueError: setting an array element with a sequence.\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3043/comments",
    "author": "pethor",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-10-10T23:48:11Z",
        "body": "Hmm, if the state size is changed, did you make sure to change the initial state as well? I'd try to start with the included LSTM model and adapt it to your needs."
      },
      {
        "user": "llan-ml",
        "created_at": "2018-10-11T01:01:29Z",
        "body": "@pethor The reason for that error is the value of a feed is a list of a single tensor.\r\n<pre>\r\nIn [1]: import tensorflow as tf\r\nIn [2]: import numpy as np\r\nIn [3]: x = tf.placeholder(tf.float32, [10, 10])\r\nIn [4]: y = tf.zeros([10, 10], dtype=tf.float32)\r\nIn [5]: sess = tf.InteractiveSession()\r\n2018-10-11 18:17:59.095853: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nIn [6]: try:\r\n   ...:     sess.run(x, feed_dict={x: [y]})\r\n   ...: except Exception as e:\r\n   ...:     print(e)\r\n   ...:\r\nsetting an array element with a sequence.\r\nIn [7]: try:\r\n   ...:     sess.run(x, feed_dict={x: y})\r\n   ...: except Exception as e:\r\n   ...:     print(e)\r\n   ...:\r\nThe value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.For reference, the tensor object was Tensor(\"zeros:0\", shape=(10, 10), dtype=float32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(10, 10), dtype=float32).\r\nIn [19]: try:\r\n    ...:     np.asarray([y], dtype=x.dtype.as_numpy_dtype)\r\n    ...: except Exception as e:\r\n    ...:     print(e)\r\n    ...:\r\nsetting an array element with a sequence.\r\n</pre>\r\n"
      },
      {
        "user": "pethor",
        "created_at": "2018-10-11T14:09:03Z",
        "body": "@ericl I changed the state according to the needs of the RNN I'm using, the problem was with the wrapping and unwrapping of the `state_in` into named tuples. "
      }
    ]
  },
  {
    "number": 2854,
    "title": "[tune] ImportError when importing TrainingResult",
    "created_at": "2018-09-10T14:48:43Z",
    "closed_at": "2018-09-14T23:26:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/2854",
    "body": "<!--\r\nGeneral questions should be asked on the mailing list ray-dev@googlegroups.com.\r\n\r\nBefore submitting an issue, please fill out the following form.\r\n-->\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18.3\r\n- **Ray installed from (source or binary)**: binary\r\n- **Ray version**: 0.5.2\r\n- **Python version**: 3.6.6\r\n- **Exact command to reproduce**: `from ray.tune import TrainingResult`\r\n\r\n<!--\r\nYou can obtain the Ray version with\r\n\r\npython -c \"import ray; print(ray.__version__)\"\r\n-->\r\n\r\n### Describe the problem\r\n<!-- Describe the problem clearly here. -->\r\nWhen I try to import TrainingResult I get an ImportError. It was working well in version 0.5.0.\r\n\r\nI have looked for references to this class in the code, but I cannot find it. It would like to know if this class is now located in another module or it has been eliminated altogether.\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/2854/comments",
    "author": "albertaparicio",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-09-10T18:50:37Z",
        "body": "Hey @albertaparicio, in 0.5.2 it has been replaced by just a `dict`."
      },
      {
        "user": "richardliaw",
        "created_at": "2018-09-14T23:26:25Z",
        "body": "I think we can close this for now and reopen if any followup questions arise."
      }
    ]
  },
  {
    "number": 1756,
    "title": "How should we do resource requirements and scheduling for actor creation tasks?",
    "created_at": "2018-03-20T07:39:40Z",
    "closed_at": "2018-05-09T20:39:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/1756",
    "body": "Summarizing some thoughts from discussions with @ericl @richardliaw @pcmoritz @atumanov and @stephanie-wang.\r\n\r\n_Some details about the current approach:_\r\n- When we create an actor, e.g., with `a = Actor.remote()`, an **actor creation task** is submitted (as a regular task, and executed on some worker (turning that worker into the actor).\r\n- When a method is invoked on an actor, e.g., with `a.method.remote()`, an **actor method task** is scheduled on the actor and executed by the actor.\r\n- The actor creation task may require resources (e.g., 1 CPU and 1 GPU). These resources are acquired for the lifetime of the actor. By default, if nothing is specified in the `@ray.remote` decorator, then no resources are required by the actor creation task.\r\n- The actor methods may also require resources (currently the only options are 0 CPUs or 1 CPU, but this would be easy to change). These resources are acquired for the duration of the method. Currently, if nothing is specified in the `@ray.remote` decorator, then 1 CPU is required by each actor method, and otherwise 0 CPUs are required by each actor method.\r\n\r\n_Properties we'd like to have:_\r\n1. **Actor bin-packing.** That is, if we have 20 machines, each with 10 CPUs, in our cluster, and we create 200 actors, then the actors are evenly spread throughout the cluster (and similarly if the machines are unequal in size then more actors are on the bigger machines).\r\n2. **Things shouldn't hang. ** That is, if you're running a script and you create 9 actors and you have 8 CPUs, the script should still run. Or if it really can't run, then you should get a good error message or exception. The concern about this is why the default behavior is to associate 0 CPUs with the actor lifetime.\r\n3. Excess actors should be balanced as well. So if you have 20 machines, each with 10 CPUs, and you create 400 actors, then you should get 20 per machine.\r\n\r\n_Challenges:_\r\n1. The above properties are a bit at odds. To achieve them, we may need the ability to specify more of an \"anti-affinity\" behavior, but that will likely require resources with different semantics than the resources we currently use.\r\n2. The scheduling of the actor creation task must take into account the resource requirements of the actor methods. Currently, each actor method only takes into account its own resource requirements, but if a method requires 2 CPUs and the actor creation task requires 1 CPU, then it could get scheduled on a machine with only 1 CPU, which would be a problem. We currently work around this restricting the possible method resource requirements and preventing any actors from being created on machines with 0 CPUs.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/1756/comments",
    "author": "robertnishihara",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-03-20T07:55:12Z",
        "body": "Another challenge is autoscaling, which is at odds with \"not hanging\". If a cluster is autoscaling, you might want to wait for nodes to get added instead of committing to placing the actors immediately. "
      },
      {
        "user": "robertnishihara",
        "created_at": "2018-03-20T08:21:30Z",
        "body": "#1757 is a hack to improve actor load balancing for actors that do not acquire any lifetime resources."
      }
    ]
  },
  {
    "number": 1609,
    "title": "Ship remote function definitions lazily when they are first invoked.",
    "created_at": "2018-02-26T06:09:18Z",
    "closed_at": "2019-05-24T20:44:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/1609",
    "body": "We may want to ship remote function definitions when they are first invoked (as opposed to when they are first defined. This would help avoid problems like #1607.\r\n\r\nThis could also avoid shipping a ton of remote function definitions that are not used (e.g., because they are defined in a Python module which is imported by a user).\r\n\r\nThe main downside is that it increases latency for the first invocation.\r\n\r\ncc @ludwigschmidt",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/1609/comments",
    "author": "robertnishihara",
    "comments": [
      {
        "user": "ludwigschmidt",
        "created_at": "2018-02-26T06:30:38Z",
        "body": "A few thoughts on this:\r\n\r\n- From personal experience, Robert's point regarding slow `import` due to remote functions could become a serious usability problem. I remember similar issues from Julia where compiling packages on import was quite annoying because it led to slow program / notebook starts.\r\n\r\n- I find it more intuitive to have some extra latency on the actual invocation. As a user, I know that data is being transferred over the network now. And probably my remote code takes some time anyway.\r\n\r\n- Would lazy shipping also make the @remote annotation unnecessary? That would significantly simplify usability since I don't have to worry about annotating my functions any more (this is a nice feature of Pywren for me)."
      },
      {
        "user": "robertnishihara",
        "created_at": "2018-02-26T06:37:04Z",
        "body": "Responding to your third point, I think we'd still need the decorator if we want to do invoke functions with `f.remote()`. We could add a `ray.submit(f)` function to the API which would avoid the decorator. I like it less (aesthetically), though it does have the advantage of allowing you to pass in other arguments (e.g., resource requirements) when the function is invoked as opposed to when it is defined (so they can differ between invocations)."
      },
      {
        "user": "ludwigschmidt",
        "created_at": "2018-02-26T06:40:33Z",
        "body": "As a user who doesn't know what's going on under the hood, I prefer `ray.submit(f)` (or `ray.run(f)` or `ray.execute(f)`). It is more transparent to me because I know that `f` is still just the function I defined."
      },
      {
        "user": "ludwigschmidt",
        "created_at": "2018-02-26T06:43:10Z",
        "body": "Also, if `f.remote()` turns out to be the only thing that the decorator enables after lazy shipping, it might be good to clarify that in the documentation. It might also be good to make the decorator optional at this point."
      },
      {
        "user": "richardliaw",
        "created_at": "2018-02-27T01:27:31Z",
        "body": "One thing to note:\r\n```\r\n@ray.remote\r\ndef f():\r\n    return 1\r\n```\r\nand \r\n```\r\ndef f():\r\n    return 1\r\n\r\nf = ray.remote(f)\r\n```\r\nThe above two are equivalent and currently work. @ludwigschmidt does the latter approach provide the semantics you were suggesting above?\r\n\r\nJust as a note, often times I don't use the annotation, but it's helpful to have it for fast scripting."
      },
      {
        "user": "ludwigschmidt",
        "created_at": "2018-03-05T00:48:04Z",
        "body": "I forgot to comment on this earlier.\r\n\r\nThe latter approach does not quite have the semantics I had in mind because f is still a special object and not the function I initially defined. With the `ray.submit(f)` proposed by @robertnishihara , f can remain the \"plain\" Python function I define."
      }
    ]
  },
  {
    "number": 1071,
    "title": "Consider removing Redis clients from worker processes.",
    "created_at": "2017-10-03T18:59:22Z",
    "closed_at": "2018-01-28T07:12:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/1071",
    "body": "This could be done by having the global state APIs go through the local scheduler or perhaps through a different process.\r\n\r\nThis would help with the problem where we reach the maximum number of Redis clients.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/1071/comments",
    "author": "robertnishihara",
    "comments": [
      {
        "user": "mitar",
        "created_at": "2017-10-03T20:59:56Z",
        "body": "You could probably increase the number of Redis clients somehow?"
      },
      {
        "user": "robertnishihara",
        "created_at": "2017-10-03T21:06:03Z",
        "body": "Yes, from a redis client you can do something like `redis_client.config_set('maxclients', 65534)`. Before that, you need to do `ulimit -n 65536`. You can potentially increase this number further if you have `sudo`.\r\n\r\nThis gets us pretty far, although it'd be nice to do it automatically."
      },
      {
        "user": "robertnishihara",
        "created_at": "2018-01-28T07:12:55Z",
        "body": "Duplicate of #952."
      }
    ]
  },
  {
    "number": 517,
    "title": "What should the behavior of ray.init be when it is called multiple times?",
    "created_at": "2017-05-05T22:57:02Z",
    "closed_at": "2019-03-12T19:18:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/517",
    "body": "`ray.init` currently throws an exception the second time it is called. However, it can be pretty common to call it multiple times (e.g., in an jupyter notebook where it is common to rerun cells).\r\n\r\nThere are a couple options.\r\n- Throw an exception.\r\n- Do nothing and print a warning.\r\n- If it is called with the same arguments that were called the first time, do nothing, otherwise throw an exception.\r\n- Kill everything from the previous `ray.init` and run it again.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/517/comments",
    "author": "robertnishihara",
    "comments": [
      {
        "user": "xgong",
        "created_at": "2017-05-07T18:21:06Z",
        "body": "I would like to vote the option \"Do nothing, and probably print a warning.\" I did not go through the code base very deeply. But if we have something like, internal state, we should base on the current internal state to make the decision on what is the next action."
      },
      {
        "user": "robertnishihara",
        "created_at": "2017-05-07T18:33:30Z",
        "body": "Thanks! That option will definitely improve the experience of using Ray interactively (e.g., in an interpreter). However, if it happens in a script, then it is probably a bug. That said, it's probably the way to go."
      },
      {
        "user": "pcmoritz",
        "created_at": "2017-05-07T18:47:39Z",
        "body": "Either this option or \"If it is called with the same arguments that were called the first time, do nothing, otherwise throw an exception.\" sound good to me. The second one has the advantage that it can catch some kinds of errors (i.e. having ray.init twice in a script), any downside compared to \"Do nothing, and probably print a warning.\"?"
      },
      {
        "user": "mitar",
        "created_at": "2017-06-09T08:08:29Z",
        "body": "Why not having this as an option argument to `ray.init`? Or we could have `ray.init` and `ray.ensure_init`? The first one throws, the second one makes sure it was called once, but then it is happy."
      },
      {
        "user": "robertnishihara",
        "created_at": "2017-06-10T00:13:16Z",
        "body": "An optional argument to `ray.init` might not be so bad, but I'd argue for having a very high bar for adding more functionality and new methods to the API."
      },
      {
        "user": "alok",
        "created_at": "2018-05-16T18:49:46Z",
        "body": "I'd lean towards having an optional argument. I'm not sure what the default to that argument should be, but since people writing scripts are generally more OK with typing a bit more, perhaps we should favor interactive users for this."
      },
      {
        "user": "robertnishihara",
        "created_at": "2019-03-12T19:18:31Z",
        "body": "You can call `ray.init(ignore_reinit_error=True)`, which is a no-op if `ray.init()` has already been called."
      }
    ]
  },
  {
    "number": 144,
    "title": "Should we provide functionality for running a function on all workers (or all nodes)?",
    "created_at": "2016-12-22T03:11:43Z",
    "closed_at": "2017-01-10T21:45:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/144",
    "body": "Should we expose the ability to run a given function on all workers? This can be useful for a bunch of reasons (for example, properly setting the Python path on each worker).\r\n\r\nShould we also expose the ability to run a given function once on each node? This can be useful for setting things up (e.g., installing some stuff, downloading some files, copying application files).\r\n\r\nOf course, we shouldn't add any functionality without a very good reason...",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/144/comments",
    "author": "robertnishihara",
    "comments": [
      {
        "user": "pcmoritz",
        "created_at": "2016-12-23T00:08:10Z",
        "body": "Another use case is to load a dataset in a balanced way.\r\n\r\nIt is also similar to Environment variables, maybe we can unify these two functionalities in some way or provide a better one that replaces both.\r\n\r\nWe also need to think a little about the api, i.e. which arguments the function should have (maybe total # of nodes or workers and current worker index). And how does it interplay with the scheduler? How do we make sure people don't abuse it to circumvent the scheduler?\r\n\r\nI'd say add it with a disclaimer that it is experimental, encourage people to use it rarely and see what it gets used for."
      },
      {
        "user": "robertnishihara",
        "created_at": "2016-12-23T01:19:00Z",
        "body": "As we discussed earlier, one elegant approach is to provide a single method for running a function on all workers, and pass a counter into that function indicating how many other workers on that machine have already started executing that function (using Redis to do this atomically).\r\n\r\nSo if there are 4 worker on one machine, and we call something like `ray.run_function_on_all_workers(f)`, then one worker will call `f(1)`, one worker will call `f(2)`, one will call `f(3)`, and one will call `f(4)`."
      },
      {
        "user": "atumanov",
        "created_at": "2016-12-23T02:40:12Z",
        "body": "This is a very interesting question/discussion. To address this specific use case, I think what you are looking for is the anti-affinity placement constraint, ability to run one task per locality domain (e.g., per node, per worker, per rack). We could provide this functionality in the global scheduler, by extending the scheduler API to accept a bag of tasks (collectively defined as a job) and a placement constraint associated with that job. Then, when the placement decision is made for this bag of tasks, it will be done in a way that honors the constraint, or the job is atomically rejected, if the constraint cannot be satisfied. Of course, it will be ideal to do it in a general way as well as make the placement constraint specification optional (perhaps even as a separate loadable module).\r\n\r\nAnother take on this would be to approach the problem from the OS systems perspective. We could think about a basic primitive Ray could expose (for instance a _scoped atomic counter_ primitive backed by Redis) that enables ensembles of distributed Ray tasks to do leader election, for example. Counters could be _node- or worker-scoped_ and persist for the lifetime of the task ensemble. It is easy to see how node-scoped atomic counters would enable \"at most once per node\" functionality, while worker-scoped atomic counters would enable \"at most once per worker\" functionality. So the Ray function that relies on some \"once per worker\" or \"once per node\" pre-processing will add a simple if statement checking the worker-scoped or node-scoped atomic counter and calls the init() function if the atomic counter is zero. The init() could either run in the same task context or as a separate task. The latter requires a mechanism that guarantees init() to run in the same locale as the caller, thus some minimal placement/locality awareness is still needed here. BUT, we could make it _relative_ (as opposed to absolute) to preserve the resource abstraction. Locality constraint could be supported in the form \"same locale as me\" (affinity) or \"not same locale as me\" (anti-affinity).\r\n\r\nAttempting to achieve everything we need by using what the system already provides is the way to go. As tempting as it is, I would discourage side-channel (i.e. internal/invisible) data/task propagation/distribution/broadcast. Thinking about and exposing expressive/composable basic system primitives will make Ray feel more and more like a microkernel!"
      },
      {
        "user": "robertnishihara",
        "created_at": "2017-01-10T21:45:14Z",
        "body": "Closing for now."
      },
      {
        "user": "AdamGleave",
        "created_at": "2018-05-19T04:09:18Z",
        "body": "Just wanted to second this request. In my context, I want to set some defaults for libraries I use (Python logging, NumPy print options, etc) on all workers. I'm working around it by just stuffing this code in __init__.py which will get executed on all the workers, but this is pretty nasty."
      },
      {
        "user": "liujie329",
        "created_at": "2022-06-28T02:47:08Z",
        "body": "@robertnishihara \r\nhi,  at the released version: 1.13.0  , worker does't have   the :  ray.worker.get_global_worker()   function . and how can i running a function on all workers now ?\r\n\r\n"
      }
    ]
  }
]