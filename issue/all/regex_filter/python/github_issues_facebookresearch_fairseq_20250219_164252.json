[
  {
    "number": 5466,
    "title": "Question about data preparation with speech data alignment in speech matrix dataset",
    "created_at": "2024-03-24T03:42:51Z",
    "closed_at": "2024-03-24T19:17:27Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5466",
    "body": "During data preparation of speech matrix, for aligned_speech tsv files, the files shown as: \r\n\r\n```\r\nscore\tlt_audio\tsl_audio\r\n1.121542\tlt_aud.zip:5203779181:21527\tsl_aud.zip:50446110544:22547\r\n1.1027344\tlt_aud.zip:132563238:14033\tsl_aud.zip:3224296345:11940\r\n1.1023445\tlt_aud.zip:6292033729:49818\tsl_aud.zip:17374011756:20890\r\n```\r\n\r\nwhich have different formats with the audio titles in raw audio folders for each language, for example in the folder audios/lt/, there is:\r\n\r\n```\r\nls | head -n 5\r\n20090112-0900-PLENARY-10_lt_1079616_1086270.ogg\r\n20090112-0900-PLENARY-10_lt_1133568_1136670.ogg\r\n20090112-0900-PLENARY-10_lt_1238304_1242270.ogg\r\n20090112-0900-PLENARY-10_lt_1288704_1292862.ogg\r\n20090112-0900-PLENARY-10_lt_1288704_1296606.ogg\r\n```\r\n\r\nSo how do these two formats align with each other? I thought they could somehow be the same number pairs, but there are actually not.\r\n\r\nCould anybody help? Thank you so much!\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5466/comments",
    "author": "tony-sensei",
    "comments": [
      {
        "user": "shanhaidexiamo",
        "created_at": "2024-10-31T09:13:32Z",
        "body": "can I ask you about the download url of speech matrix dataset ? "
      }
    ]
  },
  {
    "number": 5302,
    "title": "Question reg. fairseq mms/data_prep/align_and_segment.py",
    "created_at": "2023-08-30T01:37:20Z",
    "closed_at": "2023-09-06T21:56:12Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5302",
    "body": " I am testing the mms/data_prep/align_and_segment.py code with some datasets and wanted to know some info. In the paper, its mentioned that \"After manual inspection of sample quality and their corresponding score for several languages, we select −0.2 as the threshold and choose samples with scores greater than this threshold.\" \r\nSo does the mms/data_prep/align_and_segment.py code do the above mentioned qc, in essence I want to know if the outputs from this code were given directly as inputs to the tts model as training data? If not could you point me to the code that does the qc . I am just testing with various inputs to see if TTS model can be improved for a specific language.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5302/comments",
    "author": "spanta28",
    "comments": [
      {
        "user": "vineelpratap",
        "created_at": "2023-09-06T21:33:52Z",
        "body": "The \"align_and_segment.py\" doesn't do the filtering mentioned above. \r\nThe reason we filter is because we were using a specific dataset based on religious texts which we found to be noisy. It won't be needed for cases where audio and the corresponding text match. \r\n\r\n\r\n\r\n"
      },
      {
        "user": "spanta28",
        "created_at": "2023-09-06T21:56:09Z",
        "body": "Got it. Thanks"
      }
    ]
  },
  {
    "number": 5232,
    "title": "What happened to the facebook repo on HuggingFace?",
    "created_at": "2023-07-01T18:45:12Z",
    "closed_at": "2023-07-02T17:58:13Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5232",
    "body": "Could I ask for that? I couldn't find any models from Facebook at this time (just today).",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5232/comments",
    "author": "tarudesu",
    "comments": [
      {
        "user": "psamathido",
        "created_at": "2023-07-02T00:28:07Z",
        "body": "same here :("
      }
    ]
  },
  {
    "number": 5205,
    "title": "[MMS ASR] The doc for calc WER/CER can be unclear",
    "created_at": "2023-06-15T10:16:55Z",
    "closed_at": "2023-06-17T02:34:30Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5205",
    "body": "As the doc says:\r\n\r\n> $ ls /path/to/manifest\r\n> dev.tsv\r\n> dev.wrd\r\n> dev.ltr\r\n> dev.uid\r\n> \r\n> \\# dev.tsv each line contains <audio>  <number_of_sample>\r\n> $ cat dev.tsv\r\n> /\r\n> /path/to/audio_1  180000\r\n> /path/to/audio_2  200000\r\n> \r\n> $ cat dev.ltr\r\n> t h i s | i s | o n e |\r\n> t h i s | i s | t w o |\r\n> \r\n> $ cat dev.wrd\r\n> this is one\r\n> this is two\r\n> \r\n> $ cat dev.uid\r\n> audio_1\r\n> audio_2\r\n\r\nThere are several points that can lead to misunderstanding. Is the  /path/to/audio_1 indicates a directory or a wav file? What does the number of samples mean? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5205/comments",
    "author": "CopyNinja1999",
    "comments": [
      {
        "user": "androstj",
        "created_at": "2023-06-17T02:34:30Z",
        "body": "I added new readme and scripts on how to get the sample for waveform. Please check examples/mms/misc/get_sample_size.py"
      }
    ]
  },
  {
    "number": 5169,
    "title": "MMS ASR models with shared multilingual vocabulary",
    "created_at": "2023-05-29T11:26:51Z",
    "closed_at": "2023-05-31T19:52:55Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5169",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nAre there any plans to release MMS ASR models with shared multilingual vocabulary (as described in the first paragraph of section 5.1 of the paper)?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5169/comments",
    "author": "akreal",
    "comments": [
      {
        "user": "androstj",
        "created_at": "2023-05-31T19:52:55Z",
        "body": "At this moment, we don't plan to release a shared multilingual vocab model."
      }
    ]
  },
  {
    "number": 5162,
    "title": "For MMS TTS, is it possible to add pauses, emotion, inflection, ect?",
    "created_at": "2023-05-26T08:36:49Z",
    "closed_at": "2023-05-31T15:36:51Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5162",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI am playing with and learning about the MMS TTS. I have it running and am curious if it is possible to adjust the output to have things like pauses, emotion, & inflection.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5162/comments",
    "author": "JWesorick",
    "comments": [
      {
        "user": "chevalierNoir",
        "created_at": "2023-05-26T21:59:50Z",
        "body": "The MMS TTS model (VITS) is a probabilistic model. Thus you will get a different audio each time you run (suppose the random seed is not specified). For more controllable generation (e.g., generate an utterance of a particular type of emotion), it's not supported yet. And we will incorporate that in our next release.  "
      },
      {
        "user": "Adlinga",
        "created_at": "2023-06-20T10:24:07Z",
        "body": "I've found that in the given state pauses could be ajusted by adding spaces and apostrophes.\r\nFor example try to generate:\r\n\"Hello my name is Gosha\"\r\n\"Hello  '  my name is Gosha\""
      }
    ]
  },
  {
    "number": 5150,
    "title": "How can we use Massively Multilingual Speech (MMS) commercially?",
    "created_at": "2023-05-25T01:23:35Z",
    "closed_at": "2023-05-25T18:42:48Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5150",
    "body": "Can MMS be used commercially? How?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5150/comments",
    "author": "troublesprouter",
    "comments": [
      {
        "user": "vineelpratap",
        "created_at": "2023-05-25T18:42:48Z",
        "body": "MMS is released under CC-BY-NC 4.0 license. It cannot be used for commercial purposes."
      },
      {
        "user": "TechInterMezzo",
        "created_at": "2024-06-20T15:02:41Z",
        "body": "@vineelpratap \r\n> MMS is released under CC-BY-NC 4.0 license. It cannot be used for commercial purposes.\r\n\r\nIs it possible to create a synthetic dataset with MMS output and allow commercial use for that dataset?"
      }
    ]
  },
  {
    "number": 5041,
    "title": "Textless NLP / dGSLM: the pre-trained  SpeechDLM model with poor performance",
    "created_at": "2023-03-23T08:59:05Z",
    "closed_at": "2023-04-11T09:05:16Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5041",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nThe pre-trained SpeechDLM model gets poor performance on the Fisher valid dataset. \r\n\r\nMoreover, when loading the checkpoint, there is no criterion named 'duration_cross_multichannel_cross_entropy'. The `state['cfg']['criterion']` of the checkpoint is `{'_name': 'duration_cross_multichannel_cross_entropy', 'sentence_avg': False, 'main_and_cross_weights': '1,0', 'include_edge_loss': True, 'edge_loss_weight': 1.0, 'general_loss_weight': 0.0, 'normalize_edge_loss': True, 'duration_loss_weight': 1.0, 'shift_duration_target': True, 'round_duration_prediction': False, 'normalize_duration_loss': False}`.\r\n\r\nWe replaced the `state['cfg']['criterion']` with` {'_name': 'speech_dlm_criterion', 'sentence_avg': False, 'main_and_cross_weights': '1,0', 'general_unit_loss_weight': 0.0, 'edge_unit_loss_weight': 1.0, 'duration_loss_weight': 1.0}`.\r\n\r\nThen the checkpoint can be loaded sucessfully but the loss on the valid dataset is abnormal. The loss is shown as follows. Is there any wrong with the checkpoint?\r\n\r\n#### Code\r\nWe run the command\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1 fairseq-validate {BIN_DATA_DIR} \\\r\n    --task speech_dlm_task \\\r\n    --path {CHECKPOINT_DIR} \\\r\n    --max-tokens 6144\r\n```\r\nand the result shows \"2023-03-23 16:23:22 | INFO | valid | valid on 'valid' subset | [unitA-unitA]edge_token_loss 9.351 | [unitA-unitA]edge_token_pred_acc 8.689 | [unitA-unitA]edge_duration_loss 1.324 | [unitA-unitA]edge_duration_pred_acc 52.054 | [unitB-unitB]edge_token_loss 9.418 | [unitB-unitB]edge_token_pred_acc 7.937 | [unitB-unitB]edge_duration_loss 1.39 | [unitB-unitB]edge_duration_pred_acc 52.463 | edge_token_loss 22.028 | edge_token_pred_acc 8.317 | edge_duration_loss 3.441 | edge_duration_pred_acc 52.256 | loss 11.459 | [unitA-unitA]edge_token_ppl 653.15 | [unitB-unitB]edge_token_ppl 684.18 | edge_token_ppl 4.27656e+06 | wps 0 | wpb 6.40306e+06 | bsz 522\".\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n#### What have you tried?\r\nread the code and paper, and check the dataset.\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main):`0.12.2`\r\n - PyTorch Version (e.g., 1.0):`1.12`\r\n - OS (e.g., Linux):`Ubuntu 18.04.4 LTS`\r\n - How you installed fairseq (`pip`, source): `source`\r\n - Build command you used (if compiling from source): `pip3 install --editable ./`\r\n - Python version: `3.9.13`\r\n - CUDA/cuDNN version:`CUDA10.2`\r\n - GPU models and configuration:`NVIDIA RTX 2080Ti`\r\n - Any other relevant information:-\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5041/comments",
    "author": "mkd-mkd",
    "comments": [
      {
        "user": "mkd-mkd",
        "created_at": "2023-04-11T09:05:16Z",
        "body": "I'm sorry, it is my fault"
      },
      {
        "user": "tuanh208",
        "created_at": "2023-06-06T12:54:06Z",
        "body": "Thanks for pointing out the criterion name mismatch. I have updated the criterion in the model checkpoint. "
      }
    ]
  },
  {
    "number": 5037,
    "title": "Where to get data2vec 2.0 token dictionary?",
    "created_at": "2023-03-22T23:05:49Z",
    "closed_at": "2023-03-27T20:58:35Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5037",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nIs the token dictionary for the data2vec 2.0 text model available anywhere? The 'data' field in the 'task' dictionary points to '/fsx-wav2vec/abaevski/data/nlp/bookwiki_aml-full-mmap2-bin', and I'm unable to load the checkpoint without the dict.text file. Is the dictionary identical to the RoBERTa 50k BPE or is yours different due to only being trained on BooksCorpus and English Wikipedia? Any help here is appreciated, thanks! @alexeib\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n`model, args, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([path_to_d2v2_text_cp], strict=False)`\r\n\r\n>FileNotFoundError: [Errno 2] No such file or directory: '/fsx-wav2vec/abaevski/data/nlp/bookwiki_aml-full-mmap2-bin/dict.txt'\r\n\r\n#### What have you tried?\r\n\r\nRead the README file and tried looking through the checkpoint to find any dictionary.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): 0.12.2\r\n - PyTorch Version (e.g., 1.0): 1.13\r\n - OS (e.g., Linux): Ubuntu 20.04.5 LTS\r\n - How you installed fairseq (`pip`, source): `pip install ./` after cloning fairseq repo\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.16\r\n - CUDA/cuDNN version: 11.8\r\n - GPU models and configuration: Tesla T4\r\n - Any other relevant information: running in Google Colab\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5037/comments",
    "author": "DanielFLevine",
    "comments": [
      {
        "user": "wnhsu",
        "created_at": "2023-03-27T20:58:35Z",
        "body": "Fixed in #5045 "
      }
    ]
  },
  {
    "number": 4967,
    "title": "fail to convert data2vec2 model with torch.jit.script",
    "created_at": "2023-02-01T17:36:52Z",
    "closed_at": "2023-02-04T12:42:14Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4967",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\ntried to convert data2vec2 model with torch.jit.script\r\nbut got the following error\r\n\r\n```\r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py\", line 375, in get_type_hint_captures                                                   \r\n    src = inspect.getsource(fn)                                                                                                                               \r\n  File \"/opt/conda/lib/python3.8/inspect.py\", line 1012, in getsource          \r\n    lines, lnum = getsourcelines(object)                                                                                                                      \r\n  File \"/opt/conda/lib/python3.8/inspect.py\", line 994, in getsourcelines                                                                                     \r\n    lines, lnum = findsource(object)                                                                                                                          \r\n  File \"/opt/conda/lib/python3.8/inspect.py\", line 813, in findsource                                                                                         \r\n    raise OSError('could not get source code')\r\n```\r\n\r\nthe error is raised because it fails to get source file from the object(<function \\_\\_create_fn\\_\\_.<locals>.\\_\\_init\\_\\_ at 0x7fda58ad08b0>), which is the instance of the module(<module 'examples.data2vec.models.modalities.audio' from '/fairseq/examples/data2vec/models/modalities/audio.py'>)\r\n\r\nso i just directly give the correct path, but got another error\r\n\r\n```\r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py\", line 455, in createResolutionCallbackForClassMethods                                  \r\n    captures.update(get_type_hint_captures(fn))                                \r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py\", line 396, in get_type_hint_captures                                                   \r\n    raise RuntimeError(f\"Expected {fn} to be a function\")                                                                                                     \r\nRuntimeError: Expected <function __create_fn__.<locals>.__init__ at 0x7fea5ae888b0> to be a function                                                          \r\n                                                                                                                                                              \r\nDuring handling of the above exception, another exception occurred:\r\n...\r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py\", line 230, in infer_concrete_type_builder                                             \r\n    sub_concrete_type = get_module_concrete_type(item, share_types)                                                                                           \r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py\", line 424, in get_module_concrete_type                                                \r\n    concrete_type = concrete_type_store.get_or_create_concrete_type(nn_module)                                                                                \r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py\", line 365, in get_or_create_concrete_type                                             \r\n    concrete_type_builder = infer_concrete_type_builder(nn_module)                                                                                            \r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py\", line 329, in infer_concrete_type_builder                                             \r\n    attr_type, inferred = infer_type(name, value)                                                                                                             \r\n  File \"/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py\", line 183, in infer_type                                                              \r\n    raise RuntimeError(                                                                                                                                       \r\nRuntimeError: Error inferring type for modality_cfg: {'type': <Modality.AUDIO: 1>, 'prenet_depth': 4, 'prenet_layerdrop': 0.1, 'prenet_dropout': 0.0, 'start_d\r\nrop_path_rate': 0.0, 'end_drop_path_rate': 0.0, 'num_extra_tokens': 0, 'init_extra_token_zero': True, 'mask_noise_std': 0.01, 'mask_prob_min': None, 'mask_pro\r\nb': 0.45, 'inverse_mask': False, 'mask_prob_adjust': 0.05, 'keep_masked_pct': 0.0, 'mask_length': 10, 'add_masks': False, 'remove_masks': False, 'mask_dropout\r\n': 0.0, 'encoder_zero_mask': False, 'mask_channel_prob': 0.1, 'mask_channel_length': 64, 'ema_local_encoder': False, 'local_grad_mult': 0.0, 'use_alibi_encode\r\nr': True, 'alibi_scale': 1.0, 'learned_alibi': False, 'alibi_max_pos': None, 'learned_alibi_scale': True, 'learned_alibi_scale_per_head': True, 'learned_alibi\r\n_scale_per_layer': False, 'num_alibi_heads': 12, 'model_depth': 8, 'decoder': {'decoder_dim': 384, 'decoder_groups': 16, 'decoder_kernel': 7, 'decoder_layers'\r\n: 4, 'input_dropout': 0.1, 'add_positions_masked': False, 'add_positions_all': False, 'decoder_residual': True, 'projection_layers': 1, 'projection_ratio': 2.\r\n0}, 'extractor_mode': 'layer_norm', 'feature_encoder_spec': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_pos_width': 95, 'conv_pos_\r\ngroups': 16, 'conv_pos_depth': 5, 'conv_pos_pre_ln': False}: Expected <function __create_fn__.<locals>.__init__ at 0x7fea5ae888b0> to be a function\r\n```\r\n\r\nany idea?\r\nThank you for the help\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version (e.g., 1.0) : 1.13\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4967/comments",
    "author": "oewi",
    "comments": [
      {
        "user": "oewi",
        "created_at": "2023-02-04T12:42:14Z",
        "body": "solved"
      },
      {
        "user": "ArtyZe",
        "created_at": "2023-06-27T07:11:44Z",
        "body": "> the error is raised because it fails to get source file from the object(<function __create_fn__..__init__ at 0x7fda58ad08b0>), which is the instance of the module(<module 'examples.data2vec.models.modalities.audio' from '/fairseq/examples/data2vec/models/modalities/audio.py'>)\r\n\r\nhello, how could you be sure fn is for examples.data2vec.models.modalities.audio ?  and add path to where ? thanks"
      }
    ]
  },
  {
    "number": 4912,
    "title": "multilingual finetuning NLLB translation",
    "created_at": "2022-12-20T15:34:53Z",
    "closed_at": "2022-12-20T22:23:30Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4912",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nHow can I finetune the NLLB multilingual translation model on multiple language pairs together?\r\nI find the document for finetuning NLLB but it only supports one language pair. \r\nAnyone has experience or how should I adapt the training code to do it?\r\nThanks in advance! @elbayadm @vedanuj",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4912/comments",
    "author": "edchengg",
    "comments": [
      {
        "user": "elbayadm",
        "created_at": "2022-12-20T19:57:47Z",
        "body": "Hi @edchengg!\r\nFor finetuning on a single pair we provided the command\r\n```\r\nDROP=0.1\r\npython examples/nllb/modeling/train/train_script.py \\\r\n    cfg=nllb200_dense3.3B_finetune_on_fbseed \\\r\n    cfg/dataset=$DATA_CONFIG \\\r\n    cfg.dataset.lang_pairs=\"$SRC-$TGT\" \\\r\n    cfg.fairseq_root=$(pwd) \\\r\n    cfg.output_dir=$OUTPUT_DIR \\\r\n    cfg.dropout=$DROP \\\r\n    cfg.warmup=10 \\\r\n    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt\r\n\r\n```\r\nIf you want to finetune NLLB on multiple language pairs at the same time you can list all directions  with:\r\n`cfg.dataset.lang_pairs=\"$SRC1-$TGT1,$SRC2-$TGT2\"`\r\nThis assumes that the dataset config you point to with\r\n`cfg/dataset=$DATA_CONFIG` has all those pairs in it.\r\nSpecifying `cfg.dataset.lang_pairs` in the command above overwrites the list of pairs in the data config; if you want to train/finetune on all pairs in the dataset, you can skip that line all together. "
      },
      {
        "user": "edchengg",
        "created_at": "2022-12-20T22:23:30Z",
        "body": "Thanks a lot!"
      }
    ]
  },
  {
    "number": 4900,
    "title": "Details about wav2vec2.0 pretraining hyper parameter ?",
    "created_at": "2022-12-10T12:50:26Z",
    "closed_at": "2022-12-13T07:43:29Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4900",
    "body": "What hyperparameters to look for when pretraining a wav2vec2 model.\r\nwhen can one tell that the model is sufficiently trained enough, and by looking at what parameters, \r\nwhat  range will it be for those parameter, to say  that model is now trained enough. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4900/comments",
    "author": "mujhenahiata",
    "comments": [
      {
        "user": "mujhenahiata",
        "created_at": "2022-12-10T13:04:59Z",
        "body": "@alexeib please help"
      },
      {
        "user": "alexeib",
        "created_at": "2022-12-13T07:43:29Z",
        "body": "it is not really possible to tell if the model is \"sufficiently trained\" without evaluating it on some downstream task. you could implement periodic evaluation of checkpoints for instance. however that is still not great because we tend to use annealing learning rate schedules, and the models improve significantly when they are exposed to lower learning rates. the best advice i have is to train a few models for various lengths and see when it stops improving\r\n\r\nas for hyper-parameters, we have a good starting set up of parameters in our config files (which the example in the README points to)"
      }
    ]
  },
  {
    "number": 4874,
    "title": "AttributeError: module 'fairseq' has no attribute 'checkpoint_utils'",
    "created_at": "2022-11-20T10:22:08Z",
    "closed_at": "2022-11-20T13:50:23Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4874",
    "body": "# Code\r\nimport torch\r\nimport fairseq\r\n\r\ncp_path = '/path/to/wav2vec.pt'\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\r\n\r\n# Error\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'fairseq' has no attribute 'checkpoint_utils'\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4874/comments",
    "author": "tickwizard",
    "comments": [
      {
        "user": "lcc-404",
        "created_at": "2023-04-11T06:38:55Z",
        "body": "Hi,\r\nI have the same problem, how did you solve it? tkx!"
      },
      {
        "user": "liutaocode",
        "created_at": "2023-05-05T01:24:13Z",
        "body": "Current path may contain fairseq file. just remove or rename it."
      },
      {
        "user": "Ezreal11",
        "created_at": "2023-07-21T07:09:19Z",
        "body": "My code was at the same level of fairseq folder, move the code to the new folder and I solved it."
      }
    ]
  },
  {
    "number": 4863,
    "title": "Closing a massive performance gap on 4xV100 vs 4xA100",
    "created_at": "2022-11-14T09:59:38Z",
    "closed_at": "2022-11-15T08:40:27Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4863",
    "body": "I am training a seq2seq model with the `translation` task and I noticed massive performance discrepancy depending on where I train it. I have access to 4x32GB V100 GPUs and 4x80GB A100 GPUs. Basically, the V100s outperform A100s from epochs 3 onwards. I ran two identical configs on both and these are the training and validation logs for the first 5 epochs (the discrepancies get larger and larger over time):\r\n\r\n```\r\nA100:\r\n2022-11-13 23:31:37 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.329 | nll_loss 5.056 | ppl 33.26 | wps 70888.8 | wpb 16761.7 | bsz 1666.7 | num_updates 1134\r\n2022-11-14 01:41:24 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.773 | nll_loss 4.445 | ppl 21.78 | wps 67359.6 | wpb 16761.7 | bsz 1666.7 | num_updates 2274 | best_loss 5.773\r\n2022-11-14 03:51:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.655 | nll_loss 4.32 | ppl 19.98 | wps 68505.8 | wpb 16761.7 | bsz 1666.7 | num_updates 3415 | best_loss 5.655\r\n2022-11-14 06:01:08 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.597 | nll_loss 4.258 | ppl 19.13 | wps 66829.6 | wpb 16761.7 | bsz 1666.7 | num_updates 4555 | best_loss 5.597\r\n2022-11-14 08:10:29 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.526 | nll_loss 4.176 | ppl 18.08 | wps 65220.1 | wpb 16761.7 | bsz 1666.7 | num_updates 5695 | best_loss 5.526\r\nV100:\r\n2022-11-09 12:53:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.345 | nll_loss 5.055 | ppl 33.23 | wps 46087.1 | wpb 16761.7 | bsz 1666.7 | num_updates 1134\r\n2022-11-09 15:59:28 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.786 | nll_loss 4.449 | ppl 21.84 | wps 44076.7 | wpb 16761.7 | bsz 1666.7 | num_updates 2274 | best_loss 5.786\r\n2022-11-09 19:05:16 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.549 | nll_loss 4.187 | ppl 18.22 | wps 44535.1 | wpb 16761.7 | bsz 1666.7 | num_updates 3415 | best_loss 5.549\r\n2022-11-09 22:11:50 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.475 | nll_loss 4.103 | ppl 17.19 | wps 46866.6 | wpb 16761.7 | bsz 1666.7 | num_updates 4555 | best_loss 5.475\r\n2022-11-10 01:17:39 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.387 | nll_loss 4.014 | ppl 16.15 | wps 45486.7 | wpb 16761.7 | bsz 1666.7 | num_updates 5695 | best_loss 5.387```\r\n\r\nA100:\r\n2022-11-13 23:32:02 | INFO | train | epoch 001 | loss 7.531 | nll_loss 6.505 | ppl 90.8 | wps 81361.4 | ups 0.15 | wpb 554217 | bsz 56777.6 | num_updates 1134 | lr 0.0002835 | gnorm 1.176 | clip 2.4 | loss_scale 1 | train_wall 6925 | gb_free 67.1 | wall 7884\r\n2022-11-14 01:41:48 | INFO | train | epoch 002 | loss 6.007 | nll_loss 4.75 | ppl 26.91 | wps 81171.9 | ups 0.15 | wpb 554381 | bsz 56793.4 | num_updates 2274 | lr 0.0005685 | gnorm 0.445 | clip 0 | loss_scale 0.5 | train_wall 6923 | gb_free 67.3 | wall 15669\r\n2022-11-14 03:51:59 | INFO | train | epoch 003 | loss 5.701 | nll_loss 4.409 | ppl 21.24 | wps 80983.5 | ups 0.15 | wpb 554388 | bsz 56768.1 | num_updates 3415 | lr 0.00085375 | gnorm 0.347 | clip 0 | loss_scale 0.5 | train_wall 6933 | gb_free 64.3 | wall 23480\r\n2022-11-14 06:01:33 | INFO | train | epoch 004 | loss 5.589 | nll_loss 4.283 | ppl 19.47 | wps 81290.6 | ups 0.15 | wpb 554364 | bsz 56787.5 | num_updates 4555 | lr 0.0009371 | gnorm 0.345 | clip 0 | loss_scale 1 | train_wall 6964 | gb_free 66.8 | wall 31255\r\n2022-11-14 08:10:53 | INFO | train | epoch 005 | loss 5.525 | nll_loss 4.212 | ppl 18.54 | wps 81436.1 | ups 0.15 | wpb 554346 | bsz 56770.4 | num_updates 5695 | lr 0.000838075 | gnorm 0.326 | clip 0 | loss_scale 0.5 | train_wall 6878 | gb_free 65.3 | wall 39015\r\nV100:\r\n2022-11-09 12:53:51 | INFO | train | epoch 001 | loss 7.533 | nll_loss 6.506 | ppl 90.92 | wps 56458.9 | ups 0.1 | wpb 554265 | bsz 56783.9 | num_updates 1134 | lr 0.0002835 | gnorm 1.175 | clip 2.5 | loss_scale 1 | train_wall 10190 | gb_free 19.6 | wall 11366\r\n2022-11-09 16:00:14 | INFO | train | epoch 002 | loss 6.006 | nll_loss 4.749 | ppl 26.88 | wps 56505.9 | ups 0.1 | wpb 554291 | bsz 56778.7 | num_updates 2274 | lr 0.0005685 | gnorm 0.448 | clip 0.1 | loss_scale 0.5 | train_wall 10170 | gb_free 19.8 | wall 22549\r\n2022-11-09 19:06:01 | INFO | train | epoch 003 | loss 5.7 | nll_loss 4.408 | ppl 21.23 | wps 56750.5 | ups 0.1 | wpb 554454 | bsz 56778.3 | num_updates 3415 | lr 0.00085375 | gnorm 0.349 | clip 0 | loss_scale 1 | train_wall 10139 | gb_free 16.8 | wall 33696\r\n2022-11-09 22:12:37 | INFO | train | epoch 004 | loss 5.587 | nll_loss 4.28 | ppl 19.43 | wps 56447.8 | ups 0.1 | wpb 554364 | bsz 56787.5 | num_updates 4555 | lr 0.0009371 | gnorm 0.35 | clip 0 | loss_scale 1 | train_wall 10203 | gb_free 19.4 | wall 44892\r\n2022-11-10 01:18:24 | INFO | train | epoch 005 | loss 5.517 | nll_loss 4.203 | ppl 18.42 | wps 56692.7 | ups 0.1 | wpb 554361 | bsz 56774.3 | num_updates 5695 | lr 0.000838075 | gnorm 0.327 | clip 0 | loss_scale 0.5 | train_wall 10164 | gb_free 17.9 | wall 56039\r\n```\r\n\r\nThe training performance gap also becomes larger over time (at epoch 20 it's 0.3 loss points).\r\n   \r\n   I run training with\r\n   \r\n   ```CUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train $setting \\\r\n    --memory-efficient-fp16 \\\r\n    --wandb-project $model \\\r\n    --log-interval 10 \\\r\n    --max-update 1000000 --patience 5 \\ \r\n    --task translation --arch transformer_wmt_en_de_big \\\r\n    --share-all-embeddings \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n    --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --label-smoothing 0.1 --criterion label_smoothed_cross_entropy \\\r\n    --dropout 0.3 --weight-decay 0.0 \\\r\n    --save-dir ${ckpt} --no-epoch-checkpoints \\\r\n    --max-tokens 21288 --update-freq 16 --lr 0.001 \\\r\n    --ddp-backend=$backend --clip-norm 5.0 \\\r\n    --seed 1\r\n```\r\n   \r\n   \r\nMy environment:\r\n - fairseq Version (e.g., 1.0 or main): 0.12.2\r\n - PyTorch Version (e.g., 1.0):\r\n   - A100: 1.12 (py3.10_cuda11.3_cudnn8.3.2_0 from channel pytorch)\r\n   - V100: 1.12 (installed with pip)\r\n - OS (e.g., Linux): CentOS 7\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable fairseq`\r\n - Python version: 3.10\r\n - CUDA/cuDNN version:\r\n   - A100: 1.12 (cuda 11.3)\r\n   - V100: unk\r\n   \r\nI might be wrong but with identical setups the two nodes should perform *very* similarly. Is there any way I could close this performance gap? Could it be the pytorch environment issue? (The A100s had very specific cuda needs, hence the non-standard install).",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4863/comments",
    "author": "st-vincent1",
    "comments": [
      {
        "user": "WilliamTambellini",
        "created_at": "2022-11-14T16:37:50Z",
        "body": "cuda 11.3 could be slightly too old to take full advantage of Ampere gpus. Perhaps try to upgrade to a more recent build of pytorch 1.12 but using recent cuda.\r\nAlso keep an eye on the nvidia drivers version (reported by nvidia-smi): old drivers could run recent Ampere gpus but not at optimal speed. "
      },
      {
        "user": "st-vincent1",
        "created_at": "2022-11-15T08:40:27Z",
        "body": "Thank you so much William!\r\n\r\nThe NVIDIA drivers were up to date, but I installed a newer version of PyTorch 1.12 with CUDA 11.6 and this seemingly closed the gap (posting logs from A100 run with CUDA 11.6 for completeness):\r\n\r\n```\r\n2022-11-14 21:50:18 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.328 | nll_loss 5.055 | ppl 33.24 | wps 70713.7 | wpb 16761.7 | bsz 1666.7 | num_updates 1134\r\n2022-11-14 23:54:53 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.784 | nll_loss 4.451 | ppl 21.87 | wps 74273.7 | wpb 16761.7 | bsz 1666.7 | num_updates 2274 | best_loss 5.784\r\n2022-11-15 01:59:42 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.592 | nll_loss 4.228 | ppl 18.74 | wps 71179.6 | wpb 16761.7 | bsz 1666.7 | num_updates 3414 | best_loss 5.592\r\n2022-11-15 04:04:45 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.472 | nll_loss 4.096 | ppl 17.1 | wps 72783.2 | wpb 16761.7 | bsz 1666.7 | num_updates 4554 | best_loss 5.472\r\n2022-11-15 06:09:33 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.375 | nll_loss 3.997 | ppl 15.97 | wps 73489.5 | wpb 16761.7 | bsz 1666.7 | num_updates 5694 | best_loss 5.375\r\n\r\n2022-11-14 21:50:43 | INFO | train | epoch 001 | loss 7.532 | nll_loss 6.505 | ppl 90.85 | wps 83983.3 | ups 0.15 | wpb 554217 | bsz 56777.6 | num_updates 1134 | lr 0.0002835 | gnorm 1.175 | clip 2.4 | loss_scale 1 | train_wall 6871 | gb_free 73.8 | wall 7621\r\n2022-11-14 23:55:20 | INFO | train | epoch 002 | loss 6.008 | nll_loss 4.752 | ppl 26.94 | wps 84527 | ups 0.15 | wpb 554381 | bsz 56793.4 | num_updates 2274 | lr 0.0005685 | gnorm 0.446 | clip 0 | loss_scale 0.5 | train_wall 6825 | gb_free 73.7 | wall 15098\r\n2022-11-15 02:00:06 | INFO | train | epoch 003 | loss 5.702 | nll_loss 4.41 | ppl 21.26 | wps 84434.5 | ups 0.15 | wpb 554487 | bsz 56774.7 | num_updates 3414 | lr 0.0008535 | gnorm 0.35 | clip 0 | loss_scale 0.5 | train_wall 6816 | gb_free 72.2 | wall 22584\r\n2022-11-15 04:05:12 | INFO | train | epoch 004 | loss 5.582 | nll_loss 4.275 | ppl 19.36 | wps 84195.3 | ups 0.15 | wpb 554342 | bsz 56796 | num_updates 4554 | lr 0.000937203 | gnorm 0.345 | clip 0 | loss_scale 0.5 | train_wall 6863 | gb_free 73.6 | wall 30090\r\n2022-11-15 06:10:00 | INFO | train | epoch 005 | loss 5.508 | nll_loss 4.193 | ppl 18.29 | wps 84391.3 | ups 0.15 | wpb 554295 | bsz 56780.9 | num_updates 5694 | lr 0.000838149 | gnorm 0.322 | clip 0 | loss_scale 0.5 | train_wall 6838 | gb_free 72.4 | wall 37578\r\n2022-11-15 08:14:46 | INFO | train | epoch 006 | loss 5.46 | nll_loss 4.139 | ppl 17.62 | wps 84433.4 | ups 0.15 | wpb 554479 | bsz 56783.4 | num_updates 6834 | lr 0.000765055 | gnorm 0.305 | clip 0 | loss_scale 0.5 | train_wall 6812 | gb_free 74.6 | wall 45064\r\n```\r\n\r\nThese results more or less match the V100 runs from above."
      }
    ]
  },
  {
    "number": 4834,
    "title": "AttributeError: 'dict' object has no attribute '_get_node_flag'",
    "created_at": "2022-10-31T17:53:24Z",
    "closed_at": "2022-11-01T16:50:11Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4834",
    "body": "root/abc/venv/lib/python3.8/site-packages/omegaconf/omegaconf.py\", line 670, in open_dict\r\nprev_state = config._get_node_flag(\"struct\")\r\nAttributeError: 'dict' object has no attribute '_get_node_flag'\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4834/comments",
    "author": "MLDeep16",
    "comments": [
      {
        "user": "8904591612",
        "created_at": "2023-03-21T14:07:29Z",
        "body": "can you please tell me how did you resolve this issue\r\n"
      }
    ]
  },
  {
    "number": 4731,
    "title": "Question about the Product Kmeans in HuBERT",
    "created_at": "2022-09-16T06:17:01Z",
    "closed_at": "2022-09-19T08:33:48Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4731",
    "body": "## ❓ Questions and Help\r\nHi, Im trying to pretraining a new HuBERT model. I noticed the paper used Product K-means-{0,1,2}-100 to do opimization. But I dont find how and where to use it in the opensource code, which confusing me for a long time. Dose anyone know how to do it?\r\n\r\nThanks!!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4731/comments",
    "author": "wangkunpeng1215",
    "comments": [
      {
        "user": "Labyrintbs",
        "created_at": "2022-10-19T09:24:54Z",
        "body": "Thanks for your issue, I have the same question too, did you find the answer?"
      },
      {
        "user": "wangkunpeng1215",
        "created_at": "2022-10-20T03:10:07Z",
        "body": "> Thanks for your issue, I have the same question too, did you find the answer?\r\n\r\nHi, i dont know my understanding correct or not. I think the 'Product K-means-{0,1,2}' is just the concat of the zeroth,\r\nfirst, and second-order derivatives in MFCC extraction.  So it just used the original MFCC to do kmeans, as those in the open source  code."
      },
      {
        "user": "Labyrintbs",
        "created_at": "2022-10-20T12:29:06Z",
        "body": "\r\n\r\n\r\n> > Thanks for your issue, I have the same question too, did you find the answer?\r\n> \r\n> Hi, i dont know my understanding correct or not. I think the 'Product K-means-{0,1,2}' is just the concat of the zeroth, first, and second-order derivatives in MFCC extraction. So it just used the original MFCC to do kmeans, as those in the open source code.\r\n\r\nThank you! I understood :D"
      }
    ]
  },
  {
    "number": 4716,
    "title": "Parallel (batched) translation from different source languages",
    "created_at": "2022-09-12T06:46:11Z",
    "closed_at": "2022-09-14T12:30:53Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4716",
    "body": "Hi team, \r\n\r\nThe opportunity of parallel translation (in a single batch) from different source languages is of a particular interest,.\r\nThe current obstacle lies in the fact that the tokenizer depends on a single specified source language prior to make input embeddings. E.g., in the frame of `transformers` package, initially I should load a tokenizer with an indication of source language to use,\r\n`tokenizer = AutoTokenizer.from_pretrained(\"nllb-200-3.3B\", use_auth_token=True, src_lang='eng_Latn')`,\r\nand only then, I can define a batch of inputs:\r\n`inputs = tokenizer.prepare_seq2seq_batch(list_of_texts, return_tensors=\"pt\").to(device)`.\r\n\r\nIs there a possibility to organize the workflow in another manner, for example, to make batches of language-independent input embeddings feasible? Maybe somebody already had experience of real usage? Any proposals about code snippet to use?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4716/comments",
    "author": "molokanov50",
    "comments": [
      {
        "user": "gmryu",
        "created_at": "2022-09-12T13:31:00Z",
        "body": "I am no expert and I merely want to share my point of view.\r\n\r\nThe actual question is how do you make a tokenizer judge in which language this sentence is written? \r\nThere must be a ID token in front of that sentence, i.e. `<en> I like apples.`, to tell a tokenizer the language. (or an exhaustive program to identify it by reading characters)\r\n\r\nIf it is so, you can actually load all tokenizer you need first, then switch tokenizers if a sentence's language is changed from the previous sentence's.\r\nWell, actually you can also collect sentences written in the same language to form a batch. Then you shuffle/sample from those batches to make a multilingual batch.\r\nThis is the only way I believe."
      },
      {
        "user": "molokanov50",
        "created_at": "2022-09-14T12:30:53Z",
        "body": "I found out everything i need to sample `BatchEncoding` objects as you said - `input_ids` and `attention_mask` attributes need to be sampled correctly.\r\nAnd on the whole, your methodics helped me a lot, thanks!\r\n"
      }
    ]
  },
  {
    "number": 4687,
    "title": "The normalization settings of input audio",
    "created_at": "2022-09-01T13:50:32Z",
    "closed_at": "2022-09-18T06:06:09Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4687",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nIn wav2vec2.0 and hubert, the config `task.normalize` is set to `False` (which means not to normalize the input audio), but data2vec is set to `True`, and the original paper also mentioned it. Will it have a big effect on experiment result?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4687/comments",
    "author": "Ther-nullptr",
    "comments": [
      {
        "user": "Ther-nullptr",
        "created_at": "2022-09-01T13:50:53Z",
        "body": "@alexeib "
      },
      {
        "user": "alexeib",
        "created_at": "2022-09-18T06:06:03Z",
        "body": "it wont have a much of an effect, but you have to match the feature extractor to the normalization setting\r\n\r\nnormalize in dataloader -> layer norm in feature extractor\r\nno normalization in dataloader -> group norm in first block of feature extractor + feature_grad_mult = 0.1 (rescale feature extractor grads by 0.1)"
      }
    ]
  },
  {
    "number": 4636,
    "title": "Where is the list with all the possible languages?",
    "created_at": "2022-08-08T16:45:47Z",
    "closed_at": "2022-08-09T14:32:06Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4636",
    "body": "\r\nI need to translate texts with a lot of different languages.\r\nI have found some language's names at documentation, however not all.\r\n\r\nWhere can I get the full list of possible languages like `rus_Cyrl` and `eng_Latn`?\r\nOr is it possible to detect the `src_lang` inside the model?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4636/comments",
    "author": "lena-kru",
    "comments": [
      {
        "user": "gmryu",
        "created_at": "2022-08-09T10:07:46Z",
        "body": "You can load a fairseq checkpoint and see what is inside.\r\nSays:\r\n```\r\nfrom fairseq.checkpoint_utils import load_checkpoint_to_cpu\r\n\r\nall_state=load_checkpoint_to_cpu(\"{path_to_the_checkpoint.pt}\")\r\ncfgargs=all_state[\"cfg\"]\r\nmdlargs=cfgargs[\"model\"].__dict__\r\nprint(mdlargs[\"source_lang\"])\r\n```"
      }
    ]
  },
  {
    "number": 4587,
    "title": "[LanguagePairDataset]: Simplest way of loading embedding vectors as input (Encoder)?",
    "created_at": "2022-07-18T12:56:04Z",
    "closed_at": "2022-07-22T09:00:10Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4587",
    "body": "#### What is your question?\r\nI'm experimenting with loading **embedding vectors** into an Encoder in the \"transformer\" architecture. My inputs would look like this:\r\n`batch_size x embedding_dim x padded_input_length`\r\n\r\nand my outputs would be standard TranslationTask outputs:\r\n`batch_size x padded_input_length`\r\n\r\nWhile the outputs are tokens, the inputs are already encoded embeddings (hence the extra dimension).\r\n\r\nI'm fairly confident in modifying the `LanguagePairDataset` class to accept this input. However, I'm not sure how to change `load_langpair_dataset` within `fairseq/tasks/translation.py` to load numerical data of shape, as some helper functions used here (such as `load_indexed_dataset` within `fairseq/data/data_utils.py` presuppose token data and the use of a dictionary (which I will not need for the Encoder). \r\n\r\nWhat would be a minimal example/best way of going about loading this type of input? Perhaps the answer lies in using the `dataset_impl` argument?\r\n\r\nCould I get some pointers? :)\r\n\r\n#### Code\r\n\r\nI'm happy to share any snippets of what I have so far if needed.\r\n\r\nI'm using fairseq 0.12.2, installed with `pip install --editable ./` on Python 3.10.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4587/comments",
    "author": "st-vincent1",
    "comments": [
      {
        "user": "gmryu",
        "created_at": "2022-07-19T10:34:26Z",
        "body": "I assume you are using copies of `data/language_pair_dataset.py` and `tasks/translation.py`. (If you are not, copy them and modify the copy, not the origin)\r\n\r\nThen, `fairseq-preprocess`ed data is numerical. Dictionaries are passed in `load_indexed_dataset` but they are not used here.\r\nSo please calm down a bit. I do not believe it is a wise choice to mess with `dataset_impl` as they are mostly meant to save memory / speed up during excution.\r\n\r\nActually one step back, do you really need to use `load_indexed_dataset`? \r\nIf you do not require loading a huge amount of data via mmap, you can use your own script to load your binary data. \r\n(well, you can use `load_indexed_dataset` as well. In that case read through their implementation. They have some format to follow. Though I guess you want to start from `fairseq-preprocess`.)\r\n\r\nThe minimal way of creating a `LanguagePairDataset` is:\r\n```\r\nlines=[\"I am John.\", \"You are Alice.\"] # raw inputs.\r\nsrc_tokens = [\r\n        #each is a 1d LongTensor. pad, non-pad, length do not matter.\r\n        fs_dict.encode_line( _str , add_if_not_exist=False).long()\r\n        for _str in lines\r\n]\r\nsrc_lengths = [t.numel() for t in src_tokens]\r\n# the same for tgt_tokens\r\n\r\nLanguagePairDataset(\r\n        src_tokens, src_lengths, src_dict,\r\n        tgt_tokens, tgt_lengths, tgt_dict,\r\n        left_pad_source=True, ...\r\n)\r\n```\r\nA proof that you can use `list[torch.LongTensor]` instead of those fancy dataset. (may take more time during loading, but loading is seldom a bottle neck compared to training I belive)"
      },
      {
        "user": "st-vincent1",
        "created_at": "2022-07-21T08:28:11Z",
        "body": "@gmryu, Thanks for the reply! I'm working on copies of course :)\r\n\r\nI took your advice and looked away from `load_indexed_dataset` and `dataset_impl` and instead I just load the dataset directly in the `load_dataset` function within the `tasks/mytask.py` implementation. So far it seems to be working :)"
      }
    ]
  },
  {
    "number": 4559,
    "title": "how to get higher evaluate wmt14 blue ",
    "created_at": "2022-07-09T14:52:49Z",
    "closed_at": "2022-07-15T06:35:01Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4559",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nas title,the question is the same as #4477 \r\n#### Code\r\nBinarize the dataset\r\n\r\nfairseq-preprocess \\\r\n    --source-lang en --target-lang de \\\r\n    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\r\n    --destdir data-bin/wmt14_en_de --thresholdtgt 0 --thresholdsrc 0 --nwordssrc 44000 --nwordstgt 44000\\\r\n    --joined-dictionary --workers 20\r\n\r\nPYTHONIOENCODING=utf-8  fairseq-train \\\r\n    data-bin/wmt14_en_de \\\r\n    --arch transformer_wmt_en_de --share-all-embeddings \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 4096 \\\r\n    --max-tokens-valid 4096 \\\r\n    --update-freq 1 \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-detok moses \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\r\n    --save-dir checkpoints/wmt14_en_de/transformer/ckpt \\\r\n    --log-format json \\\r\n    --keep-last-epochs 5 \\\r\n    --max-epoch 30 \\\r\n    --fp16 \\\r\n\r\nEvaluate\r\nfairseq-generate data-bin/wmt14_en_de\r\n--path checkpoints/wmt14_en_de/transformer/ckpt/checkpoint_best.pt\r\n--batch-size 128 --beam 5 --remove-bpe\r\n--scoring sacrebleu\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n#### What have you tried?\r\nadd --scoring sacrebleu\r\nand change --nwordssrc 44000 --nwordstgt 44000 ->--nwordssrc 32768 --nwordstgt 32768 #3807\r\nand try compound_split_bleu.sh\r\n But there are still huge differences between valid(27.21) and evaluate(24.37(add --scoring sacrebleu))\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main):main\r\n - PyTorch Version (e.g., 1.0):1.10.1+cu111\r\n - OS (e.g., Linux):Linux\r\n - How you installed fairseq (`pip`, source):source\r\n - Build command you used (if compiling from source):\r\n - Python version:3.8\r\n - CUDA/cuDNN version:11.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\nthe problem did not appear in IWSLT'14",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4559/comments",
    "author": "tjshu",
    "comments": [
      {
        "user": "tjshu",
        "created_at": "2022-07-15T06:34:57Z",
        "body": "the question is in --update-freq 1 \r\ntrain in --update-freq 8 can get normal BLUE"
      },
      {
        "user": "BaohaoLiao",
        "created_at": "2022-10-25T19:00:39Z",
        "body": "> the question is in --update-freq 1 train in --update-freq 8 can get normal BLUE\r\n\r\nMay I ask why you use 44000 as the vocabulary size rather than 37000 in  \"attention is all you need\"?\r\n"
      },
      {
        "user": "tjshu",
        "created_at": "2022-10-26T03:14:25Z",
        "body": "> > the question is in --update-freq 1 train in --update-freq 8 can get normal BLUE\r\n> \r\n> May I ask why you use 44000 as the vocabulary size rather than 37000 in \"attention is all you need\"?\r\nI try two  vocabulary size,44000 better then 37000 a little \r\n37000  vocabulary size also can achieve the BLUE of \"attention is all you need\" \r\n"
      }
    ]
  },
  {
    "number": 4492,
    "title": "Model m2m-100 in fairseq-interactive mode",
    "created_at": "2022-06-15T11:27:53Z",
    "closed_at": "2022-06-16T08:56:56Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4492",
    "body": "I have a nice implementation of `fairseq-generate` on the pretrained m2m-100 model for EN-RU language pair. At the binarization stage with `fairseq-preprocess`, I noticed that `--srcdict` and `--tgtdict` options contain **the same** file, `model_dict.128k.txt` - exactly as it is recommended on the m2m-100 web page. That file is subsequently used in `--fixed-dictionary` option in `fairseq-generate`.\r\nThe main lack of this workflow is that I need to preprocess a **reference** translation file, which is a priori absent when my goal is simple translation without necessity to measure the quality of the translation. Thereby it's reasonable to assume that, if my EN-RU translation successfully completes with `fairseq-generate`, there is an opportunity to do the same with `fairseq-interactive`.\r\n\r\nInitially, when I tried `fairseq-interactive`, I got the error that files `dict.en.txt` and `dict.ru.txt` are not found. Really these dictionaries are not distributed with pretrained m2m-100 models. Then, if I copy the only available model dictionary `model_dict.128k.txt` twice, and rename one of those copies to `dict.en.txt`, the other - to `dict.ru.txt`, then there is no more error, but the output text turns out to be translated into a **random** language (EL, PT, etc. or even EN), as if my option `--target-lang ru` was ignored. The same occurs if I use `dict.en.txt` and `dict.ru.txt` files from my binarized data folder - they are the same as `model_dict.128k.txt`.\r\n\r\nThe full `fairseq-interactive` translation command which I use:\r\n\r\n`fairseq-interactive --input=testdata/ex.txt --path 1.2B_last_checkpoint.pt . --source-lang en --target-lang ru --tokenizer moses --bpe sentencepiece --sentencepiece-model spm.128k.model > testdata/ex.txt.out`\r\n\r\nAny ideas and suggestions to use m2m-100 in fairseq-interactive correctly?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4492/comments",
    "author": "molokanov50",
    "comments": [
      {
        "user": "gmryu",
        "created_at": "2022-06-16T01:11:44Z",
        "body": "To be honest, I have not tested m2m-100 myself.\r\nFrom a side perspecitive, I guess you should give the following arguments:\r\n```\r\n--task translation_multi_simple_epoch \\\r\n--lang-pairs language_pairs.txt \\\r\n--decoder-langtok --encoder-langtok src \r\n```\r\nor what is the command you used to generate? just switch that generate to interactive, add tokenizer,bpe.\r\n\r\n\r\nAnyway, for this case I believe TranslationTask (default task if not specified) read the model_dict.128k.txt, moses tokenized and spm your data correctly.\r\nThe reason you got random language is m2m-100 model requires a special token in sentences to identify which language pair is used. A normal translation task do not need to."
      },
      {
        "user": "molokanov50",
        "created_at": "2022-06-16T08:56:56Z",
        "body": "The addition of `--decoder-langtok --encoder-langtok src` solved my question, now the target translation language is taken from `--target-lang` option. Thx a lot."
      }
    ]
  },
  {
    "number": 4491,
    "title": "Pretrain HuBert Model error",
    "created_at": "2022-06-15T08:57:43Z",
    "closed_at": "2022-06-16T02:59:25Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4491",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n<!-- If you still can't find what you need: -->\r\n#### Code\r\npython  hydra_train.py  --config-dir \"fairseq/examples/hubert/config/pretrain/\" --config-name hubert_base_librispeech\r\n\r\n#### What is your question?\r\nWhen the code try to init Trainer in the hubert model,  I meet the following errors. How can I fix it, thanks.\r\n\r\n#### Code\r\npython  hydra_train.py  --config-dir \"fairseq/examples/hubert/config/pretrain/\" --config-name hubert_base_librispeech\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n#### What have you tried?\r\nTraceback (most recent call last):\r\n  File \"/data/NAS_PLUS/zhuang/CodeBase/001_fairseq/fairseq_cli/hydra_train.py\", line 27, in hydra_main\r\n    _hydra_main(cfg)\r\n  File \"/data/NAS_PLUS/zhuang/CodeBase/001_fairseq/fairseq_cli/hydra_train.py\", line 56, in _hydra_main\r\n    distributed_utils.call_main(cfg, pre_main, **kwargs)\r\n  File \"/data/NAS_PLUS/zhuang/CodeBase/001_fairseq/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/data/NAS_PLUS/zhuang/CodeBase/001_fairseq/fairseq_cli/train.py\", line 147, in main\r\n    trainer = Trainer(cfg, task, model, criterion, quantizer)\r\n  File \"/data/NAS_PLUS/zhuang/CodeBase/001_fairseq/fairseq/trainer.py\", line 164, in __init__\r\n    if self.data_parallel_rank == 0:\r\n  File \"/data/NAS_PLUS/zhuang/CodeBase/001_fairseq/fairseq/trainer.py\", line 197, in data_parallel_rank\r\n    return distributed_utils.get_data_parallel_rank()\r\n  File \"/data/NAS_PLUS/zhuang/CodeBase/001_fairseq/fairseq/distributed/utils.py\", line 463, in get_data_parallel_rank\r\n    return get_rank(get_data_parallel_group())\r\n  File \"/data/NAS_PLUS/zhuang/CodeBase/001_fairseq/fairseq/distributed/utils.py\", line 405, in get_rank\r\n    return dist.get_rank(group=group)\r\n  File \"/home/zhuang/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 688, in get_rank\r\n    default_pg = _get_default_group()\r\n  File \"/home/zhuang/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 347, in _get_default_group\r\n    raise RuntimeError(\"Default process group has not been initialized, \"\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version 0.12.1\r\n - PyTorch Version 1.11\r\n - Linux\r\n - How you installed fairseq  source\r\n - Build command you used pip install --editable ./\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 11.3\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4491/comments",
    "author": "NiniAndy",
    "comments": [
      {
        "user": "hamidun123",
        "created_at": "2022-06-15T08:59:31Z",
        "body": "I have the same question"
      },
      {
        "user": "hamidun123",
        "created_at": "2022-06-16T02:58:24Z",
        "body": "I have find the solution.\r\nYou only need delete the paramater \"distributed_training.distributed_port\" and set \"distributed_training.distributed_world_size\"=\"number of your gpus\""
      }
    ]
  },
  {
    "number": 4470,
    "title": "Why is the translation of long texts limited to a few initial sentences?",
    "created_at": "2022-06-09T15:03:01Z",
    "closed_at": "2022-06-15T09:48:42Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4470",
    "body": "Hi team,\r\n\r\nI'm just trying to translate (EN-RU) long texts consisting of 6 - 8 sentences but not exceeding 100 tokens in general (in order to not overcome model's memory consumption) using Fairseq's pretrained models (m2m, mBart, etc.). My input file is composed of every such text per line. According to the recommended workflow, I initially perform spm_encode.py, then binarization with fairseq-preprocess, and then translation with fairseq-generate. But in my output, I typically see no more than 3 - 4 translated sentences from the beginning of every text. Why does this occur? \r\nMeanwhile, in several papers, there is a scattered info that only one sentence per line is recommended in input files at all, but there is no clear explanation of it.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4470/comments",
    "author": "molokanov50",
    "comments": [
      {
        "user": "erip",
        "created_at": "2022-06-10T12:03:46Z",
        "body": "When you say \"100 tokens\", do you mean 100 tokens (~words) or 100 subword tokens? 100 \"words\" can easily surpass the 256 subword token max and then the documents might get truncated. "
      },
      {
        "user": "molokanov50",
        "created_at": "2022-06-10T14:26:18Z",
        "body": "I meant regular words but now I verified that my **spm-encoded** text has 144 subword tokens - of course, if only whitespaces separate subword tokens from each other. Besides I have there 100 'long underscores' which represent whitespaces of the **source** text. Even if long underscores are regarded as independent subword tokens (BTW are they?), I have in total 244 subword tokens. That should satisfy the network's limits. The truncation occurs after the 121st whitespace of the **spm-encoded** text. Until this position, 84 long underscores are encountered. And there is a sentence boundary at this position.\r\nIs there maybe any config in which I can specify the upper limit of subword tokens to translate?\r\nAny other workarounds?"
      },
      {
        "user": "erip",
        "created_at": "2022-06-10T15:02:47Z",
        "body": "In general you have `len(spm_encoded_line.strip().split())` subwords for a given `spm_encoded_line`. When you train a model you specify `--max-source-positions` which defines the maximum source sequence length handleable by your model. Check what this is for your model."
      },
      {
        "user": "molokanov50",
        "created_at": "2022-06-14T07:58:33Z",
        "body": "@erip \r\nUntil now I performed tests only on **pretrained** models available in fairseq examples. There is no change in my result if I vary my `--max-source-positions` option. For all my input sentences, the truncation always takes place around 110th-125th subwords exactly at the sentence boundary, and the boundary of the next sentence is already further than 135th subword.\r\nLooks like there is a default input length set up to ~128 subwords.\r\nShould I re-train my model with a different `--max-source-positions` value to have an opportunity to translate longer input sequences?"
      },
      {
        "user": "erip",
        "created_at": "2022-06-14T11:13:27Z",
        "body": "Suboptimally you could feed smaller chunks in and recombine in post-processing."
      },
      {
        "user": "molokanov50",
        "created_at": "2022-06-15T09:48:42Z",
        "body": "Can be closed, thanks."
      }
    ]
  },
  {
    "number": 4195,
    "title": "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.",
    "created_at": "2022-02-08T12:51:20Z",
    "closed_at": "2022-05-17T12:46:11Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4195",
    "body": "i want to train a new wav2vec base model, when i run:\r\n**fairseq-hydra-train \\\r\n    task.data=/Home/code/guozhi/slr \\\r\n    --config-dir ~/Home/code/guozhi/fairseq/examples/wav2vec/config/pretraining \\\r\n    --config-name wav2vec2_base_librispeech \\\r\n**\r\nthere are some error:\r\n**\r\nAdditional config directory '/home/gaogzh/Home/code/guozhi/fairseq/examples/wav2vec/config/pretraining' not found\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n**\r\nfor why?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4195/comments",
    "author": "pupil011",
    "comments": [
      {
        "user": "guobabaya",
        "created_at": "2022-05-17T12:35:08Z",
        "body": "hi，bro. Have you finally solved the problem?"
      },
      {
        "user": "pupil011",
        "created_at": "2022-05-17T12:46:04Z",
        "body": "> hi，bro. Have you finally solved the problem?\r\n\r\nsure, i used the relative path to solve."
      },
      {
        "user": "Freedomcls",
        "created_at": "2022-06-16T02:13:04Z",
        "body": "> > hi，bro. Have you finally solved the problem?\r\n> \r\n> sure, i used the relative path to solve.\r\n\r\nhi, bro, I also met the same problem, could you please tell me the specific operation, thanks!"
      },
      {
        "user": "wccccp",
        "created_at": "2023-01-16T09:45:11Z",
        "body": "> hi, bro, I also met the same problem, could you please tell me the specific operation, thanks!\r\n\r\n"
      }
    ]
  },
  {
    "number": 4186,
    "title": "speech_to_text's example might have a typo",
    "created_at": "2022-02-06T08:31:02Z",
    "closed_at": "2022-02-13T14:10:06Z",
    "labels": [
      "question",
      "needs triage",
      "speech"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4186",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nAm I misusing script or do we have a typo?\r\n\r\n#### Code\r\n```python3\r\npython fairseq/examples/speech_to_text/prep_librispeech_data.py\r\n```\r\n \r\ncreates an error:\r\n```python3\r\nTraceback (most recent call last):\r\n  File \"prep_librispeech_data.py\", line 14, in <module>\r\n    from examples.speech_to_text.data_utils import (\r\nModuleNotFoundError: No module named 'examples'\r\n```\r\n#### What have you tried?\r\nchanging source code to:\r\n\r\n```python3\r\nfrom data_utils import (\r\n    create_zip,\r\n    extract_fbank_features,\r\n    gen_config_yaml,\r\n    gen_vocab,\r\n    get_zip_manifest,\r\n    save_df_to_tsv,\r\n)\r\n```\r\nseems to work then...\r\n#### What's your environment?\r\n\r\n - fairseq Version 0.10.2\r\n - PyTorch Version 1.10.2+cu113\r\n - OS Debian GNU/Linux 11 (bullseye)\r\n - How you installed fairseq (`pip`, source): clone source, pip install path/to/fairseq\r\n - Python version: 3.7.11\r\n - CUDA/cuDNN version: cu113\r\n - GPU models and configuration: RTX 3050 TI\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4186/comments",
    "author": "ZurabDz",
    "comments": [
      {
        "user": "duj12",
        "created_at": "2022-02-12T06:08:24Z",
        "body": "you may add the path of fairseq root to your PYTHONPATH, input this in shell script:\r\nexport PYTHONPYTH=$PYTHONPATH:/path/to/your/fairseq\r\nso that python can find 'examples/speech_to_text/prep_librispeech_data.py' to execute."
      },
      {
        "user": "ZurabDz",
        "created_at": "2022-02-12T07:21:46Z",
        "body": "Yep you are right, sorry didn't think of that. "
      }
    ]
  },
  {
    "number": 4159,
    "title": "About the machine setting of data2vec training",
    "created_at": "2022-01-28T09:58:18Z",
    "closed_at": "2022-10-26T01:31:38Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4159",
    "body": "Sorry to disturb, I tried using data2vec to train my own audio model on my wav datas which are cutted into 30 seconds each file. I found it very easy to out of memory when the `max_tokens` of `examples/data2vec/config/audio/pretraining/base_librispeech.yaml` is setting bigger than 960000 (The original is 3800000). Below is my environment.\r\n - fairseq Version (e.g., 1.0 or main): 1.0.0a0\r\n - PyTorch Version (e.g., 1.0): 1.8.1\r\n - OS (e.g., Linux): ubuntu\r\n - How you installed fairseq (`pip`, source): source\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: 2GPUs, Tesla T4\r\n\r\nAny suggestions? Is my file too big (30 seconds)? I also tried decrease the max_tokens, or decrease file size to 10 seconds, **another problem happened often: gradient overflow detected, ignoring gradient, setting loss scale to ...**, is this mattered? The loss is going down firstly and going up after several iterations. I can't figure out what happend. What setting may cause this? Look forward for you reply, thanks a lot.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4159/comments",
    "author": "Ramlinbird",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T10:21:24Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 4145,
    "title": "Wav2vec Fine-tuning error  ",
    "created_at": "2022-01-19T01:14:33Z",
    "closed_at": "2022-01-26T22:21:11Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4145",
    "body": "Hi, I am working on finetuning the base_960h model on my own set of audio and transcript data.\r\n\r\nHowever, I am facing the issue while trying to train the model with this command.\r\n\r\n```cmd\r\n!fairseq-hydra-train task.data=/kaggle/working/data distributed_training.distributed_world_size=1 dataset.valid_subset=valid model.w2v_path=/kaggle/working/model/wav2vec_small_960h.pt --config-dir /kaggle/working/fairseq/examples/wav2vec/config/finetuning/ --config-name base_960h\r\n```\r\nI have added all the files to the data folder starting with:\r\n1. audio_split/  \r\n2. lexicon.txt  \r\n3. train.ltr  \r\n4. train.wrd  \r\n5. valid.tsv\r\n6. dict.ltr.txt \r\n7. train.tsv\r\n8. valid.ltr\r\n9. valid.wrd\r\n\r\nHere is the output of the error.\r\n```python\r\n[2022-01-19 01:02:49,288][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 320000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/kaggle/working/model/wav2vec_small_960h.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 64, 'mask_channel_prob': 0.1, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 0, 'feature_grad_mult': 0.0, 'layerdrop': 0.1, 'mask_channel_min_space': 1, 'mask_channel_before': False, 'normalize': False, 'data': '/kaggle/working/data', 'w2v_args': None, 'checkpoint_activations': False, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'ddp_backend': 'legacy_ddp', 'blank_weight': 0.0, 'blank_mode': 'add'}, 'task': {'_name': 'audio_finetuning', 'data': '/kaggle/working/data', 'labels': 'ltr', 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': 'none', 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'eval_bleu': False, 'eval_bleu_detok': None, 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': '{}', 'eval_bleu_print_samples': False, 'autoregressive': False}, 'criterion': {'_name': 'ctc', 'zero_infinity': True, 'sentence_avg': True, 'post_process': 'letter', 'wer_kenlm_model': None, 'wer_lexicon': None, 'wer_lm_weight': 2.0, 'wer_word_score': -1.0, 'wer_args': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 0, 'hold_steps': 0, 'decay_steps': 0, 'phase_ratio': [0.1, 0.4, 0.5], 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 320000, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}\r\nTraceback (most recent call last):\r\n  File \"/kaggle/working/fairseq/fairseq_cli/hydra_train.py\", line 27, in hydra_main\r\n    _hydra_main(cfg)\r\n  File \"/kaggle/working/fairseq/fairseq_cli/hydra_train.py\", line 56, in _hydra_main\r\n    distributed_utils.call_main(cfg, pre_main, **kwargs)\r\n  File \"/kaggle/working/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/kaggle/working/fairseq/fairseq_cli/train.py\", line 94, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/kaggle/working/fairseq/fairseq/tasks/audio_finetuning.py\", line 193, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/kaggle/working/fairseq/fairseq/tasks/audio_pretraining.py\", line 197, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/kaggle/working/fairseq/fairseq/tasks/fairseq_task.py\", line 335, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/kaggle/working/fairseq/fairseq/models/__init__.py\", line 105, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/kaggle/working/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 197, in build_model\r\n    w2v_encoder = Wav2VecEncoder(cfg, len(task.target_dictionary))\r\n  File \"/kaggle/working/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 388, in __init__\r\n    model = task.build_model(w2v_args.model)\r\n  File \"/kaggle/working/fairseq/fairseq/tasks/audio_pretraining.py\", line 197, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/kaggle/working/fairseq/fairseq/tasks/fairseq_task.py\", line 335, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/kaggle/working/fairseq/fairseq/models/__init__.py\", line 105, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/kaggle/working/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 197, in build_model\r\n    w2v_encoder = Wav2VecEncoder(cfg, len(task.target_dictionary))\r\nTypeError: object of type 'NoneType' has no len()\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n````\r\n\r\n**Can you please help me what is the problem?**",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4145/comments",
    "author": "jainilpatel",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2022-01-25T23:50:57Z",
        "body": "someone recently changed audio_pretraining to audio_finetuning for the finetuning task, so you need to switch to that (task._name=audio_finetuning)"
      },
      {
        "user": "jainilpatel",
        "created_at": "2022-01-26T22:20:40Z",
        "body": "@alexeib Thank you so much for the response. I will try to look into it. "
      },
      {
        "user": "Pchatain",
        "created_at": "2022-08-31T00:49:30Z",
        "body": "@jainilpatel was this every resolved? I'm getting the exact same error stack trace as in this post, except I triple checked that the exact config file I'm calling has task._name=audio_finetuning. I'm also using the same base model and training command. "
      }
    ]
  },
  {
    "number": 4142,
    "title": "how to load preprocessd file?",
    "created_at": "2022-01-18T02:04:35Z",
    "closed_at": "2022-04-27T23:21:57Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4142",
    "body": "I got test.src-tgt.src.bin file after fairseq-preprocess, how to open it with python code?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4142/comments",
    "author": "renmada",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:27Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:28Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4140,
    "title": "Initialize linear weights before softmax in MLM",
    "created_at": "2022-01-16T07:26:22Z",
    "closed_at": "2022-05-02T10:22:13Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4140",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nHi guys, I am training a masked language model using **masked_lm task** and **masked_lm criterion**. Now I want to initialize the linear weights (before softmax) of the model, then how could I do it? It seems that there is no command-line argument to achieve the thing. I really appreciate any reply from you guys.\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\r\n    --task masked_lm data-bin/mlm \\\r\n    --save-dir checkpoints/mlm \\\r\n    --restore-file checkpoints/mlm/checkpoint_last.pt \\\r\n    --max-epoch 20 --save-interval 1 --no-epoch-checkpoints \\\r\n    --arch cl_encoder --criterion masked_lm \\\r\n    --optimizer adam --lr 0.0001 --stop-min-lr 1e-09 \\\r\n    --dropout 0.1 \\\r\n    --batch-size 128 --tokens-per-sample 256 \\\r\n    --ddp-backend=legacy_ddp\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version (e.g., 1.0): 1.10.0+cu111\r\n - OS (e.g., Linux): Linux (Colab)\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version: \r\n - GPU models and configuration:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4140/comments",
    "author": "vkhangpham",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T13:21:25Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4139,
    "title": "Evaluation Error: TypeError: forward() missing 1 required positional argument: 'prev_output_tokens'",
    "created_at": "2022-01-15T17:50:39Z",
    "closed_at": "2022-02-07T17:55:10Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4139",
    "body": "## ❓ Questions and Help\r\nI added a new classification task using the transformer model. This is what my code looks like:\r\n`import os\r\nimport torch\r\n\r\nfrom fairseq.data import Dictionary, LanguagePairDataset\r\nfrom fairseq.tasks import FairseqTask, register_task\r\n\r\n@register_task('simple_classification')\r\nclass SimpleClassificationTask(FairseqTask):\r\n\r\n    @staticmethod\r\n    def add_args(parser):\r\n        #Add some command-line arguments for specifying where the data is located and the maximum supported input length.\r\n        parser.add_argument('data', metavar='FILE', help='file prefix for data')\r\n        parser.add_argument('--max-positions', default=1024, type=int, help='max input length')\r\n\r\n    @classmethod\r\n    def setup_task(cls, args, **kwargs):\r\n        #Here we can perform any setup required for the task. This may include loading Dictionaries, initializing shared\r\n        #Embedding layers, etc. In this case we'll just load the dictionaries.\r\n        input_vocab = Dictionary.load(os.path.join(args.data, 'dict.input.txt'))\r\n        label_vocab = Dictionary.load(os.path.join(args.data, 'dict.label.txt'))\r\n        print('| [input] dictionary: {} types'.format(len(input_vocab)))\r\n        print('| [label] dictionary: {} types'.format(len(label_vocab)))\r\n\r\n        return SimpleClassificationTask(args, input_vocab, label_vocab)\r\n\r\n    def __init__(self, args, input_vocab, label_vocab):\r\n        super().__init__(args)\r\n        #self.args = args\r\n        self.input_vocab = input_vocab\r\n        self.label_vocab = label_vocab\r\n    def load_dataset(self, split, **kwargs):\r\n        \"\"\" Load a given dataset split (e.g., train, dev, test.\"\"\"\r\n\r\n        prefix = os.path.join(self.cfg.data, '{}.input-label'.format(split))\r\n\r\n        # Read input sentences.\r\n        sentences, lengths = [], []\r\n        with open(prefix + '.input', encoding='utf-8') as file:\r\n\r\n            for line in file:\r\n                sentence = line.strip()\r\n\r\n                #Tokenize the sentence, splitting on spaces\r\n                tokens = self.input_vocab.encode_line(sentence, add_if_not_exist=False,)\r\n                sentences.append(tokens)\r\n                lengths.append(tokens.numel())\r\n\r\n        # Read labels.\r\n        labels = []\r\n        with open(prefix + '.label', encoding='utf-8') as file:\r\n            for line in file:\r\n                label = line.strip()\r\n                labels.append(\r\n                    # Convert labels to a numeric ID.\r\n                    torch.LongTensor([self.label_vocab.add_symbol(label)])\r\n                )\r\n        assert len(sentences) == len(labels)\r\n        print('| {} {} {} example'.format(self.cfg.data, split, len(sentences)))\r\n\r\n        # We reuse LanguagePairDataset since classification can be modeled as a\r\n        # sequence-to-sequence task where the target sequence has length 1.\r\n\r\n        self.datasets[split] = LanguagePairDataset(\r\n            src=sentences,\r\n            src_sizes=lengths,\r\n            src_dict=self.input_vocab,\r\n            tgt=labels,\r\n            tgt_sizes=torch.ones(len(labels)), #targets have length 1\r\n            tgt_dict=self.label_vocab,\r\n            left_pad_source=False,\r\n            # Since our target is a single class label, there's no need for\r\n            # teacher forcing. If we set this to \"True\" then our Model's\r\n            # \"forward()\" method would receive an additional argument called\r\n            # *prev_output_tokens* that would contain a shifted version of the\r\n            # target sequence.\r\n            input_feeding=True,\r\n        )\r\n    def max_positions(self):\r\n     #   \"\"\" Return the max input length allowed by the task.\"\"\"\r\n       # The source should be less than *args.max_positions and the \"target\"\r\n        # has max length 1.\r\n        return (self.cfg.max_positions, 1)\r\n\r\n    @property\r\n    def source_dictionary(self):\r\n        \"\"\" Return the max input length allowed by the task.\"\"\"\r\n        return self.input_vocab\r\n    @property\r\n    def target_dictionary(self):\r\n        \"\"\" Return the max input length allowed by the task.\"\"\"\r\n        return self.label_vocab`\r\n\r\nWhich runs well, but evaluation returns an error:\r\n\r\n`\r\nTypeError:forward() missing 1 required positional argument: 'prev_output_tokens'\r\n`\r\n\r\nThis is what the evaluation code looks like:\r\n\r\n```\r\nfrom fairseq import checkpoint_utils, data, options, tasks\r\n # Parse command-line arguments for generation\r\n parser = options.get_generation_parser(default_task='simple_classification')\r\n args = options.parse_args_and_arch(parser)\r\n # Setup task\r\n task = tasks.setup_task(args)\r\n # Load model\r\n print('| loading model from {}'.format(args.path))\r\n models, _model_args = checkpoint_utils.load_model_ensemble([args.path], task=task)\r\n model = models[0]\r\n print(model)\r\n while True:\r\n    sentence = input('\\nInput: ')\r\n    # Tokenize into characters\r\n    chars = ' '.join(list(sentence.strip()))\r\n    tokens = task.source_dictionary.encode_line(chars, add_if_not_exist=True,\r\n    )\r\n   \r\n    # Build mini-batch to feed to the model\r\n    batch = data.language_pair_dataset.collate(\r\n        samples=[{'id': -1, 'source': tokens}],  # bsz = 1\r\n        pad_idx=task.source_dictionary.pad(),\r\n        eos_idx=task.source_dictionary.eos(),\r\n        left_pad_source=False,\r\n        input_feeding=True,)\r\n   \r\n    # Feed batch to the model and get predictions\r\n    print(batch)\r\n    preds, _ = model(**batch['net_input'])\r\n    # Print top 3 predictions and their log-probabilities\r\n    top_scores, top_labels = preds[0].topk(k=3)\r\n    for score, label_idx in zip(top_scores, top_labels):\r\n        label_name = task.target_dictionary.string([label_idx])\r\n        print('({:.2f})\\t{}'.format(score, label_name))\r\n```\r\n\r\n#### What have you tried?\r\nI set input_feeding=True in evaluation but it doesn't change anything. \r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): v0.10.2\r\n - PyTorch Version (e.g., 1.0): Python 3.8.10\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): pip \r\n - Build command you used (if compiling from source):\r\n - Python version: '1.10.0'\r\n - CUDA/cuDNN version: cuda/11.0\r\n - GPU models and configuration: P100-PCIE\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4139/comments",
    "author": "fenimi",
    "comments": [
      {
        "user": "RuABraun",
        "created_at": "2022-06-12T09:01:15Z",
        "body": "I have this problem too"
      }
    ]
  },
  {
    "number": 4114,
    "title": "KeyError: 'transformer_monotonic_iwslt_de_en'",
    "created_at": "2022-01-08T14:47:40Z",
    "closed_at": "2022-05-02T10:22:02Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4114",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI am trying to run a simultaneous translation task using the following training command:\r\n\r\n#### What have you tried?\r\n\r\nThe training works fine. It saves the checkpoints under the 'checkpoints' folder\r\n\r\n`fairseq-train \\\r\n    data-bin/sample_wmt14_en_de \\\r\n    --simul-type waitk \\\r\n    --waitk-lagging 3 \\\r\n    --user-dir examples/simultaneous_translation \\\r\n    --mass-preservation \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --max-update 50000 \\\r\n    --arch transformer_monotonic_iwslt_de_en  \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n    --lr-scheduler 'inverse_sqrt' \\\r\n    --warmup-init-lr 1e-7  --warmup-updates 4000 \\\r\n    --lr 5e-4 --min-lr 1e-9 --clip-norm 0.0 --weight-decay 0.0001\\\r\n    --dropout 0.3 \\\r\n    --label-smoothing 0.1\\\r\n    --max-tokens 3584 \\\r\n    --encoder-embed-dim 128 \\\r\n    --decoder-embed-dim 128 \\\r\n    --encoder-layers 2 \\\r\n    --decoder-layers 2 \\\r\n    --batch-size 128 \\\r\n    --max-epoch 3 --save-interval 3 \\\r\n    --save-dir 'checkpoints/sample_fconv_wmt'`\r\n\r\nFor generation, I am running the following command:\r\n\r\n`fairseq-generate data-bin/sample_wmt14_en_de \\\r\n    --path checkpoints/sample_fconv_wmt/checkpoint_best.pt \\\r\n    --batch-size 32 --beam 5 `\r\n\r\nI am getting the following error:\r\n\r\n`Traceback (most recent call last):\r\n  File \"/Users/lc2022/miniconda/envs/torch17/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"/Users/lc2022/Desktop/Pycharmprojects/fairseq/fairseq_cli/generate.py\", line 379, in cli_main\r\n    main(args)\r\n  File \"/Users/lc2022/Desktop/Pycharmprojects/fairseq/fairseq_cli/generate.py\", line 41, in main\r\n    return _main(args, sys.stdout)\r\n  File \"/Users/lc2022/Desktop/Pycharmprojects/fairseq/fairseq_cli/generate.py\", line 94, in _main\r\n    num_shards=args.checkpoint_shard_count,\r\n  File \"/Users/lc2022/Desktop/Pycharmprojects/fairseq/fairseq/checkpoint_utils.py\", line 256, in load_model_ensemble\r\n    num_shards,\r\n  File \"/Users/lc2022/Desktop/Pycharmprojects/fairseq/fairseq/checkpoint_utils.py\", line 279, in load_model_ensemble_and_task\r\n    state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n  File \"/Users/lc2022/Desktop/Pycharmprojects/fairseq/fairseq/checkpoint_utils.py\", line 232, in load_checkpoint_to_cpu\r\n    state = _upgrade_state_dict(state)\r\n  File \"/Users/lc2022/Desktop/Pycharmprojects/fairseq/fairseq/checkpoint_utils.py\", line 435, in _upgrade_state_dict\r\n    registry.set_defaults(state[\"args\"], models.ARCH_MODEL_REGISTRY[state[\"args\"].arch])\r\nKeyError: 'transformer_monotonic_iwslt_de_en' `\r\n\r\nAm I missing some arguments in the generate command? Or is this error due to something else?\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (0.10.2):\r\n - PyTorch Version (1.10.1)\r\n - OS (Mac):\r\n - How you installed fairseq (git clone ):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration: None\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4114/comments",
    "author": "EricLina",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T13:21:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:32Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4104,
    "title": "RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select",
    "created_at": "2021-12-30T13:21:22Z",
    "closed_at": "2022-05-01T18:22:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4104",
    "body": "I got the following error while trying to generate translations with M2M-100 model:\r\n`RuntimeError: Expected object of device type Cuda but got device type cpu for argument #1 'self' in call to _th_index_select` .\r\nWhat can I do to fix this?\r\n\r\n#### Environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): 0.10.0\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip install fairseq\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 11.4\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4104/comments",
    "author": "ajesujoba",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T13:21:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4102,
    "title": "XLMR-xxl in multi-GPU ",
    "created_at": "2021-12-30T05:08:24Z",
    "closed_at": "2022-05-01T18:22:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4102",
    "body": "XLM-R xxl is too large to load in one GPU, how can I fix this? I have 4 V100-32G GPUs. Thanks\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4102/comments",
    "author": "ahtamjan",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T13:21:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4095,
    "title": "Cann't convert the tensor to GPU device",
    "created_at": "2021-12-24T08:50:59Z",
    "closed_at": "2022-05-01T18:22:14Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4095",
    "body": "## ❓ Cann't convert the tensor to GPU device\r\n\r\nwhen training the model, it can not convert the tensor from CPU to GPU device, why?\r\n```\r\n> p sample\r\n>  {'id': tensor([2000325, 1938073,  803296, 2658491, 3000699,  403808, 3547328,  840151,\r\n          32596,  193341, 2998371, 1161608, 3661565, 1790630, 3702332, 2230871,\r\n        1570542, 1467395, 3806129,  863435, 3786397, 1019630, 1891565,  810339,\r\n         607065, 2582544, 2849397, 1064850, 1352852, 3085569,  482236,  395145,\r\n        1481191,  107342, 3343505, 1843603, 1804996, 2923612, 2305186, 3411989,\r\n         384142, 1424420,  211846, 1675446, 3002466,  473706, 1065665,  277858,\r\n         721644,  605310, 1342541,  455777, 1406311, 2087088,  463274, 1625151,\r\n        2133260, 1198340,  708669,  995246, 2024422, 3632277, 1830529, 2091650])}\r\n\r\n> p sample['id'].to(device=0)\r\n> *** RuntimeError: CUDA error: device-side assert triggered\r\n> p torch.cuda.memory_allocated()\r\n> 1032658432\r\n> p torch.cuda.max_memory_allocated()\r\n> 1047600640\r\n```\r\n\r\nPlease help me, thank you!\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version (e.g., 1.0) : 1.10.0+cu102\r\n - OS (e.g., Linux): centos\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: python3.8\r\n - CUDA/cuDNN version: cuda10.0\r\n - GPU models and configuration: v100\r\n - Any other relevant information:\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4095/comments",
    "author": "lyzKF",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:23Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:45Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4093,
    "title": "How to finetune w2v_large_lv_fsh_swbd_cv_ftsb300_updated.pt on custom dataset",
    "created_at": "2021-12-23T12:31:09Z",
    "closed_at": "2022-05-01T18:22:12Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4093",
    "body": "#### I want to fine tune w2v_large_lv_fsh_swbd_cv_ftsb300_updated.pt model on my custom dataset.\r\n i have fine tuned w2v_large_lv_fsh_swbd_cv.pt on my custom dataset but when i start to fine tune w2v_large_lv_fsh_swbd_cv_ftsb300_updated model weights \r\n ### it gives me a config error. \r\n\r\n##### Can you please guide me what can i do to finetune model using w2v_large_lv_fsh_swbd_cv_ftsb300_updated as pretrained model.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4093/comments",
    "author": "shahzebali42",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:25Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4082,
    "title": "torch1.9 torch.distributed.launch: the main node keep waiting for the other nodes",
    "created_at": "2021-12-17T09:20:40Z",
    "closed_at": "2021-12-21T01:53:50Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4082",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHi, \r\nI have successfully trained with fairseq-hydra-train on a single node with multiple GPUs. \r\nAnd then I decided to finetune/pretrain wav2vec2.0 models on several gpu nodes. Each node contains 8 GPU of 3090. \r\n(The torch environment I'm using is 1.9. )\r\nHowever, when I start the processes on the nodes, the  main node kept waiting for store based barrier, the log on the main node looks like this:\r\n```\r\n[2021-12-17 16:36:32,821][torch.distributed.distributed_c10d][INFO] - Waiting in store based barrier to initialize process group for rank: 6, key: store_based_barrier_key:1 (world_size=16, worker_count=7, timeout=0:30:00)\r\n[2021-12-17 16:36:32,832][torch.distributed.distributed_c10d][INFO] - Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=16, worker_count=7, timeout=0:30:00)\r\n[2021-12-17 16:36:32,842][torch.distributed.distributed_c10d][INFO] - Waiting in store based barrier to initialize process group for rank: 4, key: store_based_barrier_key:1 (world_size=16, worker_count=7, timeout=0:30:00)\r\n[2021-12-17 16:36:32,872][torch.distributed.distributed_c10d][INFO] - Waiting in store based barrier to initialize process group for rank: 5, key: store_based_barrier_key:1 (world_size=16, worker_count=7, timeout=0:30:00)\r\n```\r\nand the log on the child node looks like this:\r\n```\r\n[INFO] 2021-12-17 16:15:54,690 api: [default] starting workers for entrypoint: python\r\n[INFO] 2021-12-17 16:15:54,690 api: [default] Rendezvous'ing worker group\r\n[INFO] 2021-12-17 16:15:54,690 dynamic_rendezvous: The node 'localhost_242249_0' attempts to join the next round of the rendezvous '114514'.\r\n[INFO] 2021-12-17 16:15:57,894 dynamic_rendezvous: The node 'localhost_242249_0' has joined round 2 of the rendezvous '114514' as rank 1 in a world of size 2.\r\n/home/shenyang/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.\r\n  warnings.warn(\r\n[INFO] 2021-12-17 16:15:58,192 api: [default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=localhost\r\n  master_port=54625\r\n  group_rank=1\r\n  group_world_size=2\r\n  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]\r\n  role_ranks=[8, 9, 10, 11, 12, 13, 14, 15]\r\n  global_ranks=[8, 9, 10, 11, 12, 13, 14, 15]\r\n  role_world_sizes=[16, 16, 16, 16, 16, 16, 16, 16]\r\n  global_world_sizes=[16, 16, 16, 16, 16, 16, 16, 16]\r\n\r\n[INFO] 2021-12-17 16:15:58,192 api: [default] Starting worker group\r\n[INFO] 2021-12-17 16:15:58,194 __init__: Setting worker0 reply file to: /tmp/torchelastic_3n9nak5r/114514_0rth0621/attempt_0/0/error.json\r\n[INFO] 2021-12-17 16:15:58,194 __init__: Setting worker1 reply file to: /tmp/torchelastic_3n9nak5r/114514_0rth0621/attempt_0/1/error.json\r\n[INFO] 2021-12-17 16:15:58,194 __init__: Setting worker2 reply file to: /tmp/torchelastic_3n9nak5r/114514_0rth0621/attempt_0/2/error.json\r\n[INFO] 2021-12-17 16:15:58,194 __init__: Setting worker3 reply file to: /tmp/torchelastic_3n9nak5r/114514_0rth0621/attempt_0/3/error.json\r\n[INFO] 2021-12-17 16:15:58,194 __init__: Setting worker4 reply file to: /tmp/torchelastic_3n9nak5r/114514_0rth0621/attempt_0/4/error.json\r\n[INFO] 2021-12-17 16:15:58,194 __init__: Setting worker5 reply file to: /tmp/torchelastic_3n9nak5r/114514_0rth0621/attempt_0/5/error.json\r\n[INFO] 2021-12-17 16:15:58,194 __init__: Setting worker6 reply file to: /tmp/torchelastic_3n9nak5r/114514_0rth0621/attempt_0/6/error.json\r\n[INFO] 2021-12-17 16:15:58,194 __init__: Setting worker7 reply file to: /tmp/torchelastic_3n9nak5r/114514_0rth0621/attempt_0/7/error.json\r\n[2021-12-17 16:16:02,034][fairseq.distributed.utils][INFO] - distributed init (rank 8): env://\r\n[2021-12-17 16:16:02,046][fairseq.distributed.utils][INFO] - distributed init (rank 10): env://\r\n[2021-12-17 16:16:02,070][fairseq.distributed.utils][INFO] - distributed init (rank 9): env://\r\n[2021-12-17 16:16:02,074][fairseq.distributed.utils][INFO] - distributed init (rank 14): env://\r\n[2021-12-17 16:16:02,108][fairseq.distributed.utils][INFO] - distributed init (rank 11): env://\r\n[2021-12-17 16:16:02,162][fairseq.distributed.utils][INFO] - distributed init (rank 12): env://\r\n[2021-12-17 16:16:02,195][fairseq.distributed.utils][INFO] - distributed init (rank 13): env://\r\n[2021-12-17 16:16:02,223][fairseq.distributed.utils][INFO] - distributed init (rank 15): env://\r\n```\r\nAnd nothing else happens on the childnode. \r\nI have already setup the sftp connection between two nodes.\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n```\r\nWORLDSIZE=16\r\nGPUS_PER_NODE=8\r\nNNODES=1:2\r\nNODE_RANK=0 # If I use two nodes, the second node's rank is 8\r\nMASTER_ADDR=\"localhost\" #  If I use two nodes, the second node's master_addr is 192.168.0.xxx\r\nMASTER_PORT=23333\r\nJOBID=TEST_JOBID\r\nBKD=c10d\r\nDISTRIBUTED_ARGS=\"--nnodes=$NNODES --nproc_per_node=$GPUS_PER_NODE --node_rank $NODE_RANK --rdzv_id=$JOBID --rdzv_backend=$BKD --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT\"\r\n\r\npython -m torch.distributed.run \\\r\n  $DISTRIBUTED_ARGS \\\r\n  $(which fairseq-hydra-train)  \\\r\n  task.data=/path/to/data \\\r\n  common.tensorboard_logdir=/path/to/tb_logdir \\\r\n  model.w2v_path=/path/to/xlsr_53_56k.pt  \\\r\n  --config-dir config/finetuning  \\\r\n  --config-name my_config\r\n```\r\n\r\n#### What have you tried?\r\n1. pass some distributed parameters into the fairseq-hydra-train instead of the default \"env://\"\r\n2. Clean all the zombie processes before a new attempt. \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version (e.g., 1.0): 1.9\r\n - OS (e.g., Linux): Ubuntu 16.04.6 \r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: XLSR-53\r\n - Any other relevant information:\r\n\r\n\r\nBest regards,\r\nYang\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4082/comments",
    "author": "Speakin-sy",
    "comments": [
      {
        "user": "Speakin-sy",
        "created_at": "2021-12-21T01:53:50Z",
        "body": "OK, I think I have solved the problem. fairseq-hydra-train is incompatible with the torch1.9/1.10 elastic launch. It's not necessary to  use torch.distributed.launch/run to launch the trainers. "
      },
      {
        "user": "zheyuanWang",
        "created_at": "2022-05-15T03:38:40Z",
        "body": "I came acrosss this problem when the number of GPUs set by \"CUDA_VISIBLE_DEVICES=4,5\" (2xGPUS) doesn't match with `--nproc_per_node=6`   (6xGPUs) when using `python -m torch.distributed.launch` , "
      },
      {
        "user": "gkeskin07",
        "created_at": "2022-05-17T23:26:28Z",
        "body": "> OK, I think I have solved the problem. fairseq-hydra-train is incompatible with the torch1.9/1.10 elastic launch. It's not necessary to use torch.distributed.launch/run to launch the trainers.\r\n\r\n@Speakin-sy\r\nCould you please elaborate on how you were able to resolve this issue? I am using hydra with torch 1.11, and getting the same errors. `Waiting in store based barrier to...`"
      }
    ]
  },
  {
    "number": 4067,
    "title": "Fairseq memory consumption increased when upgraded from torch 1.6 to 1.10",
    "created_at": "2021-12-09T06:13:57Z",
    "closed_at": "2022-05-01T18:22:03Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4067",
    "body": "Hi there,\r\nI've trained a BART model for text summary, during the inference I'm using the halved model on GPU\r\nIt was taking around 1.5 gb to load the model while using torch 1.6 with CUDA 10.1, after updating to torch 1.10.0 with CUDA 11.1 the same model is taking around 2.5 GB to load\r\n\r\nThis is how I'm loading the model\r\n```\r\ntemp_model = torch.load(halved_model_path)\r\nmodel = BARTHubInterface(args, task, temp_model)\r\nmodel.to(device)\r\nmodel.eval()\r\n```\r\nThis is my environment\r\n\r\n - fairseq 0.10.1\r\n - PyTorch Version : 1.10.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq : source\r\n - Build command you used : pip install --editable ./\r\n - Python version - 3.6.8\r\n - CUDA/cuDNN version -a gpu 3090 with cuda 11.1\r\n\r\n \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4067/comments",
    "author": "saiprasanth385",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:34Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:34Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4066,
    "title": "Training loss suddenly increase after a restart (adam optimizer)",
    "created_at": "2021-12-09T05:59:49Z",
    "closed_at": "2022-05-01T18:22:02Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4066",
    "body": "## ❓ Questions and Help\r\n\r\nHi there,\r\n\r\nI am pre-training a RoBERTa language model with the command\r\n```\r\nfairseq-train --fp16 \"train/LibriSpeech/\" \\\r\n    --task masked_lm --criterion masked_lm \\\r\n    --save-dir \"train/\" \\\r\n    --keep-last-epochs 1 \\\r\n    --train-subset train \\\r\n    --arch roberta_base \\\r\n    --num-workers 2 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr 0.0005 --total-num-update 100000 --warmup-updates 10000 \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --sample-break-mode eos --tokens-per-sample 512 --max-positions 512 \\\r\n    --mask-prob 0.2  \\\r\n    --batch-size 16 --update-freq 64 --max-update 250000 \\\r\n    --seed 5 --log-format simple --log-interval 10 --skip-invalid-size-inputs-valid-test \\\r\n    --save-interval-updates 3600 --keep-interval-updates 4 \\\r\n    --validate-interval 10\r\n  \r\n```\r\n\r\nAfter 50k updates, the training loss is around 2.3. I stopped and reinstalled the fairseq by building it from the source. Then I restarted the training process but I saw the training loss from the loaded last checkpoint was 3.0. \r\n\r\nI tested the restart checkpoint after 5k restart training updates on my downstream tasks. The results showed that the accuracy still increased as expected, compared to the last checkpoint before restarting.\r\n\r\n## my question\r\nI wonder if I made some mistakes or what is going on with the loss?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (1.0) :\r\n - PyTorch Version (1.10.0)\r\n - OS (CentOS):\r\n - How you installed fairseq (git clone then pip install ./editable):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: V100\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4066/comments",
    "author": "Lingygao",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:35Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:33Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4065,
    "title": "Tuning wav2vec2 language model parameters",
    "created_at": "2021-12-07T11:25:57Z",
    "closed_at": "2022-05-01T18:22:00Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4065",
    "body": "How can I tune my language model parameters of wav2vec2 (Kenlm)\r\n\r\nMy default paramters are :\r\n\r\nBEAM = 128\r\nbeam_threshold = 25\r\nLM_WEIGHT = 2\r\nWORD_SCORE = -1\r\nunk_weight = -np.inf\r\nsil_weight = 0\r\nnbest = 1\r\ncriterion = 'ctc'\r\nlabels = 'ltr'\r\n\r\nis there any documentation of what should be the parameters?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4065/comments",
    "author": "nome2050",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:37Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:31Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4062,
    "title": "Mbart output generation issue",
    "created_at": "2021-12-07T00:33:24Z",
    "closed_at": "2022-05-01T18:21:58Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4062",
    "body": "AttributeError: 'MBartEncoder' object has no attribute 'gradient_checkpointing'\r\n\r\nCode:\r\n\r\nwhile(True):\r\n\r\n    src_t = input(\"\\nEnter your Hindi gramatically incorrect sentence:\\n\")\r\n    src_toks = tokenizer.tokenize(src_t)\r\n    n_beam = int(input(\"\\nEnter Beam Size:\\n\"))\r\n    src_tok_ids = tokenizer.convert_tokens_to_ids(src_toks)\r\n    test_inputs = tokenizer.prepare_for_model(src_tok_ids, return_tensors=\"pt\", padding='max_length', truncation='longest_first', \r\n    max_length=35)\r\n    test_inputs = transformers.tokenization_utils_base.BatchEncoding(test_inputs)\r\n\r\n    #output = model(inputs['input_ids'].squeeze(1), inputs['attention_mask'].squeeze(1), labels=labels.squeeze(1))\r\n\r\n    output = model.generate(test_inputs['input_ids'].unsqueeze(0), num_beams=n_beam, max_length=35, early_stopping=True)\r\n\r\n    print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in output])\r\n\r\nexit()\r\n\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-18-307d4a330d53> in <module>()\r\n      9     #output = model(inputs['input_ids'].squeeze(1), inputs['attention_mask'].squeeze(1), labels=labels.squeeze(1))\r\n     10 \r\n---> 11     output = model.generate(test_inputs['input_ids'].unsqueeze(0), num_beams=n_beam, max_length=35, early_stopping=True)\r\n     12 \r\n     13     print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in output])\r\n\r\n5 frames\r\n/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in __getattr__(self, name)\r\n   1176                 return modules[name]\r\n   1177         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\n-> 1178             type(self).__name__, name))\r\n   1179 \r\n   1180     def __setattr__(self, name: str, value: Union[Tensor, 'Module']) -> None:\r\n\r\nAttributeError: 'MBartEncoder' object has no attribute 'gradient_checkpointing'",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4062/comments",
    "author": "geeksouvik",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:39Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:29Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4056,
    "title": "bart_base for masked_ml task",
    "created_at": "2021-12-06T06:39:24Z",
    "closed_at": "2022-05-01T18:21:57Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4056",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI try to fine-tuning bart_base model using masked_lm task on my own dataset, but an error occurs. Does bart_base support masked_lm task?\r\n\r\n#### Code\r\n\r\n`cmd = f\"\"\"\r\n        fairseq-train {args.dataset_dir}/bin \\\r\n        --save-dir {args.exp_dir} \\\r\n        --restore-file {args.model_path} \\\r\n        --arch bart_base  \\\r\n        --memory-efficient-fp16\t\\\r\n        --task masked_lm  \\\r\n        --criterion masked_lm  \\\r\n        --max-tokens 1536  \\\r\n        --update-freq 8 * 8 \\\r\n        --max-update 50000  \\\r\n        --required-batch-size-multiple 1  \\\r\n        --dropout 0.1  \\\r\n        --attention-dropout 0.1  \\\r\n        --relu-dropout 0.0  \\\r\n        --weight-decay 0.01  \\\r\n        --optimizer adam  \\\r\n        --adam-eps 1e-08  \\\r\n        --clip-norm 0.1  \\\r\n        --lr-scheduler polynomial_decay  \\\r\n        --lr 3e-5  \\\r\n        --total-num-update 50000  \\\r\n        --warmup-updates 5000  \\\r\n        --ddp-backend no_c10d  \\\r\n        --num-workers 20  \\\r\n        --reset-meters  \\\r\n        --reset-optimizer \\\r\n        --reset-dataloader \\\r\n        --share-all-embeddings \\\r\n        --layernorm-embedding \\\r\n        --share-decoder-input-output-embed  \\\r\n        --skip-invalid-size-inputs-valid-test  \\\r\n        --log-format json  \\\r\n        --log-interval 10  \\\r\n        --save-interval-updates\t100 \\\r\n        --validate-interval\t50 \\\r\n        --save-interval\t50 \\\r\n        --patience 200\r\n    \"\"\"`\r\n\r\n#### Error\r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/fairseq_cli/train.py\", line 169, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/fairseq_cli/train.py\", line 279, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/fairseq/trainer.py\", line 662, in train_step\r\n    loss, sample_size_i, logging_output = self.task.train_step(\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/fairseq/tasks/fairseq_task.py\", line 475, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/fairseq/criterions/masked_lm.py\", line 51, in forward\r\n    logits = model(**sample[\"net_input\"], masked_tokens=masked_tokens)[0]\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/fairseq/distributed/module_proxy_wrapper.py\", line 55, in forward\r\n    return self.module(*args, **kwargs)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/fairseq/distributed/legacy_distributed_data_parallel.py\", line 74, in forward\r\n    return self.module(*inputs, **kwargs)\r\n  File \"/home/user/anaconda3/envs/tapex/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\nTypeError: forward() got an unexpected keyword argument 'masked_tokens'`\r\n\r\n#### What have you tried?\r\nI try masked_lm task on roberta_base with following cmd, and it works.\r\n`cmd = f\"\"\"\r\n        fairseq-train {args.dataset_dir}/bin \\\r\n        --save-dir {args.exp_dir} \\\r\n        --restore-file {args.model_path} \\\r\n        --arch {args.model_arch}  \\\r\n        --memory-efficient-fp16\t\\\r\n        --task masked_lm  \\\r\n        --criterion masked_lm  \\\r\n        --max-tokens {args.max_tokens}  \\\r\n        --update-freq {args.gradient_accumulation} \\\r\n        --max-update {args.total_num_update}  \\\r\n        --required-batch-size-multiple 1  \\\r\n        --dropout 0.1  \\\r\n        --attention-dropout 0.1  \\\r\n        --weight-decay 0.01  \\\r\n        --optimizer adam  \\\r\n        --adam-eps 1e-08  \\\r\n        --clip-norm 0.1  \\\r\n        --lr-scheduler polynomial_decay  \\\r\n        --lr {args.learning_rate}  \\\r\n        --total-num-update {args.total_num_update}  \\\r\n        --warmup-updates 5000  \\\r\n        --ddp-backend no_c10d  \\\r\n        --num-workers 20  \\\r\n        --reset-meters  \\\r\n        --reset-optimizer \\\r\n        --reset-dataloader \\\r\n        --layernorm-embedding \\\r\n        --skip-invalid-size-inputs-valid-test  \\\r\n        --log-format json  \\\r\n        --log-interval 10  \\\r\n        --save-interval-updates\t100 \\\r\n        --validate-interval\t50 \\\r\n        --save-interval\t50 \\\r\n        --patience 200\r\n    \"\"\"`\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main):801a64683164680562c77b688d9ca77fc3e0cea7\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):Linux\r\n - How you installed fairseq (`pip`, source):source\r\n - Build command you used (if compiling from source):\r\n - Python version:3.8\r\n - CUDA/cuDNN version:11.2\r\n - GPU models and configuration:RTX2080Ti\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4056/comments",
    "author": "q5s2c1",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:28Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4039,
    "title": "problem with --sampling?",
    "created_at": "2021-11-25T07:12:51Z",
    "closed_at": "2022-04-18T01:21:12Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4039",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nCan anyone confirm that the behaviour of `--sampling` in `fairseq-interactive` is to generate samples from the conditional distribution using ancestral sampling?\r\n\r\n\r\n#### What have you tried?\r\n\r\nI'm trying to generate 100 samples from a trained machine translation model using ancestral sampling from the learned conditional distribution. I tried accomplishing this using the following command:\r\n```\r\nfairseq-interactive --path $ONE_SOURCE_CHECKPT_PREFIX/checkpoint_best.pt --sampling  --nbest 100  --source-lang src \\\r\n--target-lang tgt $ONE_SOURCE_PREPROC_SAVE_DIR < $ONE_SOURCE_DEVSRC > $ONE_SOURCE_SAVEPREF/results_dev_sampling.txt\r\n```\r\n\r\nbut I received an error stating that the `beam-size` and the `nbest` have to match for using `--sampling`: `--sampling requires --nbest to be equal to --beam`. I find this to be a strange error, since sampling from the conditional distribution shouldn't have anything to do with beam search (to my knowledge).\r\n\r\nAt any rate, I set `beam-size` and `nbest` to be the same:\r\n\r\n```\r\nfairseq-interactive --path $ONE_SOURCE_CHECKPT_PREFIX/checkpoint_best.pt --sampling  --nbest 100  --beam-size 100 --source-lang src \\\r\n--target-lang tgt $ONE_SOURCE_PREPROC_SAVE_DIR < $ONE_SOURCE_DEVSRC > $ONE_SOURCE_SAVEPREF/results_dev_sampling.txt\r\n```\r\n\r\nBut the results I obtained are strikingly similar to that of beam search. In particular, the first hypothesis for any input is nearly always the best (most accurate by BLEU), which is the behaviour I expect from beam search. Can anyone confirm that the behaviour of `--sampling` is to generate samples from the conditional distribution using ancestral sampling? To me, it seems that using `--sampling` still just uses beam search. \r\n\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): main\r\n - PyTorch Version (e.g., 1.0): 1.9.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): `pip`\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4039/comments",
    "author": "smfsamir",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:02Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4038,
    "title": "wav2vec 1.0 cannot replicate pretrained wav2vec_large.pt model",
    "created_at": "2021-11-24T23:56:22Z",
    "closed_at": "2022-04-18T01:21:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4038",
    "body": "## ❓ Questions and Help\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n1. Generally: cannot replicate model `wav2vec_large.pt` so any ideas would be great\r\n2. How long should I be training the model? Currently I have 3 gpus with 16GB ram. \r\n3. Are there any significant implementation changes between when wav2vec was released vs now? \r\n4. Does the checkpoint with the best validation loss generally have better performance than latest checkpoint (w.r.t comparing them using the same ASR model)?\r\n5. When I calculate validation loss for `wav2vec_large.pt` it has 42 which is really high. Is this correct?\r\n\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n```\r\npython train.py /path/manifests --save-dir /save/dir --num-workers 3 --fp16 --max-update 400000 --save-interval 1 --arch wav2vec --task audio_pretraining --min-lr 1e-06 --stop-min-lr 1e-09 --optimizer adam --lr 5e-3 --lr-scheduler cosine --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion wav2vec --num-negatives 10 --max-sample-size 150000 --max-tokens 1500000 --skip-invalid-size-inputs-valid-test --tensorboard-logdir /tensorboard/dir\r\n```\r\n\r\n\r\n#### What have you tried?\r\n1. Tried running the same command as mentioned under `examples/wav2vec` (repasted above) and also tried running with different hyperparameters (i.e. lr scheduling, warmup updates, batch size) but can't seem to get a comparable model to `wav2vec_large.pt` when running it against same ASR model to measure performance. I trained it until it seemed to converge which usually took only up to ~10 epochs. I also used the Librispeech dataset with train-* as train.tsv and dev-* as dev.tsv\r\n2. Ran code with earliest wav2vec 1.0 release and received better performance results but still could not replicate wav2vec_large.pt\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: main\r\n - PyTorch Version: 1.9.0\r\n - OS: Linux\r\n - How you installed fairseq: `pip`\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.12\r\n - CUDA/cuDNN version: 11.4\r\n - GPU models and configuration: 3 GPUs (16gb RAM)\r\n - Any other relevant information: \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4038/comments",
    "author": "GarlandZhang",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:03Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4033,
    "title": "Why does time mask work",
    "created_at": "2021-11-22T10:55:02Z",
    "closed_at": "2022-04-18T01:21:07Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4033",
    "body": "In the wav2vec2 finetuning, the default time mask prob is set to 0.65, and the freq mask prob 0.5. Despite considering the overlap which makes the final mask proportion lower, there will still be about 0.5 of the input feature get masked. \r\n\r\nI'm wondering why this is actually working well while half of the input is masked. Does it only make sense for CNN-extracted features as each frame has a large perceptual field of surrounding frames?\r\n\r\nAnd another question is, does the `dropout-input` has a similar effect to this mask mechanism (e.g. dropout-input=0.5 vs mask-prob=0.5)?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4033/comments",
    "author": "18445864529",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:07Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4030,
    "title": "Visualize model graph in Tensorboard",
    "created_at": "2021-11-21T16:51:45Z",
    "closed_at": "2022-04-18T01:21:06Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4030",
    "body": "How do we visualize the model graph in fairseq? \r\n\r\nI am getting an error \r\n\r\n`Tracer cannot infer type of ({'id': tensor([743216, 642485,  92182, 793806, 494734, 275334,  53282, 449572,   1758,\r\n  ...\r\n  20734, 469070, 678489, 473213]), 'nsentences': 832, 'ntokens': 29775, 'net_input': {'src_tokens': tensor([[  225,    30,   874,  ...,  1330,    84,     2],\r\n  [  442,   734,  7473,  ...,    38,     5,     2],\r\n  ...,\r\n  [ 6238, 11411,   428,  ..., 10387,     5,     2]]), 'src_lengths': tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\r\n  ...\r\n  21, 21, 21, 21]), 'prev_output_tokens': tensor([[   2,   22,  133,  ..., 4445,   46,    1],\r\n  ...,\r\n  [   2,   22,   59,  ...,  112,    6,    1]])}, 'target': tensor([[ 22, 133, 203,  ...,  46,   2,   1],\r\n  [132, 429,  40,  ...,  46,   2,   1],\r\n  ...,\r\n  [ 22,  59, 177,  ...,   6,   2,   1]])},)\r\n:Dictionary inputs to traced functions must have consistent type. Found Tensor and int\r\nError occurs, No graph saved`\r\n\r\nIf I pass the sample['net_inputs'] as argument as defined in criterion, I get an error \r\n\r\n`TypeError: add_graph() got an unexpected keyword argument 'src_tokens'`\r\n\r\nAny help would be really appreciated. I am getting 0 grad in one part of the model and I want to understand where it breaks. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4030/comments",
    "author": "glakshmidhar",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:08Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:36Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4023,
    "title": "imbalanced dataloading when inference on wav2vec 2.0",
    "created_at": "2021-11-19T00:36:32Z",
    "closed_at": "2022-04-18T01:21:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4023",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. have searched the issues.\r\n2. have searched the docs.\r\n\r\n\r\n#### What is your question?\r\ni have a server with 8 V100 GPU. when i run inference on small audio set (about 400s after vad), only 3 GPUs are working. After reading code (as below), i found that task.get_batch_iterator split the data to different shards and only the first 3 shards have data. Then the busiest GPU run about 30s and the last 5 GPU do nothing.\r\n\r\nMy question is: \r\nHow can i do balanced data loading? use the get_batch_iterator and change some parameters? or use other method?\r\nThanks for any help!\r\n\r\nfrom fairseq import tasks\r\nself.task = tasks.setup_task(cfg.task)\r\nreturn self.task.get_batch_iterator(\r\n            dataset=self.task.dataset(self.cfg.dataset.gen_subset),\r\n            max_tokens=self.cfg.dataset.max_tokens,\r\n            max_sentences=self.cfg.dataset.batch_size,\r\n            max_positions=(sys.maxsize, sys.maxsize),\r\n            ignore_invalid_inputs=self.cfg.dataset.skip_invalid_size_inputs_valid_test,\r\n            required_batch_size_multiple=self.cfg.dataset.required_batch_size_multiple,\r\n            seed=self.cfg.common.seed,\r\n            num_shards=self.service_num,\r\n            shard_id=self.rank_id,\r\n            num_workers=self.cfg.dataset.num_workers,\r\n            data_buffer_size=self.cfg.dataset.data_buffer_size,\r\n            disable_iterator_cache=disable_iterator_cache,\r\n        ).next_epoch_itr(shuffle=False)\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (1.0.0a0):\r\n - PyTorch Version (1.7.1)\r\n - OS (ubuntu 16.04):\r\n - How you installed fairseq (source):\r\n ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4023/comments",
    "author": "firdota",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:11Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:33Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4017,
    "title": "wav2vec finetune on large scale labeled data",
    "created_at": "2021-11-14T13:50:40Z",
    "closed_at": "2022-04-18T00:21:27Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4017",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4017/comments",
    "author": "lixx0105",
    "comments": [
      {
        "user": "lixx0105",
        "created_at": "2021-11-14T14:02:11Z",
        "body": "hi @alexeib，may i have your advice on wav2vec finetune on large scale label data(10k hour)? It seem finetune is hard. For example, I got 20% cer when I finetune on 1k hour data(data A), but can only get 30% cer when I finetune on 10k hour data which include data A. if I finetune on data A first and then continue to finetune on 10k hour, I got slightly better cer than 20%. Can I have your advice? Thanks"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:16Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:57Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4012,
    "title": "nat_loss.py  outputs, targets = outputs[masks], targets[masks]",
    "created_at": "2021-11-11T11:50:15Z",
    "closed_at": "2022-04-18T00:21:25Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4012",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n#### Code\r\n\r\n`cannot import name 'libnat_cuda' from 'fairseq' (/share/qzz/en2zh/fairseq-editor/fairseq/__init__.py)... fall back to CPU version\r\nTraceback (most recent call last):\r\n  File \"/home/qzz/anaconda3/envs/editor/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/share/qzz/en2zh/fairseq-editor/fairseq_cli/train.py\", line 321, in cli_main\r\n    main(args)\r\n  File \"/share/qzz/en2zh/fairseq-editor/fairseq_cli/train.py\", line 96, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/home/qzz/anaconda3/envs/editor/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/share/qzz/en2zh/fairseq-editor/fairseq_cli/train.py\", line 176, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/qzz/anaconda3/envs/editor/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/share/qzz/en2zh/fairseq-editor/fairseq/trainer.py\", line 319, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/share/qzz/en2zh/fairseq-editor/fairseq/tasks/translation_lev.py\", line 175, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/home/qzz/anaconda3/envs/editor/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/share/qzz/en2zh/fairseq-editor/fairseq/criterions/nat_loss.py\", line 218, in forward\r\n    factor=outputs[obj].get(\"factor\", 1.0)\r\n  File \"/share/qzz/en2zh/fairseq-editor/fairseq/criterions/nat_loss.py\", line 53, in _compute_loss\r\n    outputs, targets = outputs[masks], targets[masks]\r\nIndexError: The shape of the mask [96, 30] at index 1 does not match the shape of the indexed tensor [96, 31] at index 1\r\n`\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (0.9.0):\r\n - PyTorch Version (1.9.0)\r\n - OS (Linux):\r\n - How you installed fairseq (source):\r\n - Build command you used (if compiling from source):\r\n       cd fairseq\r\n       pip install --editable .\r\n       python setup.py build_ext --inplace\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: \r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4012/comments",
    "author": "QzzIsCoding",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:19Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:55Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4002,
    "title": "Approximately how long should fine-tuning of the wav2vec small model take with 1h librispeech data?",
    "created_at": "2021-11-06T13:08:41Z",
    "closed_at": "2022-04-18T00:21:22Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4002",
    "body": "I am fine-tuning the small wav2vec model with the 1 hour partition from the libri-light repo. I am using the base_1h.yaml configuration provided in this repo with some small changes to use 4 GPUs instead of 8:\r\nupdate_freq=2,\r\ndistributed_world_size=4, \r\nmax_tokens=3840000, \r\nfreeze_finetune_updates=0.\r\n\r\nI am not fine-tuning with a decoder. \r\nFrom reading other posted questions and issues, I understand that although the UER and raw WER will be close to or reach and stay at 100 early in the finetuning process, the fine tuned model should start producing <100 UER results once loss on the validation set drops below 4. \r\n\r\nI understand that the fine tuning setup configures for 13000 updates but I am observing that a single epoch which equates to approximately 2 - 3 updates is taking 15 seconds and it's increasing. So I am wondering if there's something wrong with my parallelization or utilization of the GPUs. How long approximately should fine-tuning take on 1h of Librispeech data? If you do not have approximate numbers for 4 GPUs, can you provide them for the 8 GPUs that were used in the 2020 paper? \r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4002/comments",
    "author": "chen7515",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:23Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4000,
    "title": "[wav2vec2] Why not use a larger batch size",
    "created_at": "2021-11-05T00:33:41Z",
    "closed_at": "2022-04-18T00:21:19Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4000",
    "body": "In the paper it says that V100s were used for training. But when I train wav2vec2 the default `--max-tokens=1400000` leaves a lot of space free. I use 3000000. But I wonder whether there might be some drawback to using a larger batch size and that's why those are the default values in the pretrain config?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4000/comments",
    "author": "RuABraun",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:26Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3991,
    "title": "Could someone give more details about fixed_shapes in fairseq_dataset.py?",
    "created_at": "2021-11-02T03:21:41Z",
    "closed_at": "2022-04-18T00:21:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3991",
    "body": "I come across there are too many dynamic shapes when training the dataset. Then I find there has been fixed_shapes method in  fairseq_dataset.py. However, as a layman I don't understand how to use this function and what the difference using fixed_shapes or not. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3991/comments",
    "author": "jingzhang0909",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:06Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3987,
    "title": "How to get top_n output results for the same sentence in Levenshtein Transformer?(Or beam search?)",
    "created_at": "2021-10-30T11:15:34Z",
    "closed_at": "2022-04-18T00:21:06Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3987",
    "body": "## ❓ Questions and Help\r\n#### What is your question?\r\n**How to get top_n output results for the same sentence in Levenshtein Transformer?(Or beam search?)**\r\n#### Code\r\n**I try add \"--nbest 3\" or \"--beam 3\"  following \"fairseq generate\"to get the top_n output results for every sentence of test.txt, but I find the result is also the only one at any time.** \r\n#### What have you tried?\r\n**I try add \"--nbest 3\" or \"--beam 3\"  following \"fairseq generate\"to get the top_n output results for every sentence of test.txt, but I find the result is also the only one at any time.** \r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3987/comments",
    "author": "tolerancecky",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:12Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:36Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3985,
    "title": "Why doing loss.backward() and optimizer.step() at different places",
    "created_at": "2021-10-29T04:03:31Z",
    "closed_at": "2022-04-18T00:21:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3985",
    "body": "The call stack of training is like `train.py - train()` → `trainer.py - train_step()` → `fairseq_task.py - train_step()` → `criterion...`. After the criterion returns the loss to `fairseq_task.py`, the loss is backward there, whereas the optimization step is called in `trainer.py`. \r\n\r\nI'm wondering what's the point of doing these two things in two different files instead of being next to each other?\r\n\r\nBecause when I return the loss to the outside-most file `train.py`, and do backward() and step() there, I found the log of 'loss' and 'nll_loss' became 'nan', why is this happening? \r\n\r\nThank you.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3985/comments",
    "author": "18445864529",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:14Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:34Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3982,
    "title": "Best/recommended method for hyperparameter tuning",
    "created_at": "2021-10-27T19:24:07Z",
    "closed_at": "2022-04-18T00:21:02Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3982",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHello everyone,\r\n\r\nI am using fairseq to implement & train a custom model. What's the best way to do a hyperparameter search (random search, grid search, etc.) within fairseq? I was hoping not to have to do it by hand :smile:  \r\n\r\nIn case it matters, here's what I'm doing now: \r\n1. I implemented a model in `my_model.py`\r\n2. Put it into the `fairseq/models` folder\r\n3. Run `fairseq-train` with CLI flags\r\n\r\nThis works fine for training. \r\n\r\n#### What have you tried?\r\nNothing, yet. My first thought was that it's possible to hook into the `Trainer` class training loop and add some code there, but I'm also not really sure how I would do that. Open to any and all suggestions!\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: `main`\r\n - PyTorch Version: 1.9.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): from source\r\n - Build command you used (if compiling from source): `python setup.py build_ext --inplace`\r\n - Python version: 3.9.7\r\n - CUDA/cuDNN version: 11.2\r\n - GPU models and configuration: GeForce RTX 3090\r\n\r\nThanks in advance!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3982/comments",
    "author": "offendo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:16Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:32Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "ashokrajab",
        "created_at": "2022-07-14T04:52:35Z",
        "body": "Hi @offendo,\r\nI'm also trying to perform hyperparameter tuning for the model that I trained using fairseq. If you had figured out an efficient way, it would be helpful if you could share the same.\r\n\r\nThank you."
      },
      {
        "user": "offendo",
        "created_at": "2022-07-14T15:43:53Z",
        "body": "Hi @ashokrajab, I wish I had a better suggestion, but I ended up doing it manually which was rather tedious and probably not ideal...but it worked well enough for my use. My method looked like this\r\n\r\n1. Calculate grid of hyperparameters (e.g., learning rate log-uniform between 1e-2 and 1e-5, dropout uniform between 0.1 and 0.5, etc.)\r\n2. Train each configuration for a small number of epochs (I just wrote a bash script to launch one after another)\r\n3. Log the performance at the end of each run, use the best configuration. \r\n"
      }
    ]
  },
  {
    "number": 3981,
    "title": "Fairseq OOM when using multi-GPUs",
    "created_at": "2021-10-27T14:02:23Z",
    "closed_at": "2022-04-18T00:21:17Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3981",
    "body": "# Fairseq OOM when using multi-GPUs\r\n## I run bart-large on my device, if I use 4\\*2080ti(4\\*12G), it will OOM. And it works well on 1\\*3090(24G). I found that it will use more than 12G each GPU even on multi-GPUs. If there is any way for me to run it on my 4\\*2080ti(4\\*12G)?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3981/comments",
    "author": "lxx909546478",
    "comments": [
      {
        "user": "lxx909546478",
        "created_at": "2021-10-27T14:13:00Z",
        "body": "If there is any way to make it use lower 12G on each GPU? \r\nIf a program use 16G on one GPU, shouldn't it use 8G on two GPUs? It makes me so confused."
      },
      {
        "user": "18445864529",
        "created_at": "2021-10-29T04:11:33Z",
        "body": "The batch-size refers to that per GPU. So when changing from 24g to 12g the bsz should be halved."
      },
      {
        "user": "lxx909546478",
        "created_at": "2021-10-29T07:28:44Z",
        "body": "> The batch-size refers to that per GPU. So when changing from 24g to 12g the bsz should be halved.\r\n\r\nyep, but I have tried to halve the batch_size to 1, it also doesn't work for me. I would appreciate if you have other solutions. "
      },
      {
        "user": "lorelupo",
        "created_at": "2021-10-29T09:24:29Z",
        "body": "This behavior is expected because when you select --batch-size or --max-tokens, they both refer to the BATCH SIZE PER GPU, as suggested by @18445864529 . Logically, a 24G has more capacity than 12G and it can therefore process larger batch sizes, as you are observing. Simply try lowering the batch size until it fits in your 4x12G setting. "
      },
      {
        "user": "zhongkang-ustc",
        "created_at": "2021-11-04T06:15:31Z",
        "body": "i suggest: max-tokens 4096 or 2048, max-sentences 128 or 64.  it works on my milti-GPU of 12Gib"
      },
      {
        "user": "lxx909546478",
        "created_at": "2021-11-04T09:57:09Z",
        "body": "> This behavior is expected because when you select --batch-size or --max-tokens, they both refer to the BATCH SIZE PER GPU, as suggested by @18445864529 . Logically, a 24G has more capacity than 12G and it can therefore process larger batch sizes, as you are observing. Simply try lowering the batch size until it fits in your 4x12G setting.\r\n\r\nIn fact, I have tried --batch-size=1, but it doesn't work for me."
      },
      {
        "user": "lxx909546478",
        "created_at": "2021-11-04T09:58:29Z",
        "body": "> i suggest: max-tokens 4096 or 2048, max-sentences 128 or 64. it works on my milti-GPU of 12Gib\r\n\r\nThanks for your reply, the setting maybe work for your dataset, but it doesn't work for me."
      },
      {
        "user": "lorelupo",
        "created_at": "2021-11-04T10:26:03Z",
        "body": "> > This behavior is expected because when you select --batch-size or --max-tokens, they both refer to the BATCH SIZE PER GPU, as suggested by @18445864529 . Logically, a 24G has more capacity than 12G and it can therefore process larger batch sizes, as you are observing. Simply try lowering the batch size until it fits in your 4x12G setting.\r\n> \r\n> In fact, I have tried --batch-size=1, but it doesn't work for me.\r\n\r\nThere might be a very long sequence in your dataset that causes that OOM even when it is the only sequence in the batch.  Can you check if this is the case?"
      },
      {
        "user": "lxx909546478",
        "created_at": "2021-11-04T10:31:26Z",
        "body": "> > > This behavior is expected because when you select --batch-size or --max-tokens, they both refer to the BATCH SIZE PER GPU, as suggested by @18445864529 . Logically, a 24G has more capacity than 12G and it can therefore process larger batch sizes, as you are observing. Simply try lowering the batch size until it fits in your 4x12G setting.\r\n> > \r\n> > \r\n> > In fact, I have tried --batch-size=1, but it doesn't work for me.\r\n> \r\n> There might be a very long sequence in your dataset that causes that OOM even when it is the only sequence in the batch. Can you check if this is the case?\r\n\r\nI set --max_tokens to 1000, if it is too large? But when I set it to 800, it will report error \"Max sentence length shouldn't exceed 800\"."
      },
      {
        "user": "18445864529",
        "created_at": "2021-11-04T10:44:03Z",
        "body": "> > The batch-size refers to that per GPU. So when changing from 24g to 12g the bsz should be halved.\r\n> \r\n> yep, but I have tried to halve the batch_size to 1, it also doesn't work for me. I would appreciate if you have other solutions.\r\n\r\nI would suggest first using `nvidia-smi` to check the memory usage before running, if there are already memory occupations, use `top` to kill the processes. The fairseq trainer has an argument 'raise_oom', if this is set false(by default), it will try to recover from the OOM, in my case, it had never succeeded in recovering but only resulted in dead processes that occupied most of the memories. As a result, once you got OOM, you are very likely to encounter OOM again as long as you don't clear the dead process manually. Anyway, I suggest setting this argument to be True, it can save you a lot of trouble."
      },
      {
        "user": "lorelupo",
        "created_at": "2021-11-04T10:50:29Z",
        "body": "It looks like you have sentences longer than 800 tokens in your dataset.\r\n\r\nIf your GPU memory is clean already (see @18445864529 comment), I would try to clean your dataset from long sentences, although I am not sure that this would solve your issue, since 1000 tokens shouldn't be too many."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:31:59Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3979,
    "title": "wav2vec 2.0 pretraining training stopping condition?",
    "created_at": "2021-10-26T08:10:18Z",
    "closed_at": "2022-04-18T00:21:01Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3979",
    "body": "Hi, I know base trains towards 400k updates. I am currently at 250k and I have noticed the following:\r\n- training and validation **accuracy** is still going up \r\n-  training and validation **loss** is still decreasing\r\n- **prob_perplexity** and **code_perplexity** have peaked and have been decreasing since then \r\n\r\nQuestion is, should I continue training despite the perplexities decreasing? They were steadily increasing since the start of pretraining but have since started reversing. Is there is any risk of overtraining considering I am still seeing regular progress on every epoch? (i.e an improved checkpoint_best, decreasing losses and improving accuracy)\r\n\r\nSome extra background:\r\n- Training on 2000h of audio \r\n- Used an older pretrained model as a base (but with more training data this time) so it's not training completely from scratch. \r\n- Have been manipulating `lr` and `update_freq` through training. Initially started with the default for `lr` and 64 for the latter but that would have taken too long, so after 9k updates, I dropped it down to 1 for the first 90k updates with a smaller `lr`. Since then it's been at 8 with the same `lr` as before for 1.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3979/comments",
    "author": "machakos23",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:17Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:31Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3967,
    "title": "Embeddings as input and output to fairseq model",
    "created_at": "2021-10-18T15:37:52Z",
    "closed_at": "2022-04-17T23:21:28Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3967",
    "body": "Hi,\r\n\r\n\r\nI am using fairseq to fine tune the model on embeddings that come from spm tokens. \r\n\r\n\r\nNow I want to run the experiment with these changes\r\n\r\nI have embeddings corresponding to the context of the Input. For example:\r\n\r\nInput : I am Batman\r\n\r\nEmbedding(“I am Batman”) = [768D]\r\n\r\nNote that rather than a matrix which contains embedding of each word, we have a row matrix.\r\n\r\nIn the similar manner the output sequence is encoded. I.e. a row matrix.\r\n\r\n\r\nHow do I use the fairseq and MBART with these types of input ?  ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3967/comments",
    "author": "kapeed1011",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:24Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:56Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3959,
    "title": "how to use Fairseq with code？",
    "created_at": "2021-10-14T11:30:55Z",
    "closed_at": "2022-04-17T23:21:19Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3959",
    "body": null,
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3959/comments",
    "author": "taoztw",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:01Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3957,
    "title": "checkpoint for roberta-base + layerdrop",
    "created_at": "2021-10-14T10:38:31Z",
    "closed_at": "2022-04-17T23:21:18Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3957",
    "body": "## ❓ Questions and Help\r\n\r\nHello, the checkpoint for roberta-base + layerdrop is can not accsessible. Could you please provide a new download link?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3957/comments",
    "author": "xyltt",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:02Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3954,
    "title": "Finetune m2m100 on multilingual data",
    "created_at": "2021-10-13T04:10:05Z",
    "closed_at": "2022-04-17T23:21:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3954",
    "body": "I am able to finetune the m2m100 model on a given pair of language e.g ja-en. But I need to finetune this on multilingual data where my dataset consists of several pairs of languages. How can I achieve this? Any suggestions would be appreciated. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3954/comments",
    "author": "nikhiljaiswal",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:04Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:47Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3949,
    "title": "Iteration amount of pre-training wav2vec 2.0 base",
    "created_at": "2021-10-12T15:04:33Z",
    "closed_at": "2022-04-18T00:21:00Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3949",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI want to know the how many epoches did you spend fully pre-training the wav2vec 2.0 base model on full 960 hours' LibriSpeech dataset.\r\nI know that you set the max_update as 400000 for stop condition. But after some trial pre-training, I think it is very hard to reach 400000 epoches in 1.6 days with 64 100v GPUs.\r\n\r\n#### What have you tried?\r\nI have did some trial pre-training with wav2vec 2.0 base model on full 960 hours' Librispeech. After estimation, I think that probably your pre-training do not reach the max_update.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): 1.0\r\n - PyTorch Version (e.g., 1.0): 1.8.1\r\n - OS (e.g., Linux): Linux\r\n - Python version: 3.8.10\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 4 v100 GPUs\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3949/comments",
    "author": "yuanfeng-yl",
    "comments": [
      {
        "user": "r03943158",
        "created_at": "2021-10-23T03:59:54Z",
        "body": "Hi, can I ask the time cost in one epoch? \r\nI use 100 hours's LibriSpeech with 8 v100. It cost 20 minutes for 73 updates. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:19Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3946,
    "title": "Best metric dosen't change",
    "created_at": "2021-10-11T02:23:46Z",
    "closed_at": "2022-04-17T23:21:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3946",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nBest metric dosen't change\r\n#### Code\r\nuse fairseq-train with --patience and define the evaluate metric as ppl, the best-ppl appear in wandb is always the same which result in the early stopping dosen' work\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (1.0.0a0+a61107e):\r\n - PyTorch Version (1.9.1)\r\n - OS (Linux):\r\n - How you installed fairseq (source):\r\n - Python version:3.8\r\n - CUDA/cuDNN version:10.1\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3946/comments",
    "author": "rattlesnakey",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:05Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3942,
    "title": "MBart issue when it encode sentence with new word!",
    "created_at": "2021-10-09T06:39:14Z",
    "closed_at": "2022-04-17T23:21:14Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3942",
    "body": "#### What is your question?\r\nwhen add_if_not_exists=True\r\nand the sentence have a new piece that not in the dictionary, the dictionary will add it to its map.\r\n\r\nBut the MBart model's embedding layer size does't change, and it will report a index out of boundary error.\r\n\r\n\r\n#### Code\r\n    def encode_line(\r\n        self,\r\n        line,\r\n        line_tokenizer=tokenize_line,\r\n        add_if_not_exist=True,\r\n        consumer=None,\r\n        append_eos=True,\r\n        reverse_order=False,\r\n    ) -> torch.IntTensor:\r\n        words = line_tokenizer(line)\r\n        if reverse_order:\r\n            words = list(reversed(words))\r\n        nwords = len(words)\r\n        ids = torch.IntTensor(nwords + 1 if append_eos else nwords)\r\n\r\n        for i, word in enumerate(words):\r\n            if add_if_not_exist:\r\n                idx = self.add_symbol(word)\r\n            else:\r\n                idx = self.index(word)\r\n            if consumer is not None:\r\n                consumer(word, idx)\r\n            ids[i] = idx\r\n        if append_eos:\r\n            ids[nwords] = self.eos_index\r\n        return ids\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n#### What have you tried?\r\n\r\nHow to resize the model's embedding?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3942/comments",
    "author": "SouthWindShiB",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:06Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:44Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3936,
    "title": "The ratio between train set (trainpref) and valid set (validpref) in pretraining",
    "created_at": "2021-10-07T15:54:19Z",
    "closed_at": "2022-04-17T23:21:13Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3936",
    "body": "Hello, I am very insterested in trying to pre-train the langage model from scratch.\r\nBut I am not sure about the ratio between validpref and trainpref in actual pretraining?\r\nFor example, BERT, the corpus is very large (wiki+bookcorpus), what is the reasonable ratio between train set and valid set? Or does BERT use cross-validation?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3936/comments",
    "author": "WangJiexin",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:08Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3928,
    "title": "Training time about Wav2vec2 pretrained on librispeech",
    "created_at": "2021-10-05T05:41:19Z",
    "closed_at": "2022-04-17T23:21:10Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3928",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nHi, I am using 8x Nvidia-V100 to train the wav2vec2 (base-model) on librispeech (100 hours).\r\nI set distributed_training.distributed_world_size=8 and the optimization.update_freq='[8]'.\r\nIt takes 20 minutes for one epoch (73 updated), and the 400000 updated need two mouths. So it seems much slower than the number in the paper (1.6 days with 64 v100 on 900 hours). Can anyone provide the experience on the training time?\r\nThank you! \r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n#### What have you tried?\r\n\r\nI tried to move the dataset to share memory to eliminate the I/O cost, but the time is still the same.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main): 1.0.0a0+f2146bd\r\n - PyTorch Version (e.g., 1.0) 1.9\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: 8x Nvidia-v100\r\n - Any other relevant information: apex 0.1\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3928/comments",
    "author": "r03943158",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:11Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "thechuong98",
        "created_at": "2022-06-03T10:14:48Z",
        "body": "Hi, I'm facing the same issue? Have you found a way to fix it yet?"
      },
      {
        "user": "ductho9799",
        "created_at": "2022-06-03T10:16:24Z",
        "body": "Hi, I'm facing the same issue? Have you found a way to fix it yet?"
      }
    ]
  },
  {
    "number": 3927,
    "title": "How to restore the checkpoint in wav2vec (fairseq-hydra-train))",
    "created_at": "2021-10-04T07:39:59Z",
    "closed_at": "2021-10-04T14:12:01Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3927",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:\r\n1. search the issues.\r\n2. search the docs.\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nHi, I want to restore the checkpoint to continue the training processing. \r\nHowever, I can't find the parser (--restore_file) in the fairseq-hydra-train. It only can be found in the fairseq-train.\r\nHow to restore the checkpoint on wav2vec (fairseq-hydra-train))?\r\n\r\nThank you.\r\n\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or main):\r\n - PyTorch Version (e.g., 1.0) 1.9\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: Nviaia-V100\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3927/comments",
    "author": "r03943158",
    "comments": [
      {
        "user": "ahazeemi",
        "created_at": "2021-10-04T08:22:43Z",
        "body": "@r03943158 We can use `checkpoint.restore_file` for it:\r\n\r\n```\r\nfairseq-hydra-train \\\r\n    task.data=/path/to/data \\\r\n    checkpoint.restore_file=/path/to/checkpoint\r\n    --config-dir /path/to/fairseq-py/examples/wav2vec/config/pretraining \\\r\n    --config-name wav2vec2_large_librivox\r\n```"
      },
      {
        "user": "r03943158",
        "created_at": "2021-10-04T14:11:47Z",
        "body": "@ahazeemi Thank you! It seems the script works fine."
      }
    ]
  },
  {
    "number": 3925,
    "title": "How to map timestamp to seconds referring to Issue #3627",
    "created_at": "2021-10-01T11:38:36Z",
    "closed_at": "2022-04-17T23:21:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3925",
    "body": "## How to map timestamp to seconds Issue #3627 \r\n\r\n### How to map timestamps to seconds using the formula : \r\n#### segment_start + timestep/total_frames * segment_duration:\r\n\r\nI am getting correct timestamps but i want to find respecting seconds of the audio.\r\nCan anyone explain this formulae?\r\n\r\n##### What do we have to put in these --> segment_start, segment_duration <-- variables to find respective seconds? \r\nMy audio's sample rate=16000 ,  sample width = 2\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3925/comments",
    "author": "shahzebali42",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:13Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "abarcovschi",
        "created_at": "2023-12-06T21:09:12Z",
        "body": "@shahzebali42 did you ever figure out where to get the values for segment_start and segment_duration?"
      },
      {
        "user": "abarcovschi",
        "created_at": "2023-12-17T14:13:27Z",
        "body": "I figured out how to map from timesteps to seconds for each non-blank token. The formula I used is the following:<br />\r\nsec_time = frame_num * (audio_len / (num_frames * sample_rate))<br />\r\nwhere:\r\n- frame_num = the timestep of the symbol, as returned in the 'timesteps' field of Wl2Decoder.decode() outputs.\r\n- audio_len = the number of samples in the loaded audio file corresponding to the transcript (if using batched w2v2 acoustic model inference, will be zero padded to the length of the longest loaded audio file in the batch). \r\n- num_frames = the number of frames in the emission matrix returned by the w2v2 acoustic model inference for that audio file (if using batched inference, the number of frames for each audio file will be the same as in this case all loaded audio files are padded to the length of the longest audio file in the batch).\r\n- sample_rate = sample rate of loaded audio files (usually 16000 Hz)."
      }
    ]
  },
  {
    "number": 3923,
    "title": "Just curious why Dockerfile is removed from the repository",
    "created_at": "2021-09-30T10:58:04Z",
    "closed_at": "2022-04-17T23:21:07Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3923",
    "body": "## ❓ Questions and Help\r\nI am just curious why Dockerfile is removed from the repository. I noticed that it exists previously.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3923/comments",
    "author": "jun-danieloh",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T23:32:14Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3893,
    "title": "Error using translate function with beam=1",
    "created_at": "2021-09-17T21:46:19Z",
    "closed_at": "2022-04-17T22:21:25Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3893",
    "body": "I am facing problem using `beam=1` parameter while using `translate` function, error and code given below. Note that other beam sizes are working fine.\r\n\r\n#### Stacktrace:\r\n```\r\n  File \"fairseq_translate.py\", line 33, in <module>\r\n    pred = en2de.translate(Lines[i*batch_size : i*batch_size+y], beam=1)\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 124, in translate\r\n    return self.sample(sentences, beam, verbose, **kwargs)\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 132, in sample\r\n    batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 178, in generate\r\n    translations = self.task.inference_step(\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/fairseq_task.py\", line 517, in inference_step\r\n    return generator.generate(\r\n  File \"/home/kgarg8/anaconda3/envs/fairseq/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 187, in generate\r\n    return self._generate(sample, **kwargs)\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 332, in _generate\r\n    lprobs, avg_attn_scores = self.model.forward_decoder(\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 778, in forward_decoder\r\n    decoder_out = model.decoder.forward(\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/fconv.py\", line 543, in forward\r\n    x, attn_scores = attention(\r\n  File \"/home/kgarg8/anaconda3/envs/fairseq/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/fconv.py\", line 340, in forward\r\n    x = self.bmm(x, encoder_out[0])\r\n  File \"/home/kgarg8/anaconda3/envs/fairseq/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/kgarg8/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/beamable_mm.py\", line 44, in forward\r\n    return output.view(bsz, 1, -1)\r\nRuntimeError: shape '[204, 1, -1]' is invalid for input of size 2800\r\n```\r\n#### Code\r\n```\r\nimport torch, pdb\r\nfrom tqdm import tqdm\r\n\r\nen2de = torch.hub.load('pytorch/fairseq', 'conv.wmt17.en-de', tokenizer='moses', bpe='subword_nmt')\r\nen2de.eval()\r\nen2de.cuda()\r\n\r\nfile1 = open('data/wmt16_en_de/test.en', 'r')\r\nLines = file1.readlines()\r\n\r\nbatch_size = 1000\r\nbatches = len(Lines) // batch_size\r\nif len(Lines) % batch_size != 0:\r\n    batches += 1\r\n\r\nfile2 = open('data/wmt16_en_de/pred.txt', 'w')\r\nfor i in tqdm(range(batches)):\r\n    if (i+1)*batch_size < len(Lines):\r\n        y = batch_size\r\n    else:\r\n        y = len(Lines) - i*batch_size\r\n    pred = en2de.translate(Lines[i*batch_size : i*batch_size+y], beam=1) # Error!!\r\n    for item in pred:\r\n        file2.write(\"%s\\n\" % item)\r\n```\r\n#### Environment\r\n```\r\n - fairseq Version: 1.0.0a0+f6abcc2\r\n - PyTorch Version: 1.9.0\r\n - OS: Linux\r\n - How you installed fairseq: <pip>\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.8.11\r\n - CUDA/cuDNN version: 11.1.74\r\n - GPU models and configuration: RTX A5000\r\n - Any other relevant information: N/A\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3893/comments",
    "author": "kgarg8",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:31:58Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:55Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "kurtabela",
        "created_at": "2023-03-25T12:49:12Z",
        "body": "@kgarg8 did you ever solve this issue? Im facing a similar issue during validation while training a model from pretrained mbart"
      }
    ]
  },
  {
    "number": 3886,
    "title": "Fine-tuning wav2veq seq2seq model ( hyperparameters)",
    "created_at": "2021-09-17T08:50:12Z",
    "closed_at": "2022-04-17T22:21:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3886",
    "body": "Hello,\r\n\r\nI have pretrained a wav2vec model using my own data (a set of 11000 samples, each sample have a frame number going from 1200 up to 2000 frames). It is not audo data. I have obtained a good accuracy of 99%.\r\n\r\nNow, I'm would like to perform the fine tuning (1000 samples) using th following command line:\r\n\r\n\r\npython train.py --distributed-world-size 1 --max-epoch 200 --batch-size 64 --distributed-port -1 \"${out_manifest_path_tuning}\" --save-dir $dirfinetuning --fp16 \\\r\n\t--autoregressive --valid-subset valid --no-epoch-checkpoints --no-epoch-checkpoints --eval-wer --num-workers 1 \\\r\n\t--max-update 10000 --sentence-avg --task audio_finetuning --arch wav2vec_seq2seq --w2v-path $dirPretrained/checkpoint_best.pt \\\r\n\t--labels ltr --apply-mask --mask-selection static --mask-other 0 --mask-length 10 --mask-prob 0.65 --layerdrop 0.1 \\\r\n\t--mask-channel-selection static --mask-channel-other 0 --mask-channel-length 64 --mask-channel-prob 0.004 \\\r\n\t--feature-grad-mult 0.0 --freeze-finetune-updates 0 --validate-after-updates 100 --optimizer adam \\\r\n\t--adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr  0.0005 --lr-scheduler tri_stage --warmup-steps 2000 --hold-steps 8000 \\\r\n\t--decay-steps 10000 --final-lr-scale 0.05 --final-dropout 0.0 --dropout 0.0 --activation-dropout 0.1 --criterion cross_entropy \\\r\n\t--attention-dropout 0.0 --seed 2337 --log-format json --min-sample-size 1200 --log-interval 500 --ddp-backend no_c10d  --tensorboard-logdir $TENSORBOARD_LOGDIR_TUNING\r\n\r\n \r\n\r\nHowever, The **wer on valid set is equal to 100%** during the tuning. I think that I should adjust some **hyperparameters**. I will appreciate any help in this field.\r\n\r\nThank you in advance.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3886/comments",
    "author": "sanakhamekhem",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:31:59Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3872,
    "title": "How can i get nbest results from TransformerModel.from_pretrained",
    "created_at": "2021-09-14T02:04:16Z",
    "closed_at": "2022-04-17T22:21:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3872",
    "body": "from fairseq.models.transformer import TransformerModel\r\nzh2en = TransformerModel.from_pretrained(\r\n  '/path/to/checkpoints',\r\n  checkpoint_file='checkpoint_best.pt',\r\n  data_name_or_path='data-bin/wmt17_zh_en_full',\r\n  bpe='subword_nmt',\r\n  bpe_codes='data-bin/wmt17_zh_en_full/zh.code'\r\n)\r\nzh2en.translate('你好 世界')\r\n\r\nAbove code only get one translation for each sentence, how can i get nbest results from that? Thanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3872/comments",
    "author": "lisasiyu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:32:02Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3871,
    "title": "Permission denied: \"from fairseq.models.bart import BARTModel\"",
    "created_at": "2021-09-14T01:34:50Z",
    "closed_at": "2022-04-17T22:21:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3871",
    "body": "Traceback (most recent call last):\r\n  File \"Generate.py\", line 7, in <module>\r\n    from fairseq.models.bart import BARTModel\r\n  File \"/dev_data/sxc/cm_bart/fairseq/fairseq/__init__.py\", line 14, in <module>\r\n    import fairseq.pdb  # noqa\r\n  File \"/dev_data/sxc/cm_bart/fairseq/fairseq/pdb.py\", line 16, in <module>\r\n    _stdin_lock = multiprocessing.Lock()\r\n  File \"/home/sxc/.conda/envs/metaphor/lib/python3.7/multiprocessing/context.py\", line 67, in Lock\r\n    return Lock(ctx=self.get_context())\r\n  File \"/home/sxc/.conda/envs/metaphor/lib/python3.7/multiprocessing/synchronize.py\", line 162, in __init__\r\n    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)\r\n  File \"/home/sxc/.conda/envs/metaphor/lib/python3.7/multiprocessing/synchronize.py\", line 59, in __init__\r\n    unlink_now)\r\nPermissionError: [Errno 13] Permission denied\r\n\r\n\r\n\r\nRequirement already satisfied: cffi in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from fairseq==0.9.0) (1.14.6)\r\nRequirement already satisfied: cython in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from fairseq==0.9.0) (0.29.24)\r\nRequirement already satisfied: numpy in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from fairseq==0.9.0) (1.21.2)\r\nRequirement already satisfied: regex in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from fairseq==0.9.0) (2021.8.28)\r\nRequirement already satisfied: sacrebleu in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from fairseq==0.9.0) (2.0.0)\r\nRequirement already satisfied: torch in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from fairseq==0.9.0) (1.8.1+cu102)\r\nRequirement already satisfied: tqdm in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from fairseq==0.9.0) (4.62.2)\r\nRequirement already satisfied: pycparser in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from cffi->fairseq==0.9.0) (2.20)\r\nRequirement already satisfied: colorama in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from sacrebleu->fairseq==0.9.0) (0.4.4)\r\nRequirement already satisfied: portalocker in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from sacrebleu->fairseq==0.9.0) (2.3.2)\r\nRequirement already satisfied: tabulate>=0.8.9 in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from sacrebleu->fairseq==0.9.0) (0.8.9)\r\nRequirement already satisfied: typing-extensions in /home/sxc/.conda/envs/metaphor/lib/python3.7/site-packages (from torch->fairseq==0.9.0) (3.10.0.2)\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3871/comments",
    "author": "patientalone",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:32:03Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3859,
    "title": "What is the first value of out_ds[\"info\"] for? ",
    "created_at": "2021-09-10T11:52:43Z",
    "closed_at": "2022-04-17T22:21:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3859",
    "body": "## ❓ Questions and Help\r\n\r\nHi, \r\nI am extracting wav2vec features. I wonder what the first value of out_ds[\"info\"] is for in `examples/wav2vec/wav2vec_featurize_debug.py`.\r\n\r\n```\r\n 97 class H5Writer:\r\n 98     \"\"\" Write features as hdf5 file in flashlight compatible format \"\"\"\r\n 99 \r\n100     def __init__(self, fname):\r\n101         self.fname = fname\r\n102         os.makedirs(os.path.dirname(self.fname), exist_ok=True)\r\n103 \r\n104     def write(self, data):\r\n105         channel, T = data.shape\r\n106         #chanho-added\r\n107         print(\"channel, T:\", channel, T)\r\n108 \r\n109         with h5py.File(self.fname, \"w\") as out_ds:\r\n110             data = data.T.flatten()\r\n111             out_ds[\"features\"] = data\r\n112             out_ds[\"info\"] = np.array([16e3 // 160, T, channel])\r\n```\r\n\r\nIn the line 112, the first value of  out_ds[\"info\"] is always 100, but I don't know why it is inserted. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3859/comments",
    "author": "cpark-dev",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:32:13Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3851,
    "title": "How can we check examples from the fairseq-preprocess output?",
    "created_at": "2021-09-08T02:26:41Z",
    "closed_at": "2022-04-17T22:21:07Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3851",
    "body": "The fairseq-preprocess would generate the train/test/dev idx and bin file. How can we inspect few examples from it to verify the input and target ids? Thanks a lot. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3851/comments",
    "author": "gyin94",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:32:17Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3831,
    "title": "Wav2vec CTC fine tuning model error",
    "created_at": "2021-08-29T04:03:26Z",
    "closed_at": "2021-08-30T06:59:08Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3831",
    "body": "## ❓ Questions and Help\r\nHi everyone, I am going to do fine-tuning my custom dataset using the `wav2vec_small_960h.pt`.\r\n\r\n<!-- If you still can't find what you need: -->\r\nHowever, I got an error which details as below:\r\n`\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.conda/envs/fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq_cli/train.py\", line 97, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/tasks/audio_finetuning.py\", line 190, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/tasks/audio_pretraining.py\", line 198, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/tasks/fairseq_task.py\", line 320, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/models/__init__.py\", line 107, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 176, in build_model\r\n    w2v_encoder = Wav2VecEncoder(cfg, len(task.target_dictionary))\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 356, in __init__\r\n    model = task.build_model(w2v_args.model)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/tasks/audio_pretraining.py\", line 198, in build_model\r\n    model = super().build_model(model_cfg)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/tasks/fairseq_task.py\", line 320, in build_model\r\n    model = models.build_model(cfg, self)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/models/__init__.py\", line 107, in build_model\r\n    return model.build_model(cfg, task)\r\n  File \"/home/ubuntu/manhlt/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 176, in build_model\r\n    w2v_encoder = Wav2VecEncoder(cfg, len(task.target_dictionary))\r\nTypeError: object of type 'NoneType' has no len()\r\n`\r\n#### Code\r\nhere is my running script:\r\n`\r\nfairseq-hydra-train task.data=/home/ubuntu/manhlt/phoST-ASR/format_dataset/phost-fairseq-test/ \\\r\n                    model.w2v_path=/home/ubuntu/wav2vec_small_960h.pt \\\r\n                    --config-dir config/finetuning \\\r\n                    --config-name base_100h\r\n`\r\nI already have these files in the data folder:\r\n- dict.ltr.txt\r\n- train.ltr\r\n- train.wrd\r\n- valid.ltr\r\n- valid.wrd\r\n- train.tsv\r\n- valid.tsv\r\n- lexicon.txt\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version : 1.0.0a0+6f847c8\r\n - PyTorch Version: 1.9\r\n - OS linux: Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): from source\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version: Cuda 11.0\r\n - GPU models and configuration: NVIDIA V100\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3831/comments",
    "author": "v-manhlt3",
    "comments": [
      {
        "user": "xiaoch2004",
        "created_at": "2021-08-30T04:20:30Z",
        "body": "wav2vec_small_960h.pt is the model after finetuned. You should use wav2vec_small.pt instead"
      },
      {
        "user": "v-manhlt3",
        "created_at": "2021-08-30T06:59:08Z",
        "body": "Thanks for your quick response! the problem is solved."
      }
    ]
  },
  {
    "number": 3829,
    "title": "Loading model for generation that was pretrained elsewhere",
    "created_at": "2021-08-27T13:22:10Z",
    "closed_at": "2022-05-01T17:22:23Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3829",
    "body": "I have trained a Transformer translation model on spm-preprocessed data, using my own implementation of the architecture. I would like to use fairseq for generation using these models. This is because fairseq implements some decoding strategies that I'm interested in (e.g. diverse beam search). I would like to:\r\n1. Load model weights, test data & vocabulary with fairseq,\r\n2. Generate top-k translations using selected decoding strategy.\r\n\r\nI would like to avoid training a new model from scratch in fairseq, or using pretrained models provided by fairseq (as they do not match my domain).\r\n\r\nIs this feasible or should I look into reimplementing these decoding strategies myself? Is there any code I can use as a reference?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3829/comments",
    "author": "st-vincent1",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:53Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3816,
    "title": "Constrained decoding",
    "created_at": "2021-08-25T15:05:54Z",
    "closed_at": "2022-05-01T17:22:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3816",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nAs I reimplement the part of examples-><constrained_decoding> I finish the example of constrained decoding, but as I want to use my model.pt to instead of the WMT's model.pt to achieve Vi-En's constrained translation, I found this issue：\r\n\r\n**RuntimeError: Error(s) in loading state_dict for TransformerModel:\r\n        Unexpected key(s) in state_dict: \"encoder.cons_pos_embed._float_tensor\", \"encoder.seg_embed.weight\", \"decoder.ptrnet.linear.weight\", \"decoder.ptrnet.linear.bias\". \r\n        size mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([42296, 512]) from checkpoint, the shape in current model is torch.Size([42295, 512]).\r\n        size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([42296, 512]) from checkpoint, the shape in current model is torch.Size([42295, 512]).\r\n        size mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([42296, 512]) from checkpoint, the shape in current model is torch.Size([42295, 512]).**\r\n \r\n#### Code\r\n\r\n**_echo -e \"Cảm ơn bạn\" \\\r\n| python normalize.py | python tok.py \\\r\n| fairseq-interactive /public/home/zhchynnu/perl5/ourmodel/examples/constrained_decoding/data \\\r\n  --path /public/home/zhchynnu/perl5/ourmodel/examples/constrained_decoding/path/ourmodel.pt \\\r\n  --bpe fastbpe \\\r\n  --bpe-codes /public/home/zhchynnu/perl5/ourmodel/examples/constrained_decoding/path/ourbpecodes \\\r\n  --constraints \\\r\n  -s vi -t en \\\r\n  --beam 10_**\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI guess the question might be related to the class of PT file\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):fairseq==0.10.2\r\n - PyTorch Version (e.g., 1.0)torch==1.5.0+cu101\r\n - OS (e.g., Linux):Linux\r\n - How you installed fairseq (`pip`, source):pip\r\n - Build command you used (if compiling from source):\r\n - Python version:3.6\r\n - CUDA/cuDNN version:10.1\r\n - GPU models and configuration: 2060\r\n - Any other relevant  @information:No\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3816/comments",
    "author": "jhkd-kevin",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3815,
    "title": "Is it needed to set mask=False when use wav2vec to inferrence ?",
    "created_at": "2021-08-25T07:57:29Z",
    "closed_at": "2022-05-01T17:22:19Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3815",
    "body": "## ❓ Questions and Help\r\n\r\nI search around but I don't find any useful help.\r\n\r\nI use wav2vec2 to do a ASR model,\r\n\r\nwhen testing, we found that the test accuracy is unstable\r\n\r\nI think when the input data and the model is stable, the result has to be stable, bue not.\r\n\r\nso I seach a lot and finally when I set mask=False\r\n` def forward(\r\n        self, source, padding_mask=None, mask=False, features_only=False,\r\n        mask_indices=None, mask_channel_indices=None,\r\n        padding_count=None,\r\n    )` \r\nat wav2vec2.py\r\n\r\nfinally the test acc is stable ?\r\n\r\nbut I don't know this setting is right ? \r\n\r\nshould I set **### mask=True** when I am doing a inferrence ? \r\n\r\nplease !\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3815/comments",
    "author": "XuLw",
    "comments": [
      {
        "user": "xiewenjing1170",
        "created_at": "2021-08-26T07:42:26Z",
        "body": "Hi, I also have the same problem. I use wav2vec2.0 as a feature extractor, and plus classifier to build an ASR model to do classification task.  During training wav2vec2.0, I set mask=True. But during inference, I am not sure what the mask value should be set (mask=False or mask=Value). \r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "Kick28",
        "created_at": "2025-02-11T13:43:05Z",
        "body": "bump 😃 \nThere's a little chance to get the answer for this, but I still do my shot.\nI also have the same question. Honestly speaking, I'm using HF Transformers implementation, but noticed in the config of my checkpoint the next:\n```json\n\"apply_spec_augment\": true,\n\"mask_time_prob\": 0.075\n```\nI used `facebook/wav2vec2-xlsr-53-espeak-cv-ft checkpoint` as a base checkpoint for my fine-tuning, and those values in this model config are the same. So should we still use time masking during fine-tuning and inference? "
      }
    ]
  },
  {
    "number": 3811,
    "title": "Could I build custom optimizer and lr scheduler with plugin way?",
    "created_at": "2021-08-20T23:48:22Z",
    "closed_at": "2022-05-01T17:22:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3811",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI want to build a new optimizer and lr scheduler with the plugin way, i.e. using the flag --user-dir. I could build custom model, criterion and task. However, when I build optimizer and lr scheduler, It doesn't work. It always show it is an invalid choice.  Is it possible for us to build a custom optimizer or lr scheduler in a plugin way? I could not find this kind of example in the source code.\r\n\r\n#### Code\r\nMy user dir looks like:\r\n  |----my_dir\r\n  |-------- criterions\r\n  |------------__init__.py\r\n  |------------my_criterion.py\r\n  |--------models\r\n  |------------__init__.py\r\n  |------------my_model.py\r\n  |--------optim\r\n  |------------__init__.py\r\n  |------------my_optim.py\r\n  |------------lr_scheduler\r\n  |----------------__init__.py\r\n  |----------------my_lr_scheduler.py\r\n\r\nOnly the lr scheduler and optimizer do not work. \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.10.2\r\n - PyTorch Version (e.g., 1.0): 1.8\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3811/comments",
    "author": "BaohaoLiao",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:26Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3808,
    "title": "What to infer from this hypothesis?",
    "created_at": "2021-08-20T06:52:07Z",
    "closed_at": "2022-04-22T20:03:58Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3808",
    "body": "## ❓ Questions and Help\r\n\r\nI am training a fairseq model for translation from an indic language to english.\r\n\r\nAfter the completion of an epoch and during the subsequent validation, the following `\"\"\"\"` output is observed.\r\n\r\n(This is a sample of that output)\r\n```\r\n20 11:47:15 | INFO | fairseq.tasks.translation | example hypothesis: \"\"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\"\r\n2021-08-20 11:47:15 | INFO | fairseq.tasks.translation | example reference: Fire at Telangana firecracker factory kills 11\r\n20 11:47:19 | INFO | fairseq.tasks.translation | example hypothesis: \"\"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\"\r\n2021-08-20 11:47:19 | INFO | fairseq.tasks.translation | example reference: Photography and shooting of videos has been banned.\r\n2021-08-20 11:50:57 | INFO | fairseq.tasks.translation | example reference: \"Justice Mukta Gupta, however, said that if the proceedings in the matter are delayed due to Kejriwals absence, the trial court has liberty to modify the order and direct the AAP leader to appear before it whenever required. It also asked the leader to give an undertaking before a trial court that he will have no objection if the matter proceeds in his absence .\"\r\n2021-08-20 11:51:19 | INFO | fairseq.tasks.translation | example hypothesis: \"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\" \"\"\"\"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\" \"\"\"\"\"\r\n2021-08-20 11:51:19 | INFO | fairseq.tasks.translation | example reference: All times local (UTC + 02) Man of the Match: Antoln Alcaraz (Paraguay) Assistant referees: Hctor Vergara (Canada) Marvin Torrentera (Mexico) Fourth official: Joel Aguilar (El Salvador) Fifth official: Juan Zumba (El Salvador) Man of the Match: Rbert Vittek (Slovakia) Assistant referees: Celestin Ntagungira (Rwanda) Enock Molefe (South Africa) Fourth official: Ravshan Irmatov (Uzbekistan) Fifth official: Rafael Ilyasov (Uzbekistan) Man of the Match: Enrique Vera (Paraguay) Assistant referees: Evarist Menkouande (Cameroon) Bechir Hassani (Tunisia) Fourth official: Joel Aguilar (El Salvador) Fifth official: Juan Zumba (El Salvador) Man of the Match: Daniele De Rossi (Italy) Assistant referees: Leonel Leal (Costa Rica) Carlos Pastrana (Honduras) Fourth official: Koman Coulibaly (Mali) Fifth official: Redouane Achik (Morocco) Man of the Match: Rbert Vittek (Slovakia) Assistant referees: Darren Cann (England) Michael Mullarkey (England) Fourth official: Stphane Lannoy (France) Fifth official: Eric Dansault (France) Man of the Match: Roque Santa Cruz (Paraguay) Assistant referees: Toru Sagara (Japan) Jeong Hae-sang (South Korea) Fourth official: Koman Coulibaly (Mali) Fifth official: Incio Manuel Cndido (Angola)\r\n2021-08-\r\n``` \r\n\r\nMore than 50% of the hypotheses observed had such `\"\"\"\"` strings.\r\n\r\nWhat does this mean?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3808/comments",
    "author": "ekdnam",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:28Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "ekdnam",
        "created_at": "2022-04-22T20:03:58Z",
        "body": "I think this means that the output word has not been generated, and thus it is represented as an empty string by a bunch of inverted commas."
      }
    ]
  },
  {
    "number": 3800,
    "title": "What BLEU score used in fconv paper",
    "created_at": "2021-08-19T02:11:28Z",
    "closed_at": "2022-05-01T17:22:02Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3800",
    "body": "I tried to replicate results obtained in the original Convolutional Sequence to Sequence Learning (Gehring et al., 2017) paper.\r\nAfter 3 epochs, evaluated on test dataset, the BLEU score is *BLEU4 =20.12*\r\n\r\nDo I need to train for more epochs or BLEU in the paper is not BLEU4, maybe BLUE2?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3800/comments",
    "author": "renmada",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:35Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:32Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3789,
    "title": "Generate Word Time Stamp for the transcription of audio after splitting it into chunks ",
    "created_at": "2021-08-17T13:59:16Z",
    "closed_at": "2022-05-01T17:21:59Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3789",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3789/comments",
    "author": "pushkal1234",
    "comments": [
      {
        "user": "pushkal1234",
        "created_at": "2021-08-17T14:01:07Z",
        "body": "How to get word time stamp in wav2vec 2.0? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:37Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3784,
    "title": "How to save w2v_args during Wav2Vec 2.0 finetuning to model checkpoints?",
    "created_at": "2021-08-12T16:40:25Z",
    "closed_at": "2022-05-01T16:22:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3784",
    "body": "#### What is your question?\r\n\r\nCurrently, it appears that the wav2vec_asr.py Wav2VecEncoder classes uses a somewhat inefficient methodology for loading in a finetuning checkpoint. In order to build the W2V Encoder class, it looks for the original pretrained model file if the w2v_args attribute is None:\r\n\r\n```\r\nif cfg.w2v_args is None:\r\n            state = checkpoint_utils.load_checkpoint_to_cpu(cfg.w2v_path, arg_overrides)\r\n            w2v_args = state.get(\"cfg\", None)\r\n            if w2v_args is None:\r\n                w2v_args = convert_namespace_to_omegaconf(state[\"args\"])\r\n            cfg.w2v_args = w2v_args\r\n```\r\n\r\nThis means that if you finetune a W2V model, the checkpoint for the finetuning is incomplete without the original pretrained model file, which adds a large disk space footprint.\r\n\r\nIs there any way to have the w2v_args attribute populated with the pretrained model params during finetuning somehow in order to bypass this requirement? I note that the finetuned models available for download already have this done, but they use a pre-Hydra format, so there is just the `args` key in their state file, but that args['model']['w2v_args'] is populated, but for post-Hydra finetuned models, it is left as None:\r\n\r\n```\r\nstate = torch.load('my_finetuned_hydra_model/checkpoints/checkpoint_best.pt', 'cpu')\r\n\r\nstate['args']  # returns None\r\n\r\nstate['cfg']['model']['w2v_args']  # returns None\r\nstate['cfg']['model']['w2v_path']   # path to the original pretrained model file\r\n```\r\n\r\nIdeally, I am looking for some mechanism that during finetuning it saves the pretrained model config to `state['cfg']['model']['w2v_args']` so that I do not need to transfer over the pretrained model file in addition to the finetuned model file. I want the finalised, finetuning checkpoint to have all of the information required to reconstruct the model, and not be reliant on any external model files.\r\n\r\nIs there any way I can do this easily during finetuning? Some flag I can set maybe?\r\n\r\nWould greatly appreciate any help.\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master but from May 10, 2021\r\n - PyTorch Version (e.g., 1.0): 1.8.1\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): python setup.py bdist_wheel\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 11.1/8.0.0.5\r\n - GPU models and configuration: 8X A100\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3784/comments",
    "author": "trias702",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3782,
    "title": "How to preprocess the perfectly mismatched text when training roberta",
    "created_at": "2021-08-12T06:56:26Z",
    "closed_at": "2022-04-17T02:40:03Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3782",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n#### What is your question?\r\nWhen training roberta, did you preprocess perfectly mismatched text ?\r\nCan you tell me how to preprocess the perfectly mismatched text when training roberta?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3782/comments",
    "author": "upskyy",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:45Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3780,
    "title": "sequence generator generation when prefix_tokens ends with <eos>",
    "created_at": "2021-08-11T08:49:50Z",
    "closed_at": "2022-05-01T16:22:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3780",
    "body": "Hi, I'm trying to generate a new sentence based on previous sentences. \r\nThe data format is like` s1 <eos> s2 <eos> s3 <eos>`\r\nHowever, I found that SequenceGenerator stops generation at the step where prefix_token is  `<eos>`.\r\nIs there a simple way to get rid of it?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3780/comments",
    "author": "YuxianMeng",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3779,
    "title": "I got AttributeError: module 'sacrebleu' has no attribute 'compute_bleu'.",
    "created_at": "2021-08-11T07:02:02Z",
    "closed_at": "2022-05-01T17:22:03Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3779",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nAttributeError: module 'sacrebleu' has no attribute 'compute_bleu'.\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nCUDA_VISIBLE_DEVICES=0 fairseq-train     data-bin/iwslt14.tokenized.de-en     --arch transformer_iwslt_de_en --share-decoder-input-output-embed     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0     --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000     --dropout 0.3 --weight-decay 0.0001     --criterion label_smoothed_cross_entropy --label-smoothing 0.1     --max-tokens 4096     --eval-bleu     --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}'     --eval-bleu-detok moses     --eval-bleu-remove-bpe     --eval-bleu-print-samples     --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version 1.0.0a0+9825786\r\n - PyTorch Version 1.6\r\n - OS ubuntu 18.04\r\n - How you installed fairseq: source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: cuda10.2\r\n - GPU models and configuration: tesla p40\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3779/comments",
    "author": "husthkk",
    "comments": [
      {
        "user": "matthijsvk",
        "created_at": "2021-08-19T05:59:39Z",
        "body": "I think it's version 2.0 of sacrebleu causing issues.\r\nYou can do `pip uninstall sacrebleu; pip install sacrebleu==1.5.1`"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:33Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:34Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3769,
    "title": "How can I load decoder from a Machine Translation model to my Speech to text translation model?",
    "created_at": "2021-08-07T05:44:29Z",
    "closed_at": "2022-05-01T16:22:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3769",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI'm training a speech to text model using COVOST dataset, just like it is written in the examples. There is a command tool parameter (**--load-pretrained-encoder-from**) which initializes the encoder of my speech-to-text model with a pre-trained ASR model. I was wondering if there's a way to do the same with the decoder of  ST network and a pretrained Machine translation model. \r\n\r\nI know that to be able to load the 'encoder' or 'decoder' of another pretrained model, the architectures should be the same. For ASR there's already a pretrained model using **\"st_transformer_s\"** architecture, which matches with the ST model. Another complication is that MT architectures don't match. \r\n\r\nI would appreciate any kind of help. Thanks. \r\n\r\n#### Code\r\n\r\nfairseq-train ${COVOST_ROOT}/fr \\\r\n  --config-yaml config_st_fr_en.yaml --train-subset train_st_fr_en --valid-subset dev_st_fr_en \\\r\n  --save-dir ${ST_SAVE_DIR} --num-workers 4 --max-update 30000 --max-tokens 40000 \\  # --max-tokens 50000 for en-*\r\n  --task speech_to_text --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --report-accuracy \\\r\n  --arch s2t_transformer_s --encoder-freezing-updates 1000 --optimizer adam --lr 2e-3 \\\r\n  --lr-scheduler inverse_sqrt --warmup-updates 10000 --clip-norm 10.0 --seed 1 --update-freq 8 \\\r\n  _--load-pretrained-encoder-from ${ASR_SAVE_DIR}/${CHECKPOINT_FILENAME}_\r\n\r\n#### What have you tried?\r\n\r\nI have searched all over the code to find an option that lets me load pretrained decoder to my model. Seems like there IS such an option from other tasks, just not for ST.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0.0a0+fc391ff\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3769/comments",
    "author": "amirfaghihi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3767,
    "title": "BPE and dictionary",
    "created_at": "2021-08-07T00:44:50Z",
    "closed_at": "2022-05-01T16:22:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3767",
    "body": "I run:\r\n```bash\r\nfairseq-preprocess --tokenizer moses --bpe characters --source-lang src --target-lang tgt --trainpref $TEXT/train --testpref $TEXT/test --destdir data-bin2/data.toked.src-tgt\r\n```\r\nWith tokenizer and without, and with different tokenizers and bpes {characters/subword/..} but I always get the same vocabularies, with full words, shouldn't I see the words broken down in the vocabulary if I am using BPE? \r\n\r\nEnv:\r\n - fairseq master:\r\n - PyTorch Version (e.g., 1.0)\r\n - OS WSL/ Ubuntu /Windows doesn't make a difference\r\n - How you installed fairseq pip/source doesn't make a difference\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3767/comments",
    "author": "ImanHosseini",
    "comments": [
      {
        "user": "ImanHosseini",
        "created_at": "2021-08-08T19:40:50Z",
        "body": "What is bpe flag even doing? I put a breakpoint on \"build_bpe\" function and it is not hit, so even if I specify \"bpe\" in options, no bpe?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3764,
    "title": "fairseq-hydra-train: how to change the location of the outputs directory containing hydra_train.log ",
    "created_at": "2021-08-06T06:44:48Z",
    "closed_at": "2021-08-06T07:26:16Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3764",
    "body": "it currently populates outputs/ in the location where `fairseq-hydra-train` is called but i'd like it to go somewhere else more sensible. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3764/comments",
    "author": "tensorfoo",
    "comments": [
      {
        "user": "tensorfoo",
        "created_at": "2021-08-06T07:26:16Z",
        "body": "Think i got it, needed to add `-m hydra.sweep.dir=pathtodir`"
      },
      {
        "user": "chengrunyang",
        "created_at": "2021-10-22T14:21:06Z",
        "body": "Hi @tensorfoo , did you add `-m hydra.sweep.dir=<pathtodir>` as an argument to the `fairseq-hydra-train` command? \r\n\r\nOn my end, adding `-m hydra.sweep.dir=<pathtodir>` gives `fairseq-hydra-train: error: unrecognized arguments: hydra.sweep.dir=<pathtodir>`, and adding `hydra.sweep.dir=<pathtodir>` (without `-m`) does not change the output location."
      },
      {
        "user": "tensorfoo",
        "created_at": "2021-10-23T16:29:58Z",
        "body": "Hi @chengrunyang. I don't know why I wrote -m either now. But for pretraining you're right it doesn't work but it did work for  fine-tuning (without the -m). I would also like to do it for pretraining so please let me know if you figure it out. "
      },
      {
        "user": "chengrunyang",
        "created_at": "2021-10-23T16:37:28Z",
        "body": "Thanks for the information. Weird, I'm fine-tuning RoBERTa on GLUE tasks, and neither with nor without the `-m` works. Not sure if it's specific to your fine-tuning task. Would you mind sharing an example command that you used? "
      }
    ]
  },
  {
    "number": 3762,
    "title": "Where can I find the wmt 15 dataset for the paper Monotonic Multihead attention?",
    "created_at": "2021-08-04T15:39:33Z",
    "closed_at": "2022-04-17T22:21:13Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3762",
    "body": "\r\n\r\n\r\nHi I am reading the paper \"Monotonic Multihead Attention\" for simultaneous machine translation. \r\n\r\nThe readme document said \"Please follow the instructions to download and preprocess the WMT'15 En-De dataset.\" But I only found the script for wmt 14 and 17 but 15 is not there. Is the script the same for them?\r\n\r\nThank you in advance!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3762/comments",
    "author": "MingyangHeseri",
    "comments": [
      {
        "user": "kurtisxx",
        "created_at": "2021-09-11T20:20:33Z",
        "body": "Is there any progress on this issue? I'm having the same question."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:32:11Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3756,
    "title": "How do I load a Wav2Vec2 Model from a checkpoint and run it over a list of audio files for inference?",
    "created_at": "2021-08-02T08:29:21Z",
    "closed_at": "2022-05-01T16:22:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3756",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI have tried all the other similar questions asked by other members but still not able to load and run it for inferences. \r\n#### Code\r\nimport torch\r\nimport fairseq\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(['output_col/checkpoint_best.pt'], arg_overrides={\"data\": './col_dataset/'})\r\nmodel = model[0]\r\n\r\ntype(model) = > fairseq.models.wav2vec.wav2vec2_asr.Wav2VecCtc\r\n\r\nmodel.eval() => Prints the model architecture but not the weights\r\n\r\nwav_input_16khz = torch.randn(1,10000) \r\nz = model.feature_extractor(wav_input_16khz)\r\nc = model.feature_aggregator(z)\r\nprint(c) => AttributeError: 'Wav2VecCtc' object has no attribute 'feature_extractor'\r\n\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3756/comments",
    "author": "sourabharsh",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:38Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:34Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3752,
    "title": "TransformerDecoder  CUDA out of memory",
    "created_at": "2021-07-30T07:35:23Z",
    "closed_at": "2022-05-01T16:22:00Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3752",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nWhen i inference TransformerDecoder model with varying batch size, eg: 10 x 20, 5 x 15 and so on, error  occurs:\r\nerror:CUDA out of memory. Tried to allocate 7.91 GiB (GPU 0; 14.76 GiB total capacity; 4.44 GiB already allocated; 7.89 GiB free; 4.45 GiB reserved in total by PyTorch)\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI tried to add c10::cuda::CUDACachingAllocator::emptyCache() after model.forword(), but CUDA out of memory still occurs.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0.0\r\n - PyTorch Version (e.g., 1.0) : 1.6\r\n - OS (e.g., Linux): centos\r\n - How you installed fairseq (`pip`, source): pip \r\n - Build command you used (if compiling from source):\r\n - Python version:  python3\r\n - CUDA/cuDNN version: cuda10.2\r\n - GPU models and configuration:  TransformerDecoder \r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3752/comments",
    "author": "ismymajia",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:29Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3749,
    "title": "RuntimeError when execute w2vu_generate.py",
    "created_at": "2021-07-28T10:34:34Z",
    "closed_at": "2022-05-01T17:22:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3749",
    "body": "## ❓ Questions and Help\r\n\r\nHi, I am using Wav2vec Unsupervised (wav2vec-U) framework and at the step of generation by follow command \r\n```bash\r\npython3 w2vu_generate.py --config-dir config/generate --config-name viterbi \\ \r\nfairseq.common.user_dir=$FAIRSEQ_ROOT/examples/wav2vec/unsupervised \\\r\nfairseq.task.data=/workspace/wav2vec_u/workplace/prepare_timit/unmatched/feat \\\r\nfairseq.common_eval.path=/workspace/wav2vec_u/fairseq2/examples/wav2vec/unsupervised/multirun/2021-07-28/08-50-33/0/checkpoint_last.pt \\\r\nfairseq.dataset.gen_subset=valid \\\r\nresults_path=./transcriptions \\\r\nlm_model=/workspace/wav2vec_u/workplace/prepare_timit/unmatched/phones/train_text_phn.04.bin \\\r\n```\r\nI got an RuntimeError as below : \r\n```python\r\n[2021-07-28 10:23:17,042][__main__][INFO] - | loading model(s) from /workspace/wav2vec_u/fairseq2/examples/wav2vec/unsupervised/multirun/2021-07-28/08-50-33/0/checkpoint_last.pt\r\n[2021-07-28 10:23:22,497][unsupervised.data.extracted_features_dataset][INFO] - loaded 620, skipped 0 samples\r\n[2021-07-28 10:23:22,497][unsupervised.tasks.unpaired_audio_text][INFO] - split valid has unpaired text? False\r\n[2021-07-28 10:23:22,498][__main__][INFO] - | /workspace/wav2vec_u/workplace/prepare_timit/unmatched/feat valid 620 examples\r\nTraceback (most recent call last):                                                                                                                                                         \r\n  File \"w2vu_generate.py\", line 692, in hydra_main\r\n    _, score = main(cfg)\r\n  File \"w2vu_generate.py\", line 621, in main\r\n    gen_result = generate(cfg, models, saved_cfg, use_cuda)\r\n  File \"w2vu_generate.py\", line 456, in generate\r\n    generator, models, num_feats, sample, task, use_cuda\r\n  File \"w2vu_generate.py\", line 568, in gen_hypos\r\n    hypos = task.inference_step(generator, models, sample, None)\r\n  File \"/workspace/wav2vec_u/fairseq2/fairseq/tasks/fairseq_task.py\", line 528, in inference_step\r\n    models, sample, prefix_tokens=prefix_tokens, constraints=constraints\r\n  File \"/workspace/wav2vec_u/fairseq2/examples/speech_recognition/w2l_decoder.py\", line 76, in generate\r\n    emissions = self.get_emissions(models, encoder_input)\r\n  File \"/workspace/wav2vec_u/fairseq2/examples/speech_recognition/w2l_decoder.py\", line 82, in get_emissions\r\n    encoder_out = model(**encoder_input)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/workspace/wav2vec_u/fairseq2/examples/wav2vec/unsupervised/models/wav2vec_u.py\", line 538, in forward\r\n    gen_result = self.generator(features, random_label, padding_mask)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/workspace/wav2vec_u/fairseq2/examples/wav2vec/unsupervised/models/wav2vec_u.py\", line 310, in forward\r\n    dense_x = self.proj(dense_x)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\", line 119, in forward\r\n    input = module(input)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 263, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 260, in _conv_forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: Given groups=1, weight of size [43, 512, 4], expected input[1, 1024, 237] to have 512 channels, but got 1024 channels instead\r\n```\r\nI don't get where the error is. Can someone help ? Thanks in advance !\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3749/comments",
    "author": "Ning107",
    "comments": [
      {
        "user": "prokotg",
        "created_at": "2021-08-20T11:49:11Z",
        "body": "Seems like you use initial features\r\n\r\n`fairseq.task.data=/workspace/wav2vec_u/workplace/prepare_timit/unmatched/feat`\r\n\r\ninstead of processed feats in subfolders, probably something like:\r\n\r\n`fairseq.task.data=/workspace/wav2vec_u/workplace/prepare_timit/unmatched/feat/precompute_pca512_cls128_mean_pooled`"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:27Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3748,
    "title": "How to fairseq-preprocess without applying BPE?",
    "created_at": "2021-07-28T03:00:45Z",
    "closed_at": "2022-05-01T16:22:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3748",
    "body": "How to binarize dataset but without applying BPE with `fairseq-preprocess`?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3748/comments",
    "author": "speedcell4",
    "comments": [
      {
        "user": "kellycyy",
        "created_at": "2021-08-07T06:02:25Z",
        "body": "The `fairseq-preprocess` includes the binarizing process in its source code. Did you state the `--dataset-impl=raw` argument instead of using the default value? If you use the former one, the output files will not be `.bin` and `.idx`."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3740,
    "title": "RuntimeError: Function MmBackward returned an invalid gradient at index 0 ",
    "created_at": "2021-07-26T14:12:39Z",
    "closed_at": "2022-05-01T02:22:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3740",
    "body": "I'm using this example code, \"speech recognition\", for a similar task ,\"handwriting recognition\", and my input is image.\r\nrecently I faced this error : \"RuntimeError: Function MmBackward returned an invalid gradient at index 0 - got [2320, 29] but expected shape compatible with [2320, 128]\"\r\nI'm using google colab(GPU) to run my code.\r\n\r\nthe encoder out is of size : [29, 80, 1024]\r\nthe decoder out is of size : [67, 80, 176]\r\nlprobs is of size: [67, 80, 176]\r\n\r\ncan anyone help me with this?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3740/comments",
    "author": "hosein-khodadi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3736,
    "title": "How to speed up Wav2vec 2.0 model performance and deploy this model on production?",
    "created_at": "2021-07-24T17:49:44Z",
    "closed_at": "2022-05-01T16:21:55Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3736",
    "body": "I fine-tuned the Wav2vec model on my database and we want to deploy it on the production, due to the high parameters (317M) of this model and the high speed of converting speech to text, so I can not deploy it on the production.\r\nplease help me!!!\r\n ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3736/comments",
    "author": "miladfa7",
    "comments": [
      {
        "user": "jubick1337",
        "created_at": "2021-07-28T07:55:56Z",
        "body": "Hi @miladfa7 \r\nI don't think there are any tools for speed up in fairseq \r\nAlthough you can search for pruning/quantization etc "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:25Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3733,
    "title": "Logging weight histogram to tensorboard",
    "created_at": "2021-07-23T11:39:57Z",
    "closed_at": "2022-05-01T02:22:14Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3733",
    "body": "### How can I add histogram of weights to the tensorboard logs?\r\nI see that in model code I'm using (transformer model) there's this:\r\n```python\r\n metrics.log_scalar( \"gb_total\", gb_total, priority=1600, round=1, weight=0)\r\n```\r\nBut metrics has nothing similar to tensorboard utils \"add_historgram\" frunction, and I don't where and how I can make it add histogram of the weights to the logs.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3733/comments",
    "author": "ImanHosseini",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:30Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:44Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3732,
    "title": "incremental states in multi-head attention",
    "created_at": "2021-07-23T07:19:52Z",
    "closed_at": "2022-05-01T02:22:12Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3732",
    "body": "Hello,\r\n\r\nI want to use only one decoder layer to loop (stack) several times for generation (seq2seq or language modeling). Although it can be trained properly, during inference the results will be terrible because of the issues of incremental state updates while looping. Although there is a workaround that uses multiple decoder layers with shared parameters, I wonder if there is a direct way to solve the issue. And, is there any instructions about how incremental state update in multihead attention are implemented.\r\n\r\nThanks,",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3732/comments",
    "author": "getao",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "getao",
        "created_at": "2022-05-01T03:20:58Z",
        "body": "I think this issue is still worth handling because the training and inference's multi-head attn will be different due to this issue."
      }
    ]
  },
  {
    "number": 3718,
    "title": "wav2vec 2.0 pre-training get stuck when using a 10k hours dataset, works fine with only 1k hours",
    "created_at": "2021-07-16T10:27:37Z",
    "closed_at": "2022-04-18T00:21:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3718",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n#### What is your question?\r\n\r\nI am trying to pre-train wav2vec 2 on a custom dataset (10k hours). When pre-training on the complete dataset with the \"_base_librispeech_\" config, **the loss value get stuck at 6.658 and accuracy is oscillating from 0.2 up to 0.8** (exact same loss value as in #3661). The same problem occurs when pre-training with the same config but using only 1k hours from the dataset.\r\n\r\nSeeing #3661, I tried to lower the lr from 0.0005 to 0.0001, which works but only when using 1k hours. The initial behavior still occurs when trying this solution for a training on the complete dataset. \r\n\r\nIs this behavior to be expected when training with this much data and no change to the '_base_librispeech_' config? And if so, how should I adjust hyperparameters to account for that ?\r\n\r\nCustom daset info : \r\n\r\n- French speakers audio\r\n- segmented using a CNN to supress silences and music\r\n- 16 kHz, 16 bits, mono .wav files\r\n- Avg. clip duration : 30 sec.\r\n- Cumul. duration : 10k hours\r\n\r\n#### Code\r\n\r\nConfig 1 : Running fairseq-hydra-train with default parameters and no change to the '_base_librispeech_' config.\r\nConfig 2 : Running fairseq-hydra-train with default parameters and lowering lr from 0.0005 to 0.0001 in the '_base_librispeech_' config. \r\n\r\n#### What have you tried?\r\n\r\nLowering lr from 0.0005 to 0.0001 in the '_base_librispeech_' config\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.9.0\r\n - OS (e.g., Linux): Redhat Entreprise Linux 8.1\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: pip install --editable ./\r\n - CUDA/cuDNN version: 3.7.10\r\n - GPU models and configuration: 8x Nvidia Tesla V100 \r\n - Any other relevant information: \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3718/comments",
    "author": "yamsok",
    "comments": [
      {
        "user": "jubick1337",
        "created_at": "2021-07-16T12:26:17Z",
        "body": "Hi @YamSok \r\nI'm facing similar issues about pretraining w2v2\r\nHave you tested running with ```optimization.update_freq='[8]'```?\r\nIn my case it helped \r\n"
      },
      {
        "user": "RuABraun",
        "created_at": "2021-10-28T22:51:05Z",
        "body": "Also having similar problems, gonna try out increasing value of update_freq thanks @jubick1337"
      },
      {
        "user": "bmilde",
        "created_at": "2021-11-06T11:21:04Z",
        "body": "What exactly is optimization.update_freq='[8]' doing? @jubick1337 "
      },
      {
        "user": "bmilde",
        "created_at": "2021-11-06T11:25:06Z",
        "body": "Ok got it, its for simulating multi-GPU:\r\n\r\n> Note: you can simulate 64 GPUs by using k GPUs and adding command line parameters (before --config-dir) distributed_training.distributed_world_size=k +optimization.update_freq='[x]' where x = 64/k"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:24Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3715,
    "title": "Controlling optimizer.step()",
    "created_at": "2021-07-15T21:04:36Z",
    "closed_at": "2022-04-26T03:04:46Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3715",
    "body": "## ❓ Questions and Help\r\nHi,\r\n\r\nI'm dealing with a very special loss which requires significant amount of memory to calculate. Unfortunately I don't have GPUs with enough memory. So I decide to calculate the first part of the loss, perform loss.backward() and then calculate the second part, loss.backward() and finally optimizer.step(). \r\n\r\nI'm wondering where should I modify so that optimizer.step() is performed after more than one back propagation?\r\n\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3715/comments",
    "author": "marcmk6",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:46Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3710,
    "title": "What does 1 update in the log file correspond to? Does it refer to update after a pass through a batch?",
    "created_at": "2021-07-14T05:53:47Z",
    "closed_at": "2022-05-01T02:21:59Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3710",
    "body": "What does 1 update in the log file correspond to? Does it refer to update after a pass through a batch?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3710/comments",
    "author": "kanchanpoudel",
    "comments": [
      {
        "user": "RuABraun",
        "created_at": "2021-07-15T08:30:12Z",
        "body": "What is `log_interval` set to in your config?"
      },
      {
        "user": "kanchanpoudel",
        "created_at": "2021-07-16T01:52:34Z",
        "body": "100\r\n\r\n"
      },
      {
        "user": "RuABraun",
        "created_at": "2021-07-16T07:54:41Z",
        "body": "Then it's every 100 updates, as you should see from what is bring printed (`num_updates` should be increasing by 100 each time)."
      },
      {
        "user": "jubick1337",
        "created_at": "2021-07-16T08:35:21Z",
        "body": "I might be wrong but it seems to be number of optimizer.step() calls.\r\nOne can control update frequency in optimization part of config and if one sets it to be higher than 1 training would be slower (same amount of num updates but more time)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:45Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:29Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3707,
    "title": "Computing dataset size in hours from the average stats of a finetuned wav2vec2 model",
    "created_at": "2021-07-12T16:22:32Z",
    "closed_at": "2022-05-01T02:21:56Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3707",
    "body": "#### How to compute dataset size in hours from the average epoch stats of a finetuned wav2vec2 model\r\n\r\n{\"epoch\": 10, \"train_loss\": \"48.102\", \"train_ntokens\": \"12450.2\", \"train_nsentences\": \"244.602\", \"train_nll_loss\": \"0.945\", \"train_wps\": \"35062.9\", \"train_ups\": \"2.82\", \"train_wpb\": \"12450.2\", \"train_bsz\": \"244.6\", \"train_num_updates\": \"130460\", \"train_lr\": \"0.0001\", \"train_gnorm\": \"69.339\", \"train_loss_scale\": \"4\", \"train_train_wall\": \"4384\", \"train_gb_free\": \"12.4\", \"train_wall\": \"46412\"}\r\n\r\nIt is evident that number of updates per epoch = 13046\r\nand in each update, we use a total words of 12450.2 * 244.6\r\n\r\nMeaning dataset size if sample rate is 16k = 13046 * 12450.2 * 244.6 /16000/3600 hours\r\n\r\nIs this way of calculating right?\r\nAlso does words mean tokens here?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3707/comments",
    "author": "tahirjmakhdoomi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:26Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3703,
    "title": "How to save a transformer model into Torch Script ?",
    "created_at": "2021-07-10T14:16:54Z",
    "closed_at": "2022-04-30T11:22:22Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3703",
    "body": "## ❓ Questions and Help\r\n\r\nBecause I want to serving the trained Transformer model (Fairseq), I should **save the model as a Torch Scripts.** I have read documents and other issues and not found an example to do that in Fairseq.\r\n\r\nDo you have any guide to help me ? Or any successful experience to do it. Thank you very much.\r\n\r\n\r\n\r\n#### What have you tried?\r\nI learned from the unittest as the below code. So, I guess that Fairseq may support save model or sequence_generator as Torch Scripts.\r\n```\r\nclass TestJitSequenceGenerator(TestJitSequenceGeneratorBase):\r\n    def test_export_transformer(self):\r\n        model = self.transformer_model\r\n        torch.jit.script(model)\r\n\r\n    def test_ensemble_sequence_generator(self):\r\n        model = self.transformer_model\r\n        generator = SequenceGenerator(\r\n            [model],\r\n            self.task.tgt_dict,\r\n            beam_size=2,\r\n            no_repeat_ngram_size=2,\r\n            max_len_b=10,\r\n        )\r\n        scripted_model = torch.jit.script(generator)\r\n        self._test_save_and_load(scripted_model)\r\n\r\n    def test_export_ensemble_model(self):\r\n        model = self.transformer_model\r\n        ensemble_models = EnsembleModel([model])\r\n        torch.jit.script(ensemble_models)\r\n```\r\n\r\nSo I continue to have a try. I know that the interactive.py contains these step, load checkpont.pt, model, and sequence_generator. Therefore, I add\r\n```\r\nscripted_model = torch.jit.script(generator)\r\nscripted_model.save(\"./filename\")\r\n```\r\nto the interactive.py in order to save Torch Script. **But I failed.**  \r\n```\r\n  File \"/Users/pshi/opt/anaconda3/envs/speller3/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 328, in init_fn\r\n    scripted = recursive_script(orig_value)\r\n  File \"/Users/pshi/opt/anaconda3/envs/speller3/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 534, in recursive_script\r\n    return create_script_module(nn_module, infer_methods_to_compile(nn_module))\r\n  File \"/Users/pshi/opt/anaconda3/envs/speller3/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 296, in create_script_module\r\n    return create_script_module_impl(nn_module, concrete_type, cpp_module, stubs)\r\n  File \"/Users/pshi/opt/anaconda3/envs/speller3/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 340, in create_script_module_impl\r\n    create_methods_from_stubs(concrete_type, stubs)\r\n  File \"/Users/pshi/opt/anaconda3/envs/speller3/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 259, in create_methods_from_stubs\r\n    concrete_type._create_methods(defs, rcbs, defaults)\r\nRuntimeError: \r\nTried to access nonexistent attribute or method 'type' of type 'Device'.:\r\n  File \"/Users/pshi/opensource/fairseq/fairseq/modules/multihead_attention.py\", line 147\r\n            need_weights = True\r\n\r\n        is_tpu = query.device.type == \"xla\"\r\n                 ~~~~~~~~~~~~~~~~~ <--- HERE\r\n\r\n        tgt_len, bsz, embed_dim = query.size()\r\n```\r\n\r\n - fairseq Version: 1.0 master:\r\n - PyTorch Version: 1.8\r\n - OS: Linux:\r\n - How you installed fairseq: source\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.1\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3703/comments",
    "author": "shipengAlan",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:52Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-30T11:21:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3701,
    "title": "The best way to build two transformer sub-models with different configs in my custom model",
    "created_at": "2021-07-10T00:43:59Z",
    "closed_at": "2022-04-30T11:22:22Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3701",
    "body": "## ❓ Questions and Help\r\n\r\nWhat is the best way to build two transformer sub-models with different configs in my custom model?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.10.2\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3701/comments",
    "author": "zhengxxn",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:53Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-30T11:21:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3691,
    "title": "Training Fairseq model using pre-trained embeddings",
    "created_at": "2021-07-07T14:06:12Z",
    "closed_at": "2022-05-01T16:21:57Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3691",
    "body": "## ❓ Questions and Help\r\n\r\nI'm using a big model from fairseq. And I'm experimenting using pre-trained embedding  with fairseq. But when i start training with embeddings parameters in the encoder side  on fp16, this error generated (FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n#### Code\r\n\r\nI used this command to train a fasttext model using custom data:\r\ntraining fasttext command (skipgram -input data.en -output fasttext.en -lr 0.025 -dim 768 -ws 5 -epoch 10 -minCount 5 -neg 5 -loss ns -bucket 2000000   -minn 3 -maxn 6 -thread 16 -t 1e-4 -lrUpdateRate 100)\r\n\r\npart of my training fairseq command that related to embeddings is (  --encoder-embed-path  fasttext.en.vec ---encoder-embed-dim 768 --decoder-embed-dim 768)\r\n\r\n#### What have you tried?\r\n\r\nAs well i tried solutions such as:\r\n1) decreasing the learning rate.\r\n2) increase the batch size.\r\n3) add these parameters ( --fp16-scale-tolerance, --min-loss-scale, --update-freq).\r\n\r\n\r\n\r\nBut none of them can solve the problem. The only thing that worked was switching to FP32 training, but that is not an option since the training will be slower.\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0.0\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.6.9\r\n - CUDA/cuDNN version:10.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n#### What is your question?\r\nHow to solve this? and what is the proper way to use pre-trained embedding?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3691/comments",
    "author": "NourKhdour",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:45Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:27Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "LoganLiu66",
        "created_at": "2022-08-12T08:54:17Z",
        "body": "Have you found out any solution for this? I'm facing the same issue"
      }
    ]
  },
  {
    "number": 3688,
    "title": "How to generate config.yaml file for inferencing wav2vec_small_960h using fairseq-interactive",
    "created_at": "2021-07-06T10:10:59Z",
    "closed_at": "2022-05-01T16:21:56Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3688",
    "body": "I am trying to run and generate a transcript for a single audio file using wav2vec_small_960h.pt model and fairseq-interactive. I need config.yaml file for running fairseq-interactive command so how to generate this file?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3688/comments",
    "author": "Hemant6174",
    "comments": [
      {
        "user": "ranveerkln",
        "created_at": "2021-07-28T09:14:31Z",
        "body": "Did you get some solution on your own to solve this issue. I am facing the same problem. I want  config.yaml file"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:46Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T16:21:26Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3686,
    "title": "Getting Error : Key 'input_feat_per_channel' is not in struct",
    "created_at": "2021-07-05T10:17:14Z",
    "closed_at": "2022-04-30T11:22:10Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3686",
    "body": "While inferencing wav2vecsmall model using fairseq-interactive, getting this error:\r\nError : Key 'input_feat_per_channel' is not in struct",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3686/comments",
    "author": "Hemant6174",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-30T11:21:40Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3683,
    "title": "AssertionError: Could not infer task type",
    "created_at": "2021-07-03T15:38:05Z",
    "closed_at": "2022-05-01T02:21:55Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3683",
    "body": "I'm training unsupervised wav2vec on Vietnamese language. Everything runs perfect until the GAN. When I run the following command:\r\n\r\n```\r\nPREFIX=w2v_unsup_gan_xp\r\nTASK_DATA=./output_dir/precompute_pca512_cls128_mean_pooled\r\nTEXT_DATA=./output_dir/phones  # path to fairseq-preprocessed GAN data (phones dir)\r\nKENLM_PATH=./output_dir/phones/lm.phones.filtered.04.bin  # KenLM 4-gram phoneme language model (LM data = GAN data here)\r\n\r\nPYTHONPATH=$FAIRSEQ_ROOT PREFIX=$PREFIX fairseq-hydra-train -m --config-dir ./fairseq/examples/wav2vec/unsupervised/config/gan --config-name w2vu task.data=${TASK_DATA} task.text_data=${TEXT_DATA} task.kenlm_path=${KENLM_PATH}\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\n[2021-07-02 15:16:45,388][HYDRA] Launching 1 jobs locally\r\n[2021-07-02 15:16:45,389][HYDRA] \t#0 : task.data=/content/output_dir/precompute_pca512_cls128_mean_pooled task.text_data=/content/output_dir/phones task.kenlm_path=/content/output_dir/phones/lm.phones.filtered.04.bin\r\n[2021-07-02 15:16:49,198][fairseq_cli.train][INFO] - {'common': {'fp16': False, 'fp16_no_flatten_grads': True, 'log_format': 'json', 'log_interval': 100, 'tensorboard_logdir': 'tb', 'reset_logging': False, 'suppress_crashes': False, 'profile': True, 'tpu': False, 'log_file': None, 'seed': 4}, 'checkpoint': {'save_interval': 1000, 'save_interval_updates': 1000, 'no_epoch_checkpoints': True, 'best_checkpoint_metric': 'weighted_lm_ppl', 'save_dir': '.', 'write_checkpoints_asynchronously': False}, 'distributed_training': {'distributed_world_size': 1, 'distributed_init_method': None, 'tpu': False, 'pipeline_model_parallel': False, 'distributed_port': 0, 'distributed_no_spawn': True, 'distributed_rank': 0}, 'task': {'_name': 'unpaired_audio_text', 'data': '/content/output_dir/precompute_pca512_cls128_mean_pooled', 'text_data': '/content/output_dir/phones', 'labels': 'phn', 'sort_by_length': False, 'unfiltered': False, 'max_length': None, 'append_eos': False, 'kenlm_path': '/content/output_dir/phones/lm.phones.filtered.04.bin'}, 'dataset': {'num_workers': 6, 'batch_size': 160, 'skip_invalid_size_inputs_valid_test': True, 'valid_subset': 'valid', 'validate_interval': 1000, 'validate_interval_updates': 1000, 'max_tokens': 1000}, 'criterion': {'_name': 'model', 'loss_weights': {}, 'log_keys': ['accuracy_dense', 'accuracy_token', 'temp', 'code_ppl']}, 'optimization': {'max_update': 150000, 'clip_norm': 5.0, 'lr': [0]}, 'optimizer': {'_name': 'composite', 'groups': {'generator': {'lr': [0.0004], 'lr_float': None, 'optimizer': {'_name': 'adam', 'adam_betas': [0.5, 0.98], 'adam_eps': 1e-06, 'weight_decay': 0, 'amsgrad': False}, 'lr_scheduler': {'_name': 'fixed', 'warmup_updates': 0}}, 'discriminator': {'lr': [0.0005], 'lr_float': None, 'optimizer': {'_name': 'adam', 'adam_betas': [0.5, 0.98], 'adam_eps': 1e-06, 'weight_decay': 0.0001, 'amsgrad': False}, 'lr_scheduler': {'_name': 'fixed', 'warmup_updates': 0}}}}, 'lr_scheduler': {'_name': 'pass_through'}, 'model': {'_name': 'wav2vec_u', 'discriminator_dim': 384, 'discriminator_depth': 2, 'discriminator_kernel': 6, 'discriminator_linear_emb': False, 'discriminator_causal': True, 'discriminator_max_pool': False, 'discriminator_act_after_linear': False, 'discriminator_dropout': 0.0, 'discriminator_weight_norm': False, 'generator_stride': 1, 'generator_kernel': 4, 'generator_bias': False, 'generator_dropout': 0.1, 'smoothness_weight': 0.5, 'smoothing': 0, 'smoothing_one_sided': False, 'gumbel': False, 'hard_gumbel': False, 'gradient_penalty': 1.5, 'code_penalty': 4.0, 'temp': [2, 0.1, 0.99995], 'input_dim': 512, 'segmentation': {'type': 'JOIN', 'mean_pool_join': False, 'remove_zeros': False}}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}\r\nTraceback (most recent call last):\r\n  File \"/content/fairseq/fairseq_cli/hydra_train.py\", line 43, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/content/fairseq/fairseq_cli/train.py\", line 88, in main\r\n    task = tasks.setup_task(cfg.task)\r\n  File \"/content/fairseq/fairseq/tasks/__init__.py\", line 44, in setup_task\r\n    ), f\"Could not infer task type from {cfg}. Available tasks: {TASK_REGISTRY.keys()}\"\r\nAssertionError: Could not infer task type from {'_name': 'unpaired_audio_text', 'data': '/content/output_dir/precompute_pca512_cls128_mean_pooled', 'text_data': '/content/output_dir/phones', 'labels': 'phn', 'sort_by_length': False, 'unfiltered': False, 'max_length': None, 'append_eos': False, 'kenlm_path': '/content/output_dir/phones/lm.phones.filtered.04.bin'}. Available tasks: dict_keys(['translation_multi_simple_epoch', 'audio_pretraining', 'denoising', 'speech_to_text', 'translation', 'hubert_pretraining', 'simul_speech_to_text', 'simul_text_to_text', 'legacy_masked_lm', 'online_backtranslation', 'multilingual_denoising', 'translation_lev', 'multilingual_translation', 'translation_from_pretrained_bart', 'language_modeling', 'cross_lingual_lm', 'masked_lm', 'sentence_ranking', 'sentence_prediction', 'semisupervised_translation', 'translation_from_pretrained_xlm', 'multilingual_masked_lm', 'dummy_lm', 'dummy_masked_lm', 'dummy_mt'])\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n```\r\n\r\nmy wavu.yaml contains the following content:\r\n\r\n```\r\n# @package _group_\r\n\r\ncommon:\r\n  fp16: false\r\n  fp16_no_flatten_grads: true\r\n  log_format: json\r\n  log_interval: 100\r\n  tensorboard_logdir: tb\r\n  reset_logging: false\r\n  suppress_crashes: false\r\n  profile: true\r\n  tpu: false\r\n  log_file: null\r\n  seed: 4\r\n\r\ncheckpoint:\r\n  save_interval: 1000\r\n  save_interval_updates: 1000\r\n  no_epoch_checkpoints: true\r\n  best_checkpoint_metric: weighted_lm_ppl\r\n  save_dir: .\r\n  write_checkpoints_asynchronously: false\r\n\r\ndistributed_training:\r\n  distributed_world_size: 1\r\n  distributed_init_method: null\r\n  tpu: false\r\n  pipeline_model_parallel: false\r\n  distributed_port: 0\r\n  distributed_no_spawn: true\r\n  distributed_rank: 0\r\n\r\ntask:\r\n  _name: unpaired_audio_text\r\n  data: ???\r\n  text_data: ???\r\n  labels: phn\r\n  sort_by_length: false\r\n  unfiltered: false\r\n  max_length: null\r\n  append_eos: false\r\n  kenlm_path: ???\r\n\r\ndataset:\r\n  num_workers: 6\r\n  batch_size: 160\r\n  skip_invalid_size_inputs_valid_test: true\r\n  valid_subset: valid\r\n  validate_interval: 1000\r\n  validate_interval_updates: 1000\r\n  max_tokens: 1000\r\n\r\ncriterion:\r\n  _name: model\r\n  log_keys:\r\n    - accuracy_dense\r\n    - accuracy_token\r\n    - temp\r\n    - code_ppl\r\n\r\noptimization:\r\n  max_update: 150000\r\n  clip_norm: 5.0\r\n  lr: [0]\r\n\r\noptimizer:\r\n  _name: composite\r\n  groups:\r\n    generator:\r\n      lr: [0.0004]\r\n      lr_float: null\r\n      optimizer:\r\n        _name: adam\r\n        adam_betas: [0.5,0.98]\r\n        adam_eps: 1e-06\r\n        weight_decay: 0\r\n        amsgrad: false\r\n      lr_scheduler:\r\n        _name: fixed\r\n        warmup_updates: 0\r\n    discriminator:\r\n      lr: [ 0.0005 ]\r\n      lr_float: null\r\n      optimizer:\r\n        _name: adam\r\n        adam_betas: [0.5,0.98]\r\n        adam_eps: 1e-06\r\n        weight_decay: 0.0001\r\n        amsgrad: false\r\n      lr_scheduler:\r\n        _name: fixed\r\n        warmup_updates: 0\r\n\r\nlr_scheduler: pass_through\r\n\r\nmodel:\r\n  _name: wav2vec_u\r\n\r\n  discriminator_dim: 384\r\n  discriminator_depth: 2\r\n  discriminator_kernel: 6\r\n  discriminator_linear_emb: false\r\n  discriminator_causal: true\r\n  discriminator_max_pool: false\r\n  discriminator_act_after_linear: false\r\n  discriminator_dropout: 0.0\r\n  discriminator_weight_norm: false\r\n\r\n  generator_stride: 1\r\n  generator_kernel: 4\r\n  generator_bias: false\r\n  generator_dropout: 0.1\r\n\r\n  smoothness_weight: 0.5\r\n  smoothing: 0\r\n  smoothing_one_sided: false\r\n  gumbel: false\r\n  hard_gumbel: false\r\n  gradient_penalty: 1.5\r\n  code_penalty: 4.0\r\n  temp: [ 2,0.1,0.99995 ]\r\n  input_dim: 512\r\n\r\n  segmentation:\r\n    type: JOIN\r\n    mean_pool_join: false\r\n    remove_zeros: false\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3683/comments",
    "author": "soroushhashemifar",
    "comments": [
      {
        "user": "marcinkusz",
        "created_at": "2021-07-04T22:24:09Z",
        "body": "I don't know what causes it exactly but it seems to have something to do with 9bee82e4a7b73249a88f2e2d286e991493ef13c2. I temporarily fixed it by temporarily reverting the lines added by the commit, let me know if that works for you too. "
      },
      {
        "user": "soroushhashemifar",
        "created_at": "2021-07-06T11:35:03Z",
        "body": "It didn't solve the issue. It's weird to modify the w2vu.yaml file manually to fix the errors!"
      },
      {
        "user": "mjurkus",
        "created_at": "2021-07-11T09:58:29Z",
        "body": "Facing the same issue. Any workaround for that?"
      },
      {
        "user": "mjurkus",
        "created_at": "2021-07-11T10:15:37Z",
        "body": "Temporarily worked around by moving task and data to `fairseq/data` and `fairseq/tasks` from `wav2ver/unsupervised`."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:25Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3679,
    "title": "wa2vec-u releasing the set of hyperparameters for LibriSpeech and MLS languages?",
    "created_at": "2021-07-03T05:41:55Z",
    "closed_at": "2022-04-30T11:22:12Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3679",
    "body": "#### What is your question?\r\n1. Is it possible to release the yaml files that contains the best hyperparameters for MLS languages (possibly as well as LibriSpeech), both for GAN training and Kaldi word-lm decoding? Running the model for 40 different hyperparameter setting basically takes way too long to replicate the results. The default one provided in the repo with a random seed of 0 seems too far from obtaining the reported results. For example, the Viterbi PER I got just after GAN training using the default hyperparameters provided in the repo's yaml file is 62% and way too high for the language model fusion to achieve near a 58% WER as reported in the paper.\r\n2. What part of unlabeled 100h audio do you use from the MLS?\r\n3. Will distributed training on more than 1 GPUs by changing world_size but at the same time reducing batch_size (so that the total batch size across multiple GPU is still 160) affect the result?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3679/comments",
    "author": "JeromeNi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-30T11:21:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3657,
    "title": "wrong results for wav2vec2 inference",
    "created_at": "2021-06-28T10:17:16Z",
    "closed_at": "2022-05-01T17:22:06Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3657",
    "body": "## ❓ Questions and Help\r\n#### question?\r\nTrained pretrained wav2vec2 base model for english language. Then finetuned for labeled data. After model finetuning used following code to get results \r\n\r\n`python examples/speech_recognition/infer.py fairseq/data --task audio_pretraining --nbest 1 --path checkpoint_best.pt --gen-subset $subset --results-path results_temp --w2l-decoder viterbi --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000 --post-process letter`\r\n\r\nResults: \r\nINFO:__main__:HYPO:T\r\nINFO:__main__:TARGET:ACTOR DEV PATEL SAYS HIS ROLE IN THE FILM SLUMDOG MILLIONAIRE WAS BOTH A\r\nINFO:__main__:___________________\r\nINFO:__main__:HYPO:T\r\nINFO:__main__:TARGET:THE COUNTRYS AFFAIRS AS PART OF A MAJOR SHAKE UP OF THE MILITARY TOP BRASS \r\n\r\nI'm getting only T as output.  I didn't change any code in infer.py\r\n\r\n\r\n###How i trained model??\r\n1. Initially trained  wav2vec2 base model using following command\r\nfairseq-hydra-train \\\r\n    task.data=/path/to/data \\\r\n    --config-dir /path/to/fairseq-py/examples/wav2vec/config/pretraining \\\r\n    --config-name wav2vec2_base_librispeech\r\n\r\nI got checkpoint_pretrained.pt\r\n\r\n2. Data preparation for finetunung\r\n\r\ntrain.tsv\r\n/path/english_wav_files\r\nesp_12.wav\t49280\r\nesp_13.wav\t50000\r\n\r\ntrain.wrd\r\nHELLO SO ANYTHING ELSE NO\r\nANUSHKA SHARMA CONTINUE MAKING\r\n\r\ntrain.ltr \r\nH E L L O | S O | A N Y T H I N G | E L S E | N O |\r\nA N U S H K A | S H A R M A | C O N T I N U E | M A K I N G |\r\n\r\n\r\n3. Now finetuned for labeled data using following command \r\nfairseq-hydra-train \\\r\n    task.data=/path/to/data \\\r\n    model.w2v_path=checkpoint_pretrained.pt\\\r\n    --config-dir /path/to/fairseq-py/examples/wav2vec/config/finetuning \\\r\n    --config-name base_100h\r\n\r\nNow i got checkpoint_best.pt then used this model for inference after installing flashlight bindings. \r\n\r\n#how finetuned model log ? How it converging?  This may help to give inputs to me\r\n[INFO] - {\"epoch\": 1, \"update\": 0.224, \"loss\": \"1695.09\", \"ntokens\": \"7910.48\", \"nsentences\": \"117.02\", \"nll_loss\": \"25.075\", \"wps\": \"1722.8\", \"ups\": \"0.22\", \"wpb\": \"7910.5\", \"bsz\": \"117\", \"num_updates\": \"200\", \"lr\": \"1.0425e-06\", \"gnorm\": \"6068.3\", \"loss_scale\": \"0.125\", \"train_wall\": \"319\", \"gb_free\": \"8.9\", \"wall\": \"970\"}\r\n\r\n[INFO] - {\"epoch\": 29, \"update\": 28.218, \"loss\": \"537.408\", \"ntokens\": \"7965.03\", \"nsentences\": \"115.58\", \"nll_loss\": \"7.798\", \"wps\": \"4463.3\", \"ups\": \"0.56\", \"wpb\": \"7965\", \"bsz\": \"115.6\", \"num_updates\": \"25200\", \"lr\": \"3e-05\", \"gnorm\": \"322.429\", \"loss_scale\": \"0.25\", \"train_wall\": \"304\", \"gb_free\": \"8.9\", \"wall\": \"47018\"}\r\n\r\n\r\ncan you please suggest any solution to overcome this issue.\r\nThank you in advance ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3657/comments",
    "author": "shiva1393",
    "comments": [
      {
        "user": "shiva1393",
        "created_at": "2021-06-28T11:50:34Z",
        "body": "i took model from fairseq/wav2vec_small_100h.pt and decoded it. Decoding part working fine. fairseq/wav2vec_small_100h.pt model asking dict.ltr.txt while decoding but finetuned  above instructed model not asking dict.ltr.txt file. I think finetune model i made mistake. Can any one suggest where i'm doing mistake\r\n\r\nFinetuned model:  160 downsampling and 12 transformers followed by linear layer with 32 letters as output.\r\n"
      },
      {
        "user": "medabalimi",
        "created_at": "2021-06-30T04:11:31Z",
        "body": "I don't see the validation WER and loss scores. How many hours  of data did you train on?"
      },
      {
        "user": "shiva1393",
        "created_at": "2021-06-30T04:25:55Z",
        "body": "Hi,\r\naround 150 hours used for finetuning wav2vec2 model and 450 hours used in pretraining.\r\nIn above  discussion i didn't mentioned some more files used in finetuning.\r\n├──  train.tsv\r\n├──  valid.tsv\r\n├── dev_other.ltr\r\n├── dev_other.tsv\r\n├── dev_other.wrd\r\n├── dict.ltr.txt\r\n├── train.ltr\r\n├── train.tsv\r\n├── train.wrd\r\n\r\nI think model not at all training from below log:\r\n[INFO] - begin validation on \"dev_other\" subset\r\n[2021-06-28 11:49:06,986][dev_other][INFO] - {\"epoch\": 26, \"dev_other_loss\": \"588.494\", \"dev_other_ntokens\": \"1947.75\", \"dev_other_nsentences\": \"25.5254\", \"dev_other_nll_loss\": \"7.712\", \"dev_other_uer\": \"98.696\", \"dev_other_wer\": \"99.868\", \"dev_other_raw_wer\": \"99.868\", \"dev_other_wps\": \"6712.6\", \"dev_other_wpb\": \"1947.8\", \"dev_other_bsz\": \"25.5\", \"dev_other_num_updates\": \"23222\", \"dev_other_best_wer\": \"99.868\"}\r\n\r\nsomewhere i made mistake can you suggest which information need to  share to analyze this error\r\n\r\nIn pretraining used 4 lack steps for model training. Here giving information regarding pretraining. Exactly used same command(from fairseq) without changing any parameter except used 160 downsampling instead of 320.\r\n\r\n[INFO] - {\"epoch\": 1, \"update\": 0.019, \"loss\": \"9.477\", \"ntokens\": \"2934.13\", \"nsentences\": \"9.43\", \"prob_perplexity\": \"350.496\", \"code_perplexity\": \"342.206\", \"temp\": \"1.999\", \"loss_0\": \"6.685\", \"loss_1\": \"0.064\", \"loss_2\": \"2.727\", \"accuracy\": \"0.01225\", \"wps\": \"10494.3\", \"ups\": \"3.57\", \"wpb\": \"2934.1\", \"bsz\": \"9.4\", \"num_updates\": \"200\", \"lr\": \"3.125e-06\", \"gnorm\": \"1.416\", \"loss_scale\": \"128\", \"train_wall\": \"56\", \"gb_free\": \"4.5\", \"wall\": \"58\"}\r\n\r\n{\"epoch\": 39, \"update\": 38.0, \"loss\": \"6.658\", \"ntokens\": \"3009.07\", \"nsentences\": \"9.625\", \"prob_perplexity\": \"639.979\", \"code_perplexity\": \"262.02\", \"temp\": \"0.5\", \"loss_0\": \"6.658\", \"loss_1\": \"0\", \"loss_2\": \"0\", \"accuracy\": \"0.00079\", \"wps\": \"9871.4\", \"ups\": \"3.28\", \"wpb\": \"3009.1\", \"bsz\": \"9.6\", \"num_updates\": \"397200\", \"lr\": \"3.80435e-06\", \"gnorm\": \"0\", \"loss_scale\": \"4096\", \"train_wall\": \"57\", \"gb_free\": \"10.9\", \"wall\": \"113933\"}\r\n"
      },
      {
        "user": "medabalimi",
        "created_at": "2021-06-30T04:49:21Z",
        "body": "You probably should train for more epochs. What's the config file / config parameters used for training? Refer to the Wav2Vec 2 paper and the yaml files."
      },
      {
        "user": "shiva1393",
        "created_at": "2021-06-30T05:17:43Z",
        "body": "Thank you @medabalimi,\r\n\r\n  Used exact file from fairseq wav2vec2_base_librispeech.yaml(used in wav2vec2 paper), used 2 gpus instead of 64 in config file.\r\n\r\nChanges in pretraining : \r\nIn fairseq/models/wav2vec/wav2vec2.py:     conv_feature_layers: str = field(\r\n        default=\"[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)]+[(512,2,2)]\")  removed one stride[(512,2,2)]  to get 160 downsample.\r\n\r\nFor thousand hours of speech fairseq models trained with 4lack updates and for 450 hours  i also did  4 lack updates. I think by increasing number of epochs  it may not converge more. But  I will try for more epochs. Thank you i will update if i won't get any output. Please suggest if you feel any changes. "
      },
      {
        "user": "shiva1393",
        "created_at": "2021-07-01T04:49:09Z",
        "body": "Finally i understood pretrained model is not properly trained. If i re-train some times weights are exploding. By observing issues many people struggling to overcome this issue. \r\n\r\n @alexeib which hyperparameters need to modify to overcome weights exploding problem, In issues some peoples suggested to move fp32 instead of fp16. \r\n\r\nI'm using following command:\r\nfairseq-hydra-train\r\ntask.data=/path/to/data\r\n--config-dir /path/to/fairseq-py/examples/wav2vec/config/pretraining\r\n--config-name wav2vec2_base_librispeech\r\n\r\n  Can you provide some inputs. Can any one trained wav2vec2 pretrained model successfully for own data set?  please provide some inputs.\r\n"
      },
      {
        "user": "alexeib",
        "created_at": "2021-07-01T05:23:31Z",
        "body": "are you training on fewer gpus than what we did in the paper? in that case you need to reduce learning rate. when dealing with exploding fp16 the first thing to do is always to try to lower the learning rate"
      },
      {
        "user": "shiva1393",
        "created_at": "2021-07-01T05:28:54Z",
        "body": "Used  same hyper-parameters in librispeech_base config file, here lr: [0.0005], \r\nI'm using 4 GPUs. \r\nThank you i will reduce learning rate and check the performance."
      },
      {
        "user": "lixx0105",
        "created_at": "2021-08-19T10:09:03Z",
        "body": "@shiva1393 hi, do you have any update for this problem? thanks~"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:30Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:36Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3654,
    "title": "TypeError: build_generator() got an unexpected keyword argument 'prefix_allowed_tokens_fn'",
    "created_at": "2021-06-28T05:22:28Z",
    "closed_at": "2022-04-30T11:21:59Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3654",
    "body": "## ❓ TypeError: build_generator() got an unexpected keyword argument 'prefix_allowed_tokens_fn'\r\n\r\n\r\n#### When attempting to translate using a pre-trained mBART model, the above error occurs. The pre-trained model loads without throwing any error, however when `translate()` function or `sample()` function the above error occurs. \r\n\r\n#### The full error is:\r\n          TypeError  Traceback (most recent call last)\r\n          \r\n          <ipython-input-8-12941c7c49bc> in <module>()\r\n          ----> 1 bart.sample(sentences=sentence_list[0])\r\n          \r\n          3 frames\r\n          \r\n          /content/fairseq/fairseq/hub_utils.py in sample(self, sentences, beam, verbose, **kwargs)\r\n              128     ) -> List[str]:\r\n              129         if isinstance(sentences, str):\r\n          --> 130             return self.sample([sentences], beam=beam, verbose=verbose, **kwargs)[0]\r\n              131         tokenized_sentences = [self.encode(sentence) for sentence in sentences]\r\n              132         batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\r\n          \r\n          /content/fairseq/fairseq/hub_utils.py in sample(self, sentences, beam, verbose, **kwargs)\r\n              130             return self.sample([sentences], beam=beam, verbose=verbose, **kwargs)[0]\r\n              131         tokenized_sentences = [self.encode(sentence) for sentence in sentences]\r\n          --> 132         batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\r\n              133         return [self.decode(hypos[0][\"tokens\"]) for hypos in batched_hypos]\r\n              134 \r\n          \r\n          /content/fairseq/fairseq/models/bart/hub_interface.py in generate(self, tokenized_sentences, inference_step_args, skip_invalid_size_inputs, *args, **kwargs)\r\n              110                 inference_step_args=inference_step_args,\r\n              111                 skip_invalid_size_inputs=skip_invalid_size_inputs,\r\n          --> 112                 **kwargs\r\n              113             )\r\n              114             for id, hypos in zip(batch['id'].tolist(), results):\r\n          \r\n          /content/fairseq/fairseq/hub_utils.py in generate(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, inference_step_args, prefix_allowed_tokens_fn, **kwargs)\r\n              169             self.models,\r\n              170             gen_args,\r\n          --> 171             prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\r\n              172         )\r\n              173 \r\n          \r\n          TypeError: build_generator() got an unexpected keyword argument 'prefix_allowed_tokens_fn'`\r\n\r\n#### Code\r\n\r\nLoading - Does not throw any error\r\n          `bart = BARTModel.from_pretrained( \r\n                  BASEDIR, \r\n                  checkpoint_file='checkpoint_89_75000.pt',           \r\n                  data_name_or_path='pretrained_models/preprocess',   \r\n                  bpe='sentencepiece',\r\n                  sentencepiece_model=f'{BASEDIR}/sentence.bpe.model', \r\n                  source_dict=f'{BASEDIR}/dict.si_LK.txt',     \r\n                  target_dict=f'{BASEDIR}/dict.en_XX.txt'\r\n                  ) `\r\n        `bart.eval()`\r\n\r\nTranslate - This is the function that throws the above error\r\n        `sentence_list = [\"රටක ප්‍රවේශ මාර්ග ජාලයක් ඉදිකිරීම\"]`\r\n        `bart.translate(sentences=sentence_list[0])`\r\n\r\n#### What's your environment?\r\n- Colab pro environment\r\n- Fairseq latest stable release - v0.10.2\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3654/comments",
    "author": "iTharindu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-30T11:21:29Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "nikhiljaiswal",
        "created_at": "2023-09-29T05:45:05Z",
        "body": "hi @iTharindu  did you find solution to this?"
      },
      {
        "user": "yugaljain1999",
        "created_at": "2023-10-27T18:57:58Z",
        "body": "@nikhiljaiswal @iTharindu  Did u find solution to this?"
      }
    ]
  },
  {
    "number": 3647,
    "title": "Integration with PyTorch Lightning ",
    "created_at": "2021-06-25T13:29:33Z",
    "closed_at": "2022-04-30T11:21:56Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3647",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nAre there any tutorials/guides for integrating or using PyTorch Lightning with fairseq?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3647/comments",
    "author": "FallRoll",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:45Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-30T11:21:26Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3635,
    "title": "How can I return nbest result in `GeneratorHubInterface`?",
    "created_at": "2021-06-22T08:35:43Z",
    "closed_at": "2022-04-29T16:22:31Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3635",
    "body": "## ❓ How can I return nbest result in `GeneratorHubInterface`?\r\n\r\nI use `GeneratorHubInterface`'s translate method.  \r\nI used like that:  \r\n```python\r\noutput = self._model.translate(\r\n    text,\r\n    beam=10,\r\n    sampling=sampling,\r\n    temperature=temperature,\r\n    sampling_topk=top_k,\r\n    sampling_topp=top_p,\r\n    max_len_a=1,\r\n    max_len_b=50,\r\n    no_repeat_ngram_size=no_repeat_ngram_size,\r\n    lenpen=len_penalty,\r\n    nbest=10,\r\n)\r\n```\r\n  \r\nBut I got a just one result. How can I get nbest result?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3635/comments",
    "author": "sooftware",
    "comments": [
      {
        "user": "sooftware",
        "created_at": "2021-06-22T13:05:16Z",
        "body": "@myleott Can you help me?"
      },
      {
        "user": "sooftware",
        "created_at": "2021-06-23T07:31:40Z",
        "body": "@alexeib Can you help me?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:22:02Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3632,
    "title": "Sample code for fine-tuning HuBERT with Persian Dataset",
    "created_at": "2021-06-19T06:03:58Z",
    "closed_at": "2022-04-29T16:22:26Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3632",
    "body": "## ❓ Questions and Help\r\nHello, is there any sample code for fine-tuning the HuBERT with Persian audio dataset?\r\nI followed the ReadMe Part but that was not clear for me.\r\n\r\n - OS (e.g., Linux): linux 20.04\r\n - Python version: 3.8",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3632/comments",
    "author": "MelikaBahmanabadi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:29Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:57Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3629,
    "title": "Derived metric not showing up in Tensorboard",
    "created_at": "2021-06-18T13:08:56Z",
    "closed_at": "2022-04-29T16:22:25Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3629",
    "body": "If I add a derived metric, something like\r\n```\r\nmetrics.log_derived(\r\n  \"nll_renorm\",\r\n  lambda meters: meters[\"nll_loss\"].avg - meters[\"nll_unnormalized\"].avg\r\n)\r\n```\r\nit is not showing up in TensorBoard event files.  Other custom metrics added with log_scalar are showing, so they should be available to the metric they are derived from.\r\n\r\nI notice that the existing perplexity metric is derived, and shows up in Tensorboard just fine.\r\n```\r\nmetrics.log_derived(\r\n  \"ppl\", lambda meters: utils.get_perplexity(meters[\"nll_loss\"].avg)\r\n)\r\n```\r\n\r\nI can't seem to find much documentation of this functionality, and grepping through the code doesn't seem to uncover any special treatment of derived metric keys like `\"ppl\"`\r\n\r\nRelatedly, is it also possible to nest derived metrics? (using the value of one derived metric, to compute another) for example with something like:\r\n```\r\nmetrics.log_derived(\r\n  \"nll_renorm\",\r\n  lambda meters: meters[\"nll_loss\"].avg - meters[\"nll_unnormalized\"].fn(meters)\r\n)\r\n```\r\n? (if for example nll_unnormalized is itself derived instead of a scalar)",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3629/comments",
    "author": "compwiztobe",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:56Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3628,
    "title": "Roberta classification head output not invariant to padding",
    "created_at": "2021-06-17T17:52:21Z",
    "closed_at": "2021-06-18T22:13:44Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3628",
    "body": "\r\nI pretrained a Roberta model on my own data.\r\nNow, I've tried registering a new classification head, but its outputs seem to vary with padding. \r\nIs this expected behaviour or did I make a mistake in the pre-training stage?\r\n\r\n#### Code\r\nin this example, `inp[0]` is longer than `inp[1]`, causing `inp[1]` to be padded by `collate`:\r\n```python\r\ncoll = collate_tokens(inp[:2], pad_idx=1)\r\nfeat = self.roberta.extract_features(coll[1])\r\npadded = self.roberta.model.classification_heads['my_head'].forward(feat)\r\nfeat = self.roberta.extract_features(inp[1])\r\nunpadded = self.roberta.model.classification_heads['my_head'].forward(feat)\r\nprint(torch.any(unpadded == padded))\r\n# > tensor(False)\r\n```\r\nThe same happens when using `predict('my_head, x)` from `hub_interface`.\r\n\r\n#### Environnment\r\n\r\n - fairseq 1.0.0\r\n - PyTorch 1.7\r\n - Linux\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3628/comments",
    "author": "mogeid",
    "comments": [
      {
        "user": "knetch",
        "created_at": "2021-06-17T20:56:37Z",
        "body": "Just trying to understand this question myself. But it looks like your tensors are offset by one. Have you tried something like np.isin instead of torch.any?"
      },
      {
        "user": "mogeid",
        "created_at": "2021-06-18T16:34:06Z",
        "body": "> Just trying to understand this question myself. But it looks like your tensors are offset by one. Have you tried something like np.isin instead of torch.any?\r\n\r\nWell, I guess I'm running into the more basic issue that I'm getting different embeddings from the exact same input\r\n```python\r\nfeat1 = self.roberta.extract_features(inp[0])\r\nfeat2 = self.roberta.extract_features(inp[0])\r\nprint(feat1.shape == feat2.shape)\r\n# > True\r\nprint(torch.all(inp[0] == inp[0]))\r\n# > True\r\nprint(torch.any(feat1 == feat2))\r\n# > tensor(False)\r\n```\r\nIs there some resetting I am supposed to be doing?"
      },
      {
        "user": "mogeid",
        "created_at": "2021-06-18T22:13:44Z",
        "body": "nevermind, I forgot to disable dropout."
      }
    ]
  },
  {
    "number": 3622,
    "title": "Train on common voice dataset",
    "created_at": "2021-06-16T05:02:11Z",
    "closed_at": "2022-04-29T16:22:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3622",
    "body": "I wonder if there is any guidance on how to train wav2vec-U on common voice dataset? Thanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3622/comments",
    "author": "soroushhashemifar",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:34Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3621,
    "title": "How to finetune a big model trained with FSDP？",
    "created_at": "2021-06-16T03:34:36Z",
    "closed_at": "2022-04-29T16:22:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3621",
    "body": "I encountered some difficulties when I finetuned a big model pretrained with FSDP. How can I load the fully shared models？",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3621/comments",
    "author": "logicwong",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:35Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3619,
    "title": "torch.hub.load('pytorch/fairseq', . . .)   expected str, bytes or os.PathLike object, not NoneType",
    "created_at": "2021-06-15T03:26:21Z",
    "closed_at": "2022-04-29T16:22:19Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3619",
    "body": " **Could you please help with the following error message:**\r\n\r\n      File \"/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 56, in from_pretrained\r\n          kwargs[\"data\"] = os.path.abspath(os.path.join(model_path, data_name_or_path))\r\n        File \"/usr/local/lib/python3.6/posixpath.py\", line 80, in join\r\n          a = os.fspath(a)\r\n      TypeError: expected str, bytes or os.PathLike object, not NoneType\r\n\r\n**I am running a straightforward example from fairseq:**\r\n\r\n#### English to German translation\r\n\r\n//filename: en_de.py \r\nimport torch\r\n\r\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de', checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt', tokenizer='moses', bpe='fastbpe')\r\nen2de.translate(\"Machine learning is great!\")  # 'Maschinelles Lernen ist großartig!'\r\n\r\n\r\n**TO RECREATE:**\r\n//directory 'my_dir':\r\n    Dockerfile\r\n    requirements.txt\r\n    en_de.py\r\n\r\n//Dockerfile:\r\n      FROM python:3.6\r\n      COPY requirements.txt ./mnt/\r\n      # install required packages\r\n      RUN pip3 install --no-cache-dir -r /mnt/requirements.txt\r\n      CMD [ \"bin/bash\" ]\r\n\r\n//requirements.txt\r\n    soundfile\r\n    torch\r\n    fairseq\r\n    fastBPE\r\n    sacremoses\r\n    requests\r\n\r\n//create docker image, run container, and execute en_de.py example\r\n    docker build -t fairseq .\r\n    docker run -it -v C:\\Users\\.......\\my_dir:/mnt/ --name fairseq_container fairseq bash\r\n    cd mnt\r\n    python en_de.py\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3619/comments",
    "author": "psabela",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3616,
    "title": "(wav2vec 2.0) Inquiry about wav2vec 2.0 decoding parameters",
    "created_at": "2021-06-13T16:10:51Z",
    "closed_at": "2021-06-23T09:04:20Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3616",
    "body": "I am trying to reproduce your fantastic work. And when I am trying to do the evaluation part, I notice that you only provide part of decoding parameters in the paper (wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations). After some experiments, I think it is for base model, right? Could you please also provide the parameters which you use for Large model?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3616/comments",
    "author": "yuanfeng-yl",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2021-06-22T17:05:33Z",
        "body": "to be honest it might be painful to dig it out from the notes. I think using params for the vox model should probably also work for the librispeech model (maybe not exactly same results but close enough) - or you can just run the ax tuning yourself, it takes about a day"
      },
      {
        "user": "yuanfeng-yl",
        "created_at": "2021-06-23T09:04:20Z",
        "body": "I see, thanks for your kind reply. I will close this issue."
      }
    ]
  },
  {
    "number": 3602,
    "title": "Fine Tuning Transformer Model on Seq2Seq Task.",
    "created_at": "2021-06-09T13:21:14Z",
    "closed_at": "2022-04-29T16:22:15Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3602",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### Question?\r\nI have a sequence2sequence task in which input length of sequences are close to 1024 and output lengths are below 512. I am looking for a pre-trained model like BART or any other seq2seq model that I can directly fine tune on my dataset. I have the dataset in form of pandas dataframe as:\r\n\r\n```\r\nInput     Output\r\nISeq1     OSeq1\r\nISeq2     OSeq2\r\n...       ....\r\n```\r\nwhere ISeq(k) & OSeq(k) are the sequences consisting of words. \r\nI am looking for a minimal start code for this type of data.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3602/comments",
    "author": "rajgar114",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:41Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3589,
    "title": "score",
    "created_at": "2021-06-02T13:05:24Z",
    "closed_at": "2022-04-29T16:22:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3589",
    "body": "Hello, I have a question that I have been blocking for several days. I use RoBERTa to classify texts and I would like to display the misclassified texts with their \"score\" for each of my classes. RoBERTa returns an entity tensor per BPE token which are values between -2 and 7 and I would like to know either the maximum and minimum value of my tensor or put the values of my tensors as a probability in order to see more clear. Do you know how I will be able to display this information?\r\nI thank you in advance.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3589/comments",
    "author": "juliette-sch",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3588,
    "title": "how to use many-to-one model setting of \"Deep Transformers with Latent Depth\"? ",
    "created_at": "2021-06-02T04:48:32Z",
    "closed_at": "2022-04-29T16:22:07Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3588",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nhow to use many-to-one model setting of \"Deep Transformers with Latent Depth\"? \r\n\r\nThe script in example (`fairseq/examples/latent_depth/`) is only about one-to-many setting.\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n\r\n#### What have you tried?\r\n\r\nI tried following setting for many-to-one (en,ru2zh) model.\r\n\r\n```\r\nlang_pairs_str=\"ru-zh,en-zh\"\r\ndatabin_dir={databin_dir}\r\n\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train ${databin_dir} \\\r\n  --user-dir examples/latent_depth/latent_depth_src \\\r\n  --lang-pairs \"${lang_pairs_str}\" \\\r\n  --arch latent_multilingual_transformer \\\r\n  --task multilingual_translation_latent_depth \\\r\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n  --share-encoders \\\r\n  --share-decoders \\\r\n  --encoder-langtok \"src\"\\\r\n  --decoder-langtok \\\r\n  --share-decoder-input-output-embed \\\r\n  --dropout 0.3 --attention-dropout 0.3 \\\r\n  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\r\n  --lr-scheduler inverse_sqrt --stop-min-lr 1e-9 --warmup-init-lr 1e-7 --warmup-updates 8000 \\\r\n  --max-tokens 4096 --update-freq 8  \\\r\n  --lr 0.0015 \\\r\n  --clip-norm 1.0 \\\r\n  --seed 42 \\\r\n  --ddp-backend=legacy_ddp \\\r\n  --encoder-layers 24 \\\r\n  --decoder-layers 24 \\\r\n  --decoder-latent-layer \\\r\n  --encoder-latent-layer \\\r\n  --sparsity-weight 0.1 \\\r\n  --anneal-updates 5000 \\\r\n  --soft-update 500  \\\r\n  --target-layers 12 \\\r\n  --share-weight 0.1\r\n```\r\n\r\n```\r\n2021-06-02 13:42:20 | INFO | fairseq_cli.train | Start iterating over samples\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/spawn.py\", line 115, in _main\r\n    self = reduction.pickle.load(from_parent)\r\n_pickle.UnpicklingError: pickle data was truncated\r\nTraceback (most recent call last):\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/resource_sharer.py\", line 142, in _serve\r\n    with self._listener.accept() as conn:\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/connection.py\", line 456, in accept\r\n    answer_challenge(c, self._authkey)\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\r\n    response = connection.recv_bytes(256)        # reject large message\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\nTraceback (most recent call last):\r\n  File \"/home1/irteam/.pyenv/versions/jhpark_3.6.5/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/fairseq_cli/train.py\", line 496, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/fairseq/distributed/utils.py\", line 351, in call_main\r\n    join=True,\r\n  File \"/home1/irteam/.pyenv/versions/jhpark_3.6.5/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/home1/irteam/.pyenv/versions/jhpark_3.6.5/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\r\n    while not context.join():\r\n  File \"/home1/irteam/.pyenv/versions/jhpark_3.6.5/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 150, in join\r\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\ntorch.multiprocessing.spawn.ProcessRaisedException: \r\n\r\n-- Process 2 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home1/irteam/.pyenv/versions/jhpark_3.6.5/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/fairseq_cli/train.py\", line 173, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/fairseq_cli/train.py\", line 284, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home1/irteam/.pyenv/versions/3.6.5/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/fairseq/trainer.py\", line 674, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/examples/latent_depth/latent_depth_src/multilingual_translation_latent_depth.py\", line 119, in train_step\r\n    sample, model, criterion, optimizer, update_num, ignore_grad\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/fairseq/tasks/multilingual_translation.py\", line 365, in train_step\r\n    ignore_grad,\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/examples/latent_depth/latent_depth_src/multilingual_translation_latent_depth.py\", line 102, in _per_lang_pair_train_loss\r\n    sample_size,\r\n  File \"/home1/irteam/.pyenv/versions/jhpark_3.6.5/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home1/irteam/users/parkjh/fairseq_210427/examples/latent_depth/latent_depth_src/loss/latent_depth.py\", line 37, in forward\r\n    kl_loss /= layer_samples[0].size()[0]\r\nAttributeError: 'NoneType' object has no attribute 'size'\r\n```\r\n\r\ndoes anyone know a suitable configuration for many-to-one model?\r\nthanks:)\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1\r\n - PyTorch Version (e.g., 1.0):1.8.1\r\n - OS (e.g., Linux):CentOS\r\n - How you installed fairseq (`pip`, source):pip \r\n - Build command you used (if compiling from source):\r\n - Python version:3.6.7\r\n - CUDA/cuDNN version:10.2\r\n - GPU models and configuration:4 P40\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3588/comments",
    "author": "tmtmaj",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "michelleqyhqyh",
        "created_at": "2023-01-11T07:23:17Z",
        "body": "How did you solve this issure?"
      }
    ]
  },
  {
    "number": 3585,
    "title": "Load Fairseq model with `BARTModel.from_pretrained` using external BPE",
    "created_at": "2021-05-31T17:53:48Z",
    "closed_at": "2022-05-02T10:21:58Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3585",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI've got a BART model that was pretrained with the `fairseq-train` CLI, and that now consists of three files: `model.pt`, `dict.source.txt` and `dict.target.txt`.  I can load the model successfully with `BARTModel.from_pretrained(\"./path/to/model\")`.\r\n\r\n**However,** the two dict files only map numbers to numbers, e.g. the lines in `dict.source.txt` just look like\r\n```\r\n620 22459230\r\n673 22176757\r\n616 21469320\r\n```\r\n\r\nSo, I need some vocabulary/ID mapping from somewhere, and I noticed that the model is elsewhere used with an external BPE vocabulary, provided in a directory that again contains three files: `dict.txt`, `encoder.json`, `vocab.bpe`.\r\n\r\nI assume I somehow have to generate BPE encodings from my input before passing it to the model, using the data provided in the BPE directory. **But how, and using which source?** The `dict.txt` again just maps numbers (and has the same number of lines as the `dict.*.txt` files in the model directory, while the `vocab.bpe` has one item per line, where one item is what looks like subword units, though with some formatting I haven't previously seen. Some lines from the file (which is a few hundred lines shorter that the `dict*txt` files):\r\n\r\n```\r\nĠm on\r\nĠl ast\r\nĠ2 00\r\n1 0\r\nĠst ud\r\nu res\r\n```\r\n\r\nAre the leading `Ġ` just an equivalent of what is elsewhere often `##`? And what do the spaces mean?\r\n\r\n**So, in short, my question is:** How do I enable my model to use the BPE information above, not using the Fairseq CLI, but just the `BARTModel` class?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.10.0\r\n - PyTorch Version (e.g., 1.0) 1.8.1\r\n - OS (e.g., Linux): Ubuntu 20.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3585/comments",
    "author": "jbingel",
    "comments": [
      {
        "user": "jbingel",
        "created_at": "2021-05-31T20:14:38Z",
        "body": "Update: I noticed that when I just load the model as above (`bart = BARTModel.from_pretrained(\"./path/to/model\")`) and then call `bart.bpe.bpe.decoder`, I get a dictionary that maps IDs to subwords. **But where does this mapping come from?** Since it's not coded into the `dict.*.txt` files, is it encoded in the `model.pt`? Or is it derived from some implicit default in fairseq?"
      },
      {
        "user": "pangsg",
        "created_at": "2022-01-04T12:22:15Z",
        "body": "How can I generate the dict.source.txt ? I hava already finetune bart.large on my own dataset, but when I load `bart = BARTModel.from_pretrained(\"./path/to/model\"))`, it shows me that no such file or directory."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T13:21:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:28Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3582,
    "title": "no_repeat_ngram speedup",
    "created_at": "2021-05-31T07:30:15Z",
    "closed_at": "2022-04-29T16:22:06Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3582",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHow to speed up decoding when `no_repeat_ngram_size>0`\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\nI tried profiling model inference with `no_repeat_ngram_size>0`, and found that most of the time(~90%) was in function `_no_repeat_ngram`. I'm wondering is there any way I can speed up this procedure?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3582/comments",
    "author": "YuxianMeng",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3573,
    "title": "wav2vec 2.0 finetuning problem with padding mask",
    "created_at": "2021-05-27T02:19:08Z",
    "closed_at": "2021-06-01T19:25:02Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3573",
    "body": "I have the following error when fine-tuning a pre-trained wav2vec 2.0 model. Anyone with similar problems?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/fairseq/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/fairseq/fairseq_cli/train.py\", line 178, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/fairseq/fairseq_cli/train.py\", line 289, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/fairseq/fairseq/trainer.py\", line 705, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/fairseq/fairseq/tasks/fairseq_task.py\", line 475, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/fairseq/fairseq/criterions/ctc.py\", line 114, in forward\r\n    non_padding_mask = ~net_output[\"padding_mask\"]\r\nTypeError: bad operand type for unary ~: 'NoneType'\r\n```\r\n\r\nDefault settings (below) was used:\r\n```\r\nfairseq-hydra-train \\\r\n    distributed_training.distributed_port=$PORT \\\r\n    task.data=/path/to/data \\\r\n    model.w2v_path=/path/to/model.pt \\\r\n    --config-dir /path/to/fairseq-py/examples/wav2vec/config/finetuning \\\r\n    --config-name base_100h\r\n```\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.8.1\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): `pip install --editable ./`\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: P40 x 8\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3573/comments",
    "author": "joonson",
    "comments": [
      {
        "user": "innarid",
        "created_at": "2021-05-27T10:22:14Z",
        "body": "Same issue"
      },
      {
        "user": "alexeib",
        "created_at": "2021-06-01T19:25:02Z",
        "body": "fixed in e6eddd805ebbc5c17bf5100c2fde6e0dfc946d2c"
      },
      {
        "user": "jinhao2557",
        "created_at": "2021-06-13T16:48:50Z",
        "body": "> I have the following error when fine-tuning a pre-trained wav2vec 2.0 model. Anyone with similar problems?\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n>     fn(i, *args)\r\n>   File \"/fairseq/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n>     main(cfg, **kwargs)\r\n>   File \"/fairseq/fairseq_cli/train.py\", line 178, in main\r\n>     valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n>   File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n>     return func(*args, **kwds)\r\n>   File \"/fairseq/fairseq_cli/train.py\", line 289, in train\r\n>     log_output = trainer.train_step(samples)\r\n>   File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n>     return func(*args, **kwds)\r\n>   File \"/fairseq/fairseq/trainer.py\", line 705, in train_step\r\n>     ignore_grad=is_dummy_batch,\r\n>   File \"/fairseq/fairseq/tasks/fairseq_task.py\", line 475, in train_step\r\n>     loss, sample_size, logging_output = criterion(model, sample)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/fairseq/fairseq/criterions/ctc.py\", line 114, in forward\r\n>     non_padding_mask = ~net_output[\"padding_mask\"]\r\n> TypeError: bad operand type for unary ~: 'NoneType'\r\n> ```\r\n> \r\n> Default settings (below) was used:\r\n> \r\n> ```\r\n> fairseq-hydra-train \\\r\n>     distributed_training.distributed_port=$PORT \\\r\n>     task.data=/path/to/data \\\r\n>     model.w2v_path=/path/to/model.pt \\\r\n>     --config-dir /path/to/fairseq-py/examples/wav2vec/config/finetuning \\\r\n>     --config-name base_100h\r\n> ```\r\n> \r\n> #### What's your environment?\r\n> \r\n>     * fairseq Version (e.g., 1.0 or master): master\r\n> \r\n>     * PyTorch Version (e.g., 1.0) 1.8.1\r\n> \r\n>     * OS (e.g., Linux): linux\r\n> \r\n>     * How you installed fairseq (`pip`, source): pip\r\n> \r\n>     * Build command you used (if compiling from source): `pip install --editable ./`\r\n> \r\n>     * Python version: 3.6.9\r\n> \r\n>     * CUDA/cuDNN version: 11.1\r\n> \r\n>     * GPU models and configuration: P40 x 8\r\n> \r\n>     * Any other relevant information:\r\n\r\n\r\n\r\n> I have the following error when fine-tuning a pre-trained wav2vec 2.0 model. Anyone with similar problems?\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n>     fn(i, *args)\r\n>   File \"/fairseq/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n>     main(cfg, **kwargs)\r\n>   File \"/fairseq/fairseq_cli/train.py\", line 178, in main\r\n>     valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n>   File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n>     return func(*args, **kwds)\r\n>   File \"/fairseq/fairseq_cli/train.py\", line 289, in train\r\n>     log_output = trainer.train_step(samples)\r\n>   File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n>     return func(*args, **kwds)\r\n>   File \"/fairseq/fairseq/trainer.py\", line 705, in train_step\r\n>     ignore_grad=is_dummy_batch,\r\n>   File \"/fairseq/fairseq/tasks/fairseq_task.py\", line 475, in train_step\r\n>     loss, sample_size, logging_output = criterion(model, sample)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/fairseq/fairseq/criterions/ctc.py\", line 114, in forward\r\n>     non_padding_mask = ~net_output[\"padding_mask\"]\r\n> TypeError: bad operand type for unary ~: 'NoneType'\r\n> ```\r\n> \r\n> Default settings (below) was used:\r\n> \r\n> ```\r\n> fairseq-hydra-train \\\r\n>     distributed_training.distributed_port=$PORT \\\r\n>     task.data=/path/to/data \\\r\n>     model.w2v_path=/path/to/model.pt \\\r\n>     --config-dir /path/to/fairseq-py/examples/wav2vec/config/finetuning \\\r\n>     --config-name base_100h\r\n> ```\r\n> \r\n> #### What's your environment?\r\n> \r\n>     * fairseq Version (e.g., 1.0 or master): master\r\n> \r\n>     * PyTorch Version (e.g., 1.0) 1.8.1\r\n> \r\n>     * OS (e.g., Linux): linux\r\n> \r\n>     * How you installed fairseq (`pip`, source): pip\r\n> \r\n>     * Build command you used (if compiling from source): `pip install --editable ./`\r\n> \r\n>     * Python version: 3.6.9\r\n> \r\n>     * CUDA/cuDNN version: 11.1\r\n> \r\n>     * GPU models and configuration: P40 x 8\r\n> \r\n>     * Any other relevant information:\r\n\r\nHave you solved this problem? Please help me. Thanks."
      }
    ]
  },
  {
    "number": 3570,
    "title": "Call model.encoder in Criterion function cause runtime error",
    "created_at": "2021-05-26T12:49:10Z",
    "closed_at": "2022-04-28T22:22:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3570",
    "body": "I want to use encoder out of transformer to compute loss, but i want to keep a standrad transformer model. \r\n\r\nWhen i try to do like this:\r\n```python\r\nclass myloss(FairseqCriterion):\r\n    def forward(self, model, sample, reduce=True):\r\n        ...\r\n        logits = model.encoder(**sample['net_input'])['encoder_out']\r\n        ...\r\n```\r\nIt doesn't work.\r\nIt cause error during gradient consistency verification in trainer.py `TypeError: must be real number, not NoneType`\r\n\r\nbut when i try to do like this:\r\n```python\r\nclass mymodel(TransformerModel):\r\n    def forward(self, src_tokens,  **kwargs):\r\n        all_features = self.encoder(src_tokens, **kwargs)\r\n        return all_features['encoder_out']\r\n\r\nclass myloss(FairseqCriterion):\r\n    def forward(self, model, sample, reduce=True):\r\n        ...\r\n        logits = model(**sample['net_input'])\r\n        ...\r\n```\r\nIt work well!\r\n\r\nWhy didn't the former work?\r\nIs there any mechanism that causes this?\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3570/comments",
    "author": "nomadlx",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3565,
    "title": "How to use porcess group in optim.shard_",
    "created_at": "2021-05-24T05:36:58Z",
    "closed_at": "2022-04-28T22:22:15Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3565",
    "body": "## environment\r\n - fairseq Version: 1.0.0a0+e7bf13b\r\n - PyTorch Version: 1.7.1\r\n - OS (e.g., Linux): CentOs\r\n - How you installed fairseq : source\r\n - Build command you used:  pip install -e .\r\n - CUDA/cuDNN version: cu101 cuDNN 7.6\r\n - configuration: \r\n \r\n ```\r\nTOKENS_PER_SAMPLE=1024\r\nfairseq-train --memory-efficient-fp16 $DATA_DIR \\\r\n  --zero-sharding os --distributed-world-size $2 --distributed-port 12343 \\\r\n    --model-parallel-size 1 \\\r\n    --task lm_gpt2 --criterion clm \\\r\n    --arch transformer_lm_gpt2_big --tokens-per-sample $TOKENS_PER_SAMPLE \\\r\n    --tensorboard-logdir log\\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-06 --clip-norm 1.0 \\\r\n    --lr-scheduler cosine --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES \\\r\n    --weight-decay 0.1 \\\r\n    --batch-size $BATCH_SIZE --update-freq $UPDATE_FREQ \\\r\n    --max-update $TOTAL_UPDATES\r\n``` \r\n\r\n## question\r\nThe training speed becomes much slower as the GPU increases e.g(1595.77 token per second per gpu for world_size=32, 1281.029 token per second per gpu for world_size=104). \r\nTo speed up, I try to use zero stage1 (`optim.shard_`) with process group like this:\r\n\r\n```\r\ndef get_global_group_shard():\r\n    if not hasattr(get_global_group_shard, \"_global_group_shard\"):\r\n        rank = torch.distributed.get_rank()\r\n        sz = 8\r\n        gid = rank // sz\r\n        ranks = [i for i in range(gid * sz, (gid + 1) * sz)]\r\n        get_global_group._global_group_shard = dist.new_group(ranks=ranks)\r\n    return get_global_group._global_group_shard \r\n```\r\n\r\nHowever, I got the following error\r\n\r\n```\r\n File \"~/.conda/envs/torch1.7.1/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 864, in broadcast                                                                                                                                                     work = group.broadcast([tensor], opts)                                                                                                                                                                                                                                                    RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\r\n```\r\n\r\nCan I use and how to use porcess group in `optim.shard_` ?\r\n\r\nBy the way, how to speed up the model training when using hundreds of GPUs ？\r\n\r\nThank you  very much.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3565/comments",
    "author": "philokey",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:28Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3554,
    "title": "AttributeError: 'MaskedLMModel' object has no attribute 'register_classification_head'",
    "created_at": "2021-05-17T04:27:37Z",
    "closed_at": "2022-04-18T02:21:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3554",
    "body": "## ❓ Questions and Help\r\nHello,\r\nWhen trying to fine-tune my own pretrained BERT_base model, I come across the issue of \"AttributeError: 'MaskedLMModel' object has no attribute 'register_classification_head'\". \r\n\r\nAny help would be appreciated, thanks.\r\n\r\n\r\n - fairseq Version (e.g., 1.0 or master): \r\n - PyTorch Version (e.g., 1.0) 1.8.1+cu102\r\n - OS (e.g., Linux): Linux 4.15.0-143-generic x86_64\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: V10.0.130\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3554/comments",
    "author": "nuthatch827",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:47:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:40Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3551,
    "title": "roberta fine-tuning parameters",
    "created_at": "2021-05-14T12:21:05Z",
    "closed_at": "2022-04-18T02:21:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3551",
    "body": "I used this command to fine-tune roberta-base with MNLI train set\r\nbut I can't get the same accuracy in the paper (my accuracy was 65%)\r\nI don't know why.  maybe this command is for trainning roberta-large\r\nhow can I make the command for roberta-base with MNLI\r\n\r\nTOTAL_NUM_UPDATES=123873  \r\nWARMUP_UPDATES=7432      # 6 percent of the number of updates\r\nLR=1e-5                \r\nNUM_CLASSES=3\r\nMAX_SENTENCES=32       # Batch size.\r\nROBERTA_PATH=/path/to/roberta.base/model.pt\r\n\r\nCUDA_VISIBLE_DEVICES=0 fairseq-train MNLI-bin/ \\\r\n    --restore-file $ROBERTA_PATH \\\r\n    --max-positions 512 \\\r\n    --batch-size $MAX_SENTENCES \\\r\n    --max-tokens 4400 \\\r\n    --task sentence_prediction \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --init-token 0 --separator-token 2 \\\r\n    --arch roberta_base \\\r\n    --criterion sentence_prediction \\\r\n    --num-classes $NUM_CLASSES \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\r\n    --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\r\n    --max-epoch 10 \\\r\n    --find-unused-parameters \\\r\n    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric;\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3551/comments",
    "author": "yuyi5187",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:47:41Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3549,
    "title": "How to change hop length and number of fft-point  for training wav2vec",
    "created_at": "2021-05-14T03:34:16Z",
    "closed_at": "2022-05-01T17:22:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3549",
    "body": "In wav2vec, I could not find out the config for adaptation about audio feature like hop-length , window-length and n-fft. \r\nCould you tell me how to change these parameter？\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3549/comments",
    "author": "yixiangchen1995",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:47:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "turian",
        "created_at": "2021-08-26T14:43:31Z",
        "body": "I am also curious what the default hop size is."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:46Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3544,
    "title": "Question about quant-noise example",
    "created_at": "2021-05-10T09:40:39Z",
    "closed_at": "2022-04-30T11:22:13Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3544",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI trained Transformer language model with quant-noise and iterative product quantization.\r\n\r\nBut the inference time is not reduced.\r\n\r\nWhy it does? is there any API for quantized model that i didn't know?\r\n\r\n#### Code\r\n\r\n#### What have you tried?\r\n\r\nI trained quantized model following way\r\n\r\n```bash\r\nFAIRSEQ=$1\r\nMODEL_DST=$2\r\n\r\nmkdir -p \"$MODEL_DST/adaptive_transformer_charlm_quant_v2\"\r\n\r\npython3 $FAIRSEQ/train.py \"$MODEL_DST/decoder/fairseq_charlm_data\" \\\r\n    --save-dir \"$MODEL_DST/adaptive_transformer_charlm_quant_v2\" \\\r\n    --restore-file \"$MODEL_DST/adaptive_transformer_charlm/checkpoint_last.pt\" \\\r\n    --reset-lr-scheduler --reset-optimizer --reset-meters --reset-dataloader \\\r\n    --task language_modeling_with_generation \\\r\n    --arch transformer_lm \\\r\n    --optimizer=nag \\\r\n    --lr=0.0000004 \\\r\n    --lr-scheduler=fixed \\\r\n    --fp16 \\\r\n    --optimizer adam \\\r\n    --weight-decay 1e-07 \\\r\n    --clip-norm 0.1 \\\r\n    --criterion=candidate_penalty_cross_entropy --rank-alpha 1.0 \\\r\n    --batch-size 64 \\\r\n    --update-freq 1 \\\r\n    --tokens-per-sample 256 \\\r\n    --seed 42 \\\r\n    --sample-break-mode none \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --shorten-method=truncate \\\r\n    --log-format=json --log-interval=100 \\\r\n    --save-interval-updates=50000 --keep-interval-updates=10 \\\r\n    --quant-noise-pq 0.05 --quant-noise-pq-block-size 8 \\\r\n    --max-epoch 12 \\\r\n    --quantization-config-path examples/quant_noise/transformer_quantization_config.yaml \\\r\n    --ddp-backend=no_c10d --distributed-world-size=2 > $MODEL_DST/adaptive_transformer_charlm_quant_v2/train.quant_noise.log\r\n```\r\n\r\nquantization_config file\r\n```yaml\r\n\r\n# Number of Centroids for Product Quantization, by default 256 (byte-aligned)\r\nn_centroids:\r\n    Linear:\r\n        key: in_features\r\n        value: {\"*\": 256}\r\n    Embedding:\r\n        key: embedding_dim\r\n        value: {\"*\": 256}\r\n\r\n# Block Sizes for Product Quantization\r\n# We suggest: 8 for FFN, 4 for ATTN, 4 for embedding projections, 8 for embeddings\r\nblock_sizes:\r\n  Linear:\r\n      key: fuzzy_name\r\n      value: {fc: 8, attn: 4, emb: 4}\r\n  Embedding:\r\n      key: fuzzy_name\r\n      value: {emb: 8}\r\n\r\n# Layers to Quantize Sequentially\r\n# We suggest: first FFN, then EMB, then ATTN\r\nlayers_to_quantize:\r\n    - decoder\\.layers\\.\\d+\\.fc[12]\r\n    - decoder\\.embed_tokens\\.embeddings\\.[012]\\.[01]\r\n    - decoder\\.layers\\.\\d+\\.self_attn\\.(k_proj|v_proj|q_proj|out_proj)\r\n```\r\n\r\nThen, the size of model is reduced 201MB -> 48MB\r\n\r\nbut the inference time is not reduced.\r\n\r\nI tried the inference by the way.\r\n\r\n```python\r\nlm_step_outputs, state.lm_states = self.lm_model.extract_features(\r\n    state.lm_token.to(self.device),\r\n    incremental_state=state.lm_states\r\n)\r\nlm_step_outputs = self.lm_model.output_layer(lm_step_outputs)\r\nlm_step_outputs = lm_step_outputs.squeeze(1).squeeze(0).log_softmax(dim=-1)\r\n```\r\n(self.lm_model : `TransformerDecoder` object in fairseq) \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0\r\n - PyTorch Version (e.g., 1.0) : 1.6.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: \r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3544/comments",
    "author": "DongChanS",
    "comments": [
      {
        "user": "Shamdan17",
        "created_at": "2021-07-06T09:15:59Z",
        "body": "If I am not mistaken, iterative product quantization only decreases model size (by sharing \"blocks\" of parameters within weight matrices. As a result, it does not decrease the number of parameters in a weight matrix, only the size (by only needing to store 1 copy if a block is shared). Hence, I do not think that it has any effect on inference time. In fact, it will probably increase the inference time in overhead. To decrease inference time, you should probably look into either scalar quantization (eg. FP32->INT8) which should increase inference speed or structured pruning."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:29Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-30T11:21:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3534,
    "title": "Where can I find pretrained embeddings?",
    "created_at": "2021-05-05T13:48:53Z",
    "closed_at": "2021-08-24T17:32:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3534",
    "body": "Hello, I see some of the models for mt task, such as fconv, can be supplied with pretrained embeddings given argument ```--encoder-embed-path``` or ```--decoder-embed-path```. Docs do not specify where pretrained embeddings files can be downloaded from. I am interested in embeddings for a iwslt14 with BPE for research purposes. Could you help me with it?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3534/comments",
    "author": "grypesc",
    "comments": [
      {
        "user": "zxk19981227",
        "created_at": "2021-05-14T05:30:47Z",
        "body": "This is easy for that if you could load a pretrained or simply create model with named_parameters function, you could change and get all parameters in the model"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:47:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3532,
    "title": "Translation with FSDP gets stuck in validation step",
    "created_at": "2021-05-05T11:24:12Z",
    "closed_at": "2022-04-18T02:21:01Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3532",
    "body": "#### What is your question?\r\n\r\nHow do I run training&validation of a translation model with FSDP?\r\n\r\n#### Code\r\n```\r\nOMP_NUM_THREADS=20 CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\n    fairseq-train data-bin/iwslt14.tokenized.de-en \\\r\n    --ddp-backend fully_sharded --fp16 --fp16-init-scale 4 \\\r\n    --cpu-offload --checkpoint-activations \\\r\n    --task translation --max-tokens 4096 \\\r\n    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\r\n    --optimizer cpu_adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --log-format json --log-interval 1 \\\r\n    --save-interval-updates 5 \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-detok moses \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\r\n```\r\n#### What have you tried?\r\nThe example with the language model is running fine. Then I tried to merge the LM FSDP example with the ordinary example for translation. The above script runs training fine (5 steps), but afterwards it gets stuck in the validation part after the first example.\r\nGPUs and CPUs are continuing working.\r\n\r\nOutput:\r\n```\r\n2021-05-05 13:07:40 | INFO | fairseq_cli.train | task: TranslationTask\r\n2021-05-05 13:07:40 | INFO | fairseq_cli.train | model: FullyShardedDataParallel\r\n2021-05-05 13:07:40 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\r\n2021-05-05 13:07:40 | INFO | fairseq_cli.train | num. shared model params: 4,933,632 (num. trained: 4,933,632)\r\n2021-05-05 13:07:40 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\r\n2021-05-05 13:07:40 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de\r\n2021-05-05 13:07:40 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en\r\n2021-05-05 13:07:40 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples\r\n2021-05-05 13:07:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************\r\n2021-05-05 13:07:40 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \r\n2021-05-05 13:07:40 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \r\n2021-05-05 13:07:40 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \r\n2021-05-05 13:07:40 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \r\n2021-05-05 13:07:40 | INFO | fairseq.utils | rank   4: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \r\n2021-05-05 13:07:40 | INFO | fairseq.utils | rank   5: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \r\n2021-05-05 13:07:40 | INFO | fairseq.utils | rank   6: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \r\n2021-05-05 13:07:40 | INFO | fairseq.utils | rank   7: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \r\n2021-05-05 13:07:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************\r\n2021-05-05 13:07:40 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\r\n2021-05-05 13:07:40 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\r\n2021-05-05 13:07:40 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last-shard0.pt\r\n2021-05-05 13:07:40 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last-shard0.pt\r\n2021-05-05 13:07:40 | INFO | fairseq.trainer | loading train data for epoch 1\r\n2021-05-05 13:07:40 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de\r\n2021-05-05 13:07:40 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en\r\n2021-05-05 13:07:40 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples\r\nUsing /secondary/thies/.cache/torch_extensions as PyTorch extensions root...\r\nUsing /secondary/thies/.cache/torch_extensions as PyTorch extensions root...\r\nUsing /secondary/thies/.cache/torch_extensions as PyTorch extensions root...\r\nUsing /secondary/thies/.cache/torch_extensions as PyTorch extensions root...\r\nUsing /secondary/thies/.cache/torch_extensions as PyTorch extensions root...\r\nUsing /secondary/thies/.cache/torch_extensions as PyTorch extensions root...\r\nUsing /secondary/thies/.cache/torch_extensions as PyTorch extensions root...\r\nUsing /secondary/thies/.cache/torch_extensions as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /secondary/thies/.cache/torch_extensions/cpu_adam/build.ninja...\r\nBuilding extension module cpu_adam...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nninja: no work to do.\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 0.7473657131195068 seconds\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 0.7493686676025391 seconds\r\nLoading extension module cpu_adam...\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 0.8453831672668457 seconds\r\nTime to load cpu_adam op: 0.7807090282440186 seconds\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 0.7946493625640869 seconds\r\nLoading extension module cpu_adam...\r\nLoading extension module cpu_adam...\r\nLoading extension module cpu_adam...\r\nTime to load cpu_adam op: 0.7924172878265381 seconds\r\nTime to load cpu_adam op: 0.8085505962371826 seconds\r\nTime to load cpu_adam op: 0.7884249687194824 seconds\r\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\r\nConfig: alpha=0.000500, betas=(0.900000, 0.980000), weight_decay=0.000100, adam_w=1\r\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\r\nConfig: alpha=0.000500, betas=(0.900000, 0.980000), weight_decay=0.000100, adam_w=1\r\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\r\nConfig: alpha=0.000500, betas=(0.900000, 0.980000), weight_decay=0.000100, adam_w=1\r\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\r\nConfig: alpha=0.000500, betas=(0.900000, 0.980000), weight_decay=0.000100, adam_w=1\r\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\r\nConfig: alpha=0.000500, betas=(0.900000, 0.980000), weight_decay=0.000100, adam_w=1\r\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\r\nConfig: alpha=0.000500, betas=(0.900000, 0.980000), weight_decay=0.000100, adam_w=1\r\n2021-05-05 13:07:43 | INFO | fairseq.trainer | begin training epoch 1\r\n2021-05-05 13:07:43 | INFO | fairseq_cli.train | Start iterating over samples\r\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\r\nConfig: alpha=0.000500, betas=(0.900000, 0.980000), weight_decay=0.000100, adam_w=1\r\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\r\nConfig: alpha=0.000500, betas=(0.900000, 0.980000), weight_decay=0.000100, adam_w=1\r\n2021-05-05 13:07:47 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.007, \"loss\": \"13.498\", \"nll_loss\": \"13.506\", \"ppl\": \"11636.6\", \"wps\": \"0\", \"ups\": \"0\", \"wpb\": \"28516\", \"bsz\": \"1272\", \"num_updates\": \"1\", \"lr\": \"1.25e-07\", \"gnorm\": \"5.345\", \"loss_scale\": \"4\", \"train_wall\": \"2\", \"gb_free\": \"14.4\", \"wall\": \"7\"}\r\n2021-05-05 13:07:48 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.014, \"loss\": \"13.472\", \"nll_loss\": \"13.477\", \"ppl\": \"11405.9\", \"wps\": \"31140.7\", \"ups\": \"1.06\", \"wpb\": \"29497\", \"bsz\": \"1080\", \"num_updates\": \"2\", \"lr\": \"2.5e-07\", \"gnorm\": \"5.429\", \"loss_scale\": \"4\", \"train_wall\": \"1\", \"gb_free\": \"14.3\", \"wall\": \"8\"}\r\n2021-05-05 13:07:49 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.022, \"loss\": \"13.474\", \"nll_loss\": \"13.48\", \"ppl\": \"11426.5\", \"wps\": \"28879\", \"ups\": \"1\", \"wpb\": \"28881\", \"bsz\": \"1056\", \"num_updates\": \"3\", \"lr\": \"3.75e-07\", \"gnorm\": \"5.46\", \"loss_scale\": \"4\", \"train_wall\": \"1\", \"gb_free\": \"14.3\", \"wall\": \"9\"}\r\n2021-05-05 13:07:50 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.029, \"loss\": \"13.458\", \"nll_loss\": \"13.462\", \"ppl\": \"11286.3\", \"wps\": \"23210.6\", \"ups\": \"0.81\", \"wpb\": \"28624\", \"bsz\": \"824\", \"num_updates\": \"4\", \"lr\": \"5e-07\", \"gnorm\": \"5.482\", \"loss_scale\": \"4\", \"train_wall\": \"1\", \"gb_free\": \"14.3\", \"wall\": \"10\"}\r\n2021-05-05 13:07:51 | INFO | train_inner | {\"epoch\": 1, \"update\": 0.036, \"loss\": \"13.469\", \"nll_loss\": \"13.475\", \"ppl\": \"11384.5\", \"wps\": \"31104.4\", \"ups\": \"1.06\", \"wpb\": \"29284\", \"bsz\": \"960\", \"num_updates\": \"5\", \"lr\": \"6.25e-07\", \"gnorm\": \"5.432\", \"loss_scale\": \"4\", \"train_wall\": \"1\", \"gb_free\": \"14.3\", \"wall\": \"11\"}\r\n2021-05-05 13:07:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2021-05-05 13:07:55 | INFO | fairseq.tasks.translation | example hypothesis: senses senses senses texas texas texas texas texas texas texas texas texas texas texas texas texas texas texas\r\n2021-05-05 13:07:55 | INFO | fairseq.tasks.translation | example reference: they're just not moving.\r\n```\r\n(not continues, waited for ~20 min)\r\n#### What's your environment?\r\n\r\n - fairseq Version :1.0.0a0+a4e1d4a (master from 2021-05-02 )\r\n - fairscale 0.3.6\r\n - deepspeed 0.3.16\r\n - PyTorch Version 1.8.1\r\n - OS Linux:\r\n - How you installed fairseq `pip`:\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.5\r\n - CUDA/cuDNN version: 11.1.105\r\n - GPU models and configuration: 8x T4\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3532/comments",
    "author": "thies1006",
    "comments": [
      {
        "user": "thies1006",
        "created_at": "2021-05-05T11:41:32Z",
        "body": "Apparently the script works when removing  `--checkpoint-actications`."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:48:00Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:33Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3529,
    "title": "Does UPDATE_FREQ (gradient accumulation) affect the TOTAL_NUM_UPDATES?",
    "created_at": "2021-05-04T12:29:15Z",
    "closed_at": "2022-04-18T01:21:28Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3529",
    "body": "If we set TOTAL_NUM_UPDATES=10 we would have 10 iterations and 10 weight update. What if we have TOTAL_NUM_UPDATES=10, and UPDATE_FREQ=2 (gradient accumulation=2)! Do we have still 10 updates (or 5)? (assume MAX_TOKENS is fixed)\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3529/comments",
    "author": "ASoleimaniB",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:48:05Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:58Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3527,
    "title": "xlm-roberta vocabulary",
    "created_at": "2021-05-02T19:24:54Z",
    "closed_at": "2022-04-18T02:20:58Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3527",
    "body": "Hello. I need to preprocess xlm-roberta vocabulary file. But how can I read it properly or remake .model into .txt? Help me please!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3527/comments",
    "author": "Syavaprd",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-05-03T13:50:05Z",
        "body": "What have you tried so far?  Please follow the issue templates"
      },
      {
        "user": "Syavaprd",
        "created_at": "2021-05-05T03:18:43Z",
        "body": "I mean where can I get vocab.txt for xlm-roberta. I only found the .model file.\r\nI need to run fairseq-preprocess with it\r\n\r\n> What have you tried so far? Please follow the issue templates\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:48:03Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:29Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3525,
    "title": "Check probability for certain outputs",
    "created_at": "2021-04-30T08:47:28Z",
    "closed_at": "2022-04-18T01:21:29Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3525",
    "body": "I need to test the probabilities that the model would produce certain outputs. Let me give you an example:\r\nI have a source sentence X, and several possible target sentences Y1, Y2, Y3, Y4 , ... \r\n\r\nI want to know if I can compute the probability that the model would give to each of the translation Y, given X. \r\nIs there a function to compute these values?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3525/comments",
    "author": "fferlito",
    "comments": [
      {
        "user": "uralik",
        "created_at": "2021-05-04T15:50:51Z",
        "body": "is your data iterator augmented with the sequences Y1 Y2 etc? If yes, then you may extend e.g. the valid step function in your `--user-dir` fairseq module. Essentially the model outputs for given input are produced like this:\r\n`target_net_output = model(**sample['net_input'])`\r\n\r\nSo you may substitute the target input part of the sample with whatever sequence you need and get desired scores:\r\n\r\n```\r\nbeam_hyps_target = data_utils.collate_tokens(beam_hyps, pad_idx=self.tgt_dict.pad(), move_eos_to_beginning=False)\r\nbeam_hyps_input = data_utils.collate_tokens(beam_hyps, pad_idx=self.tgt_dict.pad(), move_eos_to_beginning=True)\r\n\r\ngenerated_net_output = model(sample['net_input']['src_tokens'], sample['net_input']['src_lengths'], beam_hyps_input)\r\n```\r\n\r\nThen you gather specific target token scores (`beam_hyps_target `) from `generated_net_output`. This may be not the best way to do this though."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:48:04Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:59Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3523,
    "title": "How to disable shuffle  in wav2vec during inference",
    "created_at": "2021-04-29T12:08:32Z",
    "closed_at": "2021-05-03T13:49:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3523",
    "body": "Hi,\r\n   I just modified the code to work on batch inference. Now it able to do batch inference but I am getting shuffled predicted output. I tried to turn off all shuffle flags. but till the shuffled result. please help me out\r\n\r\nThanks, ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3523/comments",
    "author": "vigneshgig",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-04-29T14:55:20Z",
        "body": "Can you provide more context?  Please follow the issue templates.  Can you share a branch/fork?  What is the command that you ran."
      },
      {
        "user": "lematt1991",
        "created_at": "2021-05-03T13:49:49Z",
        "body": "Closing due to inactivity."
      },
      {
        "user": "ThuPro27",
        "created_at": "2021-06-17T04:39:39Z",
        "body": " The order of your predicted output is based on the size of each audio file. You can, for example, fake the same value of frame for all files in order to get the same order of predicted output as the order of input."
      }
    ]
  },
  {
    "number": 3520,
    "title": "Wav2Vec 2.0 checkpints",
    "created_at": "2021-04-29T10:09:11Z",
    "closed_at": "2022-04-18T01:21:23Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3520",
    "body": "What is the difference between current Wav2Vec 2.0 checkpoint called \"wav2vec_vox_new\" and the one that used to be before  - \"wav2vec_vox\". Thanks in advance for any clarifications.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3520/comments",
    "author": "Shyrm",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2021-05-01T19:25:50Z",
        "body": "the new one is trained for 1m updates and uses a different normalization order in the transformer to avoid instability (and this leads to using higher LR)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:48:13Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:53Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3509,
    "title": "Fighting deadlocks",
    "created_at": "2021-04-26T17:17:24Z",
    "closed_at": "2021-05-19T08:16:12Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3509",
    "body": "## ❓ Questions and Help\r\n\r\nI wrote a lot of code on the top of Fairseq repository (mostly new models, new evaluation script, some changes for model training in Trainer class), but it should not affect multiprocessing/multithreading or related stuff. Each launch after about 1.5 to 2.5 epochs (in 7-GPU setup) I am experiencing deadlock. During this deadlock, all 7 GPUs are at 100% load.\r\n\r\nThis deadlock was looking like a problem in the wrong calls to torch.distributed, but after I patched most torch.distributed methods to log their calls to separate files, I found that this is not the case (but I might forget about some methods).\r\n\r\nThe question: what should I use to find the actual line of code which causes deadlock? As I understand, Fairseq uses spawn method to create GPU workers, and calling pdb, for example, will not help (It will show ^C for main thread), not for workers.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3509/comments",
    "author": "zixeljupo",
    "comments": [
      {
        "user": "villmow",
        "created_at": "2021-04-30T08:40:53Z",
        "body": "Had the same symptoms half a year ago, once one GPU received an OOM the others started having a 100% load."
      },
      {
        "user": "zixeljupo",
        "created_at": "2021-05-01T10:08:48Z",
        "body": "> Had the same symptoms half a year ago, once one GPU received an OOM the others started having a 100% load.\r\n\r\nUsually, when one worker OOMs or dies with another exception, this frees GPU for this worker, while others hang on distributed communication, which shows 100% load for other GPUs. However, in my case, all 7 (out of 7 available) GPUs are at 100% load. Did you have 100% load for all selected GPUs, or for all except one?"
      },
      {
        "user": "zixeljupo",
        "created_at": "2021-05-19T08:16:12Z",
        "body": "Ok, this was a bug on my side. The problem was with custom gradient synchronization and with the fact that sometimes some of the gradients were None."
      }
    ]
  },
  {
    "number": 3508,
    "title": "[fairseq wav2vec2.0] Can't predict with my own finetuned model",
    "created_at": "2021-04-26T13:30:18Z",
    "closed_at": "2022-04-28T22:22:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3508",
    "body": "## ❓ Questions and Help\r\n\r\nI fintuned the base model 1h on my own data without language model. Now that I have my checkpoints_best.pt, I don't know how to make prediction with. \r\n\r\nI used the following code to load my model:\r\n```\r\nimport fairseq\r\nimport torch\r\nimport soundfile as sf\r\n\r\ncp = '/path/to/best_checkpoint.pt'\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp])\r\nmodel = model[0]\r\n```\r\nThen:\r\n```\r\naudio_input, _  = sf.read(fileName) \r\nz = model.feature_extractor(audio_input)\r\nc = model.feature_aggregator(z)\r\n```\r\nI got the following error `AttributeError: 'Wav2VecCtc' object has no attribute 'feature_extractor'`. \r\n\r\nI understand that `feature_extractor` method no longer exists for Wav2vec2.0 but I don't know how can I run inference ?\r\n\r\n\r\nI tested `predict = model.forward(audio_input)` and `predict = model(audio_input)` but get the following error `TypeError: forward() takes 1 positional argument but 2 were given`.\r\n\r\nCan you please help me ?\r\n\r\n - fairseq Version : master\r\n - PyTorch Version 1.8.1\r\n - OS : Ubuntu 18.04\r\n - How you installed fairseq : pip install -e .\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.1\r\n ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3508/comments",
    "author": "kamil-bentounes",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3501,
    "title": "Unable to train/eval a ASR model on LibriSpeech",
    "created_at": "2021-04-23T08:00:19Z",
    "closed_at": "2022-04-18T01:21:22Z",
    "labels": [
      "question",
      "stale",
      "speech"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3501",
    "body": "## ❓ Questions and Help\r\n\r\nWhen I follow fairseq/examples/speech_to_text/docs/librispeech_example.md to train a ASR model on LibriSpeech, the following error occurred:\r\n \r\n`File \"/fairseq/models/speech_to_text/s2t_transformer.py\", line 337, in forward\r\n    if self.num_updates < self.encoder_freezing_updates:\r\nTypeError: '<' not supported between instances of 'int' and 'NoneType'`\r\n\r\n1. I find a similar issue: #3457 , and following his suggestion,  I checked fairseq/models/fairseq_model.py, but this file had been fixed in lastest commit, it still shows the same error message.\r\n\r\n2. Following the error message, I check `s2t_transformer.py`,  I think the parameter `encoder_freezing_updates` should be given in the config.yaml.  The config file `config.yaml` is generated automatically during data preprocessing. is it incomplete?  If it incomplete, anyone can show a complete one?\r\n```\r\n**config.yaml**\r\n\r\nbpe_tokenizer:\r\n  bpe: sentencepiece\r\n  sentencepiece_model: /home/zhuriyong/wanchengyi/fair_librispeech_preprocessed_data/spm_unigram10000.model\r\ninput_channels: 1\r\ninput_feat_per_channel: 80\r\nsampling_alpha: 1.0\r\nspecaugment:\r\n  freq_mask_F: 27\r\n  freq_mask_N: 2\r\n  time_mask_N: 2\r\n  time_mask_T: 100\r\n  time_mask_p: 1.0\r\n  time_wrap_W: 0\r\ntransforms:\r\n  '*':\r\n  - utterance_cmvn\r\n  _train:\r\n  - utterance_cmvn\r\n  - specaugment\r\nvocab_filename: spm_unigram10000.txt\r\n```\r\n3. Finally, I give the parameter a default value 1 (None, before), and ran the training command again, nothing error shows this time.\r\n\r\n```\r\n**/fairseq/models/speech_to_text/s2t_transformer.py, line 204.**\r\n\r\n        parser.add_argument(\r\n            '--encoder-freezing-updates',\r\n            default=1,\r\n            type=int,\r\n            metavar='N',\r\n            help='freeze encoder for first N updates'\r\n        )\r\n```\r\n4. what's the parameter `encoder-freezing-updates` mean?  what's the reasonale value?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3501/comments",
    "author": "cywan1998",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:48:14Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "793328114",
        "created_at": "2022-04-25T13:42:02Z",
        "body": "hey, Have you solved this problem?\r\n"
      }
    ]
  },
  {
    "number": 3493,
    "title": "Error in multi-node multi-gpu running with fairseq",
    "created_at": "2021-04-20T08:37:31Z",
    "closed_at": "2021-04-22T11:32:05Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3493",
    "body": "## Error in multi-node multi-gpu running with fairseq\r\n\r\npython3 -m torchelastic.distributed.launch  --nnodes=1:1 --rdzv_backend=\"etcd\" --rdzv_endpoint=\"fission-elastic-etcd2.fission-system:2379\" --rdzv_id=fairseq-wav2vec-pretrain-base-56p --nproc_per_node=8 --max_restarts=1 --rdzv_conf=\"timeout=86400\" /workspace/train/././fairseq/fairseq-hydra-train.py --config-dir /workspace/train/fairseq/examples/wav2vec/config/pretraining --config-name wav2vec2_base_librispeech.yaml\r\n\r\n---------------------------------------------------------------------\r\n$cat wav2vec2_base_librispeech.yaml \r\ncommon:\r\n  fp16: false\r\n  log_format: json\r\n  log_interval: 200\r\n\r\ncheckpoint:\r\n  save_dir: /fairseq/model/wav2vec/zstart_gpu16_lr1e-04_maxtokens80k \r\n  save_interval_updates: 1000\r\n  keep_interval_updates: 1\r\n  no_epoch_checkpoints: true\r\n\r\ntask:\r\n  _name: audio_pretraining\r\n  data: /workspace/train/fairseq/manifest/train\r\n  max_sample_size: 250000\r\n  min_sample_size: 32000\r\n  normalize: false\r\n\r\ndataset:\r\n  num_workers: 4\r\n  max_tokens: 400000\r\n  skip_invalid_size_inputs_valid_test: true\r\n\r\ndistributed_training:\r\n  distributed_world_size: 8\r\n  ddp_backend: legacy_ddp\r\n\r\ncriterion:\r\n  _name: wav2vec\r\n  infonce: true\r\n  log_keys: [\"prob_perplexity\",\"code_perplexity\",\"temp\"]\r\n  loss_weights: [0.1, 10]\r\n\r\noptimization:\r\n  max_update: 800000\r\n  lr: [0.0001]\r\n\r\noptimizer:\r\n  _name: adam\r\n  adam_betas: (0.9,0.98)\r\n  adam_eps: 1e-06\r\n  weight_decay: 0.01\r\n\r\nlr_scheduler:\r\n  _name: polynomial_decay\r\n  warmup_updates: 32000\r\n\r\nmodel:\r\n  _name: wav2vec2\r\n  quantize_targets: true\r\n  final_dim: 256\r\n  encoder_layerdrop: 0.05\r\n  dropout_input: 0.1\r\n  dropout_features: 0.1\r\n  feature_grad_mult: 0.1\r\n  encoder_embed_dim: 768\r\n\r\n---------------------------------------------------------------------\r\n\r\n[2021-04-20 07:07:16,784][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 3164472, skipped 566116 samples\r\n[2021-04-20 07:07:17,097][fairseq.tasks.fairseq_task][INFO] - dataset set epoch 1\r\nTraceback (most recent call last):\r\n  File \"/workspace/train/fairseq/fairseq_cli/hydra_train.py\", line 45, in hydra_main\r\n    distributed_utils.call_main(cfg, pre_main)\r\n  File \"/workspace/train/fairseq/fairseq/distributed/utils.py\", line 349, in call_main\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n  File \"/workspace/train/fairseq/fairseq/distributed/utils.py\", line 326, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/workspace/train/fairseq/fairseq_cli/train.py\", line 126, in main\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/workspace/train/fairseq/fairseq/checkpoint_utils.py\", line 221, in load_checkpoint\r\n    epoch=1, load_dataset=True, **passthrough_args\r\n  File \"/workspace/train/fairseq/fairseq/trainer.py\", line 449, in get_train_iterator\r\n    self.model.max_positions(),\r\n  File \"/workspace/train/fairseq/fairseq/trainer.py\", line 199, in model\r\n    device=self.device,\r\n  File \"/workspace/train/fairseq/fairseq/models/distributed_fairseq_model.py\", line 71, in DistributedFairseqModel\r\n    module=model.to(device),\r\n  File \"/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 607, in to\r\n    return self._apply(convert)\r\n  File \"/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 354, in _apply\r\n    module._apply(fn)\r\n  File \"/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 354, in _apply\r\n    module._apply(fn)\r\n  File \"/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 354, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 376, in _apply\r\n    param_applied = fn(param)\r\n  File \"/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\", line 605, in convert\r\n    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\r\nRuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 343.91 MiB already allocated; 12.12 MiB free; 380.00 MiB reserved in total by PyTorch)\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n[ERROR] 2021-04-20 07:07:22,098 local_elastic_agent: [default] Worker group failed\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/torchelastic/agent/server/local_elastic_agent.py\", line 190, in _monitor_workers\r\n    if self._process_context.join(timeout=-1):\r\n  File \"/usr/local/lib64/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException: \r\n Process 4 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib64/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/usr/local/lib/python3.6/site-packages/torchelastic/agent/server/local_elastic_agent.py\", line 79, in _wrap\r\n    ret = fn(*args)\r\n  File \"/usr/local/lib/python3.6/site-packages/torchelastic/distributed/launch.py\", line 392, in wrapper_fn\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', '/workspace/train/././fairseq/fairseq-hydra-train.py', '--config-dir', '/workspace/train/fairseq/examples/wav2vec/config/pretraining', '--config-name', 'wav2vec2_base_librispeech.yaml']' returned non-zero exit status 1.\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib64/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/site-packages/torchelastic/distributed/launch.py\", line 510, in <module>\r\n    main()\r\n  File \"/usr/local/lib/python3.6/site-packages/torchelastic/distributed/launch.py\", line 499, in main\r\n    elastic_agent.run(spec.role)\r\n  File \"/usr/local/lib/python3.6/site-packages/torchelastic/agent/server/api.py\", line 535, in run\r\n    monitor_result.exceptions,\r\ntorchelastic.agent.server.api.WorkerGroupFailureException: [default] exceeded max_restarts=1\r\n-------------------------------------------------\r\n\r\nIn one node multi-gpu mode, running the command below is ok.\r\n\r\nfairseq-hydra-train --config-dir /workspace/train/fairseq/examples/wav2vec/config/pretraining --config-name wav2vec2_base_librispeech.yaml\r\n\r\n-------------------------------------------------\r\nAny help will be greatly appreciated!  @myleott @alexeib \r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3493/comments",
    "author": "Gavin90s",
    "comments": [
      {
        "user": "aijanai",
        "created_at": "2021-05-08T11:16:24Z",
        "body": "@Gavin90s would you please report how did you sort it out?"
      },
      {
        "user": "heibaidaolx123",
        "created_at": "2021-05-12T06:13:52Z",
        "body": "Is there any example for fairseq-hydra-train multi-node training? @alexeib "
      }
    ]
  },
  {
    "number": 3487,
    "title": "Replace unk tokens at generation time - Dataset not found",
    "created_at": "2021-04-18T23:22:51Z",
    "closed_at": "2022-05-02T22:22:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3487",
    "body": "I am trying to generate translations from a trained model and replace the generate <unk> tokens with the original words from the source sentence. To this end, I am calling generate.py as follows:\r\n\r\n`python generate.py my_processed_test_data --path checkpoint_best.pt --batch-size 128 --beam 5 --source-lang src --target-lang dst  > output_no_unk.txt --replace-unk --raw-text `\r\n\r\nEven though my_processed_test_data folder does contain test.*.bin and test.*.idx files, I am getting the error:\r\n\r\n`Traceback (most recent call last):\r\n  File \"generate.py\", line 192, in <module>\r\n    cli_main()\r\n  File \"generate.py\", line 188, in cli_main\r\n    main(args)\r\n  File \"generate.py\", line 35, in main\r\n    task.load_dataset(args.gen_subset)\r\n  File \"/usr/fairseq/tasks/translation.py\", line 154, in load_dataset\r\n    raise FileNotFoundError('Dataset not found: {} ({})'.format(split, data_path))\r\nFileNotFoundError: Dataset not found: test (/usr/my_processed_test_data/test)\r\n`\r\n\r\nHow to fix this problem? Thank you!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3487/comments",
    "author": "Crista23",
    "comments": [
      {
        "user": "Crista23",
        "created_at": "2021-04-19T17:33:16Z",
        "body": "In addition, I am wondering how the align-file option should be used and how the contents of the align-file look like - is this simply the output of an alignment or attention based model? And can this option be used to replace unk words with the original words in the source sentence?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:03Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3481,
    "title": "run M2M_100 12B model, I use 8 Tesla T4 GPUs,but It's very inefficient. (0.34 sentences/s, 9.99 tokens/s). I want to be more efficient,help me.",
    "created_at": "2021-04-15T10:41:00Z",
    "closed_at": "2022-05-02T22:22:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3481",
    "body": "These are my parameters\r\n\r\nfairseq-generate \\\r\n    en-zh/data_bin \\\r\n    --batch-size 1 \\\r\n    --path ../../12b_last_chk_8_gpus/12b_last_chk_8_gpus.pt \\\r\n    --fixed-dictionary model_dict.128k.txt \\\r\n    -s en -t zh \\\r\n    --remove-bpe 'sentencepiece' \\\r\n    --beam 5 \\\r\n    --task translation_multi_simple_epoch \\\r\n    --lang-pairs language_pairs.txt \\\r\n    --decoder-langtok --encoder-langtok src \\\r\n    --gen-subset test \\\r\n    --fp16 \\\r\n    --dataset-impl mmap \\\r\n    --distributed-world-size 1 --distributed-no-spawn \\\r\n    --pipeline-model-parallel \\\r\n    --pipeline-chunks 1 \\\r\n    --pipeline-encoder-balance '[1,6,6,6,7]' \\\r\n    --pipeline-encoder-devices '[0,4,5,1,0]' \\\r\n    --pipeline-decoder-balance '[1,6,6,6,6,1]' \\\r\n    --pipeline-decoder-devices '[0,2,6,7,3,0]' \\",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3481/comments",
    "author": "im-yangp",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:09Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3478,
    "title": "M2M-100 12B model,I use 4 GPUs with the same efficiency as 8 GPUs.why?The result is (0.40 sentences/s, 11.68 tokens/s).The GPU is  Tesla T4",
    "created_at": "2021-04-14T06:41:37Z",
    "closed_at": "2022-04-29T16:22:17Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3478",
    "body": "\r\n12b_last_chk_4_gpus  config\r\nfairseq-generate \\\r\n    data_bin \\\r\n    --batch-size 8 \\\r\n    --path ../../12b_last_chk_4_gpus/12b_last_chk_4_gpus.pt \\\r\n    --fixed-dictionary model_dict.128k.txt \\\r\n    -s de -t zh \\\r\n    --remove-bpe 'sentencepiece' \\\r\n    --beam 5 \\\r\n    --task translation_multi_simple_epoch \\\r\n    --lang-pairs language_pairs.txt \\\r\n    --decoder-langtok --encoder-langtok src \\\r\n    --gen-subset test \\\r\n    --fp16 \\\r\n    --dataset-impl mmap \\\r\n    --distributed-world-size 1 --distributed-no-spawn \\\r\n    --pipeline-model-parallel \\\r\n    --pipeline-chunks 2 \\\r\n    --pipeline-encoder-balance '[1,15,10]' \\\r\n    --pipeline-encoder-devices '[0,1,0]' \\\r\n    --pipeline-decoder-balance '[3,11,11,1]' \\\r\n    --pipeline-decoder-devices '[0,2,3,0]' \r\n\r\n######################################\r\n\r\n12b_last_chk_8_gpus config\r\nfairseq-generate \\\r\n    data_bin \\\r\n    --batch-size 32 \\\r\n    --path ../../12b_last_chk_8_gpus/12b_last_chk_8_gpus.pt \\\r\n    --fixed-dictionary model_dict.128k.txt \\\r\n    -s de -t zh \\\r\n    --remove-bpe 'sentencepiece' \\\r\n    --beam 5 \\\r\n    --task translation_multi_simple_epoch \\\r\n    --lang-pairs language_pairs.txt \\\r\n    --decoder-langtok --encoder-langtok src \\\r\n    --gen-subset test \\\r\n    --fp16 \\\r\n    --dataset-impl mmap \\\r\n    --distributed-world-size 1 --distributed-no-spawn \\\r\n    --pipeline-model-parallel \\\r\n    --pipeline-chunks 2 \\\r\n    --pipeline-encoder-balance '[1,6,6,6,7]' \\\r\n    --pipeline-encoder-devices '[0,4,5,1,0]' \\\r\n    --pipeline-decoder-balance '[1,6,6,6,6,1]' \\\r\n    --pipeline-decoder-devices '[0,2,6,7,3,0]' \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3478/comments",
    "author": "im-yangp",
    "comments": [
      {
        "user": "leonodelee",
        "created_at": "2021-04-28T06:09:46Z",
        "body": "请问你复现m2m出现这个问题了没？"
      },
      {
        "user": "leonodelee",
        "created_at": "2021-04-28T06:09:59Z",
        "body": "I want to implement example/M2M  but got a error while loading model(s) from 12b_last_chk_2_gpus.pt:\r\nERROR | fairseq.dataclass.utils | Error when composing. Overrides:[…]\r\n…\r\nomegaconf.errors.ValidationError: Invalid value 'simple', expected one of [c10d, fully_sharded, legacy_ddp, no_c10d, pytorch_ddp,_mo]\r\n\tfull_key: distributed_training.ddp_backend\r\n\treference_type=DistributedTrainingConfig\r\nobject_type=DistributedTrainingConfig\r\n"
      },
      {
        "user": "im-yangp",
        "created_at": "2021-05-11T05:53:37Z",
        "body": "@leonodelee 你检查一下参数配置 ，应该是参数配错了，按官方文档的配置是运行成功的，如果你用12b_last_chk_2_gpus，必须有两块32G内存以上的GPU"
      },
      {
        "user": "Hanlard",
        "created_at": "2021-06-10T07:55:35Z",
        "body": "> @leonodelee 你检查一下参数配置 ，应该是参数配错了，按官方文档的配置是运行成功的，如果你用12b_last_chk_2_gpus，必须有两块32G内存以上的GPU\r\n\r\n就是那个官方文档的配置，一样会有他说的这个报错"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:39Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3476,
    "title": "Adopting for other languages",
    "created_at": "2021-04-13T20:41:22Z",
    "closed_at": "2022-05-02T22:22:19Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3476",
    "body": "Hey guys,\r\n\r\nwhat changes would be necessary to adopt your approach for German? \r\nAs far as i understood, the vocoder is universal but the Wav2Vec is not.\r\nDo you know of any equivalent for German? Besides that, why would I \r\nhave to have multiple utterances for target speaker? Seems to me 1:1 would be fair.\r\n\r\nBest regards \r\nChris \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3476/comments",
    "author": "ChrisDelClea",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:12Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3473,
    "title": "save_encoder.py in criss is slow",
    "created_at": "2021-04-13T12:49:16Z",
    "closed_at": "2022-05-02T22:22:18Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3473",
    "body": "@chtran Hi, I feel that save_encoder.py is slow. It took over 10 hours to process 10M sentences. I want to know if there is an optimized script to process large-scale data? Thanks.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3473/comments",
    "author": "wangyong1122",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:13Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3470,
    "title": "Questions about mBART",
    "created_at": "2021-04-13T05:38:27Z",
    "closed_at": "2022-05-02T22:22:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3470",
    "body": "## ❓ Questions and Help\r\n\r\nQuestions about mBART model.\r\n\r\n#### What is your question?\r\nHi, I want to have a try for the mBART code. At the first step, I have already downloaded the model mbart.CC25. At the second step, I am quite confused about the code.\r\n\r\n#### Code\r\n\r\n```\r\nSPM=/path/to/sentencepiece/build/src/spm_encode\r\nMODEL=sentence.bpe.model\r\n${SPM} --model=${MODEL} < ${DATA}/${TRAIN}.${SRC} > ${DATA}/${TRAIN}.spm.${SRC} &\r\n${SPM} --model=${MODEL} < ${DATA}/${TRAIN}.${TGT} > ${DATA}/${TRAIN}.spm.${TGT} &\r\n${SPM} --model=${MODEL} < ${DATA}/${VALID}.${SRC} > ${DATA}/${VALID}.spm.${SRC} &\r\n${SPM} --model=${MODEL} < ${DATA}/${VALID}.${TGT} > ${DATA}/${VALID}.spm.${TGT} &\r\n${SPM} --model=${MODEL} < ${DATA}/${TEST}.${SRC} > ${DATA}/${TEST}.spm.${SRC} &\r\n${SPM} --model=${MODEL} < ${DATA}/${TEST}.${TGT} > ${DATA}/${TEST}.spm.${TGT} &\r\n```\r\n#### What have you tried?\r\nWhat is this ${DATA} I need to give?\r\n\r\nAlso, from multi-lingual doc, I want to try mBERT50. After downloading the file, \r\n\r\n> use the tool binarize.py to binarize your data using sentence.bpe.model and dict.{lang}.txt, and copy the dictionaries to your data path\r\n\r\nI wonder how to use this binarize code?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3470/comments",
    "author": "PingYu-iris",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:15Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3468,
    "title": "Measure diversity in Neural Machine translation",
    "created_at": "2021-04-12T19:32:21Z",
    "closed_at": "2022-05-02T22:22:14Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3468",
    "body": "Hi\r\nI would like to know how to implement diversity metric in fairseq.I have trained NMT model and I am generating translations through different decoding algorithms. I would like to measure diversity of the generated sentences besides BLUE score. Is there any command line tool that can be used? \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3468/comments",
    "author": "zoki701",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:22Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:44Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3456,
    "title": "How to initiate the decoder",
    "created_at": "2021-04-07T16:47:22Z",
    "closed_at": "2022-05-02T22:22:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3456",
    "body": "I use transformer in the fairseq package to pretrain a model with some data. In this procedure, I train both encoder and decoder, and update their parameters. However , when I fintune my data with another dataset, I must use different dictionary to process my targrt corpus, but there is an error saying: \r\n\r\n1. size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([18000, 512]) from checkpoint, the shape in current model is torch.Size([22336, 512]).\r\n2. size mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([18000, 512]) from checkpoint, the shape in current model is torch.Size([22336, 512]).\r\n\r\nI know that's because I use different dictionary, Howerver , I wonder if there are some methods to initiate the decoder parameters and only use encoder that pretrained already to fintune my model ? **(I use transformer model)**",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3456/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T02:04:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3450,
    "title": "the released model of pre-trained asr & streaming st cann't be loaded correctly",
    "created_at": "2021-04-06T12:11:16Z",
    "closed_at": "2022-05-02T22:22:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale",
      "speech"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3450",
    "body": "In simultaneous translation, the released model of pre-trained asr & st cann't be loaded correctly. \r\n\r\nthe error information is:\r\n\r\nTraceback (most recent call last):\r\n  File \"/data00/home/xxx/miniconda3/envs/fairseq_new/bin/simuleval\", line 33, in <module>\r\n    sys.exit(load_entry_point('simuleval', 'console_scripts', 'simuleval')())\r\n  File \"/data00/home/xxx/codes/SimulEval/simuleval/cli.py\", line 167, in main\r\n    _main(args.client_only)\r\n  File \"/data00/home/xxx/codes/SimulEval/simuleval/cli.py\", line 194, in _main\r\n    evaluate(args, client, server_process)\r\n  File \"/data00/home/xxx/codes/SimulEval/simuleval/cli.py\", line 147, in evaluate\r\n    decode(args, client, result_queue, indices)\r\n  File \"/data00/home/xxx/codes/SimulEval/simuleval/cli.py\", line 91, in decode\r\n    parser = options.general_parser()\r\n  File \"/data00/home/xxx/codes/fairseq/examples/speech_to_text/simultaneous_translation/agents/fairseq_simul_st_agent.py\", line 139, in __init__\r\n    self.load_model_vocab(args)\r\n  File \"/data00/home/xxx/codes/fairseq/examples/speech_to_text/simultaneous_translation/agents/fairseq_simul_st_agent.py\", line 223, in load_model_vocab\r\n    state = checkpoint_utils.load_checkpoint_to_cpu(filename)\r\n  File \"/data00/home/xxx/codes/fairseq/fairseq/checkpoint_utils.py\", line 263, in load_checkpoint_to_cpu\r\n    state = torch.load(f, map_location=torch.device(\"cpu\"))\r\n  File \"/data00/home/xxx/miniconda3/envs/fairseq_new/lib/python3.7/site-packages/torch/serialization.py\", line 595, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/data00/home/xxx/miniconda3/envs/fairseq_new/lib/python3.7/site-packages/torch/serialization.py\", line 764, in _legacy_load\r\n    magic_number = pickle_module.load(f, **pickle_load_args)\r\n_pickle.UnpicklingError: A load persistent id instruction was encountered,\r\nbut no persistent_load function was specified.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3450/comments",
    "author": "dqqcasia",
    "comments": [
      {
        "user": "dqqcasia",
        "created_at": "2021-04-07T07:37:27Z",
        "body": "@xutaima Would you have a suggestion?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:26Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3443,
    "title": "How can I modify the code to apply knowledge distillation (not sequential KD)",
    "created_at": "2021-04-05T02:28:07Z",
    "closed_at": "2022-05-02T22:22:01Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3443",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHi! I try to apply knowledge distillation and I need to: \r\n\r\n1. load two training sets, i.e. original training set and the new training set distilled by the teacher model. Both data sets have the same amount of examples.\r\n2. load the logits predicted by the teacher model. Because it can be time-consuming to let the teacher model produce the same logits again and again. \r\n\r\nCan you give me some advice on how to modify the source code to achieve that? Thank you!\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0\r\n - PyTorch Version (e.g., 1.0): 1.6\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3443/comments",
    "author": "haorannlp",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T02:04:39Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3441,
    "title": "Merging custom trained model with (ctc) finetuned projections of pretrained models",
    "created_at": "2021-04-04T16:08:35Z",
    "closed_at": "2022-05-02T22:21:57Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3441",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI have trained a custom base model (without finetuning) on a different dataset derived from librispeech. I want to save the fine-tuning effort of CTC training from scratch. The paper states that for the finetuning a randomized linear projection layer is added on top and trained. I want to use this projection layer from the pre-trained + finetuned models that are already provided in the repository by modifying the checkpoint of the model I have trained. How can I do that?\r\n\r\n#### Code\r\n\r\nN/A\r\n\r\n#### What have you tried?\r\nI have looked at the model (after loading the `.pt` file), and figured out `model['model']` dict needs to be altered. However, the naming convention in the pre-trained models and pre-trained + CTC finetuned models are different and so is the `len(model['model'].keys())`. I'm not sure how to proceed after this point.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0.0a0+16c1a20\r\n - PyTorch Version (e.g., 1.0): 1.8.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): `pip install --editable ./`\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 11.2\r\n - GPU models and configuration: NVIDIA GTX TITAN X\r\n - Any other relevant information: Built flashlight python bindings from source\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3441/comments",
    "author": "archiki",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T02:04:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:26Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3439,
    "title": "Cannot pass `--phase-ratio` when using `fairseq-train` command line tool with tristage scheduler",
    "created_at": "2021-04-04T05:28:17Z",
    "closed_at": "2022-05-02T22:21:56Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3439",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI am trying to use the tristage scheduler with `--phase-ratio` through `fairseq-train`. However, the command line tool doesn't seem to recognizer this parameter.\r\n\r\n#### Code\r\n\r\n`fairseq-train ...... --lr-scheduler tri_stage --lr 5e-05 --phase-ratio '(0.1,0.0,0.9)' ...` will return an error of `fairseq-train: error: unrecognized arguments: --phase-ratio (0.1,0.0,0.9)`\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.8\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): pip install ./\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: NA\r\n - GPU models and configuration: NA\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3439/comments",
    "author": "zijwang",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T02:04:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:25Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3432,
    "title": "rxf pre-trained model",
    "created_at": "2021-04-01T15:27:29Z",
    "closed_at": "2022-05-02T21:22:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3432",
    "body": "## ❓ Questions and Help\r\n\r\nHey,\r\n\r\ncould I ask if there is any pre-trained model for rxf available?\r\nor if it will be released in the future?\r\n\r\nThank you\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3432/comments",
    "author": "sofiaperosin",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T02:04:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3428,
    "title": "wav2vect+ctc  self-training",
    "created_at": "2021-04-01T06:35:57Z",
    "closed_at": "2022-04-18T02:21:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3428",
    "body": "## ❓ Questions and Help\r\nI have some details questions about \"Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020).\"\r\nIn this paper ,after finetune wav2vec+ctc model, it use the self-training methods to finetune on the librivox dataset.  \r\nI have some questions about this.\r\n1.  Is there the self-training methods in fairseq?\r\n2. when i want to get the pseudo labels of librivox dataset, is there some pre-process? e.g. running VAD on the raw wavs? \r\n3.  If I want to training or infer the wav2vec+ctc model on my own datasets, should I run VAD pre-process on my datasets? And If I use the raw wavs of my dataset( there may some noise or silence), it would work?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3428/comments",
    "author": "liuuzexiang",
    "comments": [
      {
        "user": "olafthiele",
        "created_at": "2021-05-12T11:22:17Z",
        "body": ">     1. Is there the self-training methods in fairseq?\r\nYou self train first, then you finetune.\r\n\r\n>     2. when i want to get the pseudo labels of librivox dataset, is there some pre-process? e.g. running VAD on the raw wavs?\r\nThey are not pseudo labels, you need short audio (5-20 secs) with correct transcription.\r\n\r\n>     3. If I want to training or infer the wav2vec+ctc model on my own datasets, should I run VAD pre-process on my datasets? And If I use the raw wavs of my dataset( there may some noise or silence), it would work?\r\n\r\nCheck the Common Voice project, you need input like that. You should have just regular silence. Check Common Voice.\r\n\r\nPlease close the issue if that answers your questions.\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:47:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:35Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3426,
    "title": "numpy and cpython binary incompatibility",
    "created_at": "2021-04-01T03:43:38Z",
    "closed_at": "2022-04-18T01:21:25Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3426",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nFile \"fairseq/data/data_utils_fast.pyx\", line 1, in init fairseq.data.data_utils_fast\r\ncython: language_level=3\r\nValueError: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216 from C header, got 192 from PyObject\r\n\r\n#### Code\r\nth2en = TransformerModel.from_pretrained(\r\n        model_name_or_path=MODEL_BASE_DIR,\r\n        checkpoint_file='checkpoint.pt',\r\n        data_name_or_path=VOCAB_BASE_DIR,\r\n        bpe='sentencepiece', sentencepiece_model=BPE_BASE_DIR + 'spm.th.model')\r\n        # sentencepiece_vocab=BPE_BASE_DIR + 'spm.th.model')\r\n\r\nprint(th2en.translate([\"งั้นเอาเป็นเวนติ แบล็คแอนด์ไวท์มอคค่าใส่นมสดกับวิปครีม จากสตาร์บัคส์สาขาห้างเบิร์ชวิลล์นะคะ\"])\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (0.9.0):\r\n - PyTorch Version (1.6)\r\n - OS (Linux):\r\n - How you installed fairseq (pip install --editable .):\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information: numpy==1-19.5  Cython==0.29.22\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3426/comments",
    "author": "lsjiiia",
    "comments": [
      {
        "user": "haorannlp",
        "created_at": "2021-04-05T02:08:53Z",
        "body": "You can  try to update your numpy version."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:48:12Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:55Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3420,
    "title": "Does KenLM need to be trained with the same training corpus?",
    "created_at": "2021-03-31T05:44:40Z",
    "closed_at": "2022-04-28T22:22:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3420",
    "body": "Hi guys, I see that the KenLM binary that comes with LibriSpeech dataset is trained based on the LibriSpeech data. Does it work or perhaps even better if let's say the KenLM is trained from a different and bigger corpus(eg. Wikipedia). Is it okay if the vocab is different?\r\n\r\nThanks!!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3420/comments",
    "author": "weiyengs",
    "comments": [
      {
        "user": "harveenchadha",
        "created_at": "2021-04-23T21:46:11Z",
        "body": "The dict for language model and the trained ASR model has to be the same!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:35Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3419,
    "title": "Get an error  when using \" from fariseq import libnat\"",
    "created_at": "2021-03-31T02:27:47Z",
    "closed_at": "2022-05-02T22:22:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3419",
    "body": "I have installed the fairseq by pip install successfully.  But when I used the command \" from fairseq import libnat\"， it cause the error report: ImportError: cannot import name 'libnat'..\r\nMy fairseq version is 0.10.2 ----I also tryed to install the latest version of 1.0.0a0 by source code from github, but still had same problem. \r\nHope someone can help me~~",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3419/comments",
    "author": "lmw0320",
    "comments": [
      {
        "user": "haorannlp",
        "created_at": "2021-04-05T02:07:49Z",
        "body": "Did you try \"pip install --editable .\" ?"
      },
      {
        "user": "lmw0320",
        "created_at": "2021-04-06T08:24:45Z",
        "body": "> Did you try \"pip install --editable .\" ?\r\n\r\nYes, I have download the source code from github, and install the fairseq by the suggestion command:  pip install --editable . , but still got the error report of  can not import name 'libnat'.."
      },
      {
        "user": "i13harada1s",
        "created_at": "2021-04-14T10:16:20Z",
        "body": "@lmw0320 \r\nI had the same issue, but I can solve it by the command described in #2010: \"python setup.py build_ext --inplace\"."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:10Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T22:21:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "conan1024hao",
        "created_at": "2023-04-07T05:26:50Z",
        "body": "I solved it by `python setup.py build_ext`"
      }
    ]
  },
  {
    "number": 3404,
    "title": "Using word2vec",
    "created_at": "2021-03-27T08:43:21Z",
    "closed_at": "2022-05-01T02:22:13Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3404",
    "body": "Hey,\r\n\r\nI'm trying to create a loss function based on embedding distance from the target word for translation (to replace CE). \r\nIv'e used the 'examples/translation' code to create my dataset and vocabulary based on the \"prepare-iwslt14.sh\".\r\nIt created in data-bin the dictionary files and and .bin and .idx for the sentences.\r\n\r\nHow can I run word2vec over the target vocabulary, using some corpus, to create the word2vec embeddings?\r\nI want the token split to be the same as the one created by fairseq-preprocess, and the token indexes to be consistent between word2vec and fairseq.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3404/comments",
    "author": "YoadTew",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-28T12:36:14Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "gayaninan",
        "created_at": "2021-07-23T11:15:58Z",
        "body": "Hello,\r\n\r\nI'm also facing a similar issue with word2vec embeddings. Were you able to resolve it?\r\n\r\nThanks in advance."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3400,
    "title": "FileNotFoundError: [Errno 2] No such file or directory: 'RTE-bin/input0/dict.txt'",
    "created_at": "2021-03-26T09:02:43Z",
    "closed_at": "2022-04-18T00:21:10Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3400",
    "body": "Trying to run training for rxf example for summarization. Faced with this error, but I do not even have RTE-bin folder, do I need to install something additionally to fairseq? \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/pavel.pugach/.pyenv/versions/3.7.9/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/pavel.pugach/fairseq/fairseq_cli/train.py\", line 477, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/pavel.pugach/fairseq/fairseq/distributed/utils.py\", line 364, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/pavel.pugach/fairseq/fairseq_cli/train.py\", line 82, in main\r\n    task = tasks.setup_task(cfg.task)\r\n  File \"/home/pavel.pugach/fairseq/fairseq/tasks/__init__.py\", line 44, in setup_task\r\n    return task.setup_task(cfg, **kwargs)\r\n  File \"/home/pavel.pugach/fairseq/fairseq/tasks/sentence_prediction.py\", line 118, in setup_task\r\n    source=True,\r\n  File \"/home/pavel.pugach/fairseq/fairseq/tasks/sentence_prediction.py\", line 106, in load_dictionary\r\n    dictionary = Dictionary.load(filename)\r\n  File \"/home/pavel.pugach/fairseq/fairseq/data/dictionary.py\", line 215, in load\r\n    d.add_from_file(f)\r\n  File \"/home/pavel.pugach/fairseq/fairseq/data/dictionary.py\", line 228, in add_from_file\r\n    raise fnfe\r\n  File \"/home/pavel.pugach/fairseq/fairseq/data/dictionary.py\", line 225, in add_from_file\r\n    with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'RTE-bin/input0/dict.txt'\r\n```\r\n\r\nPython 3.7.9\r\npytorch 1.8.0+cu111\r\nFairseq - cloned from current master branch",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3400/comments",
    "author": "Worien",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-28T12:36:17Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "jmusiel",
        "created_at": "2021-11-01T17:49:19Z",
        "body": "bump"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:07Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T00:20:40Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "huchinlp",
        "created_at": "2022-04-19T06:10:41Z",
        "body": "Download the data and preprocess it, and if every thing goes smoothly, you will get the binarized data in the directory `RTE-bin`.\r\nDo not forget replacing the `RTE-bin` arg in your command with the absolute path to your data directory."
      }
    ]
  },
  {
    "number": 3394,
    "title": "Relation of Wav2vec2.0 \"max-sample-size\" and audio time",
    "created_at": "2021-03-25T02:29:51Z",
    "closed_at": "2021-04-25T02:08:40Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3394",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI want to know the relation of Wav2vec2.0 \"max-sample-size\" and audio time. For example, when I set the \"max_sample_size=320000\", what is the duration of wav audios(16000Hz) ?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3394/comments",
    "author": "CSLujunyu",
    "comments": [
      {
        "user": "harveenchadha",
        "created_at": "2021-04-23T21:48:12Z",
        "body": "If your max_sample_size is 320000 and your sample rate is 16000 then it means at once you are allowing a file of 20 seconds.\r\n\r\n320000/16000 = 20 seconds"
      },
      {
        "user": "CSLujunyu",
        "created_at": "2021-04-25T02:08:40Z",
        "body": "Thanks~"
      }
    ]
  },
  {
    "number": 3378,
    "title": "Training a Multilingual system on multi-gpus",
    "created_at": "2021-03-19T15:10:14Z",
    "closed_at": "2021-03-23T09:08:08Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3378",
    "body": "## ❓ Questions and Help\r\n\r\n### What is your question?\r\n\r\nHello! \r\nI'm trying to train a multilingual model using a dataset with 15 language-pairs and 500M sentence pairs. I'm using a machine with 4 Tesla T4. When I try to run the training process I get an error.  It is a memory issue that I do not know how to solve. I have tried to use different dataset implementations (lazy and mmap). I'm using master version of fairseq.\r\n\r\n**My question is** : What is the right way of training a multilingual model on multiple gpus? \r\n\r\n### Code\r\n\r\n#### Preprocess for each language pair:\r\n\r\n```bash\r\n        fairseq-preprocess \\\r\n        --source-lang $source_language \\\r\n        --target-lang $target_language \\\r\n        --trainpref train \\\r\n        --validpref valid \\\r\n        --joined-dictionary \\\r\n        --tgtdict vocab_target \\\r\n        --destdir ${setup_directory}/data-bin \\\r\n        --workers $num_jobs\r\n```\r\n#### Train:\r\n\r\n```bash       \r\nfairseq-train \\\r\n        -- $data_bin \\\r\n        --ddp-backend=no_c10d \\\r\n        --save-dir ${setup_directory}/shared-model-checkpoints \\\r\n        --save-interval-updates $save_freq \\\r\n        --task translation_multi_simple_epoch \\\r\n        --arch transformer \\\r\n        --lang-pairs $lang_pairs \\\r\n        --lang-dict $lang_list \\\r\n        --sampling-method \"$batch_sampling_method\" \\\r\n        $sampling_temperature \\\r\n        --optimizer adam \\\r\n        --adam-betas '(0.9, 0.98)' \\\r\n        --clip-norm $clip_norm  \\\r\n        --lr $learn_rate \\\r\n        --lr-scheduler inverse_sqrt \\\r\n        --warmup-updates $lr_warmup \\\r\n        --warmup-init-lr '1e-07' \\\r\n        --dropout $dropout \\\r\n        --criterion $criterion \\\r\n        --label-smoothing $label_smoothing \\\r\n        --share-decoder-input-output-embed \\\r\n        --max-epoch $epochs \\\r\n        --patience $early_stopping \\\r\n        --keep-last-epochs $keep_last_checkpoints \\\r\n        --keep-best-checkpoints $keep_best_checkpoints \\\r\n        --max-tokens 4096 \\\r\n        --fp16 \\\r\n        --num-workers $num_jobs \\\r\n        --fix-batches-to-gpus \r\n```\r\n\r\n### Log\r\n```bash\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/mnt/data/venvs/fairseq/0.10.2/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/mnt/data/tools/fairseq/fairseq/distributed/utils.py\", line 326, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/mnt/data/tools/fairseq/fairseq_cli/train.py\", line 157, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/mnt/data/tools/fairseq/fairseq_cli/train.py\", line 263, in train\r\n    for i, samples in enumerate(progress):\r\n  File \"/mnt/data/tools/fairseq/fairseq/logging/progress_bar.py\", line 256, in __iter__\r\n    for i, obj in enumerate(self.iterable, start=self.n):\r\n  File \"/mnt/data/tools/fairseq/fairseq/data/iterators.py\", line 59, in __iter__\r\n    for x in self.iterable:\r\n  File \"/mnt/data/tools/fairseq/fairseq/data/iterators.py\", line 528, in _chunk_iterator\r\n    for x in itr:\r\n  File \"/mnt/data/tools/fairseq/fairseq/data/iterators.py\", line 59, in __iter__\r\n    for x in self.iterable:\r\n  File \"/mnt/data/venvs/fairseq/0.10.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 355, in __iter__\r\n    return self._get_iterator()\r\n  File \"/mnt/data/venvs/fairseq/0.10.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 301, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"/mnt/data/venvs/fairseq/0.10.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 914, in __init__\r\n    w.start()\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nOverflowError: cannot serialize a bytes object larger than 4 GiB\r\n```\r\n\r\n### What have you tried?\r\n\r\n- `--fix-batches-to-gpus`\r\n-  `--dataset-impl` : `lazy` and `mmap`\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3378/comments",
    "author": "joao-alves97",
    "comments": [
      {
        "user": "joao-alves97",
        "created_at": "2021-03-21T18:44:15Z",
        "body": "After solving the previous error, now I have this one:\r\n```bash\r\nTrainFairseq/Baseline.baseline: Traceback (most recent call last):TrainFairseq/Baseline.baseline:   File \"/mnt/data/venvs/fairseq/0.10.2/bin/fairseq-train\", line 33, in <module>TrainFairseq/Baseline.baseline:     sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())TrainFairseq/Baseline.baseline:   File \"/mnt/data/tools/fairseq/fairseq_cli/train.py\", line 477, in cli_mainTrainFairseq/Baseline.baseline:     distributed_utils.call_main(cfg, main)TrainFairseq/Baseline.baseline:   File \"/mnt/data/tools/fairseq/fairseq/distributed/utils.py\", line 349, in call_mainTrainFairseq/Baseline.baseline:     join=True,TrainFairseq/Baseline.baseline:   File \"/mnt/data/venvs/fairseq/0.10.2/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawnTrainFairseq/Baseline.baseline:     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')TrainFairseq/Baseline.baseline:   File \"/mnt/data/venvs/fairseq/0.10.2/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 188, in start_processesTrainFairseq/Baseline.baseline:     while not context.join():TrainFairseq/Baseline.baseline:   File \"/mnt/data/venvs/fairseq/0.10.2/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 136, in joinTrainFairseq/Baseline.baseline:     signal_name=nameTrainFairseq/Baseline.baseline: torch.multiprocessing.spawn.ProcessExitedException: process 3 terminated with signal SIGKILL\r\n```"
      },
      {
        "user": "sarthmit",
        "created_at": "2021-05-05T08:59:20Z",
        "body": "Hi, were you able to train a multilingual model from scratch?"
      },
      {
        "user": "joao-alves97",
        "created_at": "2021-05-05T09:33:51Z",
        "body": "Hi, yes I was. I just used `--num-workers 0` and it worked"
      },
      {
        "user": "sarthmit",
        "created_at": "2021-05-05T10:41:39Z",
        "body": "Would it be possible for you to share your setup? I don't know how to do the preprocessing of the datasets and the setup of the model."
      }
    ]
  },
  {
    "number": 3373,
    "title": "how to merge tag input  in LanguagePairDataset in translation task",
    "created_at": "2021-03-19T03:12:49Z",
    "closed_at": "2021-06-20T13:41:33Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3373",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI use translation task for summary generation. Original translation take \"train.document.bin\" and \"train.summary.bin\" as input. \r\nI have a tag for each document token, I want dataloader to read it and feed it  into model. How can I do？\r\n\r\nfor example(the data I have now):\r\ndocument ：A B C D E \r\ntag : 1 2 3 4 5\r\nsummary: A B C\r\n\r\ntranslation task in fairseq use MMapIndexedDataset(default indexed dataset) to load document and summary respectively And return LanguagePairDataset.  \r\nTag is not a kind of token but I have to load it and ensure alignment between annotate tag and document token at the same time.\r\n\r\n#### Code\r\n'''python\r\nreturn LanguagePairDataset(\r\n        src_dataset,\r\n        src_dataset.sizes,\r\n        src_dict,\r\n        tgt_dataset,\r\n        tgt_dataset_sizes,\r\n        tgt_dict,\r\n        left_pad_source=left_pad_source,\r\n        left_pad_target=left_pad_target,\r\n        align_dataset=align_dataset,\r\n        eos=eos,\r\n        num_buckets=num_buckets,\r\n        shuffle=shuffle,\r\n        pad_to_multiple=pad_to_multiple,\r\n    )\r\n'''\r\n#### What have you tried?\r\nI think maybe I need to generate another tag file and use fairseq-preprocess it together with ducument and summary which will create \"document.bin\"  \"summary.bin\"  and \"tag.bin\"\r\nbut how to ensure alignment between each tag and document token and insert the code to load the tag data.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (Linux):\r\n - How you installed fairseq (source):\r\n - Build command you used (official command in README):\r\n - Python version: 3.7\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3373/comments",
    "author": "Ricardokevins",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-20T13:26:27Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3371,
    "title": "[Question] wav2vec 2.0 timestamp words",
    "created_at": "2021-03-18T23:12:54Z",
    "closed_at": "2021-06-22T03:17:07Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3371",
    "body": "Is it possible to extract the Words start time and end time of the transcript? Thank you!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3371/comments",
    "author": "irux",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-16T23:13:45Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3367,
    "title": "DDP Training requires dataset item being tensor?",
    "created_at": "2021-03-18T09:39:32Z",
    "closed_at": "2021-08-20T02:06:10Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3367",
    "body": "I tried define a dataset that contain both tensors of shape [batch, seq_len] and a graph, which is an object of dgl.Graph.\r\nThis dataset works alright when using single gpu, but throws out error when I try to use distributed training.\r\n```\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 617, in forward\r\n    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 643, in scatter\r\n    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 37, in scatter_kwargs\r\n    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 28, in scatter\r\n    res = scatter_map(inputs)\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 19, in scatter_map\r\n    return list(map(type(obj), zip(*map(scatter_map, obj.items()))))\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 15, in scatter_map\r\n    return list(zip(*map(scatter_map, obj)))\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 13, in scatter_map\r\n    return Scatter.apply(target_gpus, None, dim, obj)\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 92, in forward\r\n    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\r\n  File \"/home/mengyuxian/.conda/envs/gcnnmt/lib/python3.6/site-packages/torch/nn/parallel/comm.py\", line 186, in scatter\r\n    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\r\nRuntimeError: chunk expects at least a 1-dimensional tensor\r\n```\r\n\r\nI'm wondering what's the best practice to using ddp training while having a dataset that return non-tensor item?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3367/comments",
    "author": "YuxianMeng",
    "comments": [
      {
        "user": "YuxianMeng",
        "created_at": "2021-03-18T11:41:19Z",
        "body": "I just found that adding `--ddp-backend \"no_c10d\"` could solve the problem, but I'm curious of the reason behind it :)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-16T23:13:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3361,
    "title": "How can I fine-tune BART on my own datasets with the pre-trained task (i.e., the denoised autoencoding task)",
    "created_at": "2021-03-16T12:20:33Z",
    "closed_at": "2021-03-26T09:59:49Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3361",
    "body": "How can I fine-tune BART on my own datasets with the pre-trained task (i.e., the denoised autoencoding task)?\r\n\r\nIs there any tutorial about how to pre-process the datasets and how to set the denoised task?\r\n\r\nMany THX :)",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3361/comments",
    "author": "xieyxclack",
    "comments": [
      {
        "user": "ghost",
        "created_at": "2021-04-07T17:02:30Z",
        "body": "Hi, I meet the same question, do you solve this problem ? I use a totally different dictionary to preocess my corpus, and there is error:\r\n\r\n1. size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([18000, 512]) from checkpoint, the shape in current model is torch.Size([22336, 512]).\r\n2. size mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([18000, 512]) from checkpoint, the shape in current model is torch.Size([22336, 512]).\r\n\r\nHow do you finetune your model? And do you know how to initiate the decoder with some command lines ?"
      }
    ]
  },
  {
    "number": 3358,
    "title": "fairseq with torch 1.8.0, ROCm 4.0.1 and MI50 AMD GPUs",
    "created_at": "2021-03-15T22:50:28Z",
    "closed_at": "2021-03-27T01:40:48Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3358",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI am trying to run fairseq on AMD cluster. Installation was smooth. I have some trained checkpoint and run this command to perform validation:\r\n\r\n```\r\nfairseq-validate data-bin/iwslt14.tokenized.de-en.eostask/ --task translation_eos --path ckpt_numeos_1/checkpoint_best.pt --user-dir ./fairseq_module/ --max-tokens 4096\r\n```\r\n\r\nThis works on NVIDUA & CUDA machine, but fails with rocm with the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/state/partition1/ik1147/nmt_multiple_eos/fairseq/fairseq/data/data_utils.py\", line 302, in batch_by_size\r\n    from fairseq.data.data_utils_fast import (\r\n  File \"fairseq/data/data_utils_fast.pyx\", line 1, in init fairseq.data.data_utils_fast\r\nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/ext3/miniconda3/bin/fairseq-validate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-validate')())\r\n  File \"/state/partition1/ik1147/nmt_multiple_eos/fairseq/fairseq_cli/validate.py\", line 145, in cli_main\r\n    distributed_utils.call_main(\r\n  File \"/state/partition1/ik1147/nmt_multiple_eos/fairseq/fairseq/distributed/utils.py\", line 364, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/state/partition1/ik1147/nmt_multiple_eos/fairseq/fairseq_cli/validate.py\", line 89, in main\r\n    itr = task.get_batch_iterator(\r\n  File \"/state/partition1/ik1147/nmt_multiple_eos/fairseq/fairseq/tasks/fairseq_task.py\", line 285, in get_batch_iterator\r\n    batch_sampler = dataset.batch_by_size(\r\n  File \"/state/partition1/ik1147/nmt_multiple_eos/fairseq/fairseq/data/fairseq_dataset.py\", line 145, in batch_by_size\r\n    return data_utils.batch_by_size(\r\n  File \"/state/partition1/ik1147/nmt_multiple_eos/fairseq/fairseq/data/data_utils.py\", line 313, in batch_by_size\r\n    raise ValueError(\r\nValueError: Please build (or rebuild) Cython components with: `pip install  --editable .` or `python setup.py build_ext --inplace`.\r\n```\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\nI have tried to build Cython components as suggested above, the build is successful, but the error still occurs. From my understanding Cython extensions for fast data processing may not work well with rocm. Is there some env var which will block all fast data batching etc. so that I will have higher change of success with rocm? \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.8.0\r\n - OS (e.g., Linux): Ubuntu 20.04.2 LTS, used within Singularity container\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install  --editable .` , then I also ran `python setup.py build_ext --inplace` after I got the error message.\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration: 8 x AMD MI50 (gfx906, 32gb)\r\n - Any other relevant information: ROCm 4.0.1 (used withing Singularity container with `--rocm` argument).\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3358/comments",
    "author": "uralik",
    "comments": [
      {
        "user": "uralik",
        "created_at": "2021-03-22T18:04:21Z",
        "body": "pinging this one in case anyone can give me some direction to look into"
      },
      {
        "user": "compwiztobe",
        "created_at": "2021-03-23T11:50:20Z",
        "body": "What versions of numpy and fairseq do you have installed?  I've seen that binary incompatibility error with numpy 1.19.5 and fairseq 0.10.0, but not with fairseq 0.10.2, or with numpy >= 1.20.0.\r\n\r\nI'm not sure however why training should succeed and only validation shows this problem (I saw it when running `fairseq-train`)."
      },
      {
        "user": "uralik",
        "created_at": "2021-03-23T13:20:20Z",
        "body": "@compwiztobe as written in the post that was fairseq built from sources (master branch) as of week ago which means at least 0.10.2 right? \r\nI will try to check with different numpy versions!"
      },
      {
        "user": "compwiztobe",
        "created_at": "2021-03-24T06:49:48Z",
        "body": "Right, sorry I did miss that.  In that case, with those differences (fairseq version, train vs validate), maybe it has to do with not just numpy version but also the numpy binary for your architecture?\r\n\r\nRelated: #3203 "
      },
      {
        "user": "uralik",
        "created_at": "2021-03-27T01:40:48Z",
        "body": "The issue was the numpy built with mkl which was installed together with torch through conda which seem to have issues with AMD EPYC cpu, installing numpy without MKL from pip solved the issue."
      }
    ]
  },
  {
    "number": 3351,
    "title": "ValueError: Please build (or rebuild) Cython components with: `pip install  --editable .` or `python setup.py build_ext --inplace`.",
    "created_at": "2021-03-13T08:10:32Z",
    "closed_at": "2021-03-26T08:23:04Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3351",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   not relevant for this question\r\n2. search the docs.    No\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\ni want to train my model but enounters an error.\r\n#### Code\r\nfairseq-train data-bin/iwslt14.tokenized.de-en --arch seq2seq_arch --encoder-dropout 0.2 --decoder-dropout 0.2 --optimizer adam --lr 0.005 --lr-shrink 0.5 --max-tokens 12000\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\nTraceback (most recent call last):\r\n  File \"G:\\Program Files\\PyvirtualEnv\\torchenv\\Scripts\\fairseq-train-script.py\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"e:\\learnfairseq\\fairseq\\fairseq_cli\\train.py\", line 477, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"e:\\learnfairseq\\fairseq\\fairseq\\distributed\\utils.py\", line 364, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"e:\\learnfairseq\\fairseq\\fairseq_cli\\train.py\", line 136, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(\r\n  File \"e:\\learnfairseq\\fairseq\\fairseq\\checkpoint_utils.py\", line 228, in load_checkpoint\r\n    epoch_itr = trainer.get_train_iterator(\r\n  File \"e:\\learnfairseq\\fairseq\\fairseq\\trainer.py\", line 532, in get_train_iterator\r\n    batch_iterator = self.task.get_batch_iterator(\r\n  File \"e:\\learnfairseq\\fairseq\\fairseq\\tasks\\fairseq_task.py\", line 285, in get_batch_iterator\r\n    batch_sampler = dataset.batch_by_size(\r\n  File \"e:\\learnfairseq\\fairseq\\fairseq\\data\\fairseq_dataset.py\", line 145, in batch_by_size\r\n    return data_utils.batch_by_size(\r\n  File \"e:\\learnfairseq\\fairseq\\fairseq\\data\\data_utils.py\", line 313, in batch_by_size\r\n    raise ValueError(\r\n\r\n#### What have you tried?\r\n1): I have tried pip install  --editable . and it did't work.\r\n2): When I tried this command python setup.py build_ext --inplace\r\nI got these message\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(465): warning C4814: 'c10::optional<at::Generator>::contained_val': in C++14 'constexpr' will not imply 'const'; consider explicitly specifying 'const'\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(466): error C2556: 'at::Generator &c10::optional<at::Generator>::contained_val(void) const &': overloaded function differs only by return type from 'const at::Generator &c10::optional<at::Generator>::contained_val(void) const &'\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(460): note: see declaration of 'c10::optional<at::Generator>::contained_val'\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(466): error C2373: 'c10::optional<at::Generator>::contained_val': redefinition; different type modifiers\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(460): note: see declaration of 'c10::optional<at::Generator>::contained_val'\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\ATen/core/TensorBody.h(694): error C2440: 'default argument': cannot convert from 'const c10::nullopt_t' to 'c10::optional<at::Generator>'\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\ATen/core/TensorBody.h(694): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be called\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\ATen/core/TensorBody.h(695): error C2440: 'default argument': cannot convert from 'const c10::nullopt_t' to 'c10::optional<at::Generator>'\r\nG:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\ATen/core/TensorBody.h(695): fatal error C1003: error count exceeds 100; stopping compilation\r\nerror: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\BIN\\\\x86_amd64\\\\cl.exe' failed with exit status 2\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version 1.6\r\n - OS (e.g., Linux): Win 10 64 bit\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install  --editable .\r\n - Python version: 3.8.6\r\n - CUDA/cuDNN version: 10.1/7.6.5\r\n - GPU models and configuration: rtx 2070s\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3351/comments",
    "author": "wongz97",
    "comments": [
      {
        "user": "zwhe99",
        "created_at": "2021-06-08T07:44:55Z",
        "body": "> ## ❓ Questions and Help\r\n> ### Before asking:\r\n> 1. search the issues.   not relevant for this question\r\n> 2. search the docs.    No\r\n> \r\n> #### What is your question?\r\n> i want to train my model but enounters an error.\r\n> \r\n> #### Code\r\n> fairseq-train data-bin/iwslt14.tokenized.de-en --arch seq2seq_arch --encoder-dropout 0.2 --decoder-dropout 0.2 --optimizer adam --lr 0.005 --lr-shrink 0.5 --max-tokens 12000\r\n> \r\n> Traceback (most recent call last):\r\n> File \"G:\\Program Files\\PyvirtualEnv\\torchenv\\Scripts\\fairseq-train-script.py\", line 33, in\r\n> sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n> File \"e:\\learnfairseq\\fairseq\\fairseq_cli\\train.py\", line 477, in cli_main\r\n> distributed_utils.call_main(cfg, main)\r\n> File \"e:\\learnfairseq\\fairseq\\fairseq\\distributed\\utils.py\", line 364, in call_main\r\n> main(cfg, **kwargs)\r\n> File \"e:\\learnfairseq\\fairseq\\fairseq_cli\\train.py\", line 136, in main\r\n> extra_state, epoch_itr = checkpoint_utils.load_checkpoint(\r\n> File \"e:\\learnfairseq\\fairseq\\fairseq\\checkpoint_utils.py\", line 228, in load_checkpoint\r\n> epoch_itr = trainer.get_train_iterator(\r\n> File \"e:\\learnfairseq\\fairseq\\fairseq\\trainer.py\", line 532, in get_train_iterator\r\n> batch_iterator = self.task.get_batch_iterator(\r\n> File \"e:\\learnfairseq\\fairseq\\fairseq\\tasks\\fairseq_task.py\", line 285, in get_batch_iterator\r\n> batch_sampler = dataset.batch_by_size(\r\n> File \"e:\\learnfairseq\\fairseq\\fairseq\\data\\fairseq_dataset.py\", line 145, in batch_by_size\r\n> return data_utils.batch_by_size(\r\n> File \"e:\\learnfairseq\\fairseq\\fairseq\\data\\data_utils.py\", line 313, in batch_by_size\r\n> raise ValueError(\r\n> \r\n> #### What have you tried?\r\n> 1): I have tried pip install --editable . and it did't work.\r\n> 2): When I tried this command python setup.py build_ext --inplace\r\n> I got these message\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(465): warning C4814: 'c10::optionalat::Generator::contained_val': in C++14 'constexpr' will not imply 'const'; consider explicitly specifying 'const'\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(466): error C2556: 'at::Generator &c10::optionalat::Generator::contained_val(void) const &': overloaded function differs only by return type from 'const at::Generator &c10::optionalat::Generator::contained_val(void) const &'\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(460): note: see declaration of 'c10::optionalat::Generator::contained_val'\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(466): error C2373: 'c10::optionalat::Generator::contained_val': redefinition; different type modifiers\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\c10/util/Optional.h(460): note: see declaration of 'c10::optionalat::Generator::contained_val'\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\ATen/core/TensorBody.h(694): error C2440: 'default argument': cannot convert from 'const c10::nullopt_t' to 'c10::optionalat::Generator'\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\ATen/core/TensorBody.h(694): note: No user-defined-conversion operator available that can perform this conversion, or the operator cannot be called\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\ATen/core/TensorBody.h(695): error C2440: 'default argument': cannot convert from 'const c10::nullopt_t' to 'c10::optionalat::Generator'\r\n> G:\\Program Files\\Python38\\lib\\site-packages\\torch\\include\\ATen/core/TensorBody.h(695): fatal error C1003: error count exceeds 100; stopping compilation\r\n> error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit status 2\r\n> \r\n> #### What's your environment?\r\n> * fairseq Version (e.g., 1.0 or master):\r\n> * PyTorch Version 1.6\r\n> * OS (e.g., Linux): Win 10 64 bit\r\n> * How you installed fairseq (`pip`, source): source\r\n> * Build command you used (if compiling from source): pip install  --editable .\r\n> * Python version: 3.8.6\r\n> * CUDA/cuDNN version: 10.1/7.6.5\r\n> * GPU models and configuration: rtx 2070s\r\n> * Any other relevant information:\r\n\r\nHi, Have you solved this error?"
      },
      {
        "user": "YBZh",
        "created_at": "2022-01-08T06:18:54Z",
        "body": "install the newest numpy version can solve this problem:\r\npip uninstall numpy\r\npip install numpy"
      }
    ]
  },
  {
    "number": 3348,
    "title": "Multilingual Translation - language-specific components",
    "created_at": "2021-03-12T15:24:16Z",
    "closed_at": "2021-03-18T18:55:10Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3348",
    "body": "## ❓ Questions and Help\r\n\r\nIs there any way of identifying the source/target language inside the `transformer_layer.py`. I'm trying to have a language specific layer inside each transformer layer when performing multilingual translation with the task `translation_multi_simple_epoch`. But to do that I need to know which is the current language-pair. Is it possible?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3348/comments",
    "author": "joaomcalves",
    "comments": [
      {
        "user": "joaomcalves",
        "created_at": "2021-03-16T11:10:09Z",
        "body": "no one knows?"
      },
      {
        "user": "felixkreuk",
        "created_at": "2021-08-29T14:57:46Z",
        "body": "@JoaoMCAlves , did you figure it out? I am also wondering about this.\r\nAn ugly way to do that would be to use the --encoder-langtok and --decoder-langtok. "
      }
    ]
  },
  {
    "number": 3345,
    "title": "How to use fairseq for unsupervised or semisupervised translation？",
    "created_at": "2021-03-12T11:33:26Z",
    "closed_at": "2022-03-25T14:43:46Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3345",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n#### What is your question?\r\nHow to use fairseq for unsupervised or unsupervised translation？\r\nI found the _SemisupervisedTranslationTask_. But i don't know how to use it.\r\n\r\n\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.10.2\r\n - PyTorch Version (e.g., 1.0):1.6\r\n - OS (e.g., Linux):Linux\r\n - How you installed fairseq (`pip`, source): pip \r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3345/comments",
    "author": "zwhe99",
    "comments": [
      {
        "user": "zwhe99",
        "created_at": "2021-03-12T11:35:34Z",
        "body": "@myleott "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-16T23:13:57Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3342,
    "title": "Wav2Vec 2.0 pretraining limited by CPU even on large machine",
    "created_at": "2021-03-11T13:47:51Z",
    "closed_at": "2024-05-27T17:04:05Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3342",
    "body": "I'm running wav2vec 2.0 pretraining on a DGX A100  and I seem to be CPU-limited which is a bit surprising given the amount of CPU resources the machine has. The GPUSs seem to be working at barely 50%. When I lower the GPU count to four I get basically the same updates / time unit but with higher GPU load per GPU.\r\n\r\nI have tried running with and without `+optimization.update_freq='[x]'` parameter with somewhat similar result. The CPU load is lower without it, bit GPU utilization is about the same.\r\n\r\nAny thoughts?\r\n\r\n**Setup**:\r\nNVIDIA DGX A100\r\n8 x A100 GPU\r\n2 x 64 core / 128 thread CPU\r\n1TB RAM\r\nUbuntu 20.04\r\nCode runs inside NVIDIA NGC container",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3342/comments",
    "author": "marma",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-16T23:13:59Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "Slyne",
        "created_at": "2022-05-10T14:06:41Z",
        "body": "same issue here. Can anyone share the GPU utilization ?"
      },
      {
        "user": "lubossmidl",
        "created_at": "2022-05-16T12:17:38Z",
        "body": "I have the same problem..."
      },
      {
        "user": "lubossmidl",
        "created_at": "2022-05-17T05:27:53Z",
        "body": "the training process seems to be over-optimized on large machine ...\r\ntry to use parameter OMP_NUM_THREADS=1\r\nlike\r\nOMP_NUM_THREADS=1 fairseq-train ...\r\n\r\n(8 x A100 GPU / 128 thread CPU: GPU utilization approx. 97-100% and CPU 8% instead of GPU 30% and 100% CPU)"
      },
      {
        "user": "marma",
        "created_at": "2024-05-27T17:04:05Z",
        "body": "Thank you @lubossmidl! I did not see this as I had moved on to other things. Closing issue.\r\n\r\nFunny story: we debugged a similar issue today and found this exact solution. I remembered this issue and went back to look at it. If only I had read you answer two years ago we would have saved a few hours :)"
      }
    ]
  },
  {
    "number": 3330,
    "title": "Train a wav2vec2.0 base model on my dataset, and the train_accuracy and val_accuracy  look terrible.",
    "created_at": "2021-03-09T07:14:23Z",
    "closed_at": "2022-05-01T02:22:00Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3330",
    "body": "## ❓ Questions and Help\r\n\r\nI train a wav2vec2.0 base model on my dataset，valid-percent set 0.05.\r\n\r\nAt epoch 20 the valid_accuracy: \"0.68451\", train_accuracy: \"0.81603\"\r\n[2021-03-09 04:44:18,059][fairseq.trainer][INFO] - begin training epoch 20\r\n[2021-03-09 04:44:18,059][fairseq_cli.train][INFO] - Start iterating over samples\r\n[2021-03-09 04:52:41,134][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\r\n[2021-03-09 04:53:15,141][valid][INFO] - {\"epoch\": 20, \"valid_loss\": \"4.602\", \"valid_ntokens\": \"6728.91\", \"valid_nsentences\": \"26.0111\", \"valid_prob_perplexity\": \"602.726\", \"valid_code_perplexity\": \"23.827\", \"valid_temp\": \"1.979\", \"valid_loss_0\": \"4.594\", \"valid_loss_1\": \"0.008\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.68451\", \"valid_wps\": \"24608.5\", \"valid_wpb\": \"6728.9\", \"valid_bsz\": \"26\", \"valid_num_updates\": \"2132\", \"valid_best_loss\": \"4.602\"}\r\n[2021-03-09 04:53:15,143][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 2132 updates\r\n[2021-03-09 04:53:15,144][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt\r\n[2021-03-09 04:53:18,698][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt\r\n[2021-03-09 04:53:20,712][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 2132 updates, score 4.602) (writing took 5.5689730532467365 seconds)\r\n[2021-03-09 04:53:20,713][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)\r\n[2021-03-09 04:53:20,713][train][INFO] - {\"epoch\": 20, \"train_loss\": \"6.662\", \"train_ntokens\": \"108047\", \"train_nsentences\": \"417.738\", \"train_prob_perplexity\": \"625.681\", \"train_code_perplexity\": \"47.601\", \"train_temp\": \"1.979\", \"train_loss_0\": \"6.658\", \"train_loss_1\": \"0.003\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.81603\", \"train_wps\": \"21299.4\", \"train_ups\": \"0.2\", \"train_wpb\": \"108047\", \"train_bsz\": \"417.7\", \"train_num_updates\": \"2132\", \"train_lr\": \"0.000458154\", \"train_gnorm\": \"0.005\", \"train_loss_scale\": \"64\", \"train_train_wall\": \"491\", \"train_gb_free\": \"23.1\", \"train_wall\": \"10968\"}\r\n\r\nbut at epoch 72 the valid_accuracy: \"0.92702\" is the best, but train_accuracy: \"0.00122\" is very low.\r\n[2021-03-09 12:15:17,535][fairseq.trainer][INFO] - begin training epoch 70\r\n[2021-03-09 12:15:17,535][fairseq_cli.train][INFO] - Start iterating over samples\r\n[2021-03-09 12:18:26,840][train_inner][INFO] - {\"epoch\": 70, \"update\": 69.355, \"loss\": \"6.658\", \"ntokens\": \"107885\", \"nsentences\": \"417.735\", \"prob_perplexity\": \"639.979\", \"code_perplexity\": \"517.661\", \"temp\": \"1.928\", \"loss_0\": \"6.658\", \"loss_1\": \"0\", \"loss_2\": \"0\", \"accuracy\": \"0.00104\", \"wps\": \"21240.7\", \"ups\": \"0.2\", \"wpb\": \"107885\", \"bsz\": \"417.7\", \"num_updates\": \"7400\", \"lr\": \"0.000323077\", \"gnorm\": \"0\", \"loss_scale\": \"512\", \"train_wall\": \"925\", \"gb_free\": \"23.1\", \"wall\": \"37674\"}\r\n[2021-03-09 12:23:43,935][fairseq_cli.train][INFO] - begin validation on \"valid\" subset\r\n[2021-03-09 12:24:12,892][valid][INFO] - {\"epoch\": 70, \"valid_loss\": \"1.97\", \"valid_ntokens\": \"6749.62\", \"valid_nsentences\": \"26.0111\", \"valid_prob_perplexity\": \"639.979\", \"valid_code_perplexity\": \"3.9\", \"valid_temp\": \"1.927\", \"valid_loss_0\": \"1.97\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.92702\", \"valid_wps\": \"31136.8\", \"valid_wpb\": \"6749.6\", \"valid_bsz\": \"26\", \"valid_num_updates\": \"7469\", \"valid_best_loss\": \"1.97\"}\r\n[2021-03-09 12:24:12,893][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 7469 updates\r\n[2021-03-09 12:24:12,894][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt\r\n[2021-03-09 12:24:16,075][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt\r\n[2021-03-09 12:24:18,091][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 70 @ 7469 updates, score 1.97) (writing took 5.1970200296491385 seconds)\r\n[2021-03-09 12:24:18,092][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)\r\n[2021-03-09 12:24:18,093][train][INFO] - {\"epoch\": 70, \"train_loss\": \"6.658\", \"train_ntokens\": \"107983\", \"train_nsentences\": \"417.766\", \"train_prob_perplexity\": \"639.979\", \"train_code_perplexity\": \"519.122\", \"train_temp\": \"1.927\", \"train_loss_0\": \"6.658\", \"train_loss_1\": \"0\", \"train_loss_2\": \"0\", \"train_accuracy\": \"0.00122\", \"train_wps\": \"21372\", \"train_ups\": \"0.2\", \"train_wpb\": \"107983\", \"train_bsz\": \"417.8\", \"train_num_updates\": \"7469\", \"train_lr\": \"0.000321308\", \"train_gnorm\": \"0\", \"train_loss_scale\": \"1024\", \"train_train_wall\": \"494\", \"train_gb_free\": \"23.1\", \"train_wall\": \"38025\"}",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3330/comments",
    "author": "Johumliu",
    "comments": [
      {
        "user": "heibaidaolx123",
        "created_at": "2021-05-13T03:46:43Z",
        "body": "Hi, have you found why?"
      },
      {
        "user": "jubick1337",
        "created_at": "2021-07-16T08:42:46Z",
        "body": "@heibaidaolx123 @Nemasery it is common issue if you try to run default config on hardware setup different to 64 V100\r\nYou should adjust config to your hardware setup or use ```optimization.update_freq``` to simulate setup.\r\nOtherwise some gradient errors happen during pretraining."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3315,
    "title": "wav2vec 2.0: L2 penalty on features",
    "created_at": "2021-03-06T08:08:47Z",
    "closed_at": "2021-03-29T20:04:23Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3315",
    "body": "\r\n\r\n#### What is your question?\r\nThe Wav2vec 2.0 paper just mentioned two losses, namely, contrastive loss and diversity loss. But in the implementation, I see there is also a `features_pen` loss, which is the L2 norm of features extracted from CNN. Moreover, this loss has a very large weight parameter (10, while only 0.1 for diversity loss) when pretraining.\r\n\r\nSo my question is,\r\nWhat’s the use of this `features_pen` loss?\r\nWhy it has such a large weight?\r\nIs there any ablation study about this loss?\r\n\r\n@alexeib ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3315/comments",
    "author": "yuanenming",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2021-03-29T20:04:23Z",
        "body": "this parameter is mostly to stabilize a model that uses the \"default\" extractor mode (i.e. prevent it from crashing late in training). it has very little effect on final accuracy. for new models, i recommend using task.normalize: true, extractor_mode: layer_norm and set the penalty weight for this parameter to 0 (i.e. disable it)"
      }
    ]
  },
  {
    "number": 3308,
    "title": "Single machine with multiple GPU raise Found at least two devices Error ",
    "created_at": "2021-03-04T19:41:50Z",
    "closed_at": "2022-04-17T23:21:28Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3308",
    "body": "I am running on a single machine with multiple GPU\r\n\r\n```\r\nand raise Error\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 300, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 87, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 130, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq/trainer.py\", line 400, in train_step\r\n    assert all(norm == prev_norms[0] for norm in prev_norms) or all(\r\n  File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq/trainer.py\", line 400, in <genexpr>\r\n    assert all(norm == prev_norms[0] for norm in prev_norms) or all(\r\n  File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/torch/tensor.py\", line 27, in wrapped\r\n    return f(*args, **kwargs)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!\r\n\r\n```\r\nBelow is my shell script\r\n\r\n```\r\n> GPU_ID=0,1,2,3\r\n> DATA_BIN_DIR=\"output/bin\"\r\n> OUT_DIR=\"output/models\"\r\n> BATCH_SIZE=128\r\n> MAX_TOKENS=10000\r\n> SEED=1\r\n> \r\n> CUDA_VISIBLE_DEVICES=\"${GPU_ID}\" python3 -m fairseq_cli.train \\\r\n>     ${DATA_BIN_DIR} \\\r\n>     --save-dir ${OUT_DIR} \\\r\n>     -a fconv \\\r\n>     --num-workers=4 --skip-invalid-size-inputs-valid-test \\\r\n>     --encoder-embed-dim 300 \\\r\n>     --decoder-embed-dim 300 \\\r\n>     --decoder-out-embed-dim 500 \\\r\n>     --encoder-layers '[(1024,3)] * 5' --decoder-layers '[(1024,3)] * 5' \\\r\n>     --dropout='0.2' --clip-norm=0.1 \\\r\n>     --optimizer nag --momentum 0.99 \\\r\n>     --lr-scheduler=reduce_lr_on_plateau --lr=0.25 --lr-shrink=0.1 --min-lr=1e-4 \\\r\n>     --max-epoch 100 \\\r\n>     --batch-size ${BATCH_SIZE} \\\r\n>     --max-tokens ${MAX_TOKENS} \\\r\n>     --seed ${SEED}\r\n>     \r\n```\r\n\r\n\r\nAm I missing anything?\r\n\r\npython version 3.7.6\r\nfairseq version 0.9.0\r\nCUDA Version: 11.0  \r\npyTorch VERSION: 1.7.1",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3308/comments",
    "author": "we29758143",
    "comments": [
      {
        "user": "Hexa4C",
        "created_at": "2021-04-09T03:36:43Z",
        "body": "Same error. Have you fixed this?"
      },
      {
        "user": "Hexa4C",
        "created_at": "2021-04-09T06:08:26Z",
        "body": "> I am running on a single machine with multiple GPU\r\n> \r\n> ```\r\n> and raise Error\r\n> -- Process 0 terminated with the following error:\r\n> Traceback (most recent call last):\r\n>   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n>     fn(i, *args)\r\n>   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 300, in distributed_main\r\n>     main(args, init_distributed=True)\r\n>   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 87, in main\r\n>     train(args, trainer, task, epoch_itr)\r\n>   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 130, in train\r\n>     log_output = trainer.train_step(samples)\r\n>   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq/trainer.py\", line 400, in train_step\r\n>     assert all(norm == prev_norms[0] for norm in prev_norms) or all(\r\n>   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq/trainer.py\", line 400, in <genexpr>\r\n>     assert all(norm == prev_norms[0] for norm in prev_norms) or all(\r\n>   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/torch/tensor.py\", line 27, in wrapped\r\n>     return f(*args, **kwargs)\r\n> RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!\r\n> ```\r\n> \r\n> Below is my shell script\r\n> \r\n> ```\r\n> > GPU_ID=0,1,2,3\r\n> > DATA_BIN_DIR=\"output/bin\"\r\n> > OUT_DIR=\"output/models\"\r\n> > BATCH_SIZE=128\r\n> > MAX_TOKENS=10000\r\n> > SEED=1\r\n> > \r\n> > CUDA_VISIBLE_DEVICES=\"${GPU_ID}\" python3 -m fairseq_cli.train \\\r\n> >     ${DATA_BIN_DIR} \\\r\n> >     --save-dir ${OUT_DIR} \\\r\n> >     -a fconv \\\r\n> >     --num-workers=4 --skip-invalid-size-inputs-valid-test \\\r\n> >     --encoder-embed-dim 300 \\\r\n> >     --decoder-embed-dim 300 \\\r\n> >     --decoder-out-embed-dim 500 \\\r\n> >     --encoder-layers '[(1024,3)] * 5' --decoder-layers '[(1024,3)] * 5' \\\r\n> >     --dropout='0.2' --clip-norm=0.1 \\\r\n> >     --optimizer nag --momentum 0.99 \\\r\n> >     --lr-scheduler=reduce_lr_on_plateau --lr=0.25 --lr-shrink=0.1 --min-lr=1e-4 \\\r\n> >     --max-epoch 100 \\\r\n> >     --batch-size ${BATCH_SIZE} \\\r\n> >     --max-tokens ${MAX_TOKENS} \\\r\n> >     --seed ${SEED}\r\n> >     \r\n> ```\r\n> \r\n> Am I missing anything?\r\n> \r\n> python version 3.7.6\r\n> fairseq version 0.9.0\r\n> CUDA Version: 11.0\r\n> pyTorch VERSION: 1.7.1\r\n\r\nI think the problem might be pytorch version. I met the same problem, downgrade pytorch to 1.4.0, then problem solved."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T00:05:27Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "lazerliu",
        "created_at": "2021-10-20T04:24:09Z",
        "body": "how can I fix it with pytorch version>=1.6.0?\r\n\r\n> > I am running on a single machine with multiple GPU\r\n> > ```\r\n> > and raise Error\r\n> > -- Process 0 terminated with the following error:\r\n> > Traceback (most recent call last):\r\n> >   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n> >     fn(i, *args)\r\n> >   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 300, in distributed_main\r\n> >     main(args, init_distributed=True)\r\n> >   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 87, in main\r\n> >     train(args, trainer, task, epoch_itr)\r\n> >   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq_cli/train.py\", line 130, in train\r\n> >     log_output = trainer.train_step(samples)\r\n> >   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq/trainer.py\", line 400, in train_step\r\n> >     assert all(norm == prev_norms[0] for norm in prev_norms) or all(\r\n> >   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/fairseq/trainer.py\", line 400, in <genexpr>\r\n> >     assert all(norm == prev_norms[0] for norm in prev_norms) or all(\r\n> >   File \"/home/w29758143/anaconda3/envs/pycorrector/lib/python3.7/site-packages/torch/tensor.py\", line 27, in wrapped\r\n> >     return f(*args, **kwargs)\r\n> > RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Below is my shell script\r\n> > ```\r\n> > > GPU_ID=0,1,2,3\r\n> > > DATA_BIN_DIR=\"output/bin\"\r\n> > > OUT_DIR=\"output/models\"\r\n> > > BATCH_SIZE=128\r\n> > > MAX_TOKENS=10000\r\n> > > SEED=1\r\n> > > \r\n> > > CUDA_VISIBLE_DEVICES=\"${GPU_ID}\" python3 -m fairseq_cli.train \\\r\n> > >     ${DATA_BIN_DIR} \\\r\n> > >     --save-dir ${OUT_DIR} \\\r\n> > >     -a fconv \\\r\n> > >     --num-workers=4 --skip-invalid-size-inputs-valid-test \\\r\n> > >     --encoder-embed-dim 300 \\\r\n> > >     --decoder-embed-dim 300 \\\r\n> > >     --decoder-out-embed-dim 500 \\\r\n> > >     --encoder-layers '[(1024,3)] * 5' --decoder-layers '[(1024,3)] * 5' \\\r\n> > >     --dropout='0.2' --clip-norm=0.1 \\\r\n> > >     --optimizer nag --momentum 0.99 \\\r\n> > >     --lr-scheduler=reduce_lr_on_plateau --lr=0.25 --lr-shrink=0.1 --min-lr=1e-4 \\\r\n> > >     --max-epoch 100 \\\r\n> > >     --batch-size ${BATCH_SIZE} \\\r\n> > >     --max-tokens ${MAX_TOKENS} \\\r\n> > >     --seed ${SEED}\r\n> > >     \r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Am I missing anything?\r\n> > python version 3.7.6\r\n> > fairseq version 0.9.0\r\n> > CUDA Version: 11.0\r\n> > pyTorch VERSION: 1.7.1\r\n> \r\n> I think the problem might be pytorch version. I met the same problem, downgrade pytorch to 1.4.0, then problem solved.\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:22Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:57Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "ayushbits",
        "created_at": "2022-09-16T14:07:38Z",
        "body": "Re-opening after 1 year .\r\n@lazerliu Were you able totrain on torch >1.6.0 ?"
      },
      {
        "user": "wangpichao",
        "created_at": "2023-10-18T00:51:01Z",
        "body": "@ayushbits how you solved the problem using higher torch version? Thank you. "
      }
    ]
  },
  {
    "number": 3298,
    "title": "AttributeError: 'Namespace' object has no attribute 'conv_feature_layers'",
    "created_at": "2021-03-03T06:37:46Z",
    "closed_at": "2022-04-17T22:21:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3298",
    "body": "I was reading \"Wav2Vec 2.0 Large (LV-60) + Self Training 960 hours\" pre-trained model using torch but I am getting following error-\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-12-a6c36b388893> in <module>\r\n      2 from fairseq.models.wav2vec import Wav2Vec2Model\r\n      3 cp=torch.load(\"/media/administrator/hdd/speech_recognition_wav2vec/wav2vec_vox_960h_pl.pt\")\r\n----> 4 model=Wav2Vec2Model.build_model(cp['args'],task=None)\r\n\r\n/media/administrator/hdd/speech_recognition_wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2.py in build_model(cls, cfg, task)\r\n    329         \"\"\"Build a new model instance.\"\"\"\r\n    330 \r\n--> 331         return cls(cfg)\r\n    332 \r\n    333     def apply_mask(self, x, padding_mask):\r\n\r\n/media/administrator/hdd/speech_recognition_wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2.py in __init__(self, cfg)\r\n    226         self.cfg = cfg\r\n    227 \r\n--> 228         feature_enc_layers = eval(cfg.conv_feature_layers)\r\n    229         self.embed = feature_enc_layers[-1][0]\r\n    230 \r\n\r\nAttributeError: 'Namespace' object has no attribute 'conv_feature_layers'\r\n\r\nI wrote the following code to access pre-trained model-\r\nimport torch\r\nfrom fairseq.models.wav2vec import Wav2Vec2Model\r\ncp=torch.load(\"/media/administrator/hdd/speech_recognition_wav2vec/wav2vec_vox_960h_pl.pt\")\r\nmodel=Wav2Vec2Model.build_model(cp['args'],task=None)\r\n\r\n**System Configuration**\r\n\r\n- PyTorch Version: 1.7.1\r\n- OS: Linux\r\n- Python Version: 3.8.0\r\n- CUDA Version: 10.2\r\n- Installed fairseq from source and followed instructions mentioned in README.md",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3298/comments",
    "author": "debadityamandal",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-16T23:14:24Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "jlmarrugom",
        "created_at": "2021-09-08T16:30:21Z",
        "body": "Hi, I solve it installing the version 0.10.1 of FairSeq.\r\n`pip install fairseq==0.10.1`"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:32:15Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3293,
    "title": "fairseq with Reinforcement Learning",
    "created_at": "2021-03-01T14:34:02Z",
    "closed_at": "2021-03-01T17:18:47Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3293",
    "body": "Do fairseq have RL support?\r\nIf yes, please direct me with a link to instructions. \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3293/comments",
    "author": "babangain",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-03-01T17:18:47Z",
        "body": "Fairseq does not support any RL applications that I know of."
      }
    ]
  },
  {
    "number": 3285,
    "title": "Wav2Vec 2: Data for training a new model",
    "created_at": "2021-02-26T10:50:21Z",
    "closed_at": "2021-03-01T09:46:57Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3285",
    "body": "Hi, \r\nIs it necessary that each speech audio file in training dataset contains only one speaker?\r\nThanks.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3285/comments",
    "author": "ThuPro27",
    "comments": [
      {
        "user": "olafthiele",
        "created_at": "2021-02-27T16:17:55Z",
        "body": "No, but they should be separated without overlapping. But you would need more material if finetuning."
      },
      {
        "user": "ThuPro27",
        "created_at": "2021-03-01T03:47:06Z",
        "body": "Thank you for your answer. So what are the requirements  for constructing a new training dataset?"
      },
      {
        "user": "olafthiele",
        "created_at": "2021-03-01T08:20:06Z",
        "body": "Check the paper to see what material they used. And it depends on your use case. If you want to transcribe phone calls, I wouldn't take spotless training files :-)"
      }
    ]
  },
  {
    "number": 3272,
    "title": "How can I use XLSR-53 with Transformers?",
    "created_at": "2021-02-24T09:58:14Z",
    "closed_at": "2021-03-01T23:01:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3272",
    "body": "Hi,\r\nis it possible to use the multilingual wav2vec model called \"XLSR-53\" with transformers?\r\nThanks\r\nPhilip",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3272/comments",
    "author": "PhilipMay",
    "comments": [
      {
        "user": "rodrigoheck",
        "created_at": "2021-02-26T17:11:26Z",
        "body": "My question as well."
      },
      {
        "user": "guillefix",
        "created_at": "2021-02-26T21:24:47Z",
        "body": "+1 Is it possible to use them for ASR? And if they need fine tuning how hard is it? how do we do it?"
      },
      {
        "user": "alexeib",
        "created_at": "2021-03-01T23:01:42Z",
        "body": "you might want to open this issue in the transformers github\r\n@patrickvonplaten "
      }
    ]
  },
  {
    "number": 3269,
    "title": "Is there any way to reduce the memory size of bin and model?",
    "created_at": "2021-02-23T14:21:13Z",
    "closed_at": "2021-03-01T15:23:52Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3269",
    "body": "When I inference the model, we need bin file(dictionary) and model(checkpoint.pt), however, these two files are too large. (~ 5G)\r\n\r\nIs there any way that I can reduce the size of these two files?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3269/comments",
    "author": "we29758143",
    "comments": [
      {
        "user": "olafthiele",
        "created_at": "2021-02-23T20:01:06Z",
        "body": "Both the dictionary and model carry all the information in an already somewhat packed way. You can build a smaller KenLM model, but it will have \"less\" information in it. As for the neural net itself, there might be some way of pruning it, but again you would loose information. But especially for the XSLR I am unsure whether there is another way. Anybody?"
      },
      {
        "user": "lematt1991",
        "created_at": "2021-03-01T15:23:52Z",
        "body": "You can try reducing the size of the dictionary.  This will reduce the dictionary size and the number of parameters in the model.  You can also try reducing the size of other parameters in the model"
      }
    ]
  },
  {
    "number": 3265,
    "title": "Preprocessing: help with parameter",
    "created_at": "2021-02-22T12:31:09Z",
    "closed_at": "2021-02-22T17:12:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3265",
    "body": "I'm training a couple of transformer for some translation tasks to make a research, but I'm not sure if the fairseq-preprocess command does what I want to. Specifically, I'm wondering about the parameter --tokenizer and --bpe.\r\n\r\nWhen we specify these, like --tokenizer moses, is the preprocessing going to tokenize, or we are just telling to the script that the data was already tokenized using the one indicated? I'm wondering the same for the parameter --bpe.\r\n\r\nOn top of that, do we need to give these two parameters again to the fairseq-train command right? \r\n\r\nI know it's probably a silly question, but I would like some clarification, as the documentation is a bit vague. \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3265/comments",
    "author": "fferlito",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-02-22T16:19:45Z",
        "body": "Hmm, I'm  not sure why these arguments are even visible for `fairseq-preprocess` as they seem to meant for use with the torch hub interface (Maybe @myleott  or @alexeib have more context on this?).  To clarify, you should apply tokenization and BPE encoding prior to calling `fairseq-preprocess`."
      },
      {
        "user": "fferlito",
        "created_at": "2021-02-22T16:27:57Z",
        "body": "@lematt1991 thanks a lot for the clarification. I had this doubt as the example for the translator use the moses library and the subword-nmt before using the fairseq library, but in the documentation they were given as possible parameters. \r\nI assume that I don't need to specify these parameter in the `fairseq-preprocess` and `fairseq-train` right?\r\n\r\nThanks a lot for your time! :)"
      },
      {
        "user": "lematt1991",
        "created_at": "2021-02-22T16:28:57Z",
        "body": "> I assume that I don't need to specify these parameter in the fairseq-preprocess and fairseq-train right?\r\n\r\nThat's correct"
      },
      {
        "user": "lematt1991",
        "created_at": "2021-02-22T17:12:06Z",
        "body": "Closing for now.  Please open a new issue if you are still having problems."
      }
    ]
  },
  {
    "number": 3263,
    "title": " No module named 'fairseq.visual'",
    "created_at": "2021-02-22T03:44:42Z",
    "closed_at": "2021-02-22T16:23:26Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3263",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):0.6.1\r\n - PyTorch Version (e.g., 1.0)：\r\n - OS (e.g., Linux):linux\r\n - How you installed fairseq (`pip`, source):source\r\n - Build command you used (if compiling from source):\r\n - Python version:3.7.3\r\n - CUDA/cuDNN version:10.0/~\r\n - GPU models and configuration:2060\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3263/comments",
    "author": "jhkd-kevin",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-02-22T16:23:26Z",
        "body": "Not enough context.  Please follow the instructions in the issue template"
      }
    ]
  },
  {
    "number": 3238,
    "title": "Speech Translation -> prep_covost_data.py",
    "created_at": "2021-02-12T02:48:26Z",
    "closed_at": "2021-02-15T01:02:37Z",
    "labels": [
      "question",
      "speech"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3238",
    "body": "## ❓ Questions and Help\r\n#### What is your question?\r\n\r\nIs this step needed for flac and mp3 files loaded by torchaudio.load() [sox backend] and default args ? \r\n\r\n#### Code \r\n`_waveform = waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers`\r\n `_waveform = _waveform.squeeze().numpy()`\r\n\r\n#### What's your environment?\r\n- PyTorch 1.7.1\r\n- torchaudio 0.7.2 ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3238/comments",
    "author": "pyyush",
    "comments": [
      {
        "user": "kahne",
        "created_at": "2021-02-13T05:25:37Z",
        "body": "Hi @pyyush , this also applies to FLAC and MP3 because `torchaudio.load()` always returns 16-bit floats (normalized to [-1, 1]). However, Kaldi uses 16-bit signed integers (in range of [-2^15, 2^15]). The conversion is still needed here."
      },
      {
        "user": "pyyush",
        "created_at": "2021-02-13T17:20:32Z",
        "body": "Hi @kahne,  thank you for the answer. So torchaudio.compliance.kaldi also uses 16-bit signed integers? The reason why I asked this is because I visualized one of the fbank after data prep and the last 10 (71-80) bins seemed odd. "
      },
      {
        "user": "kahne",
        "created_at": "2021-02-13T22:24:16Z",
        "body": "> Hi @kahne, thank you for the answer. So torchaudio.compliance.kaldi also uses 16-bit signed integers? The reason why I asked this is because I visualized one of the fbank after data prep and the last 10 (71-80) bins seemed odd.\r\n\r\nYes, `torchaudio.compliance.kaldi` is designed to have exactly the same inputs/outputs as the original Kaldi implementation."
      },
      {
        "user": "pyyush",
        "created_at": "2021-02-15T01:02:37Z",
        "body": "Okay, thanks."
      }
    ]
  },
  {
    "number": 3234,
    "title": "M2M 100 for low resource languages",
    "created_at": "2021-02-11T06:38:18Z",
    "closed_at": "2021-02-15T23:15:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3234",
    "body": "I'm trying to evaluate M2M 100 for some production tasks, but having issues for certain low resource languages like fr-wo. I'm getting good performance for high-resource pairs like en-zh so I'm just wondering if this is a known issue or if I'm doing something wrong.\r\n\r\nFor example:\r\n```\r\nfairseq-generate bin --batch-size 32 --path 1.2B_last_checkpoint.pt --fixed-dictionary model_dict.128k.txt -s fr -t wo --remove-bpe 'sentencepiece' --beam 5 --task translation_multi_simple_epoch --lang-pairs language_pairs_small_models.txt --decoder-langtok --encoder-langtok src --gen-subset test\r\n\r\nS-2\t__fr__ Maguire a émigré avec sa famille de l'Irlande au Maryland à l'âge de six ans, où il a étudié sous les jésuites au Collège Saint John, et est entré dans la Compagnie de Jésus en 1837.\r\nH-2\t-0.842350959777832\tMaguire so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee so amee.\r\n\r\nS-1\t__fr__ SpaceX construit une fusée dans la région reculée de Boca Chica, située dans le comté de Cameron, au Texas, aux États-Unis.\r\nH-1\t-2.058246374130249\tSpaceX biir rakett bi Boca Chica, biir Boca Chica, Cameron County, Texas, USA\r\n\r\nS-3\t__fr__ J'aime manger du melon\r\nH-3\t-4.000425815582275\tTél. : Tél.\r\n\r\nS-0\t__fr__ Je navigue sur Facebook tous les jours\r\nH-0\t-2.928954601287842\tLu Ma Mi Ju Vi Sa Do\r\n\r\nS-4\t__fr__ Je suis mongole\r\nH-4\t-4.857517719268799\tTembam\r\n```\r\nI've gotten similar results for these languages: wo uz tn ss sd or ns my ln lg kn kk gu ga ff br ast\r\nga was particularly interesting because the output tended to be NSFW.\r\n\r\nanyways, I wanted to ask if\r\n1. is this the expected result? (I chose fr-wo because it was specifically mentioned in the paper as having had human evalutation, but maybe those sentences were biased toward longer/more semantically dense input?)\r\n2. is there any documentation on which translation directions are low resource? There's appendix A figure 13, but it says 100 languages and only displays 34. And this appears to only be the dictionary coverage and not training data.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3234/comments",
    "author": "bmtm",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2021-02-15T23:15:17Z",
        "body": "Unfortunately, we acknowledge that many languages require significant work (see Discussion section). For languages with e.g. WMT/WAT focus, particularly we saw decreases in improvement in Czech, Burmese, and Chinese, but the majority of languages that require more attention are low resource. There are a large variety of reasons, from language identification systems that require improvement (we used fasttext, which was trained on Wikipedia + Commoncrawl) producing monolingual data that is not in the correct language, to mining challenges (LASER was trained on 93 languages, but some languages have very little data), to low resource languages having poor generalization, and general necessity to improve our understanding of how to model under-resourced languages. Even backtranslation, for example, can be challenging if the original model is not high quality. \r\n\r\nAround your specific points - I agree that our fr-wo human evaluator was quite generous, but given that I do not consider myself fluent in either language, we published their evaluations as-is. For ga, I will look into the challenges there - we did identify many sources that were not appropriate, but attempted to filter them out for most languages. I would guess there is a challenge in language identification here as well. For low resource languages, we are updating the paper with greater details for an update on arXiv, but essentially we tried to follow WMT20. In WMT20, Tamil and Inuktitut were both low-resource languages. Inuktitut had about 1million training sentences (including monolingual data for BT) and Tamil had 40M (including monolingual data for BT) - we decided to treat monolingual data similar to mined data given that BT and mining are both noisy sources of aligned data. So we considered anything with less than 50M mined training examples to be low resource. However, the languages you are mentioning in your list have far far below this, some even in the low thousands. \r\n\r\nOverall, I view this as an important area of progress - in our tables, I wanted to highlight that many important language directions have 0 BLEU (e.g. translating between different African languages) - because it's also relevant to indicate clearly what needs work. I hope that we can improve this, with much careful work, soon. "
      },
      {
        "user": "Jack000",
        "created_at": "2021-02-17T00:56:46Z",
        "body": "Thanks for the explanation. I think it might be possible to augment the BT with data from aligned fasttext vectors.\r\n\r\nfor now I'll stick to the high resource translation pairs."
      }
    ]
  },
  {
    "number": 3214,
    "title": "Key error while accessing batch_iterator.first_batch",
    "created_at": "2021-02-04T10:33:02Z",
    "closed_at": "2021-02-22T22:31:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3214",
    "body": "## ❓ Questions and Help\r\nI get a key errior after loading the train dataset\r\n\r\n#### Code\r\nUsing lstm_lm model in fairseq\r\n\r\n#### What have you tried?\r\ntrain using fairseq-train command\r\n\r\n#### What's your environment?\r\n - PyTorch Version (e.g., 1.0)1.6.0\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): pip install -e .\r\n - Python version: 3\r\n\r\n####stack trace:\r\nTraceback (most recent call last):\r\n  File \"fairseq_280/venv/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"fairseq_280/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"fairseq_280/fairseq/distributed_utils.py\", line 301, in call_main\r\n    main(args, **kwargs)\r\n  File \"fairseq_280/fairseq_cli/train.py\", line 114, in main\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"fairseq_280/fairseq/checkpoint_utils.py\", line 212, in load_checkpoint\r\n    epoch=1, load_dataset=True, **passthrough_args\r\n  File \"fairseq_280/fairseq/trainer.py\", line 382, in get_train_iterator\r\n    self.reset_dummy_batch(batch_iterator.first_batch)\r\n  File \"fairseq_280/fairseq/data/iterators.py\", line 288, in first_batch\r\n    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])\r\n  File \"fairseq_280/fairseq/data/iterators.py\", line 288, in <listcomp>\r\n    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])\r\n  File \"fairseq_280/fairseq/data/monolingual_dataset.py\", line 104, in __getitem__\r\n    source, future_target, past_target = self.dataset[index]\r\n  File \"fairseq_280/fairseq/data/token_block_dataset.py\", line 125, in __getitem__\r\n    [self.dataset[idx] for idx in range(start_ds_idx, end_ds_idx + 1)]\r\n  File \"fairseq_280/fairseq/data/token_block_dataset.py\", line 125, in <listcomp>\r\n    [self.dataset[idx] for idx in range(start_ds_idx, end_ds_idx + 1)]\r\n  File \"fairseq_280/fairseq/data/indexed_dataset.py\", line 234, in __getitem__\r\n    ptx = self.cache_index[i]\r\nKeyError: 11761183",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3214/comments",
    "author": "krishnanNuance",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2021-02-12T20:51:48Z",
        "body": "havent seen this before, maybe try rebuilding the dataset using the latest fairseq version's preprocess.py?"
      },
      {
        "user": "lematt1991",
        "created_at": "2021-02-22T22:31:02Z",
        "body": "Closing due to inactivity "
      },
      {
        "user": "NonvolatileMemory",
        "created_at": "2021-03-04T15:12:59Z",
        "body": "this bug usually caused by you use an old version preprocess.py to process the data then use the 0.10 version to train"
      },
      {
        "user": "AllaeddineD",
        "created_at": "2022-07-06T14:34:35Z",
        "body": "> this bug usually caused by you use an old version preprocess.py to process the data then use the 0.10 version to train\r\n\r\nThank you ! I spent some time trying to find a solution, then I found your comment. The issue was the version of preprocess.py. It's solved now :) "
      }
    ]
  },
  {
    "number": 3201,
    "title": "How to test pretrained LM on a new dataset",
    "created_at": "2021-02-02T15:25:42Z",
    "closed_at": "2021-06-29T09:31:45Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3201",
    "body": "What is the correct way to evaluate a language modeling pretrained model, using a dataset A, on a dataset B? \r\nI ask this question because the vocab dict is not necessary compatible when moving from one dataset to another.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3201/comments",
    "author": "omarfoq",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2021-02-12T20:58:40Z",
        "body": "you would either \r\na) re-encode teh data you are testing on using the vocab used for training or\r\nb) use --dataset-impl raw and point at a unencoded text file. you would still use the dictionary from training"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-28T12:36:30Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3200,
    "title": "question about pre-processing wiki for pre-training roberta",
    "created_at": "2021-02-02T13:16:02Z",
    "closed_at": "2022-08-25T13:28:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3200",
    "body": "hi,\r\n\r\ni think there are two approaches to pre-processing wiki for pre-training roberta:\r\n\r\n1) do **sentence segmentation** first with one sentence per line, keep an empty line between two documents, then the input will be\r\n```\r\n<s> sentence1 </s> sentence2 </s> sentence3 </s> ....\r\n```\r\n2) just keep an empty line between two documents. then the input will be\r\n```\r\n<s> sentence1 sentence2 sentence3 </s> sentence4 sentence5 sentence6 </s>\r\n```\r\nwhich approach should i choose?\r\n\r\nthanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3200/comments",
    "author": "dugu9sword",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-20T13:27:03Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 3186,
    "title": "When using the default arguments, what is the criterion for fairseq-train to stop training?",
    "created_at": "2021-01-31T03:41:50Z",
    "closed_at": "2021-02-08T04:29:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3186",
    "body": "#### What is your question?\r\nWhen using the default arguments, that is, when (--max-epoch, --max-update, --stop-time-hours, --stop-min-lr) are not set, when will fairseq-train stop training?\r\n#### What's your environment?\r\n\r\n - fairseq Version (0.9.0):\r\n - PyTorch Version (1.5)\r\n - OS (Linux):\r\n - How you installed fairseq (source):\r\n - Build command you used (if compiling from source):pip Install -e\r\n - Python version: 3.7\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3186/comments",
    "author": "zwhe99",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-02-01T15:51:43Z",
        "body": "Try it out.  In general it is not recommended to not set any of these parameters"
      },
      {
        "user": "lematt1991",
        "created_at": "2021-02-01T16:00:46Z",
        "body": "It depends on your training data.  If you feel that it has stopped prematurely (i.e. your loss is still improving), then you can start another training run and resume from your latest checkpoint using the `--restore-file <path to checkpoint>` flag.  If you see that your model is still training, but your loss/validation metrics aren't improving, then you can kill the training job."
      }
    ]
  },
  {
    "number": 3174,
    "title": "Expected translation model training speed-up with apex",
    "created_at": "2021-01-28T12:09:59Z",
    "closed_at": "2021-01-29T14:40:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3174",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nHi all,\r\n\r\nwhat is the expected training speed-up once apex is activated?\r\nI installed the apex library as recommend, however, I don't see any speed-up for my setup.\r\n\r\n#### What have you tried?\r\n\r\nI compared two setups, one with apex library installed, the other without.\r\nThe command looks like: \r\n` fairseq-train data-bin --log-format simple --log-interval 100 --save-dir /output --max-tokens 7168 --update-freq 1                                          --num-workers 1 --arch transformer --optimizer adam --adam-betas '(0.9,0.98)' --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 --lr  0.0007 --min-lr 1e-09 --dropout 0.1 --attention-dropout 0.0 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.0 --fp16 --seed 2 --max-epoch 0 --task translation  --ddp-backend=no_c10d --max-update 80000 --save-interval-updates 2000 --no-epoch-checkpoints --share-all-embeddings --criterion label_smoothed_cross_entropy -distributed-world-size 32 --distributed-backend 'nccl'  --distributed-init-method 'tcp://XXX' --distributed-no-spawn`\r\n\r\nSuccessful installation is confirmed with:\r\n`Running setup.py install for apex: finished with status 'done'`\r\n`(self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)`\r\n`2021-01-27 15:52:48 | INFO | fairseq.optim.adam | using FusedAdam`\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: 0.10.1\r\n - PyTorch Version: 1.8.0.dev20210127 (with conda)\r\n - OS: Linux\r\n - How you installed fairseq: `pip install --editable .`\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1.243/7.6.3\r\n - GPU models and configuration: 32x V100 (4x 8 GPU nodes)\r\n - Any other relevant information: cudatoolkit(-dev for nvcc) was installed with conda\r\n\r\n\r\nWhat do I miss? \r\nAny experience with apex and a medium-sized transformer-based NMT setup?\r\n\r\nThanks for your help!\r\n\r\nCheers,\r\nStephan",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3174/comments",
    "author": "stephanpeitz",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-01-28T20:22:21Z",
        "body": "CC @myleott "
      },
      {
        "user": "myleott",
        "created_at": "2021-01-29T13:56:56Z",
        "body": "It should be a few percent faster. Here's what I get on a single machine with 8xV100:\r\n\r\n**command**\r\n```\r\npython -m fairseq_cli.train --task dummy_mt --dataset-size 10000000 \\\r\n  --arch transformer_wmt_en_de_big --max-tokens 4096 \\\r\n  --optimizer adam --lr 0.0001 \\\r\n  --log-format simple --log-interval 25 \\\r\n  --no-save --max-update 500 \\\r\n  --fp16\r\n```\r\n\r\n**without apex**:\r\n```\r\n2021-01-29 05:49:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\r\n2021-01-29 05:49:33 | INFO | train_inner | epoch 001:     26 / 9192 loss=4.831, ppl=28.46, wps=171455, ups=5.25, wpb=32640, bsz=1088, num_updates=25, lr=0.0001, gnorm=8.71, loss_scale=64, train_wall=6, gb_free=20.9, mb_free=21445.4, wall=28\r\n2021-01-29 05:49:37 | INFO | train_inner | epoch 001:     51 / 9192 loss=0.006, ppl=1, wps=186646, ups=5.72, wpb=32640, bsz=1088, num_updates=50, lr=0.0001, gnorm=0.105, loss_scale=64, train_wall=4, gb_free=20.9, mb_free=21445.4, wall=33\r\n2021-01-29 05:49:42 | INFO | train_inner | epoch 001:     76 / 9192 loss=0, ppl=1, wps=186931, ups=5.73, wpb=32640, bsz=1088, num_updates=75, lr=0.0001, gnorm=0.007, loss_scale=64, train_wall=4, gb_free=20.9, mb_free=21445.4, wall=37\r\n2021-01-29 05:49:46 | INFO | train_inner | epoch 001:    101 / 9192 loss=0, ppl=1, wps=186816, ups=5.72, wpb=32640, bsz=1088, num_updates=100, lr=0.0001, gnorm=0.003, loss_scale=64, train_wall=4, gb_free=20.9, mb_free=21445.4, wall=41\r\n```\r\n\r\n**with apex**:\r\n```\r\n2021-01-29 05:55:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\r\n2021-01-29 05:55:27 | INFO | train_inner | epoch 001:     26 / 9192 loss=4.831, ppl=28.46, wps=179591, ups=5.5, wpb=32640, bsz=1088, num_updates=25, lr=0.0001, gnorm=8.833, loss_scale=64, train_wall=6, gb_free=20.9, mb_free=21444.9, wall=28\r\n2021-01-29 05:55:31 | INFO | train_inner | epoch 001:     51 / 9192 loss=0.006, ppl=1, wps=193285, ups=5.92, wpb=32640, bsz=1088, num_updates=50, lr=0.0001, gnorm=0.109, loss_scale=64, train_wall=4, gb_free=20.9, mb_free=21444.9, wall=32\r\n2021-01-29 05:55:36 | INFO | train_inner | epoch 001:     76 / 9192 loss=0, ppl=1, wps=193434, ups=5.93, wpb=32640, bsz=1088, num_updates=75, lr=0.0001, gnorm=0.007, loss_scale=64, train_wall=4, gb_free=20.9, mb_free=21444.9, wall=36\r\n2021-01-29 05:55:40 | INFO | train_inner | epoch 001:    101 / 9192 loss=0, ppl=1, wps=184334, ups=5.65, wpb=32640, bsz=1088, num_updates=100, lr=0.0001, gnorm=0.003, loss_scale=64, train_wall=4, gb_free=20.9, mb_free=21444.9, wall=41\r\n2021-01-29 05:55:44 | INFO | train_inner | epoch 001:    126 / 9192 loss=0, ppl=1, wps=193405, ups=5.93, wpb=32640, bsz=1088, num_updates=125, lr=0.0001, gnorm=0.002, loss_scale=64, train_wall=4, gb_free=20.9, mb_free=21444.9, wall=45\r\n2021-01-29 05:55:48 | INFO | train_inner | epoch 001:    151 / 9192 loss=0, ppl=1, wps=193741, ups=5.94, wpb=32640, bsz=1088, num_updates=150, lr=0.0001, gnorm=0.002, loss_scale=64, train_wall=4, gb_free=20.9, mb_free=21444.9, wall=49\r\n```"
      },
      {
        "user": "stephanpeitz",
        "created_at": "2021-01-29T14:40:29Z",
        "body": "Hey,\r\n\r\nthanks for the quick and detailed answer!\r\nLooks like 3-4% in your case. I observe 1-2%.\r\nMaybe the speed-up is smaller since I train already on 32 GPUs.\r\n\r\nCheers,\r\nStephan"
      }
    ]
  },
  {
    "number": 3172,
    "title": "Multi accent Speech to Text model",
    "created_at": "2021-01-28T04:02:49Z",
    "closed_at": "2022-05-02T21:22:19Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3172",
    "body": "Hi everyone,\r\n\r\nCan we use wav2vec 2.0 to build a multi accent S2T model?\r\nexample: I want to build an ASR model that can transcribe US and Indian accent English language. I have 3-4k hours of labelled data for each accent. Is it possible to build a single model for these accents which can give me good enough WER(<15)?\r\n\r\nAny suggestion would be of great help!\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3172/comments",
    "author": "bharat-patidar",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2021-01-29T01:32:37Z",
        "body": "have you tried finetuning the released librivox model on your data? should already work reasonably well. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T02:04:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3170,
    "title": "Fairseq Model Attention Visualization",
    "created_at": "2021-01-27T08:14:01Z",
    "closed_at": "2021-01-27T15:45:42Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3170",
    "body": "Hi, \r\n\r\nI want to ask whether fairseq model can show the Attention visualization? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3170/comments",
    "author": "14H034160212",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-01-27T15:45:42Z",
        "body": "There isn't any support for visualizations, but you should be able to easily extract the attention values returned by `extract_features` and visualize them yourself"
      }
    ]
  },
  {
    "number": 3159,
    "title": "How can I print out the changes index in prediction?",
    "created_at": "2021-01-22T20:59:35Z",
    "closed_at": "2021-01-25T16:25:59Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3159",
    "body": "For example:\r\nInput: This is a cat.\r\nPrediction: This is a dog.\r\n\r\nAs the model changed the original sentence cat -> dog.\r\nWhere can I get through to get index information like\r\n\"[index:[2, 3], 'cat', 'dog']\"",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3159/comments",
    "author": "we29758143",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-01-25T16:25:59Z",
        "body": "There's not enough context here to help.  Please follow the issue templates and a reproducible example of the issue you are experiencing.  "
      }
    ]
  },
  {
    "number": 3154,
    "title": "How to train a Large-from scratch Model?",
    "created_at": "2021-01-22T11:04:14Z",
    "closed_at": "2022-05-02T21:22:06Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3154",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n**I have trained a pre-train Large model (Model-Y1) with my own unlabeled data, and finetuned it(Model-Y1) with my own labeled data. Now I get a finetuned model(Model-Y2)**\r\n**I want to know how much Model-Y2 benifits from Model-Y1. So I try to train a model from scratch（without pretrain）.**\r\n\r\n#### What have you tried?\r\n\r\nI trained a pre-train model with a few data for one epoch.So I get a fake pre-trained model(Model-F1).Then I finetuned Model-F1 with my own data to get Model-F2.\r\n\r\nthe command I use:\r\n`python train.py ${train_data} \\\r\n--no-pretrained-weights --feature-grad-mult 1 --freeze-finetune-updates 0 \\\r\n--save-dir ${save_dir} --valid-subset ${valid_subset} --w2v-path ${w2v_path} \\\r\n--distributed-world-size 16 --distributed-port 44675 --max-tokens 1920000 \\\r\n--max-update 320000 --warmup-steps 32000 --hold-steps 128000 --decay-steps 160000 \\\r\n--post-process letter --best-checkpoint-metric wer --num-workers 6 \\\r\n--sentence-avg --task audio_pretraining --arch wav2vec_ctc --labels ltr \\\r\n--optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr 3e-05 --lr-scheduler tri_stage \\\r\n--final-lr-scale 0.05 --final-dropout 0.0 --dropout 0.0 --activation-dropout 0.1 --criterion ctc \\\r\n--apply-mask --mask-selection static --mask-other 0 --mask-length 10 --mask-prob 0.5 \\\r\n--mask-channel-selection static --mask-channel-other 0 --mask-channel-length 64 --mask-channel-prob 0.256 \\\r\n--layerdrop 0.1 --zero-infinity --save-interval 1 --validate-interval 1 \\\r\n--attention-dropout 0.0 --seed 2337 --log-format json --log-interval 200 --ddp-backend no_c10d \\\r\n--skip-invalid-size-inputs-valid-test`\r\n\r\nInfering by viterbi, Model-F2 get 40% WER in libri-speech test-other while Model-Y2 get 5.2%.I think something went wrong in Model-F2.\r\nI don't know how to train a Large - from scratch Model, anyone could help?\r\n\r\n#### What's your environment?\r\n - fairseq Version (0.10.1)\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3154/comments",
    "author": "SheepAndWolf",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T03:04:39Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3127,
    "title": "Slow training speed of Levenshtein Transformer in pre-training with translation_lev",
    "created_at": "2021-01-11T14:08:09Z",
    "closed_at": "2022-05-02T21:22:11Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3127",
    "body": "Hi! \r\nWhen I increased the number of nodes for the pre-training of Levenshtein transformer (from 100x V100 to 400x V100), wpb increased as the larger nodes but wps was not increased, so the training speed is still slow. \r\n\r\n#### Ex. of 100x V100\r\n`mask_ins=1.533, word_ins=7.425, word_del=0.486, ppl=696.28, wps=110918, ups=0.31, wpb=356820, bsz=29030.6`\r\n#### Ex. of 400x V100\r\n`mask_ins=1.642, word_ins=8.158, word_del=0.552, ppl=1307.41, wps=128540, ups=0.09, wpb=1.38257e+06, bsz=112702`\r\n\r\nThe nat_loss or adding noises are the bottleneck of the performance? How to improve the training performance? Current training speed is too slow and the loss/ppl performance is too poor comparing to the standard transformer with the translation task (#2208).\r\n\r\nAny helps? \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.5 (Anaconda3)\r\n - CUDA/cuDNN version: 10.2/8.0\r\n - GPU models and configuration: 100x or 400 x V100 16GB (4x V100 / node)\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3127/comments",
    "author": "h-sugi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T03:04:34Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3123,
    "title": "Error when evaluate a finetuned model",
    "created_at": "2021-01-10T04:03:46Z",
    "closed_at": "2021-01-12T16:42:37Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3123",
    "body": "I finetuned a wav2vec model :\r\nCUDA_VISIBLE_DEVICES=0 fairseq-hydra-train   task.data=/data/dev-clean-2 model.w2v_path=/data/wav2vec_small.pt distributed_training.distributed_world_size=1 +optimization.update_freq='[24]'  --config-direxamples/wav2vec/config/finetuning   --config-name base_10m\r\n\r\n\r\nwhen i run:\r\n python examples/speech_recognition/infer.py /data/dev-clean-2/ --task audio_pretraining --nbest 1 --path outputs/2021-01-09/08-48-56/checkpoints/checkpoint_best.pt --gen-subset valid --results-path /data/result --w2l-decoder viterbi  --criterion ctc --labels ltr --max-tokens 2000000 --post-process letter\r\n\r\nINFO:__main__:Namespace(all_gather_list_size=16384, autoregressive=False, azureml_logging=False, batch_size=None, batch_s\r\nize_valid=None, beam=5, beam_size_token=100, beam_threshold=25.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, cri\r\nterion='ctc', curriculum=0, data='/data/dev-clean-2/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, dump_emissions=None, dump_features=None, empty_cache_freq=0, enable_padding=False, eos=2, eval_wer=False, eval_wer_post_process='letter', eval_wer_tokenizer=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', heartbeat_timeout=-1, iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_dec\r\node_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, keep_best_checkpoints=-1, keep_interv\r\nal_updates=-1, keep_last_epochs=-1, kenlm_model=None, kspmodel=None, labels='ltr', lenpen=1, lexicon=None, lm_path=None,lm_weight=0.0, load_checkpoint_on_all_dp_ranks=False, load_emissions=None, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sample_size=None, max_tokens=2000000, max_tokens_valid=2000000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_sample_size=None, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, normalize=False, nprocs_per_node=2, num_shards=1, num_workers=1, optimizer=None, optimizer_overrides='{}', pad=1, path='outputs/2021-01-09/08-48-56/checkpoints/checkpoint_best.pt', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, post_process='letter', prefix_size=0, print_alignment=None, print_step=False, profile=False, quantization_config_path=None, quiet=False, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path='/data/result', retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, rnnt_decoding_type='greedy', rnnt_len_penalty=-0.5, sacrebleu=False,sample_rate=16000, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, scoring='bleu', seed=1, shard_id=0, sil_weight=0.0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, task='audio_pretraining', temperature=1.0, tensorboard_\r\nlogdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', unit_lm=False, unk=3, unk_weight\r\n=-inf, unkpen=0, unnormalized=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1,\r\nvalidate_interval_updates=0, w2l_decoder='viterbi', wandb_project=None, warmup_updates=0, wer_args=None, wer_kenlm_model=None, wer_lexicon=None, wer_lm_weight=2.0, wer_word_score=-1.0, wfstlm=None, word_score=1.0, zero_infinity=False, zero_sharding='none')\r\nINFO:__main__:| decoding with criterion ctc\r\nINFO:__main__:| loading model(s) from outputs/2021-01-09/08-48-56/checkpoints/checkpoint_best.pt\r\nTraceback (most recent call last):\r\n  File \"examples/speech_recognition/infer.py\", line 428, in <module>\r\n    cli_main()\r\n  File \"examples/speech_recognition/infer.py\", line 424, in cli_main\r\n    main(args)\r\n  File \"examples/speech_recognition/infer.py\", line 240, in main\r\n    task.load_dataset(args.gen_subset, task_cfg=saved_cfg.task)\r\n  File \"/root/fairseq/fairseq/tasks/audio_pretraining.py\", line 139, in load_dataset\r\n    sample_rate=task_cfg.sample_rate,\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 297, in __getattr__\r\n    self._format_and_raise(key=key, value=None, cause=e)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 629, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 295, in __getattr__\r\n    return self._get_impl(key=key, default_value=DEFAULT_VALUE_MARKER)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 353, in _get_impl\r\n    node = self._get_node(key=key)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 375, in _get_node\r\n    self._validate_get(key)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/dictconfig.py\", line 128, in _validate_get\r\n    key=key, value=value, cause=ConfigAttributeError(msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/base.py\", line 101, in _format_and_raise\r\n    type_override=type_override,\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 694, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/usr/local/lib/python3.6/dist-packages/omegaconf/_utils.py\", line 610, in _raise\r\n    raise ex  # set end OC_CAUSE=1 for full backtrace\r\nomegaconf.errors.ConfigAttributeError: Key 'sample_rate' is not in struct\r\n        full_key: task.sample_rate\r\n        reference_type=Any\r\n        object_type=dict\r\n\r\n\r\nI tried wav2vec_small_960h.pt, it works.\r\nIs there something wrong with my fintuning？\r\n\r\n - fairseq Version : 1.0.0a0+01fcec5\r\n - PyTorch Version 1.7.1\r\n - OS: ubuntu \r\n - How you installed fairseq :pip\r\n - Python version:3.6.9\r\n - CUDA/cuDNN version: cuda 10.0\r\n - GPU models and configuration: titan rtx\r\n - Any other relevant information: in docker \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3123/comments",
    "author": "indifferen",
    "comments": [
      {
        "user": "AMOHYZ",
        "created_at": "2021-01-10T15:39:24Z",
        "body": "I'm facing the same problem"
      },
      {
        "user": "AMOHYZ",
        "created_at": "2021-01-13T13:49:41Z",
        "body": "@indifferen \r\nDid you solve the problem ?"
      },
      {
        "user": "indifferen",
        "created_at": "2021-01-13T14:01:24Z",
        "body": "> @indifferen\r\n> Did you solve the problem ?\r\n\r\nadd parameters to finetune config file: \r\n\r\ntask:\r\n  _name: audio_pretraining\r\n  data:???\r\n  max_sample_size: 250000\r\n  min_sample_size: 32000\r\n  **sample_rate: 16000\r\n  autoregressive: false**\r\n\r\nthen finetuning your model.\r\n(if you get other errors, just adding these parameters to the finetune config file)\r\n#3005 \r\n"
      }
    ]
  },
  {
    "number": 3121,
    "title": "backtranslation in fairseq/from extract_bt_data.py",
    "created_at": "2021-01-09T07:21:17Z",
    "closed_at": "2022-05-02T21:22:01Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3121",
    "body": "## ❓ Questions and Help\r\n\r\nThe question is from fairseq/examples/backtranslation/extract_bt_data.py. \r\nIt depicts in the code as follow:\r\n    parser.add_argument('--srclang', required=True, help='source language (extracted from H-* lines)')\r\n    parser.add_argument('--tgtlang', required=True, help='target language (extracted from S-* lines)')\r\nIs it right? \r\nIf I back-translate from Spanish to Chinese, which means i only have Spanish sentences, without Chinese\r\n references. So the source language \"S\" showed in generate-test.txt which is generated by fairseq-generate is Spanish, and target language \"H\" is Chinese.\r\nBut extract_bt_data is in contrast. srclang--H and tgtlang--S.\r\nIt results in the bt_data which generated by extract_bt_data.py is adverse too. Spanish sentences in bt_data.zh and Chinese sentences in bt_data.es.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3121/comments",
    "author": "wang304381190",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T03:04:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:32Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3119,
    "title": "No 'step' key in Adam optimizer state when training on PyTorch 1.7",
    "created_at": "2021-01-08T17:52:52Z",
    "closed_at": "2021-01-19T20:26:23Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3119",
    "body": "Hello,\r\n\r\nI would like to ask a question (and possibly report a potential bug as well).\r\n\r\nAfter upgrading to PyTorch 1.7, I frequently had a memory issue with my trainings:\r\n\r\n```\r\nFile \"/home/user/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 943, in all_reduce\r\n            work = group.allreduce([tensor], opts)work = group.allreduce([tensor], opts)work = group.allreduce([tensor], opts)\r\nMemoryErrorMemoryErrorMemoryError: : : std::bad_allocstd::bad_allocstd::bad_alloc\r\n```\r\n\r\n**This issue happened randomly**: for an exact same configuration (same model, training data, and everything else) across different runs: sometimes the job runs well and sometimes there is a memory error or socket timeout error.\r\n\r\n**After downgrading back to 1.6, I no longer observed this issue. However, I was not able to resume trainings that I had started on 1.7** because of the following error:\r\n\r\n```\r\nFile \"/home/user/code/fairseq/fairseq/optim/adam.py\", line 199, in step\r\n    self.optimizer.step(closure)\r\n  File \"/home/user/code/fairseq/fairseq/optim/adam.py\", line 199, in step\r\n    state[\"step\"] += 1\r\n        state[\"step\"] += 1KeyErrorstate[\"step\"] += 1\r\n: \r\n'step'KeyError\r\n: KeyError'step': \r\n'step'\r\n    state[\"step\"] += 1\r\nKeyError: 'step'\r\n```\r\n\r\nI have manually checked the checkpoints saved by 1.7 and found that their optimizer states indeed do not have the key `step`, while the 1.6 checkpoints do.\r\n\r\n**Could you please tell me how I can continue my trainings on PyTorch 1.6 by resuming from the checkpoints saved by PyTorch 1.7?**\r\n\r\nThank you so much in advance for your help! \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): `'1.0.0a0+65d88f1'`\r\n - PyTorch Version (e.g., 1.0): 1.6 and 1.7\r\n - OS (e.g., Linux): Linux (RedHat 8)\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable ./`\r\n - Python version: 3.7.5\r\n - CUDA/cuDNN version: 10.1.2/7.6.5\r\n - GPU models and configuration: Tesla V100\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3119/comments",
    "author": "formiel",
    "comments": [
      {
        "user": "parkitny",
        "created_at": "2023-08-17T07:25:41Z",
        "body": "What's the resolution to this issue?"
      }
    ]
  },
  {
    "number": 3118,
    "title": "Padding problem with short utterances with Wav2Vec2 ",
    "created_at": "2021-01-08T16:03:36Z",
    "closed_at": "2022-05-02T21:22:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3118",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nLet's consider two types of inputs:\r\n1) 100ms of speech at 16kHz: [t0,...,tn]\r\n2) this very same speech file but with zero padding at the end: [t1,....,tn,0,0,...,0,0]\r\n \r\nI give both sequences to Wav2vec2 (in eval mode) and I get completely different kind of features. \r\nMy question is: How to pad short utterances so that the zero padding does not impact the wav2vec2 features ? \r\n(I need this padding to finetune wav2vec2 on a specific dataset of variable length utterances)\r\n\r\nThe code below shows that the sum of the features extracted from 1) are differents than for 2)\r\n(I applied a masking in the second case).\r\nNote that using  CPC, I get what I want: both extracted features are identical, meaning that this problem is specific to Wav2vec2.\r\n \r\n\r\n#### Code\r\n\r\nimport torch\r\nimport fairseq\r\nfrom torch import nn\r\n\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(['wav2vec_small.pt'])\r\nmodel = model[0]\r\nmodel.eval()\r\n\r\n\r\nwinput = torch.randn(1,3000) # no padding mode\r\nc = model(winput,mask=False,features_only=True)['x'].squeeze()\r\ns=torch.sum(torch.abs(c),dim=1)\r\nprint(torch.sum(s)) # here I sum the embedding value and get 1654.80\r\n\r\n\r\nz = torch.zeros(1,3000) # padding mode\r\nwinput=torch.cat((winput,z),dim=1)\r\nc = model(winput,mask=False,features_only=True)['x'].squeeze()\r\nmask=torch.ones(c.size(0))\r\nmask[9:]=0\r\nmask=mask.reshape(-1,1)\r\nc=c*mask\r\ns=torch.sum(torch.abs(c),dim=1)\r\nprint(torch.sum(s))    # here I sum the embedding value and get 1577.70\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version 1.0\r\n - PyTorch Version 1.7\r\n - Linux\r\n - How you installed fairseq : source\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.0\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3118/comments",
    "author": "RobinAlgayres",
    "comments": [
      {
        "user": "anogkongda",
        "created_at": "2021-01-11T07:09:54Z",
        "body": "a suggestion is to\r\nconstruct padding mask, a sample code is in\r\nfairseq/data/audio/raw_audio_dataset.py: line 81 - 91 \r\n\r\nand when you wanna forward, use\r\nc = model(input,mask=padding_mask, features_only=True)['x']"
      },
      {
        "user": "RobinAlgayres",
        "created_at": "2021-01-11T11:34:18Z",
        "body": "Hi,\r\nWell it does not really solve the problem. I tried giving a padding_mask to the model and the features embeddings are still quite different. Here is the code using a padding mask.\r\n\r\nBasically I create a wav segment, I pad it with zeros, it goes through the convolutional encoder, it gets multiplied by the padding mask, go through the context network, and then I re-multiply it by the mask to show that the output is different than my previous experiment with no padding. \r\n\r\nz = torch.zeros(1,3000)\r\nwinput = torch.randn(1,3000)\r\nwinput=torch.cat((winput,z),dim=1)                                                                                                  \r\nmask=torch.ones((1,18)) # 18 is the expected size\r\nmask[:,9:]=0 # half of the input is actual speech\r\npad = mask < 1 # padding mask must be boolean\r\nc = model(winput,padding_mask=pad,mask=False,features_only=True)['x'].squeeze()\r\nc=c*(mask.reshape(-1,1)) \r\ns=torch.sum(torch.abs(c),dim=1)\r\nprint(torch.sum(s))\r\n\r\nThe thing is in Wav2Vec2 there is no difference between the context network and the predictor (as it is the case in CPC), which means that I do not think it is possible to solve the issue of short utterance embedding... \r\nAny ideas? "
      },
      {
        "user": "alexeib",
        "created_at": "2021-01-26T17:53:44Z",
        "body": "the way you are constructing padding mask seems to set the padded timesteps to False and non-padded to True, but in fairseq these masks work the other way (True for padded timesteps, False otherwise)"
      },
      {
        "user": "RobinAlgayres",
        "created_at": "2021-01-26T18:42:42Z",
        "body": "I actually set to True the padded timesteps.\r\nHere is a print of my padded mask for the tensor 'winput':\r\n\r\ntensor([[False, False, False, False, False, False, False, False, False,  True,\r\n          True,  True,  True,  True,  True,  True,  True,  True]])\r\n          "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T03:04:30Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:45Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "abarcovschi",
        "created_at": "2022-12-02T14:35:06Z",
        "body": "Hello @RobinAlgayres are there any updates on this issue? Did you get it working?"
      }
    ]
  },
  {
    "number": 3109,
    "title": "How to know if it works？Wav2Vec2.0 ",
    "created_at": "2021-01-06T09:30:07Z",
    "closed_at": "2021-01-09T07:11:16Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3109",
    "body": "## ❓ Questions and Help\r\n\r\nI try to use the code on my own data set. \r\n\r\nFrom the training logs, how can I know if it is working properly or the current performance?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3109/comments",
    "author": "Z-yq",
    "comments": [
      {
        "user": "jubick1337",
        "created_at": "2021-01-06T23:17:28Z",
        "body": "Could you specify what are you trying to do?\r\nFinetuning wav2vec2 or pretraining?"
      },
      {
        "user": "Z-yq",
        "created_at": "2021-01-07T09:08:52Z",
        "body": "@jubick1337 Thanks for help!\r\n\r\nI'm using it for pre-training and finetuning  now.\r\n\r\nFor pre-training:\r\n\r\nI tried two of my own data sets:\r\nData-1  logs:\r\n\r\n> [train_inner][INFO] - {\"epoch\": 1, \"update\": 0.885, \"loss\": \"6.658\", \"ntokens\": \"948.255\", \"nsentences\": \"18.06\", \"prob_perplexity\": \"639.979\", \"code_perplexity\": \"353.274\", \"temp\": \"0.65\", \"loss_0\": \"6.658\", \"loss_1\": \"0\", \"loss_2\": \"0\", \"accuracy\": \"0.42083\", \"wps\": \"2841.1\", \"ups\": \"3\", \"wpb\": \"948.3\", \"bsz\": \"18.1\", \"num_updates\": \"225000\", \"lr\": \"0.00047568\", \"gnorm\": \"0\", \"loss_scale\": \"4096\", \"train_wall\": \"30\", \"wall\": \"86018\"}\r\n> [fairseq_cli.train][INFO] - begin validation on \"valid\" subset\r\n> [valid][INFO] - {\"epoch\": 1, \"valid_loss\": \"6.16\", \"valid_ntokens\": \"814.394\", \"valid_nsentences\": \"18.8167\", \"valid_prob_perplexity\": \"639.979\", \"valid_code_perplexity\": \"37.608\", \"valid_temp\": \"0.649\", \"valid_loss_0\": \"6.16\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.66425\", \"valid_wps\": \"19902.9\", \"valid_wpb\": \"814.4\", \"valid_bsz\": \"18.8\", \"valid_num_updates\": \"225000\", \"valid_best_loss\": \"4.028\"}\r\n\r\n\r\nData-2  logs:\r\n\r\n> [train_inner][INFO] - {\"epoch\": 1, \"update\": 0.212, \"loss\": \"6.658\", \"ntokens\": \"1699.01\", \"nsentences\": \"5.135\", \"prob_perplexity\": \"639.979\", \"code_perplexity\": \"516.391\", \"temp\": \"0.506\", \"loss_0\": \"6.658\", \"loss_1\": \"0\", \"loss_2\": \"0\", \"accuracy\": \"0.00085\", \"wps\": \"6074\", \"ups\": \"3.58\", \"wpb\": \"1699\", \"bsz\": \"5.1\", \"num_updates\": \"275000\", \"lr\": \"0.00046938\", \"gnorm\": \"0\", \"loss_scale\": \"8\", \"train_wall\": \"51\", \"wall\": \"80966\"}\r\n> [fairseq_cli.train][INFO] - begin validation on \"valid\" subset\r\n> [valid][INFO] - {\"epoch\": 1, \"valid_loss\": \"5.053\", \"valid_ntokens\": \"1684.31\", \"valid_nsentences\": \"5.23539\", \"valid_prob_perplexity\": \"639.98\", \"valid_code_perplexity\": \"19.384\", \"valid_temp\": \"0.506\", \"valid_loss_0\": \"5.053\", \"valid_loss_1\": \"0\", \"valid_loss_2\": \"0\", \"valid_accuracy\": \"0.66461\", \"valid_wps\": \"10098.3\", \"valid_wpb\": \"1684.3\", \"valid_bsz\": \"5.2\", \"valid_num_updates\": \"275000\", \"valid_best_loss\": \"0.756\"}\r\n\r\nTheir training accuracy are quite the opposite, so which values should I pay attention to to to confirm whether the training is effective。\r\n\r\nFor Finetunning:\r\nI tried it on Data-2,and logs:\r\n\r\n> [train_inner][INFO] - {\"epoch\": 1, \"update\": 0.908, \"loss\": \"198.911\", \"ntokens\": \"11334.6\", \"nsentences\": \"729.6\", \"nll_loss\": \"12.804\", \"wps\": \"1003.1\", \"ups\": \"0.09\", \"wpb\": \"11334.6\", \"bsz\": \"729.6\", \"num_updates\": \"630\", \"lr\": \"1.60925e-05\", \"gnorm\": \"345.954\", \"loss_scale\": \"0.25\", \"train_wall\": \"5\", \"wall\": \"6975\"}\r\n \r\nSimilarly, which value should I focus on?"
      },
      {
        "user": "jubick1337",
        "created_at": "2021-01-07T09:25:38Z",
        "body": "For pretraining part look at ```valid_loss``` in ```[valid][INFO]```\r\nFor fine-tuning based on this logs validation have not passed yet but it should be similar in structure as pretraining. So it will have  ```valid_loss``` in ```[valid][INFO]``.\r\nBasically you want validation loss to decrease as time goes on. \r\nI can't tell you what loss value is good because it depends on many things."
      }
    ]
  },
  {
    "number": 3106,
    "title": "AttributeError: 'NoneType' object has no attribute 'task'",
    "created_at": "2021-01-05T22:39:07Z",
    "closed_at": "2022-05-02T10:21:56Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3106",
    "body": "Hi, when i try to generate translation from trained mode  with command:\r\n```\r\nfairseq-interactive output --path checkpoints/fconv/checkpoint1.pt --beam 5 --cpu --task translation --optimizer sgd\r\n```\r\nI get error:\r\n```\r\nAttributeError: 'NoneType' object has no attribute 'task'\r\n```\r\nI tried this on Ubunu 20.04, Google Colab and i get the same error.\r\nI am using the newest version of fairseq\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3106/comments",
    "author": "wiktor7245",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:29Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "Joshwlks",
        "created_at": "2021-09-14T14:59:41Z",
        "body": "bump\r\nI am getting the same issue"
      },
      {
        "user": "wying8349",
        "created_at": "2021-09-24T03:01:04Z",
        "body": "Same issue. Any suggestions?"
      },
      {
        "user": "Ramlinbird",
        "created_at": "2021-10-11T03:09:57Z",
        "body": "Same issue. Any suggestions?"
      },
      {
        "user": "masus04",
        "created_at": "2021-11-11T12:04:37Z",
        "body": "Has there been any progress on this?\r\n\r\nIn my case I took over a repository in which the model is loaded using the following:\r\n`fairseq.checkpoint_utils.load_model_ensemble_and_task(filepath, arg_overrides=arg_dict)`\r\n\r\nThe `arg_overrides` dict contains a key 'task' which at some point in the library is written to state[args], which seems to be None.\r\nBut why would state[args] be None?"
      },
      {
        "user": "Joshwlks",
        "created_at": "2021-11-11T12:29:52Z",
        "body": "I managed to solve this by installing Fairseq via the Github Repo rather than via Pip Install."
      },
      {
        "user": "masus04",
        "created_at": "2021-11-11T12:44:02Z",
        "body": "> I managed to solve this by installing Fairseq via the Github Repo rather than via Pip Install.\r\n\r\nAre you using a different version from GitHub than is available from Pip?"
      },
      {
        "user": "Joshwlks",
        "created_at": "2021-11-11T13:47:29Z",
        "body": "> Are you using a different version from GitHub than is available from Pip?\r\n\r\nFrom what I remember the GitHub version is quite a bit more up to date as the last release to Pip was Jan 5\r\n"
      },
      {
        "user": "Old-Young233",
        "created_at": "2021-12-05T01:32:29Z",
        "body": "bump\r\nI am getting the same issue"
      },
      {
        "user": "masus04",
        "created_at": "2021-12-05T09:27:09Z",
        "body": "Installing with pip from GitHub did the trick for me."
      },
      {
        "user": "Old-Young233",
        "created_at": "2021-12-05T09:33:37Z",
        "body": "> \r\nThe problem I ran into was that I trained a translation model in giuhub's 0.10.1 download, and then tried to use the model in a paper's open source code, but  appeared when loading `AttributeError:'NoneType' object has no attribute 'task'`\r\n"
      },
      {
        "user": "S-GeGe",
        "created_at": "2022-01-04T05:55:26Z",
        "body": "How did you solve this problem? @wiktor7245 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T13:21:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:27Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "Lihwnlp",
        "created_at": "2024-01-05T05:39:24Z",
        "body": "Check to see if it is a version issue, which can occur when the fairseq version of the training model produces a different fairseq version of the translation"
      },
      {
        "user": "tqzhou75",
        "created_at": "2024-09-13T09:43:50Z",
        "body": "@Lihwnlp Is there a suitable version"
      },
      {
        "user": "tqzhou75",
        "created_at": "2024-10-14T15:00:30Z",
        "body": "I had the same problem   AttributeError: 'NoneType' object has no attribute 'task'"
      }
    ]
  },
  {
    "number": 3098,
    "title": "training speed about fairseq",
    "created_at": "2021-01-05T09:12:54Z",
    "closed_at": "2021-01-05T19:06:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3098",
    "body": "## ❓ Questions and Help\r\nOn the machine A, wps=120000, on another machine B, wps=60000, what's wrong about this?\r\n\r\n# What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.10.1（A and B）\r\n - PyTorch Version (e.g., 1.0)：1.7.1 （A）1.7.0（B）\r\n - OS (e.g., Linux): Linux Ubuntu16.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.7.5\r\n - CUDA/cuDNN version: 11.0(A) 10.2(B)\r\n - GPU models and configuration: transformer, 6gpus\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3098/comments",
    "author": "double22a",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2021-01-05T19:06:06Z",
        "body": "It seems like the only difference is PyTorch 1.7.0 and PyTorch 1.7.1? Perhaps there's some improvement in 1.7.1 that makes training faster?\r\n\r\nWithout more details about your setup/commands run, etc., it's impossible to say for sure :)"
      }
    ]
  },
  {
    "number": 3082,
    "title": "Using Predicted Text from Fairseq-Interactive",
    "created_at": "2020-12-31T00:37:02Z",
    "closed_at": "2022-05-02T20:22:23Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3082",
    "body": "Hi,\r\n\r\nI'm trying to modify the Fairseq-interactive script I can use it's prediction to input into my own small 'algorithm' first and then display the text (I know I'll need to fork the repo). After searching the script, I've found it hard to pinpoint where the 'predicted' text variable is. I may have glossed over it because I'm doing it in a rush due to time constraints. I'd appreciate any help on this.\r\n\r\n**Edit**\r\nI think I may have found it. If anyone could confirm, is it `detok_hyp_str`?\r\n\r\n - fairseq Version: 0.9.0\r\n - How you installed fairseq: source\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3082/comments",
    "author": "JustCunn",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3065,
    "title": "Could not append to config. An item is already at 'optimization.update_freq'.",
    "created_at": "2020-12-23T12:40:56Z",
    "closed_at": "2022-05-02T20:22:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3065",
    "body": "When I followed the README \r\n```\r\n$ fairseq-hydra-train \\\r\n    task.data=/path/to/data \\\r\n    --config-dir /path/to/fairseq-py/examples/wav2vec/config/pretraining \\\r\n    --config-name wav2vec2_base_librispeech\r\n```\r\nto train a wav2vec base model, I got this error: Could not append to config. An item is already at 'optimization.update_freq'.\r\n\r\nWhat does this mean and how can I fix it? I didn't find any related results on google so I came to ask here. Thank you for the answers in advance.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3065/comments",
    "author": "18445864529",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3063,
    "title": "training doesn't begin for inputs with large size ",
    "created_at": "2020-12-23T10:03:12Z",
    "closed_at": "2022-05-02T20:22:19Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3063",
    "body": "I am running the fairseq-train command for a bart_large model. I have 8 gpus with 32gb each. However, when each file exceeds the size of max-tokens, it seems the training doesn't begin. Note that I am using truncate-source. However, I am wondering if there is a good way to ignore training data points that exceed the max-tokens-size?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3063/comments",
    "author": "darsh10",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3054,
    "title": "hub_interface",
    "created_at": "2020-12-21T08:21:27Z",
    "closed_at": "2022-05-02T20:22:14Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3054",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nsorry, May I have a question?\r\nWhat is the use of ‘hub_interface.py’？\r\nwhat the different between \"last_layer_features\" and ''transformer_out''  in the following code?\r\n\r\n\r\n#### Code\r\nfrom fairseq.models.roberta import XLMRModel\r\nxlmr = XLMRModel.from_pretrained(pretrained_path)\r\nmodel = xlmr.model\r\nlast_layer_features = xlmr.extract_features(inputs_ids)\r\ntransformer_out, _ = model(inputs_ids, features_only=True)\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3054/comments",
    "author": "charlesfufu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3050,
    "title": "Load_model_ensemble_and_task() gives error for multiple models",
    "created_at": "2020-12-20T09:45:49Z",
    "closed_at": "2020-12-25T18:27:41Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3050",
    "body": "\r\n#### What is your question?\r\n\r\nHi, I am having problems loading pretrained models. I used the code given in the readme file, and I have tried it for two models, but the load_model_ensemble_and_task()  function is raising different errors for both of them.\r\n\r\n**When I try to load \"wav2vec_large.pt\" model, I get** \r\n\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-3-ca5356f4acbd> in <module>\r\n----> 1 model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp])\r\n\r\n~/fairseq/checkpoint_utils.py in load_model_ensemble_and_task(filenames, arg_overrides, task, strict, suffix, num_shards)\r\n    277             if not PathManager.exists(filename):\r\n    278                 raise IOError(\"Model file not found: {}\".format(filename))\r\n--> 279             state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n    280             if shard_idx == 0:\r\n    281                 args = state[\"args\"]\r\n\r\n~/fairseq/checkpoint_utils.py in load_checkpoint_to_cpu(path, arg_overrides)\r\n    230         for arg_name, arg_val in arg_overrides.items():\r\n    231             setattr(args, arg_name, arg_val)\r\n--> 232     state = _upgrade_state_dict(state)\r\n    233     return state\r\n    234 \r\n\r\n~/fairseq/checkpoint_utils.py in _upgrade_state_dict(state)\r\n    432 \r\n    433     # set any missing default values in the task, model or other registries\r\n--> 434     registry.set_defaults(state[\"args\"], tasks.TASK_REGISTRY[state[\"args\"].task])\r\n    435     registry.set_defaults(state[\"args\"], models.ARCH_MODEL_REGISTRY[state[\"args\"].arch])\r\n    436     for registry_name, REGISTRY in registry.REGISTRIES.items():\r\n\r\nKeyError: 'speech_pretraining' \r\n\r\n **And when I try to load \"wav2vec_small_960h.pt\", I get:**\r\n\r\nRuntimeError: Error(s) in loading state_dict for Wav2VecCtc:\r\n\tsize mismatch for w2v_encoder.proj.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\r\n\tsize mismatch for w2v_encoder.proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\r\n\r\n#### Code\r\n\r\nimport torch\r\nimport fairseq\r\n\r\ncp = \"wav2vec_large.pt\"\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp])\r\n\r\n#### What's your environment?\r\nI am trying this in a container which is created from the jupyter/base-notebook image.\r\n\r\n - fairseq Version: 0.10.1\r\n - PyTorch Version (e.g., 1.0): 1.7.0\r\n - How you installed fairseq (`pip`, source): pip \r\n - Python version: 3.8.6\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3050/comments",
    "author": "myazann",
    "comments": [
      {
        "user": "ajmssc",
        "created_at": "2020-12-24T19:08:30Z",
        "body": "try `pip install soundfile git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d`"
      },
      {
        "user": "myazann",
        "created_at": "2020-12-25T18:27:41Z",
        "body": "> try `pip install soundfile git+git://github.com/pytorch/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d`\r\n\r\nThis works, thanks."
      }
    ]
  },
  {
    "number": 3049,
    "title": "TypeError: forward() missing 1 required positional argument: 'tgt_tokens' Levenshtein Transformer",
    "created_at": "2020-12-19T16:34:13Z",
    "closed_at": "2022-04-18T01:21:01Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3049",
    "body": "Like the title, I experience this error when the model does its first forward pass when training a machine translation model;\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/fairseq_cli/train.py\", line 296, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/fairseq_cli/train.py\", line 86, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/fairseq_cli/train.py\", line 127, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/fairseq/trainer.py\", line 330, in train_step\r\n    sample, self.model, self.criterion, self.optimizer, ignore_grad\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/fairseq/tasks/fairseq_task.py\", line 251, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/fairseq/criterions/label_smoothed_cross_entropy.py\", line 56, in forward\r\n    net_output = model(**sample['net_input'])\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 511, in forward\r\n    output = self.module(*inputs[0], **kwargs[0])\r\n  File \"/ichec/home/users/aistrigh/.conda/envs/jupyter_tools/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\nTypeError: forward() missing 1 required positional argument: 'tgt_tokens'\r\n\r\n```\r\n This is my config;\r\n```\r\nfairseq-train data-bin-0507-clean/AistNMT \\\r\n\t--lr 5e-4 \\\r\n\t--lr-scheduler inverse_sqrt \\\r\n\t--optimizer adam\\\r\n  \t--clip-norm 0.1 \\\r\n\t--dropout 0.1 \\\r\n\t--max-tokens 4096 \\\r\n  \t--arch levenshtein_transformer \\\r\n        --encoder-learned-pos \\\r\n        --decoder-learned-pos \\\r\n\t--fp16 \\\r\n        --criterion label_smoothed_cross_entropy \\\r\n        --encoder-layers 6 \\\r\n        --decoder-layers 6 \\\r\n        --adam-betas '(0.9,0.98)' \\\r\n        --label-smoothing 0.1 \\\r\n        --adam-eps 10e-9 \\\r\n        --warmup-updates 4000 \\\r\n        --warmup-init-lr '1e-07' \\\r\n```\r\n - fairseq Version (e.g., 1.0 or master): 1.0\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux): linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3049/comments",
    "author": "JustCunn",
    "comments": [
      {
        "user": "h-sugi",
        "created_at": "2021-01-06T08:34:00Z",
        "body": "try --criterion nat_loss and --task translation_lev. Maybe you should build_ext in fairseq directory.\r\nThe nat_loss option seems to be a key."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T03:04:57Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "Onkar-2803",
        "created_at": "2021-08-24T07:22:22Z",
        "body": "bump"
      },
      {
        "user": "seongjunyun",
        "created_at": "2021-11-16T17:50:28Z",
        "body": "bump"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:15Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:31Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3040,
    "title": "Will other pretrained XLSR models mentioned in the paper \"Unsupervised Cross-lingual Representation Learning for speech recognition\" be released?",
    "created_at": "2020-12-16T13:02:29Z",
    "closed_at": "2022-05-02T20:22:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3040",
    "body": "It is great that the multilingual pre-trained wav2vec 2.0 (XLSR) model (xlsr_53_56k.pt) is released. The pretraining dataset is huge, which contains Multilingual LibriSpeech, CommonVoice and Babel. \r\n\r\nHowever, I am wondering whether the models pretrained  by only one dataset can be released as well, for example, the XLSR model pretrained with Babel dataset, or just the pretrained models used in the xlsr paper(Unsupervised Cross-lingual Representation Learning for speech recognition).  It will be more convenient for us to compare the results.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3040/comments",
    "author": "JINGZIjingzi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:55Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:40Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3038,
    "title": "Trying to extract pretrained weights from Levenshtein Transformer's Decoder",
    "created_at": "2020-12-16T05:42:52Z",
    "closed_at": "2022-05-02T20:22:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3038",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHi. I'm trying to extract weights from the levenshtein transformer's Decoder, especially the part of inserting placeholders.\r\nI want to use fairseq pretrained weights to build a function that gets two words as an input and returns the proper number of placeholders between them. \r\nBut I can't figure it out how can I get the pretrained weights. Can you help me with that?\r\nThanks in advance. \r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3038/comments",
    "author": "yeounyi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:57Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3033,
    "title": "mBART w/ Arabic source language",
    "created_at": "2020-12-14T16:55:44Z",
    "closed_at": "2022-05-02T20:22:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3033",
    "body": "Hi --\r\n\r\nI was able to ~ reproduce the results of fine-tuning mBART for English->Romanian translation.  I'm interested in trying to reproduce some of the other results in the paper.\r\n\r\nI'm curious whether anything needs to be done in either the preprocessing or training scripts to account for the fact that Arabic is written right-to-left instead of left-to-right.  I'm assuming I should be using the same approach that was used in mBART pre-training, but unfortunately I don't think the source code for that is available (yet?).\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3033/comments",
    "author": "bkj",
    "comments": [
      {
        "user": "bkj",
        "created_at": "2020-12-14T16:57:47Z",
        "body": "@myleott maybe you're the right person to direct the question to? \r\nOr @sshleifer? I've seen you do a lot of inspection of the mBART model as well.\r\n\r\nThanks!"
      },
      {
        "user": "sshleifer",
        "created_at": "2020-12-16T02:59:42Z",
        "body": "I don't know the answer, but excited to learn!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:58Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3031,
    "title": "causal convolution in ConvFeatureExtractionModel wav2vec",
    "created_at": "2020-12-13T21:51:50Z",
    "closed_at": "2022-04-29T16:22:29Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3031",
    "body": "Would you please explain why in wav2vec causal convolution(adding padding on the left) is only implemented in the feature aggregator but not the feature extraction?\r\n\r\nthe paper mentions both encoder and context network implement causal conv.\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3031/comments",
    "author": "tsinggggg",
    "comments": [
      {
        "user": "SarthakYadav",
        "created_at": "2021-05-22T16:55:27Z",
        "body": "+1. I have the same question. @tsinggggg did you manage to find an answer elsewhere?"
      },
      {
        "user": "jjjjohnson",
        "created_at": "2021-06-22T02:26:05Z",
        "body": "+1 ... Not sure why..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:53Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:22:01Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3030,
    "title": "fairseq-generate Ensemble with different source directory",
    "created_at": "2020-12-12T13:56:38Z",
    "closed_at": "2022-05-02T20:22:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3030",
    "body": "I have one dataset. I have preprocessed them by two different methods. And saved them into data-bin/data1 & data-bin/data2 folder. I trained two models with the same architecture in data-bin/model1.pt & data-bin/model2.pt  \r\n\r\n\r\nNow, to generate translation,\r\n\r\n`\r\nfairseq-generate data-bin/data1 --path data-bin/model1.pt:data-bin/model2.pt   \\\r\n--beam 5 --batch-size 128 --remove-bpe --source-lang de --target-lang en --task translation | tee /tmp/gen.out.models.4 \r\n`\r\nWe can only use one data-directory.\r\n\r\nI want model 1 to use data1 & model2 to use data2. \r\nIs this possible?\r\nNote: data1 & data2 are the same but preprocessed in different ways. They use the same dict & Byte-pair-encoded with the same bpecode\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3030/comments",
    "author": "babangain",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:34Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3016,
    "title": "Fairseq stucks when resuming the training procedure by using --restore-file (multi-gpu)",
    "created_at": "2020-12-10T10:57:12Z",
    "closed_at": "2020-12-24T08:20:58Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3016",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI first trained a`transformer_big` model on 8 GPUs for some epochs, then I stoped the training procedure. When I tried to resume the training procedure on 8 GPUs by specifying `--restore-file`, `fairseq` stucked. The GPU-Util of all 8 GPUs became 100%, and Memory-Usage of each one is 8373MiB / 16160MiB.\r\n\r\nI tried to decrease the `max_tokens`, but it did not help. I had no issue when I resume the training procedure with the same configurations on a single GPU.\r\n\r\n`fairseq` stuck at the beginning.\r\n\r\nepoch 014:   0%|                                              | 0/6372 [00:00<?, ?it/s]\r\n2020-12-10 18:51:31 | INFO | fairseq.trainer | begin training epoch 14\r\n\r\n#### Code\r\n\r\n```bash\r\npython3 fairseq/fairseq_cli/train.py data-bin/*** --arch transformer_wmt_en_de_big --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 8000 --lr 5e-4 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --weight-decay 0.0 --max-tokens 1024 --save-dir checkpoints/*** --eval-bleu --eval-bleu-args '{\"beam\": 4, \"lenpen\": 0.6}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --alpha 1.0 --update-freq 6 --ddp-backend=no_c10d --skip-invalid-size-inputs-valid-test --restore-file checkpoints/***\r\n```\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 1.0.0a0+b58f4f0\r\n - PyTorch Version (e.g., 1.0): 1.7.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip install --editable ./\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3016/comments",
    "author": "gpengzhi",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-12-11T14:58:50Z",
        "body": "Can you try `--load-checkpoint-on-all-dp-ranks`? We made some recent changes to checkpoint loading that seem to be problematic. That flag should revert to the old behavior."
      },
      {
        "user": "flckv",
        "created_at": "2023-05-25T16:16:45Z",
        "body": "I tried to load last checkpoint too:\r\n\r\nsrun fairseq-hydra-train distributed_training.distributed_world_size=4 +optimization.update_freq='[16]' --config-dir /home/flck/fairseq/examples/wav2vec/config/pretraining --config-name wav2vec2_base_librispeech `--load-checkpoint-on-all-dp-ranks /home/flck/outputs/2023-05-22/07-19-53/checkpoints  `\r\n\r\nbut I got error:\r\n\r\nfairseq-hydra-train: **error**: `unrecognized arguments:` --load-checkpoint-on-all-dp-ranks /home/flck/outputs/2023-05-22/07-19-53/checkpoints\r\n\r\nhow to load the checkpoint to continue pretraining? "
      }
    ]
  },
  {
    "number": 3014,
    "title": "How to get a batch",
    "created_at": "2020-12-10T03:59:08Z",
    "closed_at": "2022-05-02T10:22:23Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3014",
    "body": "How does the code divide a batch, what is its specific process, and in which python file the division is made",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3014/comments",
    "author": "LittleRooki",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:38Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:53Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3003,
    "title": "Add new option to generate.py in fairseq-cli",
    "created_at": "2020-12-08T13:40:13Z",
    "closed_at": "2022-05-02T10:22:18Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3003",
    "body": "#### What is your question?\r\nWhere do I have to register a new option for cli-generation? \r\nI tried to add a new option to cli-generate to only output translations. I tried by adding a translations_only field in fairseq/dataclass/configs.py (see below) but when I pass the argument \"--translations_only\" to generate.py, I still get following error: \r\ngenerate.py: error: unrecognized arguments: --translations_only\r\n\r\n#### What have you tried?\r\n\r\nAdding ` translations_only: bool = field(default=False, metadata={\"help\": \"only print tranlsations\"})`\r\nin fairseq/dataclass/configs.py line 825 \r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: master \r\n - PyTorch Version: 1.7.0\r\n - OS: Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable .\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.1\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3003/comments",
    "author": "braunefe",
    "comments": [
      {
        "user": "braunefe",
        "created_at": "2020-12-08T13:42:48Z",
        "body": "task is: `--task=translation_multi_simple_epoch`"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3001,
    "title": "Reproduce doesn't match result showed",
    "created_at": "2020-12-08T07:37:30Z",
    "closed_at": "2022-05-02T10:22:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3001",
    "body": "Hi,\r\n\r\nI just reproduced id-en speech translation result and it shows the the result is:\r\n\r\nGenerate test_st_id_en with beam=5: BLEU = 10.77 61.7/19.6/6.2/2.5 (BP = 0.918 ratio = 0.921 hyp_len = 27497 ref_len = 29845)\r\n\r\nwithout pre-train ASR.\r\n\r\nBut the result shows id-en BLEU should be 2.5. I didn't change and structures, so did I done sth wrong?\r\n\r\nTo reproduce:\r\n\r\npython train.py ${COVO_ROOT} --train-subset train_st_id_en --valid-subset dev_st_id_en --save-dir ${ST_SAVE_DIR} \\\r\n    --num-workers 8 --max-tokens 8000 --task speech_to_text --criterion label_smoothed_cross_entropy \\\r\n    --report-accuracy --max-update 10000 --arch s2t_transformer_s  --optimizer adam --lr 2e-3 \\\r\n    --lr-scheduler inverse_sqrt --warmup-updates 10000 --clip-norm 10.0 --seed 1 --config-yaml ${COVO_ROOT}/config_st_ar_en.yaml\r\n\r\nTo evaluate:\r\n\r\npython scripts/average_checkpoints.py --inputs ${ST_SAVE_DIR} --num-epoch-checkpoints 10 \\\r\n    --output \"${ST_SAVE_DIR}/${CHECKPOINT_FILENAME}\"\r\n\r\npython generate.py ${COVO_ROOT} --gen-subset test_st_id_en --task task_speech_to_text_vae --batch-size 5 \\\r\n    --path ${ST_SAVE_DIR}/${CHECKPOINT_FILENAME} --beam 5 --scoring sacrebleu --config-yaml ${COVO_ROOT}/config_st_id_en.yaml\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3001/comments",
    "author": "zjw1990",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:45Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2991,
    "title": "How to fine-tune BART for translation task by replacing encoder embedding layer",
    "created_at": "2020-12-04T10:31:07Z",
    "closed_at": "2022-04-19T16:21:38Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2991",
    "body": "Hi,\r\n\r\nI want to fine-tune a pretrained BART model on WMT16 En-De translation task. To do so, I have to load a pretrained BART model and then replace the encoder embedding layer with a new randomly initilized encoder. Any suggestions on how to do that?\r\nI guess I shoul load the pretrained weights before replacing the embeding layer, otherwise the architectures won't match, but I'm not sure how to do that.\r\n\r\nThanks for you help.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2991/comments",
    "author": "sergisdeutsch",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:21:08Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "zsLin177",
        "created_at": "2022-11-28T05:45:11Z",
        "body": "hi @sergisdeutsch, I have the same question as you. Did you solve this problem? Thanks for your help."
      },
      {
        "user": "zgsldr",
        "created_at": "2023-08-29T11:51:34Z",
        "body": "hi，Have you solved the problem? I have the same problem you have, please"
      },
      {
        "user": "zgsldr",
        "created_at": "2023-08-29T11:55:12Z",
        "body": "> \r\nHave you solved the problem? I have the same problem you have, please\r\n"
      }
    ]
  },
  {
    "number": 2990,
    "title": "[New]Difficulties for reproduce XSUM results with BART",
    "created_at": "2020-12-04T08:57:50Z",
    "closed_at": "2020-12-05T08:42:06Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2990",
    "body": "I have tried CNNDM datasets with BART before, which has a great performance.\r\nHowever, when I tried the Xsum datasets, the ROUGE-1 result only have 23.8 (45.14 was claimed in paper). But I don't know where I am wrong.\r\nMy actions are below:\r\n1、I download the Xsum datasets and divide them into 6 files in [train/val/test].[source/target]\r\n2、I run the bpe and binary process which is exactly the same as what I did on CNNDM datasets.\r\n3、I download the `bart.large.xsum` which is already fine-tuned in your page.\r\n4、I run the inference script as you provided with changes `Use beam=6, lenpen=1.0, max_len_b=60, min_len=10 for Xsum Generation`\r\n5、I run code to get the ROUGE result\r\n```\r\ncat xsum_baseline.hypo | java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines > xsum_baseline.hypo.tokenized\r\ncat test.target | java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines > test.hypo.target\r\nfiles2rouge xsum_baseline.hypo.tokenized test.hypo.target\r\n```\r\n-------------------------------\r\nNow I wonder where I did wrong or which step I missed to get this awful result.\r\nThere are other issues I met during the reproduction:\r\n1、When I run the inference script in step 4, I faced an error like \"cannot find file xsum/dict.document.txt\". I find it was named \"dict.[source/target].txt\" in CNNDM datasets, so I just move the dict.[document/summary].txt file in `bart.large.xsum` to xsum folder, then the error disappears.\r\n2、I found that current inference result are likely much longer than the reference summary. For example, the reference summary is\r\n `Gaza crisis: Tel Aviv adjusts to life in rocket-range`\r\n, while the generated result is \r\n`Residents of the Israeli city of Tel Aviv are returning to work after a weekend of heightened security following a series of rocket attacks from the Gaza Strip, the BBC has learned.`\r\nI wonder if hyperparameter in inference script (`max_len_b=60, min_len=10`) are much too large for Xsum datasets. But I also have seen other people have reproduce this performance. So I don't know how to do with it. Can you help?\r\n\r\nmy inference script is below if you need:\r\n\r\n```\r\nimport torch\r\nfrom fairseq.models.bart import BARTModel\r\nbart = BARTModel.from_pretrained(\r\n    'bart.large.xsum/',\r\n    checkpoint_file='model.pt',\r\n    data_name_or_path='xsum-bin'\r\n)\r\n\r\nbart.cuda()\r\nbart.eval()\r\nbart.half()\r\ncount = 1\r\nbsz = 16\r\nwith open('xsum/test.source') as source, open('xsum/xsum_baseline.hypo', 'w') as fout:\r\n    sline = source.readline().strip()\r\n    slines = [sline]\r\n    for sline in source:\r\n        if count % bsz == 0:\r\n            with torch.no_grad():\r\n                hypotheses_batch = bart.sample(slines, beam=6, lenpen=1.0, max_len_b=60, min_len=10, no_repeat_ngram_size=3)\r\n\r\n            for hypothesis in hypotheses_batch:\r\n                fout.write(hypothesis + '\\n')\r\n                fout.flush()\r\n            slines = []\r\n\r\n        slines.append(sline.strip())\r\n        count += 1\r\n    if slines != []:\r\n        hypotheses_batch = bart.sample(slines, beam=6, lenpen=1.0, max_len_b=60, min_len=10, no_repeat_ngram_size=3)\r\n        for hypothesis in hypotheses_batch:\r\n            fout.write(hypothesis + '\\n')\r\n            fout.flush()\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2990/comments",
    "author": "monologue1107",
    "comments": [
      {
        "user": "monologue1107",
        "created_at": "2020-12-05T02:44:22Z",
        "body": "@myleott @alexeib @lematt1991 @huihuifan @tangyuq "
      },
      {
        "user": "cece00",
        "created_at": "2021-03-29T08:40:42Z",
        "body": "I wonder if this problem is solved, because I come into nearly the same problem on CNN-DM dataset. If yes, could you please tell me the solution? \r\nWith all my best."
      },
      {
        "user": "Ricardokevins",
        "created_at": "2021-11-10T05:30:30Z",
        "body": "Hey man, I meet the same problem. can you tell me how you solve the problem? (any suggestion is great appreciate!!!)"
      }
    ]
  },
  {
    "number": 2987,
    "title": "Print Alignment at word level ",
    "created_at": "2020-12-03T18:04:34Z",
    "closed_at": "2022-04-19T16:21:36Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2987",
    "body": "## ❓ Questions and Help\r\nHello, \r\n\r\nI used the ENG-GER Fairseq model with the interactive function. I'm able to print the alignment between the BPE token using **--print-alignment** (please see the example below).\r\nHowever, I want it to print the alignment between the original word in each sentence.\r\n\r\n\r\n#### Code\r\n\r\n```\r\nS-104\t the weeknd may be a god or the new age michael jackson\r\nH-104\t-0.3077760338783264\tdas wochenende kann ein gott sein oder das neue zeitalter michael jackson\r\nA-104\t1-0 1-1 2-2 4-3 7-4 7-5 8-6 5-7 9-8 13-9 11-10 12-11 12-12 9-13 13-14 14-15 15-16 16-17 17-18\r\n\r\n```\r\n\r\nAny suggestion would be appreciated. \r\n\r\n\r\nThanks\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2987/comments",
    "author": "Nagoudi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:21:06Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "Samyak005",
        "created_at": "2023-04-17T17:04:00Z",
        "body": "I am facing the same problem. any solutions? "
      }
    ]
  },
  {
    "number": 2981,
    "title": "when will wav2vec 2.0 pretraining converge?",
    "created_at": "2020-12-02T06:46:21Z",
    "closed_at": "2022-04-29T16:22:30Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2981",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI'm running wav2vec 2.0 base pretraining with mandarin(200 hours). single tesla V100 32G batch_size 64.\r\nAnd the log prints:\r\n{\"epoch\": 10, \"valid_loss\": \"3.801\", \"valid_ntokens\": \"1412.17\", \"valid_nsentences\": \"23.6442\", \"valid_prob_perplexity\": \"17.892\", \"valid_code_perplexity\": \"17.822\", \"valid_temp\": \"1.989\", \"valid_loss_0\": \"3.629\", \"valid_loss_1\": \"0.14\", \"valid_loss_2\": \"0.032\", \"valid_accuracy\": \"0.49559\", \"valid_wps\": \"23508\", \"valid_wpb\": \"1412.2\", \"valid_bsz\": \"23.6\", \"valid_num_updates\": \"1064\", \"valid_best_loss\": \"3.801\"}\r\nI noticed in the paper they trained the model for 1.5 days with 64 GPUs.\r\nFor I only have 8 GPUs, I want to know when is the pretraining model converge? what's the _**loss, valid_prob_perplexity, valid_code_perplexity and valid_accuracy**_ for Facebook libri-speech models?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2981/comments",
    "author": "mcggood",
    "comments": [
      {
        "user": "mcggood",
        "created_at": "2020-12-02T09:33:05Z",
        "body": "It seems overfit just less than 20 epochs.\r\n{\"epoch\": 18, \"valid_loss\": \"3.242\", \"valid_ntokens\": \"1406.43\", \"valid_nsentences\": \"23.6442\", \"valid_prob_perplexity\": \"19.688\", \"valid_code_perplexity\": \"19.68\", \"valid_temp\": \"1.981\", \"valid_loss_0\": \"3.086\", \"valid_loss_1\": \"0.14\", \"valid_loss_2\": \"0.017\", \"valid_accuracy\": \"0.55915\", \"valid_wps\": \"23406.9\", \"valid_wpb\": \"1406.4\", \"valid_bsz\": \"23.6\", \"valid_num_updates\": \"1917\", \"valid_best_loss\": \"3.157\"}\r\n\r\nthe best perplexity is epoch12, logs:\r\n{\"epoch\": 12, \"valid_loss\": \"3.455\", \"valid_ntokens\": \"1401.84\", \"valid_nsentences\": \"23.6442\", \"valid_prob_perplexity\": \"16.855\", \"valid_code_perplexity\": \"16.831\", \"valid_temp\": \"1.987\", \"valid_loss_0\": \"3.289\", \"valid_loss_1\": \"0.14\", \"valid_loss_2\": \"0.026\", \"valid_accuracy\": \"0.53597\", \"valid_wps\": \"23391.6\", \"valid_wpb\": \"1401.8\", \"valid_bsz\": \"23.6\", \"valid_num_updates\": \"1277\", \"valid_best_loss\": \"3.455\"}\r\n\r\nis that normal?"
      },
      {
        "user": "edosyhptra",
        "created_at": "2021-06-23T01:33:00Z",
        "body": "Mine got loss_0: 2.246 and accuracy: 0.55 in epoch 23. I dont know it seems overfit or no. I want to ask aswell. Is mine normal?\r\n\r\n[2021-06-22 14:14:45,462][fairseq.trainer][INFO] - NOTE: overflow detected, setting loss scale to: 0.03125\r\n[2021-06-22 14:16:18,056][train_inner][INFO] - {\"epoch\": 23, \"update\": 0.864, \"loss\": \"2.351\", \"ntokens\": \"23709.4\", \"nsentences\": \"145.23\", \"prob_perplexity\": \"279.599\", \"code_perplexity\": \"226.953\", \"temp\": \"1.987\", \"loss_0\": \"2.259\", \"loss_1\": \"0.081\", \"loss_2\": \"0.011\", \"accuracy\": \"0.54756\", \"wps\": \"1752.3\", \"ups\": \"0.07\", \"wpb\": \"23709.4\", \"bsz\": \"145.2\", \"num_updates\": \"1400\", \"lr\": \"7e-05\", \"gnorm\": \"0.462\", \"loss_scale\": \"0.0312\", \"train_wall\": \"1333\", \"wall\": \"17797\"}\r\n[2021-06-22 14:38:42,077][train_inner][INFO] - {\"epoch\": 23, \"update\": 0.925, \"loss\": \"2.337\", \"ntokens\": \"23686\", \"nsentences\": \"144.26\", \"prob_perplexity\": \"279.732\", \"code_perplexity\": \"227.234\", \"temp\": \"1.986\", \"loss_0\": \"2.246\", \"loss_1\": \"0.081\", \"loss_2\": \"0.011\", \"accuracy\": \"0.5496\", \"wps\": \"1762.3\", \"ups\": \"0.07\", \"wpb\": \"23686\", \"bsz\": \"144.3\", \"num_updates\": \"1500\", \"lr\": \"7.5e-05\", \"gnorm\": \"0.461\", \"loss_scale\": \"0.0312\", \"train_wall\": \"1326\", \"wall\": \"19141\"}\r\n[2021-06-22 15:01:11,834][train_inner][INFO] - {\"epoch\": 23, \"update\": 0.986, \"loss\": \"2.337\", \"ntokens\": \"23682.8\", \"nsentences\": \"145.09\", \"prob_perplexity\": \"279.642\", \"code_perplexity\": \"227.304\", \"temp\": \"1.985\", \"loss_0\": \"2.245\", \"loss_1\": \"0.081\", \"loss_2\": \"0.011\", \"accuracy\": \"0.55017\", \"wps\": \"1754.6\", \"ups\": \"0.07\", \"wpb\": \"23682.8\", \"bsz\": \"145.1\", \"num_updates\": \"1600\", \"lr\": \"8e-05\", \"gnorm\": \"0.459\", \"loss_scale\": \"0.0312\", \"train_wall\": \"1331\", \"wall\": \"20490\"}\r\n\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:52Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:22:02Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2979,
    "title": "Any way to load a checkpoint taking less RAM?",
    "created_at": "2020-12-01T17:23:29Z",
    "closed_at": "2022-04-19T16:21:31Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2979",
    "body": "A really simple question - I have a 20Gb checkpoint and I want to do inferencing with it via `fairseq-interactive`. However, I am unable to load it all into the current 12Gb System RAM I have. \r\n\r\nIs there any way to force `fairseq` to load the checkpoint in all of the TPU cores rather than the System Memory or GPU memory rather than the System RAM? (Since TPU has more memory than GPU, at least >20)?\r\n\r\nIs there any workaround for this?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2979/comments",
    "author": "neel04",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:21:01Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2975,
    "title": "Can I build&register the criterion and task outside fairseq document",
    "created_at": "2020-12-01T10:06:59Z",
    "closed_at": "2022-04-19T16:21:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2975",
    "body": "Since I need to run fairseq on server, there's permission issues when I need to add those model features, like model.py, criterion.py and task.py, into a specific document. \r\n\r\nI saw a help that I can specify a --user-dir but only for register models. Yet most models are associate with new criterions and tasks. Therefore I am wondering whether there is any way to split all my code(including criterion code, model code, task) outside fairseq. Which makes more sence as this is a package instead of kind like changing source code :)",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2975/comments",
    "author": "zjw1990",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-12-01T12:11:35Z",
        "body": "Yes, this is what --user-dir is for. You can look at examples/speech_recognition to see how you can package various components outside of the fairseq source directory. Several other examples in the examples/ directory also rely on user-dir."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T08:04:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:20:55Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2972,
    "title": "How to export wav2vec 2.0 pretrained model to ONNX and run it in ONNX Runtime",
    "created_at": "2020-11-30T14:53:10Z",
    "closed_at": "2022-04-19T16:21:32Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2972",
    "body": "Is it possible to do it?\r\nWill we have performance improvements after that?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2972/comments",
    "author": "Denovitz",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-12-01T12:13:00Z",
        "body": "I don’t know if we’ve tried that, but cc @alexeib to confirm."
      },
      {
        "user": "alexeib",
        "created_at": "2020-12-01T21:20:02Z",
        "body": "i havent tried it, but in theory it should work because its just a bunch of convolutions followed by a vanilla transformer"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:55Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:21:02Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2965,
    "title": "Transcription timestamp of every word in the output wav2vec2",
    "created_at": "2020-11-28T21:28:53Z",
    "closed_at": "2021-03-30T07:33:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2965",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n\r\n#### How to get the timestamp of each word in transcription in wav2vec?\r\n\r\nOften in ASR the timestamp of each word in duration matters a lot. Is there any mechanism that does this yet?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2965/comments",
    "author": "harveenchadha",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-12-01T21:30:49Z",
        "body": "this is not really supported at the moment. you can try to add the code yourself to infer.py for viterbi decoding, or check if wav2letter decoder supports this (since our decoding is using wav2letter decoders). alternatively you can try to implement decoding with e.g. kaldi which should be straight forward since you can easily get emissions (if you do this please contribute back!) and get the timestamps from there"
      },
      {
        "user": "irux",
        "created_at": "2021-03-24T08:31:55Z",
        "body": "@harveenchadha did you solve it ? "
      },
      {
        "user": "harveenchadha",
        "created_at": "2021-03-30T07:33:02Z",
        "body": "@irux not yet, but will be working on it."
      },
      {
        "user": "pushkal1234",
        "created_at": "2021-08-17T13:58:20Z",
        "body": "How to get a word time stamp in wav2vec 2.0?"
      }
    ]
  },
  {
    "number": 2954,
    "title": "Getting OOM system RAM when fine tuning MBart on TPU",
    "created_at": "2020-11-26T18:42:52Z",
    "closed_at": "2022-04-19T16:21:28Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2954",
    "body": "I am need to this industry and little I know. I am tryining to fine tune MBart of TPU colab pro with distributed --distributed-world-size 8. It has 24GB system memory and TPU v2. The model size is around 5.5GB with 600M parameter. I can't understand why I am getting system memory OOM issue no matter how much I minimize batch size?\r\n\r\nAt the moment, I am also fine tuning the same thing in 1 GPU colab pro with much higher batch size. Everything is fine.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.10.1\r\n - PyTorch Version (e.g., 1.0) 1.7\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): pip install --editable fairseq\r\n - Python version: 3.6\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2954/comments",
    "author": "mani-rai",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-12-01T12:31:18Z",
        "body": "TPU support is somewhat experimental currently. What command are you running? How big is your dataset?\r\n\r\nNote that we load the data in system memory, and store the model, optimizer and activations on the device (TPU/GPU) memory."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T08:04:29Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:20:58Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2949,
    "title": "what is the accuracy for wav2vec 2.0 pretrained model?",
    "created_at": "2020-11-25T11:31:42Z",
    "closed_at": "2020-12-01T21:49:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2949",
    "body": "What is the accuracy of identifying the correct latent representation for both training and testing?\r\nIn the paper, it is 62% (?)\r\n\r\n> The training accuracy of identifying the correct latent audio representation increases from 62% to 78.0% when switching from quantized to continuous targets.\r\n\r\nBut is it for the base or large model? and is it on 960 or 53k hrs of training? also, I think 62% is very low...\r\n\r\nThank you..",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2949/comments",
    "author": "LamMoh",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-12-01T21:48:24Z",
        "body": "accuracy during pretraining and performance on downstream task are not necessarily correlated - especially across different configurations (and model architectures). you could, for example, change mask length to 1 and get close to 100% accuracy, but your downstream task performance will be very bad.\r\n\r\nif i remember correctly, the base model on librispeech gets around 62%, while the large model on librivox gets around 70%"
      },
      {
        "user": "rajeevbaalwan",
        "created_at": "2023-04-16T04:41:04Z",
        "body": "> accuracy during pretraining and performance on downstream task are not necessarily correlated - especially across different configurations (and model architectures). you could, for example, change mask length to 1 and get close to 100% accuracy, but your downstream task performance will be very bad.\r\n> \r\n> if i remember correctly, the base model on librispeech gets around 62%, while the large model on librivox gets around 70%\r\n\r\n@alexeib this 62% accuracy is for wav2vec or wav2vec2.0 Base model on LS-960 ?? In the paper you have mentioned the following:\r\n**\"The training accuracy of identifying the correct latent audio representation increases from 62% to 78.0% when switching from quantized to continuous targets.\"**\r\n\r\nLooks like in above mentioned line in the paper you are comparing the LS-960 pretraining accuracy of the wav2vec-Base and wav2vec2.0-Base Models.\r\n\r\nAlso this 62% is on validation set or training set ?"
      }
    ]
  },
  {
    "number": 2930,
    "title": "How to infer a wav and output it as a subtitle file",
    "created_at": "2020-11-21T09:11:24Z",
    "closed_at": "2022-04-19T16:21:18Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2930",
    "body": "How to predict a three-minute wav file, the output subtitle format, that is, each sentence with punctuation, and the start and end time\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2930/comments",
    "author": "Deep-learning999",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T08:04:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:20:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2918,
    "title": "Transformer LM takes too long to infer on English!",
    "created_at": "2020-11-19T07:10:15Z",
    "closed_at": "2022-04-19T16:21:16Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2918",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nShould transformer LM take 7 hours to infer on Libri Test-clean dataset?\r\n\r\nData: Libri Test-clean\r\nLM checkpoint: transformer_lm.wiki103.adaptive (downloaded from repo)\r\nFinetuning checkpoint: Wav2Vec 2.0 Large (LV-60)* 960h (downloaded from repo)\r\n\r\nTime without Transformer lm: 244s for 2620 sentences (284022 tokens)\r\nTime with Transformer lm: 7.4 hours for 2620 sentences (284022 tokens)\r\n\r\n#### File/command used\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\npython examples/speech_recognition/infer.py libri-test --task audio_pretraining --nbest 1 --path english_pretrained_large_checkpoint/wav2vec2_vox_960h_new.pt --gen-subset test --results-path libri/transcript --w2l-decoder fairseqlm --lm-model adaptive_lm_wiki103.v2/model.pt --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 640000 --post-process letter\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq commit: 81677d75\r\n - OS (e.g., Linux): Linux\r\n - Python version: 3.7.9\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2918/comments",
    "author": "neerajchhimwal",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-11-19T20:25:34Z",
        "body": "yeah sorry, it is not very efficient. i shard the eval jobs over multiple nodes to get it evaluated in a few hours (with a bigger transformer lm)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T08:04:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:20:47Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2909,
    "title": "OOM in the final of the fairseq-generate",
    "created_at": "2020-11-18T07:45:30Z",
    "closed_at": "2020-11-18T13:52:26Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2909",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nOOM when the final of the fairseq-generate\r\n#### Code\r\nTraceback (most recent call last):\r\n  File \"/home/jiaxin/tools/py38tc150/bin/fairseq-generate\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq_cli/generate.py\", line 199, in cli_main\r\n    main(args)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq_cli/generate.py\", line 104, in main\r\n    hypos = task.inference_step(generator, models, sample, prefix_tokens)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq/tasks/fairseq_task.py\", line 265, in inference_step\r\n    return generator.generate(models, sample, prefix_tokens=prefix_tokens)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq/sequence_generator.py\", line 113, in generate\r\n    return self._generate(model, sample, **kwargs)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq/sequence_generator.py\", line 291, in _generate\r\n    model.reorder_incremental_state(reorder_state)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq/sequence_generator.py\", line 612, in reorder_incremental_state\r\n    model.decoder.reorder_incremental_state(self.incremental_states[model], new_order)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq/models/fairseq_incremental_decoder.py\", line 75, in reorder_incremental_state\r\n    self.apply(apply_reorder_incremental_state)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 289, in apply\r\n    module.apply(fn)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 289, in apply\r\n    module.apply(fn)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 289, in apply\r\n    module.apply(fn)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 290, in apply\r\n    fn(self)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq/models/fairseq_incremental_decoder.py\", line 73, in apply_reorder_incremental_state\r\n    module.reorder_incremental_state(incremental_state, new_order)\r\n  File \"/home/jiaxin/tools/py38tc150/lib/python3.8/site-packages/fairseq/modules/multihead_attention.py\", line 314, in reorder_incremental_state\r\n    input_buffer[k] = input_buffer[k].index_select(0, new_order)\r\nRuntimeError: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 10.76 GiB total capacity; 9.33 GiB already allocated; 261.44 MiB free; 9.66 GiB reserved in total by PyTorch)\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):0.9.0\r\n - PyTorch Version (e.g., 1.0)1.5.0\r\n - OS (e.g., Linux):unbuntu 20.4\r\n - How you installed fairseq (`pip`, source):pip\r\n - Build command you used (if compiling from source):\r\n - Python version:3.8.5\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2909/comments",
    "author": "jemmryx",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-11-18T13:52:26Z",
        "body": "What is your question?  Please follow the issue templates"
      },
      {
        "user": "junwen-austin",
        "created_at": "2021-01-02T02:34:24Z",
        "body": "@lematt1991 Happy New Year.\r\n\r\nI am having the same issue where out of memory error on CUDA appears to be at the 99% of fairest-generate which is used to translate bulk text. I checked the last few batches and it does not appear to have extremely long sentences. so I do not understand why the error would occur. I would appreciate if you could shed some light on possible cause. \r\n\r\nThank you so much!\r\n\r\n"
      }
    ]
  },
  {
    "number": 2904,
    "title": "how to save checkpoint in the middle of epoch training?",
    "created_at": "2020-11-17T14:58:35Z",
    "closed_at": "2020-11-18T03:34:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2904",
    "body": "I was about to save model every 10000 steps because the train takes almost 24 hrs every epoch.\r\nWhat I do is to add another code in def validate_and_save of train.py, turn\r\n```\r\ndo_save = (\r\n        (end_of_epoch and epoch_itr.epoch % args.save_interval == 0)\r\n        or num_updates >= max_update\r\n        or (\r\n            args.save_interval_updates > 0\r\n            and num_updates > 0\r\n            and num_updates % args.save_interval_updates == 0\r\n            and num_updates >= args.validate_after_updates\r\n        )\r\n    )\r\n```\r\ninto\r\n```\r\ndo_save = (\r\n        (end_of_epoch and epoch_itr.epoch % args.save_interval == 0)\r\n        or (epoch_itr.epoch % 10000 == 0)\r\n        or num_updates >= max_update\r\n        or (\r\n            args.save_interval_updates > 0\r\n            and num_updates > 0\r\n            and num_updates % args.save_interval_updates == 0\r\n            and num_updates >= args.validate_after_updates\r\n        )\r\n    )\r\n```\r\nby adding `(epoch_itr.epoch % 10000 == 0)`\r\nHowever, it does't work. So I wonder if there are some mistakes and what can I do in order to realize it.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2904/comments",
    "author": "monologue1107",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-11-17T16:16:43Z",
        "body": "If I understand correctly, your update will save every 10,000 epochs, not 10,000 updates.  That said, you shouldn't need to make any modifications to the code, this is what the `--save-interval-updates` argument is for.  Just add `--save-interval-updates 10000` to your command"
      }
    ]
  },
  {
    "number": 2901,
    "title": "How can I do the sampling rather than greedy or n-beam  during the translation",
    "created_at": "2020-11-17T07:54:31Z",
    "closed_at": "2020-11-17T16:19:03Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2901",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2901/comments",
    "author": "KYRIEZX",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-11-17T16:19:03Z",
        "body": "Please follow the issue templates.  What have you tried so far?"
      }
    ]
  },
  {
    "number": 2896,
    "title": "question about self-training wav2vec 2.0",
    "created_at": "2020-11-16T09:01:20Z",
    "closed_at": "2020-11-17T07:49:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2896",
    "body": "Hello, I was wondering if for finetuning the final wav2vec 2.0 model (ctc ft), you are using both the labeled and pseudo-labeled data? \r\nAlso, would it help if we continued training the finetuned wav2vec model (with 100h labeled data) on the pseudo-labeled data or is better to finetune from scratch?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2896/comments",
    "author": "natspan",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-11-16T18:40:20Z",
        "body": "yes it is using both. we finetuned the models from scratch. you can try fine-tuning a fine-tuned model. i tried it and it didnt help, but i also didnt run many experiments with this setup so it might be a question of finding the right hyper params "
      },
      {
        "user": "natspan",
        "created_at": "2020-11-17T07:50:19Z",
        "body": "Thanks for the response!! I will close the issue."
      }
    ]
  },
  {
    "number": 2892,
    "title": "Error: pip3 install fairseq",
    "created_at": "2020-11-13T13:50:26Z",
    "closed_at": "2020-11-23T16:49:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2892",
    "body": "## ❓ Questions and Help\r\n\r\nI'm trying to install fairseq but i can't. \r\nWhenever I execute this code `pip3 install fairseq`, I get this overwhelmingly long error. \r\n\r\n\r\nCollecting fairseq\r\n  Using cached fairseq-0.10.0.tar.gz (677 kB)\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Installing backend dependencies ... done\r\n    Preparing wheel metadata ... done\r\nRequirement already satisfied: sacrebleu>=1.4.12 in ./.local/lib/python3.6/site-packages (from fairseq) (1.4.14)\r\nRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.11.5)\r\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\r\nRequirement already satisfied: torch in ./.local/lib/python3.6/site-packages (from fairseq) (1.5.1)\r\nRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2017.11.9)\r\nRequirement already satisfied: dataclasses in ./.local/lib/python3.6/site-packages (from fairseq) (0.7)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.28.1)\r\nRequirement already satisfied: hydra-core in ./.local/lib/python3.6/site-packages (from fairseq) (1.0.3)\r\nRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.5.2)\r\nRequirement already satisfied: cython in ./.local/lib/python3.6/site-packages (from fairseq) (0.29.21)\r\nRequirement already satisfied: portalocker in ./.local/lib/python3.6/site-packages (from sacrebleu>=1.4.12->fairseq) (2.0.0)\r\nRequirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.19)\r\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.17.1)\r\nRequirement already satisfied: antlr4-python3-runtime==4.8 in ./.local/lib/python3.6/site-packages (from hydra-core->fairseq) (4.8)\r\nRequirement already satisfied: importlib-resources; python_version < \"3.9\" in ./.local/lib/python3.6/site-packages (from hydra-core->fairseq) (3.3.0)\r\nRequirement already satisfied: omegaconf>=2.0.2 in ./.local/lib/python3.6/site-packages (from hydra-core->fairseq) (2.0.5)\r\nRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in ./.local/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\r\nRequirement already satisfied: typing-extensions in ./.local/lib/python3.6/site-packages (from omegaconf>=2.0.2->hydra-core->fairseq) (3.7.4.3)\r\nRequirement already satisfied: PyYAML>=5.1.* in ./.local/lib/python3.6/site-packages (from omegaconf>=2.0.2->hydra-core->fairseq) (5.3.1)\r\nBuilding wheels for collected packages: fairseq\r\n  Building wheel for fairseq (PEP 517) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/bin/python3 /home/yeoun/.local/lib/python3.6/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/tmp2he2gldy\r\n       cwd: /tmp/pip-install-kgfs92dk/fairseq\r\n  Complete output (714 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-3.6\r\n  creating build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/binarizer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/legacy_distributed_data_parallel.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/nan_detector.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/checkpoint_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/quantization_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/iterative_refinement_generator.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/file_io.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/pdb.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/distributed_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/registry.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/incremental_decoding_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/search.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/trainer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/token_generation_constraints.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/options.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/file_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/hub_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  creating build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/eval_lm.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/preprocess.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/generate.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/train.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/__init__.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/interactive.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/validate.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/score.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  creating build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/legacy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/wav2vec_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/composite_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/sentence_ranking.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/nat_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/ctc.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/sentence_prediction.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/chrf.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/wer.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/bleu.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/subsample_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/nested_dictionary_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/lru_cache_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/transform_eos_lang_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/offset_tokens_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/multi_corpus_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/raw_label_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/colorize_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/append_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/prepend_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/denoising_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/add_target_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/mask_tokens_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/iterators.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/plasma_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/transform_eos_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/num_samples_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/fasta_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/pad_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/lm_context_window_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/numel_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/concat_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/roll_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/noising.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/list_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/resampling_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/shorten_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/multi_corpus_sampled_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/bucket_pad_length_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/id_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/sort_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/replace_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/strip_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/concat_sentences_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/prepend_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/base_wrapper_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  creating build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  copying fairseq/dataclass/constants.py -> build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  copying fairseq/dataclass/utils.py -> build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  copying fairseq/dataclass/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  copying fairseq/dataclass/data_class.py -> build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  creating build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_lev.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_multi_simple_epoch.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/semisupervised_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/legacy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/cross_lingual_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_from_pretrained_bart.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/multilingual_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/denoising.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/speech_to_text.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/multilingual_denoising.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/sentence_ranking.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/audio_pretraining.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/sentence_prediction.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples\r\n  copying fairseq/examples/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples\r\n  creating build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/dummy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/dummy_model.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/dummy_mt.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/dummy_lm.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer_align.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lstm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lstm_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/multilingual_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/model_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lightconv_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lightconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fused_adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adafactor.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adamax.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/dynamic_loss_scaler.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/nag.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adadelta.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fused_lamb.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/shard.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/bmuf.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/gumbel_vector_quantizer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/dynamic_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/character_token_embedder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transpose_last.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/fp32_group_norm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/layer_norm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/gelu.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/layer_drop.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/fairseq_dropout.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/unfold.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/kmeans_vector_quantizer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/dynamic_crf_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/lightweight_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/same_pad.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/quant_noise.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/adaptive_input.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/vggblock.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/logging\r\n  copying fairseq/logging/meters.py -> build/lib.linux-x86_64-3.6/fairseq/logging\r\n  copying fairseq/logging/progress_bar.py -> build/lib.linux-x86_64-3.6/fairseq/logging\r\n  copying fairseq/logging/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/logging\r\n  copying fairseq/logging/metrics.py -> build/lib.linux-x86_64-3.6/fairseq/logging\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel\r\n  copying fairseq/model_parallel/megatron_trainer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel\r\n  copying fairseq/model_parallel/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/masked_lm_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/block_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/masked_lm_dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/raw_audio_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/speech_to_text_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/audio_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/gpt2_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/nltk_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/space_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/moses_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/hf_byte_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/gpt2_bpe_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/bytes.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/subword_nmt_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/characters.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/byte_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/fastbpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/byte_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/hf_bert_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/multilingual_data_manager.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/sampled_multi_epoch_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/multilingual_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/sampled_multi_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/sampling_method.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  copying fairseq/data/audio/feature_transforms/specaugment.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  copying fairseq/data/audio/feature_transforms/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  copying fairseq/data/audio/feature_transforms/global_cmvn.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  copying fairseq/data/audio/feature_transforms/utterance_cmvn.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/rxf\r\n  copying fairseq/examples/rxf/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation\r\n  copying fairseq/examples/simultaneous_translation/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  copying fairseq/examples/speech_recognition/infer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  copying fairseq/examples/speech_recognition/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  copying fairseq/examples/speech_recognition/w2l_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_generate.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_tune.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_score_bw.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_options.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_score_lm.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/rxf/src\r\n  copying fairseq/examples/rxf/src/sentence_prediction_r3f.py -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf/src\r\n  copying fairseq/examples/rxf/src/label_smoothed_cross_entropy_r3f.py -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf/src\r\n  copying fairseq/examples/rxf/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/criterions\r\n  copying fairseq/examples/simultaneous_translation/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/criterions\r\n  copying fairseq/examples/simultaneous_translation/criterions/label_smoothed_cross_entropy_latency_augmented.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/models\r\n  copying fairseq/examples/simultaneous_translation/models/transformer_monotonic_attention.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/models\r\n  copying fairseq/examples/simultaneous_translation/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/evaluate.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/eval_latency.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/client.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/server.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/modules\r\n  copying fairseq/examples/simultaneous_translation/modules/monotonic_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/modules\r\n  copying fairseq/examples/simultaneous_translation/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/modules\r\n  copying fairseq/examples/simultaneous_translation/modules/monotonic_transformer_layer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/utils\r\n  copying fairseq/examples/simultaneous_translation/utils/latency.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/utils\r\n  copying fairseq/examples/simultaneous_translation/utils/functions.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/utils\r\n  copying fairseq/examples/simultaneous_translation/utils/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/utils\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/word_splitter.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/agent.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/simul_trans_text_agent.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/simul_trans_agent.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/scorers\r\n  copying fairseq/examples/simultaneous_translation/eval/scorers/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/scorers\r\n  copying fairseq/examples/simultaneous_translation/eval/scorers/scorer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/scorers\r\n  copying fairseq/examples/simultaneous_translation/eval/scorers/text_scorer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/scorers\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/criterions\r\n  copying fairseq/examples/speech_recognition/criterions/ASG_loss.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/criterions\r\n  copying fairseq/examples/speech_recognition/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/criterions\r\n  copying fairseq/examples/speech_recognition/criterions/cross_entropy_acc.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/replabels.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/asr_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/collaters.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/tasks\r\n  copying fairseq/examples/speech_recognition/tasks/speech_recognition.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/tasks\r\n  copying fairseq/examples/speech_recognition/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/tasks\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/models\r\n  copying fairseq/examples/speech_recognition/models/w2l_conv_glu_enc.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/models\r\n  copying fairseq/examples/speech_recognition/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/models\r\n  copying fairseq/examples/speech_recognition/models/vggtransformer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/levenshtein_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/nat_crf_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/fairseq_nat_model.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/iterative_nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/levenshtein_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/cmlm_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/insertion_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/nonautoregressive_ensembles.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/huggingface\r\n  copying fairseq/models/huggingface/hf_gpt2.py -> build/lib.linux-x86_64-3.6/fairseq/models/huggingface\r\n  copying fairseq/models/huggingface/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/huggingface\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/hub_interface.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/model.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  copying fairseq/models/wav2vec/wav2vec.py -> build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  copying fairseq/models/wav2vec/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  copying fairseq/models/wav2vec/wav2vec2_asr.py -> build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  copying fairseq/models/wav2vec/wav2vec2.py -> build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/speech_to_text\r\n  copying fairseq/models/speech_to_text/berard.py -> build/lib.linux-x86_64-3.6/fairseq/models/speech_to_text\r\n  copying fairseq/models/speech_to_text/s2t_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/speech_to_text\r\n  copying fairseq/models/speech_to_text/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/speech_to_text\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/hub_interface.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/model.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/model_camembert.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/model_xlmr.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/alignment_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  creating build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/polynomial_decay_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization\r\n  copying fairseq/modules/quantization/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization\r\n  copying fairseq/modules/quantization/quantization_options.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/dynamicconv_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/setup.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/lightconv_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/setup.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  copying fairseq/modules/quantization/pq/em.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  copying fairseq/modules/quantization/pq/utils.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  copying fairseq/modules/quantization/pq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  copying fairseq/modules/quantization/pq/pq.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar\r\n  copying fairseq/modules/quantization/scalar/ops.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar\r\n  copying fairseq/modules/quantization/scalar/utils.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar\r\n  copying fairseq/modules/quantization/scalar/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  copying fairseq/modules/quantization/pq/modules/qconv.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  copying fairseq/modules/quantization/pq/modules/qlinear.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  copying fairseq/modules/quantization/pq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  copying fairseq/modules/quantization/pq/modules/qemb.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/qconv.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/qlinear.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/qact.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/qemb.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/criterions\r\n  copying fairseq/model_parallel/criterions/vocab_parallel_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/criterions\r\n  copying fairseq/model_parallel/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/models\r\n  copying fairseq/model_parallel/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models\r\n  copying fairseq/model_parallel/models/transformer_lm.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models\r\n  copying fairseq/model_parallel/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/transformer_layer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/pipeline_parallel_transformer\r\n  copying fairseq/model_parallel/models/pipeline_parallel_transformer/model.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/pipeline_parallel_transformer\r\n  copying fairseq/model_parallel/models/pipeline_parallel_transformer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/pipeline_parallel_transformer\r\n  copying fairseq/model_parallel/models/pipeline_parallel_transformer/layers.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/pipeline_parallel_transformer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/roberta\r\n  copying fairseq/model_parallel/models/roberta/model.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/roberta\r\n  copying fairseq/model_parallel/models/roberta/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/roberta\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/grads.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/data.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/random.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/utils.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/initialize.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/mappings.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/layers.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config\r\n  copying fairseq/config/config_eval_lm.yaml -> build/lib.linux-x86_64-3.6/fairseq/config\r\n  copying fairseq/config/config.yaml -> build/lib.linux-x86_64-3.6/fairseq/config\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/optimizer\r\n  copying fairseq/config/optimizer/nag.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/optimizer\r\n  copying fairseq/config/optimizer/adam.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/optimizer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/params\r\n  copying fairseq/config/params/eval_lm_params.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/params\r\n  copying fairseq/config/params/training_params.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/params\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/task\r\n  copying fairseq/config/task/language_modeling.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/task\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/criterion\r\n  copying fairseq/config/criterion/cross_entropy.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/criterion\r\n  copying fairseq/config/criterion/adaptive_loss.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/criterion\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/lr_scheduler\r\n  copying fairseq/config/lr_scheduler/cosine.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/lr_scheduler\r\n  copying fairseq/config/lr_scheduler/inverse_sqrt.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/lr_scheduler\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_baevski_wiki103.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gpt2_big.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_wiki103.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_big.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gpt2_small.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gbw.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gpt2_medium.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gpt.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_baevski_gbw.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/examples/.gitignore -> build/lib.linux-x86_64-3.6/fairseq/examples\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/scaling_nmt\r\n  copying fairseq/examples/scaling_nmt/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/scaling_nmt\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  copying fairseq/examples/pointer_generator/preprocess.py -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  copying fairseq/examples/pointer_generator/postprocess.py -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  copying fairseq/examples/pointer_generator/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  copying fairseq/examples/pointer_generator/README.xsum.md -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator/src\r\n  copying fairseq/examples/pointer_generator/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator/src\r\n  copying fairseq/examples/pointer_generator/src/transformer_pg.py -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/quant_noise\r\n  copying fairseq/examples/quant_noise/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/quant_noise\r\n  copying fairseq/examples/quant_noise/transformer_quantization_config.yaml -> build/lib.linux-x86_64-3.6/fairseq/examples/quant_noise\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100\r\n  copying fairseq/examples/m2m_100/install_dependecies.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100\r\n  copying fairseq/examples/m2m_100/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100\r\n  copying fairseq/examples/m2m_100/tok.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/tokenizer_ar.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/seg_ja.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/tokenize_zh.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/seg_ko.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/tokenize_thai.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/tokenize_indic.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers/thirdparty\r\n  copying fairseq/examples/m2m_100/tokenizers/thirdparty/.gitignore -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers/thirdparty\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/process_data\r\n  copying fairseq/examples/m2m_100/process_data/clean_histogram.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/process_data\r\n  copying fairseq/examples/m2m_100/process_data/remove_too_much_punc.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/process_data\r\n  copying fairseq/examples/m2m_100/process_data/dedup_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/process_data\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/joint_alignment_translation\r\n  copying fairseq/examples/joint_alignment_translation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/joint_alignment_translation\r\n  copying fairseq/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/joint_alignment_translation\r\n  copying fairseq/examples/rxf/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf\r\n  copying fairseq/examples/simultaneous_translation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/docs\r\n  copying fairseq/examples/simultaneous_translation/docs/evaluation.md -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/docs\r\n  copying fairseq/examples/simultaneous_translation/docs/baseline.md -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/docs\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/layerdrop\r\n  copying fairseq/examples/layerdrop/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/layerdrop\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/camembert\r\n  copying fairseq/examples/camembert/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/camembert\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/megatron_11b\r\n  copying fairseq/examples/megatron_11b/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/megatron_11b\r\n  copying fairseq/examples/megatron_11b/detok.py -> build/lib.linux-x86_64-3.6/fairseq/examples/megatron_11b\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/paraphraser\r\n  copying fairseq/examples/paraphraser/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/paraphraser\r\n  copying fairseq/examples/paraphraser/paraphrase.py -> build/lib.linux-x86_64-3.6/fairseq/examples/paraphraser\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/conv_seq2seq\r\n  copying fairseq/examples/conv_seq2seq/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/conv_seq2seq\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/nonautoregressive_translation\r\n  copying fairseq/examples/nonautoregressive_translation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/nonautoregressive_translation\r\n  copying fairseq/examples/nonautoregressive_translation/scripts.md -> build/lib.linux-x86_64-3.6/fairseq/examples/nonautoregressive_translation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  copying fairseq/examples/language_model/prepare-wikitext-103.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  copying fairseq/examples/language_model/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  copying fairseq/examples/language_model/README.conv.md -> build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  copying fairseq/examples/language_model/README.adaptive_inputs.md -> build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  copying fairseq/examples/unsupervised_quality_estimation/repeat_lines.py -> build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  copying fairseq/examples/unsupervised_quality_estimation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  copying fairseq/examples/unsupervised_quality_estimation/aggregate_scores.py -> build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  copying fairseq/examples/unsupervised_quality_estimation/meteor.py -> build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/bart\r\n  copying fairseq/examples/bart/README.glue.md -> build/lib.linux-x86_64-3.6/fairseq/examples/bart\r\n  copying fairseq/examples/bart/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/bart\r\n  copying fairseq/examples/bart/README.summarization.md -> build/lib.linux-x86_64-3.6/fairseq/examples/bart\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/wav2vec_manifest.py -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/wav2vec_featurize.py -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/vq-wav2vec_featurize.py -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/libri_labels.py -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/prepare-wmt14en2fr.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/prepare-iwslt17-multilingual.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/prepare-iwslt14.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/prepare-wmt14en2de.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  copying fairseq/examples/multilingual/train_multilingual_model.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  copying fairseq/examples/multilingual/finetune_multilingual_model.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  copying fairseq/examples/multilingual/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  copying fairseq/examples/multilingual/multilingual_fairseq_gen.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe\r\n  copying fairseq/examples/translation_moe/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe\r\n  copying fairseq/examples/translation_moe/score.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  copying fairseq/examples/translation_moe/src/translation_moe.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  copying fairseq/examples/translation_moe/src/logsumexp_moe.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  copying fairseq/examples/translation_moe/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  copying fairseq/examples/translation_moe/src/mean_pool_gating_network.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  copying fairseq/examples/criss/download_and_preprocess_flores_test.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  copying fairseq/examples/criss/save_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  copying fairseq/examples/criss/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  copying fairseq/examples/criss/download_and_preprocess_tatoeba.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/criss/mining\r\n  copying fairseq/examples/criss/mining/mine_example.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/mining\r\n  copying fairseq/examples/criss/mining/mine.py -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/mining\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/criss/sentence_retrieval\r\n  copying fairseq/examples/criss/sentence_retrieval/encoder_analysis.py -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/sentence_retrieval\r\n  copying fairseq/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/sentence_retrieval\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/criss/unsupervised_mt\r\n  copying fairseq/examples/criss/unsupervised_mt/eval.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/unsupervised_mt\r\n  copying fairseq/examples/speech_recognition/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/datasets\r\n  copying fairseq/examples/speech_recognition/datasets/prepare-librispeech.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/datasets\r\n  copying fairseq/examples/speech_recognition/datasets/asr_prep_json.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/datasets\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/utils\r\n  copying fairseq/examples/speech_recognition/utils/wer_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/utils\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/wmt19\r\n  copying fairseq/examples/wmt19/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/wmt19\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/prep_librispeech_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/prep_covost_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/prep_mustc_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/pay_less_attention_paper\r\n  copying fairseq/examples/pay_less_attention_paper/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/pay_less_attention_paper\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/constrained_decoding\r\n  copying fairseq/examples/constrained_decoding/tok.py -> build/lib.linux-x86_64-3.6/fairseq/examples/constrained_decoding\r\n  copying fairseq/examples/constrained_decoding/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/constrained_decoding\r\n  copying fairseq/examples/constrained_decoding/normalize.py -> build/lib.linux-x86_64-3.6/fairseq/examples/constrained_decoding\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth\r\n  copying fairseq/examples/latent_depth/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src\r\n  copying fairseq/examples/latent_depth/src/multilingual_translation_latent_depth.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src\r\n  copying fairseq/examples/latent_depth/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/models\r\n  copying fairseq/examples/latent_depth/src/models/latent_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/models\r\n  copying fairseq/examples/latent_depth/src/models/latent_multilingual_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/models\r\n  copying fairseq/examples/latent_depth/src/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/modules\r\n  copying fairseq/examples/latent_depth/src/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/modules\r\n  copying fairseq/examples/latent_depth/src/modules/latent_layers.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/loss\r\n  copying fairseq/examples/latent_depth/src/loss/latent_depth.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/loss\r\n  copying fairseq/examples/latent_depth/src/loss/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/loss\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/cross_lingual_language_model\r\n  copying fairseq/examples/cross_lingual_language_model/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/cross_lingual_language_model\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/mbart\r\n  copying fairseq/examples/mbart/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/mbart\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/stories\r\n  copying fairseq/examples/stories/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/stories\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  copying fairseq/examples/byte_level_bpe/get_bitext.py -> build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  copying fairseq/examples/byte_level_bpe/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  copying fairseq/examples/byte_level_bpe/get_data.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  copying fairseq/examples/byte_level_bpe/gru_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/prepare-de-monolingual.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/sacrebleu.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/prepare-wmt18en2de.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/tokenized_bleu.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/deduplicate_lines.py -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/extract_bt_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.custom_classification.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/multiprocessing_bpe_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.glue.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.race.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/preprocess_RACE.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/preprocess_RACE.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/preprocess_GLUE_tasks.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.pretraining.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/wsc_task.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/wsc_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/wsc_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/roberta/commonsense_qa/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/roberta/commonsense_qa/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/roberta/commonsense_qa/commonsense_qa_task.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/noisychannel/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/linformer\r\n  copying fairseq/examples/linformer/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src\r\n  copying fairseq/examples/linformer/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/models\r\n  copying fairseq/examples/linformer/src/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/models\r\n  copying fairseq/examples/linformer/src/models/linformer_roberta.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  copying fairseq/examples/linformer/src/modules/linformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  copying fairseq/examples/linformer/src/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  copying fairseq/examples/linformer/src/modules/linformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  copying fairseq/examples/linformer/src/modules/multihead_linear_attention.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/xlmr\r\n  copying fairseq/examples/xlmr/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/xlmr\r\n  running build_ext\r\n  cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\r\n  cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\r\n  building 'fairseq.libbleu' extension\r\n  creating build/temp.linux-x86_64-3.6\r\n  creating build/temp.linux-x86_64-3.6/fairseq\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\n  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.data.data_utils_fast' extension\r\n  creating build/temp.linux-x86_64-3.6/fairseq/data\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\r\n                   from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12,\r\n                   from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                   from fairseq/data/data_utils_fast.cpp:624:\r\n  /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n   #warning \"Using deprecated NumPy API, disable it with \" \\\r\n    ^\r\n  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.data.token_block_utils_fast' extension\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\r\n                   from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12,\r\n                   from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                   from fairseq/data/token_block_utils_fast.cpp:625:\r\n  /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n   #warning \"Using deprecated NumPy API, disable it with \" \\\r\n    ^\r\n  fairseq/data/token_block_utils_fast.cpp: In function ‘PyArrayObject* __pyx_f_7fairseq_4data_22token_block_utils_fast__get_slice_indices_fast(PyArrayObject*, PyObject*, int, int, int)’:\r\n  fairseq/data/token_block_utils_fast.cpp:3290:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         __pyx_t_4 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\r\n                                      ^\r\n  fairseq/data/token_block_utils_fast.cpp:3485:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         __pyx_t_3 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\r\n                                      ^\r\n  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.libnat' extension\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib/libnat\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n  In file included from fairseq/clib/libnat/edit_dist.cpp:9:0:\r\n  /usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include/torch/torch.h:7:2: warning: #warning \"Including torch/torch.h for C++ extensions is deprecated. Please include torch/extension.h\" [-Wcpp]\r\n   #warning \\\r\n    ^\r\n  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -o build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.libnat_cuda' extension\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib/libnat_cuda\r\n  /usr/local/cuda-8.0/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat_cuda/edit_dist.cu -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat_cuda/edit_dist.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n  error: command '/usr/local/cuda-8.0/bin/nvcc' failed with exit status 1\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for fairseq\r\nFailed to build fairseq\r\nERROR: Could not build wheels for fairseq which use PEP 517 and cannot be installed directly\r\n\r\n\r\n#### What have you tried?\r\nSo I searched for lots of suggestions, but I couldn't because I don't have sudo. \r\n\r\n#### What's your environment?\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5.1 \r\n - OS (e.g., Linux):  Linux \r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2015 NVIDIA Corporation\r\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\r\nCuda compilation tools, release 7.5, V7.5.17\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2892/comments",
    "author": "yeounyi",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-11-16T18:36:59Z",
        "body": "Can you post the output from:\r\n\r\n```\r\n/usr/local/cuda-8.0/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat_cuda/edit_dist.cu -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat_cuda/edit_dist.o -D__CUDA_NO_HALF_OPERATORS -D__CUDA_NO_HALF_CONVERSIONS -D__CUDA_NO_HALF2_OPERATORS --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n```\r\n\r\nThis seems to be the part that failed"
      },
      {
        "user": "yeounyi",
        "created_at": "2020-11-17T00:37:48Z",
        "body": "Does \"output from\" mean I have to move to that path and install fairseq? \r\nI tried, but it turns out that I don't have cuda-8.0 directory. I changed cuda version by \r\n`export CUDA_HOME=\"/usr/local/cuda-10.0\"`\r\n`export PATH=\"/usr/local/cuda-10.0/bin:$PATH\"`\r\nI moved to `/usr/local/cuda-10.0/bin` and executed `pip3 install fairseq --user`\r\nI still get the same error. \r\nIf this isn't what you asked for, can you elaborate more what you need me to do? \r\nThanks for your help!\r\n\r\n> Building wheel for fairseq (PEP 517) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/bin/python3 /home/yeoun/.local/lib/python3.6/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/tmpvtb_lpwp\r\n       cwd: /tmp/pip-install-1_u0vikq/fairseq\r\n  Complete output (716 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-3.6\r\n  creating build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/binarizer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/legacy_distributed_data_parallel.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/nan_detector.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/checkpoint_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/quantization_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/iterative_refinement_generator.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/file_io.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/pdb.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/distributed_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/registry.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/incremental_decoding_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/search.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/trainer.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/token_generation_constraints.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/options.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/file_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  copying fairseq/hub_utils.py -> build/lib.linux-x86_64-3.6/fairseq\r\n  creating build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/eval_lm.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/preprocess.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/generate.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/train.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/__init__.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/interactive.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/validate.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  copying fairseq_cli/score.py -> build/lib.linux-x86_64-3.6/fairseq_cli\r\n  creating build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/legacy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/wav2vec_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/composite_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/sentence_ranking.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/nat_loss.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/ctc.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  copying fairseq/criterions/sentence_prediction.py -> build/lib.linux-x86_64-3.6/fairseq/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/chrf.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/wer.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  copying fairseq/scoring/bleu.py -> build/lib.linux-x86_64-3.6/fairseq/scoring\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/subsample_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/nested_dictionary_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/lru_cache_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/transform_eos_lang_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/offset_tokens_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/multi_corpus_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/raw_label_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/colorize_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/append_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/prepend_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/denoising_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/add_target_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/mask_tokens_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/iterators.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/plasma_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/transform_eos_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/num_samples_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/fasta_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/pad_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/lm_context_window_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/numel_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/concat_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/roll_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/noising.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/list_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/resampling_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/shorten_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/multi_corpus_sampled_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/bucket_pad_length_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/id_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/sort_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/replace_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/strip_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/concat_sentences_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/prepend_token_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  copying fairseq/data/base_wrapper_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data\r\n  creating build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  copying fairseq/dataclass/constants.py -> build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  copying fairseq/dataclass/utils.py -> build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  copying fairseq/dataclass/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  copying fairseq/dataclass/data_class.py -> build/lib.linux-x86_64-3.6/fairseq/dataclass\r\n  creating build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_lev.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_multi_simple_epoch.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/semisupervised_translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/legacy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/cross_lingual_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_from_pretrained_bart.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/multilingual_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/denoising.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/speech_to_text.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/multilingual_denoising.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/sentence_ranking.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/audio_pretraining.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/translation_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  copying fairseq/tasks/sentence_prediction.py -> build/lib.linux-x86_64-3.6/fairseq/tasks\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples\r\n  copying fairseq/examples/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples\r\n  creating build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/dummy_masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/dummy_model.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/dummy_mt.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  copying fairseq/benchmark/dummy_lm.py -> build/lib.linux-x86_64-3.6/fairseq/benchmark\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer_align.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer_from_pretrained_xlm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lstm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lstm_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/multilingual_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/model_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/masked_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lightconv_lm.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/lightconv.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fused_adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adafactor.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adamax.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/dynamic_loss_scaler.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/nag.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adadelta.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/adam.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fused_lamb.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/shard.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/bmuf.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/fairseq/optim\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/gumbel_vector_quantizer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/dynamic_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/character_token_embedder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transpose_last.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/fp32_group_norm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/layer_norm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/gelu.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/transformer_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/layer_drop.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/fairseq_dropout.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/unfold.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/sparse_transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/kmeans_vector_quantizer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/dynamic_crf_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/lightweight_convolution.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/same_pad.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/quant_noise.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/adaptive_input.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  copying fairseq/modules/vggblock.py -> build/lib.linux-x86_64-3.6/fairseq/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/logging\r\n  copying fairseq/logging/meters.py -> build/lib.linux-x86_64-3.6/fairseq/logging\r\n  copying fairseq/logging/progress_bar.py -> build/lib.linux-x86_64-3.6/fairseq/logging\r\n  copying fairseq/logging/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/logging\r\n  copying fairseq/logging/metrics.py -> build/lib.linux-x86_64-3.6/fairseq/logging\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel\r\n  copying fairseq/model_parallel/megatron_trainer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel\r\n  copying fairseq/model_parallel/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/masked_lm_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/block_pair_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  copying fairseq/data/legacy/masked_lm_dictionary.py -> build/lib.linux-x86_64-3.6/fairseq/data/legacy\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/raw_audio_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/speech_to_text_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  copying fairseq/data/audio/audio_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/gpt2_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/nltk_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/space_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/moses_tokenizer.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/hf_byte_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/gpt2_bpe_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/bytes.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/subword_nmt_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/characters.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/byte_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/fastbpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/byte_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  copying fairseq/data/encoders/hf_bert_bpe.py -> build/lib.linux-x86_64-3.6/fairseq/data/encoders\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/multilingual_data_manager.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/sampled_multi_epoch_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/multilingual_utils.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/sampled_multi_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  copying fairseq/data/multilingual/sampling_method.py -> build/lib.linux-x86_64-3.6/fairseq/data/multilingual\r\n  creating build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  copying fairseq/data/audio/feature_transforms/specaugment.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  copying fairseq/data/audio/feature_transforms/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  copying fairseq/data/audio/feature_transforms/global_cmvn.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  copying fairseq/data/audio/feature_transforms/utterance_cmvn.py -> build/lib.linux-x86_64-3.6/fairseq/data/audio/feature_transforms\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/rxf\r\n  copying fairseq/examples/rxf/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation\r\n  copying fairseq/examples/simultaneous_translation/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  copying fairseq/examples/speech_recognition/infer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  copying fairseq/examples/speech_recognition/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  copying fairseq/examples/speech_recognition/w2l_decoder.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_generate.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_tune.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_score_bw.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_options.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  copying fairseq/examples/noisychannel/rerank_score_lm.py -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/rxf/src\r\n  copying fairseq/examples/rxf/src/sentence_prediction_r3f.py -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf/src\r\n  copying fairseq/examples/rxf/src/label_smoothed_cross_entropy_r3f.py -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf/src\r\n  copying fairseq/examples/rxf/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/criterions\r\n  copying fairseq/examples/simultaneous_translation/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/criterions\r\n  copying fairseq/examples/simultaneous_translation/criterions/label_smoothed_cross_entropy_latency_augmented.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/models\r\n  copying fairseq/examples/simultaneous_translation/models/transformer_monotonic_attention.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/models\r\n  copying fairseq/examples/simultaneous_translation/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/evaluate.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/eval_latency.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/client.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/server.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  copying fairseq/examples/simultaneous_translation/eval/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/modules\r\n  copying fairseq/examples/simultaneous_translation/modules/monotonic_multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/modules\r\n  copying fairseq/examples/simultaneous_translation/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/modules\r\n  copying fairseq/examples/simultaneous_translation/modules/monotonic_transformer_layer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/utils\r\n  copying fairseq/examples/simultaneous_translation/utils/latency.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/utils\r\n  copying fairseq/examples/simultaneous_translation/utils/functions.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/utils\r\n  copying fairseq/examples/simultaneous_translation/utils/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/utils\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/word_splitter.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/agent.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/simul_trans_text_agent.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/simul_trans_agent.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  copying fairseq/examples/simultaneous_translation/eval/agents/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/agents\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/scorers\r\n  copying fairseq/examples/simultaneous_translation/eval/scorers/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/scorers\r\n  copying fairseq/examples/simultaneous_translation/eval/scorers/scorer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/scorers\r\n  copying fairseq/examples/simultaneous_translation/eval/scorers/text_scorer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/eval/scorers\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/criterions\r\n  copying fairseq/examples/speech_recognition/criterions/ASG_loss.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/criterions\r\n  copying fairseq/examples/speech_recognition/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/criterions\r\n  copying fairseq/examples/speech_recognition/criterions/cross_entropy_acc.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/replabels.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/asr_dataset.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  copying fairseq/examples/speech_recognition/data/collaters.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/data\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/tasks\r\n  copying fairseq/examples/speech_recognition/tasks/speech_recognition.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/tasks\r\n  copying fairseq/examples/speech_recognition/tasks/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/tasks\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/models\r\n  copying fairseq/examples/speech_recognition/models/w2l_conv_glu_enc.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/models\r\n  copying fairseq/examples/speech_recognition/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/models\r\n  copying fairseq/examples/speech_recognition/models/vggtransformer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/levenshtein_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/nat_crf_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/fairseq_nat_model.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/iterative_nonautoregressive_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/levenshtein_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/cmlm_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/insertion_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  copying fairseq/models/nat/nonautoregressive_ensembles.py -> build/lib.linux-x86_64-3.6/fairseq/models/nat\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/huggingface\r\n  copying fairseq/models/huggingface/hf_gpt2.py -> build/lib.linux-x86_64-3.6/fairseq/models/huggingface\r\n  copying fairseq/models/huggingface/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/huggingface\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/hub_interface.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/model.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  copying fairseq/models/bart/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/bart\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  copying fairseq/models/wav2vec/wav2vec.py -> build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  copying fairseq/models/wav2vec/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  copying fairseq/models/wav2vec/wav2vec2_asr.py -> build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  copying fairseq/models/wav2vec/wav2vec2.py -> build/lib.linux-x86_64-3.6/fairseq/models/wav2vec\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/speech_to_text\r\n  copying fairseq/models/speech_to_text/berard.py -> build/lib.linux-x86_64-3.6/fairseq/models/speech_to_text\r\n  copying fairseq/models/speech_to_text/s2t_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/models/speech_to_text\r\n  copying fairseq/models/speech_to_text/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/speech_to_text\r\n  creating build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/hub_interface.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/model.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/model_camembert.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/model_xlmr.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  copying fairseq/models/roberta/alignment_utils.py -> build/lib.linux-x86_64-3.6/fairseq/models/roberta\r\n  creating build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  copying fairseq/optim/lr_scheduler/polynomial_decay_schedule.py -> build/lib.linux-x86_64-3.6/fairseq/optim/lr_scheduler\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization\r\n  copying fairseq/modules/quantization/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization\r\n  copying fairseq/modules/quantization/quantization_options.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/dynamicconv_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  copying fairseq/modules/dynamicconv_layer/setup.py -> build/lib.linux-x86_64-3.6/fairseq/modules/dynamicconv_layer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/lightconv_layer.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  copying fairseq/modules/lightconv_layer/setup.py -> build/lib.linux-x86_64-3.6/fairseq/modules/lightconv_layer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  copying fairseq/modules/quantization/pq/em.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  copying fairseq/modules/quantization/pq/utils.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  copying fairseq/modules/quantization/pq/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  copying fairseq/modules/quantization/pq/pq.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar\r\n  copying fairseq/modules/quantization/scalar/ops.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar\r\n  copying fairseq/modules/quantization/scalar/utils.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar\r\n  copying fairseq/modules/quantization/scalar/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  copying fairseq/modules/quantization/pq/modules/qconv.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  copying fairseq/modules/quantization/pq/modules/qlinear.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  copying fairseq/modules/quantization/pq/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  copying fairseq/modules/quantization/pq/modules/qemb.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/pq/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/qconv.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/qlinear.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/qact.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  copying fairseq/modules/quantization/scalar/modules/qemb.py -> build/lib.linux-x86_64-3.6/fairseq/modules/quantization/scalar/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/criterions\r\n  copying fairseq/model_parallel/criterions/vocab_parallel_cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/criterions\r\n  copying fairseq/model_parallel/criterions/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/criterions\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/models\r\n  copying fairseq/model_parallel/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models\r\n  copying fairseq/model_parallel/models/transformer_lm.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models\r\n  copying fairseq/model_parallel/models/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/transformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/transformer_layer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  copying fairseq/model_parallel/modules/multihead_attention.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/pipeline_parallel_transformer\r\n  copying fairseq/model_parallel/models/pipeline_parallel_transformer/model.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/pipeline_parallel_transformer\r\n  copying fairseq/model_parallel/models/pipeline_parallel_transformer/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/pipeline_parallel_transformer\r\n  copying fairseq/model_parallel/models/pipeline_parallel_transformer/layers.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/pipeline_parallel_transformer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/roberta\r\n  copying fairseq/model_parallel/models/roberta/model.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/roberta\r\n  copying fairseq/model_parallel/models/roberta/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/models/roberta\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron\r\n  creating build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/grads.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/data.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/random.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/utils.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/cross_entropy.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/initialize.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/mappings.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/transformer.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  copying fairseq/model_parallel/megatron/mpu/layers.py -> build/lib.linux-x86_64-3.6/fairseq/model_parallel/megatron/mpu\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config\r\n  copying fairseq/config/config_eval_lm.yaml -> build/lib.linux-x86_64-3.6/fairseq/config\r\n  copying fairseq/config/config.yaml -> build/lib.linux-x86_64-3.6/fairseq/config\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/optimizer\r\n  copying fairseq/config/optimizer/nag.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/optimizer\r\n  copying fairseq/config/optimizer/adam.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/optimizer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/params\r\n  copying fairseq/config/params/eval_lm_params.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/params\r\n  copying fairseq/config/params/training_params.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/params\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/task\r\n  copying fairseq/config/task/language_modeling.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/task\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/criterion\r\n  copying fairseq/config/criterion/cross_entropy.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/criterion\r\n  copying fairseq/config/criterion/adaptive_loss.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/criterion\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/lr_scheduler\r\n  copying fairseq/config/lr_scheduler/cosine.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/lr_scheduler\r\n  copying fairseq/config/lr_scheduler/inverse_sqrt.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/lr_scheduler\r\n  creating build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_baevski_wiki103.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gpt2_big.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_wiki103.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_big.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gpt2_small.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gbw.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gpt2_medium.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_gpt.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/config/model/transformer_lm_baevski_gbw.yaml -> build/lib.linux-x86_64-3.6/fairseq/config/model\r\n  copying fairseq/examples/.gitignore -> build/lib.linux-x86_64-3.6/fairseq/examples\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/scaling_nmt\r\n  copying fairseq/examples/scaling_nmt/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/scaling_nmt\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  copying fairseq/examples/pointer_generator/preprocess.py -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  copying fairseq/examples/pointer_generator/postprocess.py -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  copying fairseq/examples/pointer_generator/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  copying fairseq/examples/pointer_generator/README.xsum.md -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator/src\r\n  copying fairseq/examples/pointer_generator/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator/src\r\n  copying fairseq/examples/pointer_generator/src/transformer_pg.py -> build/lib.linux-x86_64-3.6/fairseq/examples/pointer_generator/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/quant_noise\r\n  copying fairseq/examples/quant_noise/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/quant_noise\r\n  copying fairseq/examples/quant_noise/transformer_quantization_config.yaml -> build/lib.linux-x86_64-3.6/fairseq/examples/quant_noise\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100\r\n  copying fairseq/examples/m2m_100/install_dependecies.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100\r\n  copying fairseq/examples/m2m_100/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100\r\n  copying fairseq/examples/m2m_100/tok.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/tokenizer_ar.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/seg_ja.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/tokenize_zh.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/seg_ko.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/tokenize_thai.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  copying fairseq/examples/m2m_100/tokenizers/tokenize_indic.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers/thirdparty\r\n  copying fairseq/examples/m2m_100/tokenizers/thirdparty/.gitignore -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/tokenizers/thirdparty\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/process_data\r\n  copying fairseq/examples/m2m_100/process_data/clean_histogram.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/process_data\r\n  copying fairseq/examples/m2m_100/process_data/remove_too_much_punc.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/process_data\r\n  copying fairseq/examples/m2m_100/process_data/dedup_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/m2m_100/process_data\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/joint_alignment_translation\r\n  copying fairseq/examples/joint_alignment_translation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/joint_alignment_translation\r\n  copying fairseq/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/joint_alignment_translation\r\n  copying fairseq/examples/rxf/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/rxf\r\n  copying fairseq/examples/simultaneous_translation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/docs\r\n  copying fairseq/examples/simultaneous_translation/docs/evaluation.md -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/docs\r\n  copying fairseq/examples/simultaneous_translation/docs/baseline.md -> build/lib.linux-x86_64-3.6/fairseq/examples/simultaneous_translation/docs\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/layerdrop\r\n  copying fairseq/examples/layerdrop/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/layerdrop\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/camembert\r\n  copying fairseq/examples/camembert/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/camembert\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/megatron_11b\r\n  copying fairseq/examples/megatron_11b/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/megatron_11b\r\n  copying fairseq/examples/megatron_11b/detok.py -> build/lib.linux-x86_64-3.6/fairseq/examples/megatron_11b\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/paraphraser\r\n  copying fairseq/examples/paraphraser/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/paraphraser\r\n  copying fairseq/examples/paraphraser/paraphrase.py -> build/lib.linux-x86_64-3.6/fairseq/examples/paraphraser\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/conv_seq2seq\r\n  copying fairseq/examples/conv_seq2seq/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/conv_seq2seq\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/nonautoregressive_translation\r\n  copying fairseq/examples/nonautoregressive_translation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/nonautoregressive_translation\r\n  copying fairseq/examples/nonautoregressive_translation/scripts.md -> build/lib.linux-x86_64-3.6/fairseq/examples/nonautoregressive_translation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  copying fairseq/examples/language_model/prepare-wikitext-103.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  copying fairseq/examples/language_model/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  copying fairseq/examples/language_model/README.conv.md -> build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  copying fairseq/examples/language_model/README.adaptive_inputs.md -> build/lib.linux-x86_64-3.6/fairseq/examples/language_model\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  copying fairseq/examples/unsupervised_quality_estimation/repeat_lines.py -> build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  copying fairseq/examples/unsupervised_quality_estimation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  copying fairseq/examples/unsupervised_quality_estimation/aggregate_scores.py -> build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  copying fairseq/examples/unsupervised_quality_estimation/meteor.py -> build/lib.linux-x86_64-3.6/fairseq/examples/unsupervised_quality_estimation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/bart\r\n  copying fairseq/examples/bart/README.glue.md -> build/lib.linux-x86_64-3.6/fairseq/examples/bart\r\n  copying fairseq/examples/bart/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/bart\r\n  copying fairseq/examples/bart/README.summarization.md -> build/lib.linux-x86_64-3.6/fairseq/examples/bart\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/wav2vec_manifest.py -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/wav2vec_featurize.py -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/vq-wav2vec_featurize.py -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  copying fairseq/examples/wav2vec/libri_labels.py -> build/lib.linux-x86_64-3.6/fairseq/examples/wav2vec\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/prepare-wmt14en2fr.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/prepare-iwslt17-multilingual.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/prepare-iwslt14.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  copying fairseq/examples/translation/prepare-wmt14en2de.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/translation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  copying fairseq/examples/multilingual/train_multilingual_model.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  copying fairseq/examples/multilingual/finetune_multilingual_model.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  copying fairseq/examples/multilingual/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  copying fairseq/examples/multilingual/multilingual_fairseq_gen.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/multilingual\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe\r\n  copying fairseq/examples/translation_moe/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe\r\n  copying fairseq/examples/translation_moe/score.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  copying fairseq/examples/translation_moe/src/translation_moe.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  copying fairseq/examples/translation_moe/src/logsumexp_moe.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  copying fairseq/examples/translation_moe/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  copying fairseq/examples/translation_moe/src/mean_pool_gating_network.py -> build/lib.linux-x86_64-3.6/fairseq/examples/translation_moe/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  copying fairseq/examples/criss/download_and_preprocess_flores_test.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  copying fairseq/examples/criss/save_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  copying fairseq/examples/criss/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  copying fairseq/examples/criss/download_and_preprocess_tatoeba.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/criss/mining\r\n  copying fairseq/examples/criss/mining/mine_example.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/mining\r\n  copying fairseq/examples/criss/mining/mine.py -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/mining\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/criss/sentence_retrieval\r\n  copying fairseq/examples/criss/sentence_retrieval/encoder_analysis.py -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/sentence_retrieval\r\n  copying fairseq/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/sentence_retrieval\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/criss/unsupervised_mt\r\n  copying fairseq/examples/criss/unsupervised_mt/eval.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/criss/unsupervised_mt\r\n  copying fairseq/examples/speech_recognition/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/datasets\r\n  copying fairseq/examples/speech_recognition/datasets/prepare-librispeech.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/datasets\r\n  copying fairseq/examples/speech_recognition/datasets/asr_prep_json.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/datasets\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/utils\r\n  copying fairseq/examples/speech_recognition/utils/wer_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_recognition/utils\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/wmt19\r\n  copying fairseq/examples/wmt19/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/wmt19\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/prep_librispeech_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/prep_covost_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/data_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  copying fairseq/examples/speech_to_text/prep_mustc_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/speech_to_text\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/pay_less_attention_paper\r\n  copying fairseq/examples/pay_less_attention_paper/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/pay_less_attention_paper\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/constrained_decoding\r\n  copying fairseq/examples/constrained_decoding/tok.py -> build/lib.linux-x86_64-3.6/fairseq/examples/constrained_decoding\r\n  copying fairseq/examples/constrained_decoding/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/constrained_decoding\r\n  copying fairseq/examples/constrained_decoding/normalize.py -> build/lib.linux-x86_64-3.6/fairseq/examples/constrained_decoding\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth\r\n  copying fairseq/examples/latent_depth/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src\r\n  copying fairseq/examples/latent_depth/src/multilingual_translation_latent_depth.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src\r\n  copying fairseq/examples/latent_depth/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/models\r\n  copying fairseq/examples/latent_depth/src/models/latent_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/models\r\n  copying fairseq/examples/latent_depth/src/models/latent_multilingual_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/models\r\n  copying fairseq/examples/latent_depth/src/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/modules\r\n  copying fairseq/examples/latent_depth/src/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/modules\r\n  copying fairseq/examples/latent_depth/src/modules/latent_layers.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/loss\r\n  copying fairseq/examples/latent_depth/src/loss/latent_depth.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/loss\r\n  copying fairseq/examples/latent_depth/src/loss/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/latent_depth/src/loss\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/cross_lingual_language_model\r\n  copying fairseq/examples/cross_lingual_language_model/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/cross_lingual_language_model\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/mbart\r\n  copying fairseq/examples/mbart/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/mbart\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/stories\r\n  copying fairseq/examples/stories/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/stories\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  copying fairseq/examples/byte_level_bpe/get_bitext.py -> build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  copying fairseq/examples/byte_level_bpe/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  copying fairseq/examples/byte_level_bpe/get_data.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  copying fairseq/examples/byte_level_bpe/gru_transformer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/byte_level_bpe\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/prepare-de-monolingual.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/sacrebleu.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/prepare-wmt18en2de.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/tokenized_bleu.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/deduplicate_lines.py -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  copying fairseq/examples/backtranslation/extract_bt_data.py -> build/lib.linux-x86_64-3.6/fairseq/examples/backtranslation\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.custom_classification.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/multiprocessing_bpe_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.glue.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.race.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/preprocess_RACE.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/preprocess_RACE.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/preprocess_GLUE_tasks.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  copying fairseq/examples/roberta/README.pretraining.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/wsc_task.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/wsc_criterion.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  copying fairseq/examples/roberta/wsc/wsc_utils.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/wsc\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/roberta/commonsense_qa/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/roberta/commonsense_qa/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/roberta/commonsense_qa/commonsense_qa_task.py -> build/lib.linux-x86_64-3.6/fairseq/examples/roberta/commonsense_qa\r\n  copying fairseq/examples/noisychannel/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/noisychannel\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/linformer\r\n  copying fairseq/examples/linformer/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src\r\n  copying fairseq/examples/linformer/src/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/models\r\n  copying fairseq/examples/linformer/src/models/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/models\r\n  copying fairseq/examples/linformer/src/models/linformer_roberta.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/models\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  copying fairseq/examples/linformer/src/modules/linformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  copying fairseq/examples/linformer/src/modules/__init__.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  copying fairseq/examples/linformer/src/modules/linformer_sentence_encoder.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  copying fairseq/examples/linformer/src/modules/multihead_linear_attention.py -> build/lib.linux-x86_64-3.6/fairseq/examples/linformer/src/modules\r\n  creating build/lib.linux-x86_64-3.6/fairseq/examples/xlmr\r\n  copying fairseq/examples/xlmr/README.md -> build/lib.linux-x86_64-3.6/fairseq/examples/xlmr\r\n  running build_ext\r\n  cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\r\n  cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\r\n  building 'fairseq.libbleu' extension\r\n  creating build/temp.linux-x86_64-3.6\r\n  creating build/temp.linux-x86_64-3.6/fairseq\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib/libbleu\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\r\n  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.data.data_utils_fast' extension\r\n  creating build/temp.linux-x86_64-3.6/fairseq/data\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\r\n                   from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12,\r\n                   from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                   from fairseq/data/data_utils_fast.cpp:624:\r\n  /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n   #warning \"Using deprecated NumPy API, disable it with \" \\\r\n    ^\r\n  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.data.token_block_utils_fast' extension\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\r\n  In file included from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,\r\n                   from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12,\r\n                   from /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                   from fairseq/data/token_block_utils_fast.cpp:625:\r\n  /usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n   #warning \"Using deprecated NumPy API, disable it with \" \\\r\n    ^\r\n  fairseq/data/token_block_utils_fast.cpp: In function ‘PyArrayObject* __pyx_f_7fairseq_4data_22token_block_utils_fast__get_slice_indices_fast(PyArrayObject*, PyObject*, int, int, int)’:\r\n  fairseq/data/token_block_utils_fast.cpp:3290:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         __pyx_t_4 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\r\n                                      ^\r\n  fairseq/data/token_block_utils_fast.cpp:3485:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         __pyx_t_3 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\r\n                                      ^\r\n  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.libnat' extension\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib/libnat\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n  In file included from fairseq/clib/libnat/edit_dist.cpp:9:0:\r\n  /usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include/torch/torch.h:7:2: warning: #warning \"Including torch/torch.h for C++ extensions is deprecated. Please include torch/extension.h\" [-Wcpp]\r\n   #warning \\\r\n    ^\r\n  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/fairseq/clib/libnat/edit_dist.o -o build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so\r\n  building 'fairseq.libnat_cuda' extension\r\n  creating build/temp.linux-x86_64-3.6/fairseq/clib/libnat_cuda\r\n  /usr/local/cuda-10.0/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat_cuda/edit_dist.cu -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat_cuda/edit_dist.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n  fairseq/clib/libnat_cuda/edit_dist.cu:9:23: fatal error: edit_dist.h: No such file or directory\r\n  compilation terminated.\r\n  error: command '/usr/local/cuda-10.0/bin/nvcc' failed with exit status 1\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for fairseq\r\nFailed to build fairseq\r\nERROR: Could not build wheels for fairseq which use PEP 517 and cannot be installed directly\r\n"
      },
      {
        "user": "lematt1991",
        "created_at": "2020-11-17T00:39:33Z",
        "body": "Sorry, I meant run the following command:\r\n\r\n```\r\n/usr/local/cuda-8.0/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/include/python3.6m -c fairseq/clib/libnat_cuda/edit_dist.cu -o build/temp.linux-x86_64-3.6/fairseq/clib/libnat_cuda/edit_dist.o -D__CUDA_NO_HALF_OPERATORS -D__CUDA_NO_HALF_CONVERSIONS -D__CUDA_NO_HALF2_OPERATORS --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=libnat_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\n```\r\n\r\nAnd post the output from that command here."
      },
      {
        "user": "yeounyi",
        "created_at": "2020-11-17T01:37:31Z",
        "body": "The output is like this\r\n\r\n`gcc: error: fairseq/clib/libnat_cuda/edit_dist.cu: No such file or directory\r\ngcc: warning: ‘-x c++’ after last input file has no effect\r\ngcc: fatal error: no input files\r\ncompilation terminated.`"
      },
      {
        "user": "myleott",
        "created_at": "2020-11-23T16:49:25Z",
        "body": "Please install the latest version (0.10.1) which includes prebuilt wheels for common platforms."
      },
      {
        "user": "EricLina",
        "created_at": "2022-01-08T14:23:31Z",
        "body": "wtf! That's too long!"
      }
    ]
  },
  {
    "number": 2886,
    "title": "data augmentation issues on summarization",
    "created_at": "2020-11-12T12:13:21Z",
    "closed_at": "2020-11-12T18:20:38Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2886",
    "body": "Hi, sorry to bother.\r\nI am supposed to manually add some other tags after sentences in each line in train.target for loss computation during training. \r\n`For example, the original line(summary) in train.target file is \"hello, world.\" //The fixed line is supposed to be \"hello, world. <token> positive.\"` \r\n(I am about to insert some special tokens during them.However, I can't let the added part influence the training or fine-tuning. \r\nAs my initial idea, I want to add a value-key in dict in LanguagePairDataset so that in loss part I can use sample['target_2'] to find my adding tags such as 'positive'.(sample['target'] is the reference summary)\r\nSo how can I fix this issue? Fairseq framework is packaged so well that I can't handle it roughly. I would like to hear for some tips or advice on this idea. Thanks a lot!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2886/comments",
    "author": "monologue1107",
    "comments": [
      {
        "user": "monologue1107",
        "created_at": "2020-11-12T12:18:45Z",
        "body": "@myleott @alexeib @tangyuq "
      },
      {
        "user": "huihuifan",
        "created_at": "2020-11-12T18:20:38Z",
        "body": "zero the loss in the NLL criterion for the special token :) just like it zeroes the padding token "
      }
    ]
  },
  {
    "number": 2883,
    "title": "should I rewrite the model with dataclass if I want to use hydra_train",
    "created_at": "2020-11-11T13:21:00Z",
    "closed_at": "2022-04-19T12:21:35Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2883",
    "body": "I noticed that only language_modeling.py in tasks and transformer_lm.py in models  are implemented with the Dataclass, but if I want to  use hydra_train to train a translation task and use the vanilla transformer model, should I rewrite the model with Dataclass ? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2883/comments",
    "author": "zhengxxn",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-11-11T16:02:27Z",
        "body": "Yep, we only migrated the LM task so far, but will be migrating the rest in a few weeks. If you do migrate the translation task before then, we'd love a PR! 😄 "
      },
      {
        "user": "zhengxxn",
        "created_at": "2020-11-12T08:12:00Z",
        "body": "Ok, I will try my best to submit a PR, but maybe I need to take some time to understand the code first ..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T08:04:54Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:21:03Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2875,
    "title": "Can I increase the maximum input size using pre-trained M2M-100?",
    "created_at": "2020-11-10T06:04:16Z",
    "closed_at": "2022-05-02T20:22:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2875",
    "body": "Hi, I try to use M2M-100 to generate the translation.\r\nWhen I input my data to the pre-trained model,  an input size error has occurred.\r\n\r\n`Exception: Size of sample #154922 is invalid (=(262, 3)) since max_positions=(256, 256), skip this example with --skip-invalid-size-inputs-valid-test`\r\n\r\nI tried some options but these have not worked.\r\n```\r\nfairseq-generate \\\r\n        data_bin \\\r\n        --batch-size 32 \\\r\n        --num-workers 2 \\\r\n        --path 12b_last_chk_2_gpus.pt \\\r\n        --fixed-dictionary model_dict.128k.txt \\\r\n        -s ${source_lang} -t ${target_lang} \\\r\n        --remove-bpe 'sentencepiece' \\\r\n        --beam 5 \\\r\n        --task translation_multi_simple_epoch \\\r\n        --lang-pairs language_pairs.txt \\\r\n        --decoder-langtok --encoder-langtok src \\\r\n        --gen-subset test \\\r\n        --fp16 \\\r\n        --dataset-impl mmap \\\r\n        --distributed-world-size 1 --distributed-no-spawn \\\r\n        --max-source-positions 1024 \\\r\n        --max-target-positions 1024 \\\r\n        --max-tokens 1024 \\\r\n        --max-tokens-valid 1024 \\\r\n        --pipeline-model-parallel \\\r\n        --pipeline-chunks 1 \\\r\n        --pipeline-encoder-balance '[26]' \\\r\n        --pipeline-encoder-devices '[0]' \\\r\n        --pipeline-decoder-balance '[3,22,1]' \\\r\n        --pipeline-decoder-devices '[0,1,0]'\r\n```\r\nI'm wondering whether we can increase the maximum input size using the pre-trained model. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2875/comments",
    "author": "haruya-umemoto",
    "comments": [
      {
        "user": "SaricVr",
        "created_at": "2021-01-04T16:21:28Z",
        "body": "Hello, I'm interested in this as well. In particular, I'd like to be able to translate pieces of texts longer than 256 tokens. I understand that the quality of the results could be negatively impacted since training was performed with a limit of 256. It would be interesting to investigate the behavior anyway.\r\n\r\nThanks"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T04:04:38Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T20:21:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2868,
    "title": "cuda oom when fine-tuning BART large (distributed training)",
    "created_at": "2020-11-08T15:12:56Z",
    "closed_at": "2022-04-19T12:21:33Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2868",
    "body": "Hello, I'm using the master branch (Nov 8, 2020) to try to fine-tune bart on 3 nodes (each one has 8 32GB v100 gpus) with the following commandline. However, I'm always confronted with cuda oom error before the model starts to train on the training data. Could anyone please let me know what the reason is and how I should solve the issue?\r\n\r\nThanks\r\n\r\n**run_script.sh**\r\n\r\nTOTAL_NUM_UPDATES=1000000\r\nWARMUP_UPDATES=16000\r\nLR=1e-04\r\nMAX_TOKENS=1024\r\nUPDATE_FREQ=8\r\nBART_PATH=../bart.large/model.pt\r\n\r\nCUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\" python -m torch.distributed.launch --nproc_per_node=8 --nnodes=3 --node_rank=$2 --master_addr=$1 --master_port=12345 \\\r\n     $(which fairseq-train) ../data/bin/ \\\r\n    --restore-file $BART_PATH \\\r\n    --max-tokens $MAX_TOKENS \\\r\n    --task translation \\\r\n    --source-lang src --target-lang tgt \\\r\n    --truncate-source \\\r\n    --layernorm-embedding \\\r\n    --share-all-embeddings \\\r\n    --share-decoder-input-output-embed \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --arch bart_large \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --label-smoothing 0.1 \\\r\n    --dropout 0.2 --attention-dropout 0.1 \\\r\n    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.999)\" --adam-eps 1e-08 \\\r\n    --clip-norm 0.1 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --fp16 --update-freq $UPDATE_FREQ \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --find-unused-parameters;\r\n\r\n**error_logs**\r\nTraceback (most recent call last):\r\n  File \"/azureml-envs/azureml_7246816dec74b2f1d318f1f17ec60fe8/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/root/fairseq-20201108/fairseq_cli/train.py\", line 392, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 322, in call_main\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 300, in distributed_main\r\nTraceback (most recent call last):\r\n    main(cfg, **kwargs)\r\n  File \"/root/fairseq-20201108/fairseq_cli/train.py\", line 121, in main\r\n  File \"/azureml-envs/azureml_7246816dec74b2f1d318f1f17ec60fe8/bin/fairseq-train\", line 33, in <module>\r\nTraceback (most recent call last):\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/root/fairseq-20201108/fairseq/checkpoint_utils.py\", line 198, in load_checkpoint\r\n  File \"/azureml-envs/azureml_7246816dec74b2f1d318f1f17ec60fe8/bin/fairseq-train\", line 33, in <module>\r\n    reset_meters=reset_meters,\r\n  File \"/root/fairseq-20201108/fairseq/trainer.py\", line 331, in load_checkpoint\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/root/fairseq-20201108/fairseq_cli/train.py\", line 392, in cli_main\r\n    dist_device=self.device,\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 675, in broadcast_object\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 322, in call_main\r\n  File \"/root/fairseq-20201108/fairseq_cli/train.py\", line 392, in cli_main\r\n    [int(length_tensor.item())], dtype=dist_dtype, device=dist_device\r\nRuntimeError: CUDA out of memory. Tried to allocate 3.79 GiB (GPU 0; 31.75 GiB total capacity; 775.31 MiB already allocated; 1.11 GiB free; 782.00 MiB reserved in total by PyTorch)\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 300, in distributed_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 322, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/root/fairseq-20201108/fairseq_cli/train.py\", line 121, in main\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/root/fairseq-20201108/fairseq/checkpoint_utils.py\", line 198, in load_checkpoint\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 300, in distributed_main\r\n    reset_meters=reset_meters,\r\n  File \"/root/fairseq-20201108/fairseq/trainer.py\", line 331, in load_checkpoint\r\n    main(cfg, **kwargs)\r\n  File \"/root/fairseq-20201108/fairseq_cli/train.py\", line 121, in main\r\n    dist_device=self.device,\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 675, in broadcast_object\r\n    disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n  File \"/root/fairseq-20201108/fairseq/checkpoint_utils.py\", line 198, in load_checkpoint\r\n    reset_meters=reset_meters,\r\n  File \"/root/fairseq-20201108/fairseq/trainer.py\", line 331, in load_checkpoint\r\n    [int(length_tensor.item())], dtype=dist_dtype, device=dist_device\r\nRuntimeError: CUDA out of memory. Tried to allocate 3.79 GiB (GPU 0; 31.75 GiB total capacity; 775.31 MiB already allocated; 1.11 GiB free; 782.00 MiB reserved in total by PyTorch)\r\n    dist_device=self.device,\r\n  File \"/root/fairseq-20201108/fairseq/distributed_utils.py\", line 675, in broadcast_object\r\n    [int(length_tensor.item())], dtype=dist_dtype, device=dist_device\r\nRuntimeError: CUDA out of memory. Tried to allocate 3.79 GiB (GPU 0; 31.75 GiB total capacity; 775.31 MiB already allocated; 1.11 GiB free; 782.00 MiB reserved in total by PyTorch)\r\n\r\n**update**\r\nI add --memory-efficient-fp16. The problem still exists.\r\nAnother update: I tried using the version on July 29 this year. It works well.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2868/comments",
    "author": "getao",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T08:04:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:21:01Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2867,
    "title": "OOM occurred during the middle of training",
    "created_at": "2020-11-08T11:51:01Z",
    "closed_at": "2020-11-12T07:44:41Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2867",
    "body": "I can fine-tune the model at first, even it can train entirely in epoch 1. However, it will become OOM in epoch 2 around 4517/21194. I tried to change scripts like total_num_updates or update_freq several times, but it did't help.\r\nDo you have some idea the OOM problem occurred in the middle part of training and give me some tips? Looking forward for your kindly help.\r\nThe log shows like below:\r\n\r\n\r\n```\r\n2020-11-06 22:55:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 1; 10.92 GiB total capacity; 10.13 GiB already allocated; 13.38 MiB free; 10.33 GiB reserved in total by PyTorch)\r\n...\r\n2020-11-06 22:55:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\r\nTraceback (most recent call last):\r\n  File \"/data/rwd/anaconda3/envs/fairseq/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/data/rwd/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/data/rwd/fairseq/fairseq/distributed_utils.py\", line 254, in call_main\r\n    nprocs=args.distributed_num_procs,\r\n  File \"/data/rwd/anaconda3/envs/fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/data/rwd/anaconda3/envs/fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/data/rwd/fairseq/fairseq/distributed_utils.py\", line 339, in all_gather_list\r\n    result.append(pickle.loads(bytes(out_buffer[header_size:header_size + enc_size].tolist())))\r\n_pickle.UnpicklingError: unpickling stack underflow\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/data/rwd/anaconda3/envs/fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/data/rwd/fairseq/fairseq/distributed_utils.py\", line 238, in distributed_main\r\n    main(args, **kwargs)\r\n  File \"/data/rwd/fairseq/fairseq_cli/train.py\", line 125, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/data/rwd/anaconda3/envs/fairseq/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data/rwd/fairseq/fairseq_cli/train.py\", line 208, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/data/rwd/anaconda3/envs/fairseq/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data/rwd/fairseq/fairseq/trainer.py\", line 531, in train_step\r\n    logging_outputs, sample_size, ooms, train_time, ignore=is_dummy_batch,\r\n  File \"/data/rwd/fairseq/fairseq/trainer.py\", line 885, in _aggregate_logging_outputs\r\n    logging_outputs, *extra_stats_to_sum, ignore=ignore\r\n  File \"/data/rwd/fairseq/fairseq/trainer.py\", line 906, in _all_gather_list_sync\r\n    group=self.data_parallel_process_group,\r\n  File \"/data/rwd/fairseq/fairseq/distributed_utils.py\", line 343, in all_gather_list\r\n    'Unable to unpickle data from other workers. all_gather_list requires all '\r\nException: Unable to unpickle data from other workers. all_gather_list requires all workers to enter the function together, so this error usually indicates that the workers have fallen out of sync somehow. Workers can fall out of sync if one of them runs out of memory, or if there are other conditions in your training script that can cause one worker to finish an epoch while other workers are still iterating over their portions of the data. Try rerunning with --ddp-backend=no_c10d and see if that helps.\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2867/comments",
    "author": "monologue1107",
    "comments": [
      {
        "user": "monologue1107",
        "created_at": "2020-11-08T12:14:47Z",
        "body": "My GPUs are 3*11GB(1080Ti)\r\nHere are my command line:\r\n```\r\nTOTAL_NUM_UPDATES=1000000\r\nWARMUP_UPDATES=500\r\nLR=3e-05\r\nMAX_TOKENS=1024\r\nUPDATE_FREQ=1\r\nBART_PATH=/data/rwd/fairseq/bart.large/model.pt\r\n\r\nCUDA_VISIBLE_DEVICES=0,1,2 fairseq-train cnn_dm-bin \\\r\n    --restore-file $BART_PATH \\\r\n    --max-tokens $MAX_TOKENS \\\r\n    --task translation \\\r\n    --source-lang source --target-lang target \\\r\n    --truncate-source \\\r\n    --layernorm-embedding \\\r\n    --share-all-embeddings \\\r\n    --share-decoder-input-output-embed \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --arch bart_large \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --label-smoothing 0.1 \\\r\n    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.999)\" --adam-eps 1e-08 \\\r\n    --clip-norm 0.1 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --update-freq $UPDATE_FREQ \\\r\n    --memory-efficient-fp16 \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --tensorboard-logdir './logs6/' \\\r\n    --find-unused-parameters &\r\n```\r\n\r\n"
      },
      {
        "user": "alexeib",
        "created_at": "2020-11-08T19:51:15Z",
        "body": "have you tried following the suggestion and using --ddp-backend=no_c10d?"
      },
      {
        "user": "monologue1107",
        "created_at": "2020-11-09T10:37:04Z",
        "body": "> have you tried following the suggestion and using --ddp-backend=no_c10d?\r\n\r\nI just tried with this command. The upper problem has been solved while there seems to be occurred another problem.\r\nWhen I training my code, it have these mistakes:\r\n```\r\n2020-11-08 23:47:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\r\n/data/rwd/anaconda3/envs/fairseq/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown\r\n  len(cache))\r\n```\r\nBesides, it seems to even shut down my gpus. When I use 'nvidia-smi', it shows:\r\n`Unable to determine the device handle for GPU 0000:0D:00.0: GPU is lost.  Reboot the system to recover this GPU`\r\nI have to reboot the computer so that it can become normal. Have you ever seen this kind of problems?"
      },
      {
        "user": "alexeib",
        "created_at": "2020-11-09T18:07:41Z",
        "body": "i have not seen this before. have you tried updating your gpu drivers etc?"
      },
      {
        "user": "monologue1107",
        "created_at": "2020-11-10T01:09:08Z",
        "body": "> i have not seen this before. have you tried updating your gpu drivers etc?\r\n\r\nI don't have root access since I run this code on server. So updating gpu drivers seems to be difficult for me.\r\nWhat about the another error:\r\n```\r\n2020-11-08 23:47:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\r\n/data/rwd/anaconda3/envs/fairseq/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown\r\n  len(cache))\r\n```\r\nI don't know if my code is mistaken or what. It may related to the gpu driver problem or not."
      },
      {
        "user": "alexeib",
        "created_at": "2020-11-10T02:07:52Z",
        "body": "the semaphore thing comes from dataloader processes that asynchronously load data. thats the message you get if you kill the process somehow (e.g. your oom crashes for example)"
      },
      {
        "user": "monologue1107",
        "created_at": "2020-11-12T07:44:33Z",
        "body": "> the semaphore thing comes from dataloader processes that asynchronously load data. thats the message you get if you kill the process somehow (e.g. your oom crashes for example)\r\n\r\nThx for your answer. It seems being solved when I add --ddp-backend=no_c10d and decrease the update_freq to 1."
      },
      {
        "user": "liaohit",
        "created_at": "2022-05-12T04:06:34Z",
        "body": "I just tried add --ddp-backend=no_c10d and decrease the update_freq to 1, but the warning still exist.\r\n\r\n| WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 1; 10.92 GiB total capacity; 10.13 GiB already allocated; 13.38 MiB free; 10.33 GiB reserved in total by PyTorch)\r\n| WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\r\nTraceback"
      }
    ]
  },
  {
    "number": 2866,
    "title": "multiple tasks dataloaders ",
    "created_at": "2020-11-07T17:37:39Z",
    "closed_at": "2020-11-07T19:40:04Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2866",
    "body": "Hi,\r\n\r\nI want to train a seq2seq model, but over multiple tasks, and the datasets are large, is there any implementation of such dataloader handling multiple tasks your could point me to. I appreciate your input on this.\r\n\r\nthanks.\r\n\r\nBest\r\nRabeeh",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2866/comments",
    "author": "rabeehkarimimahabadi",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-11-07T19:39:59Z",
        "body": "Duplicate of #2865"
      }
    ]
  },
  {
    "number": 2862,
    "title": "[RoBERTa pretraining] Initialization and further pre-training with different vocabulary",
    "created_at": "2020-11-07T07:57:18Z",
    "closed_at": "2022-04-19T16:21:13Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2862",
    "body": "Hi all and thanks for the great repo.\r\n\r\nI am looking into pre-training RoBERTa and am wondering whether it is possible to initialize the pre-training with existing weights that were trained on a given pair of pretraining set/vocab A (with argument `--restore-file`)  and then further pre-train with another pair of pretraining set/vocab B?\r\n\r\nI know it is possible with BERT with the tensorflow code provided on their repo, the size of the two vocabs just have to be same. \r\n\r\nThanks a lot in advance.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2862/comments",
    "author": "manueltonneau",
    "comments": [
      {
        "user": "manueltonneau",
        "created_at": "2020-11-18T09:43:47Z",
        "body": "Let me know if you have any tips on this @ngoyal2707 :) "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T08:04:45Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:20:44Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2858,
    "title": "Unrecognized arguments from lightconv example",
    "created_at": "2020-11-06T03:16:48Z",
    "closed_at": "2020-11-06T03:33:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2858",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI try to train a lightconv translation model from scratch. I set the args like the example docs. \r\ne.g. \r\n`--lr-shrink 1 --max-lr 0.001 --lr 1e-7 --min-lr 1e-9 `. \r\nBut it caused an error :\r\n`error: unrecognized arguments: --activation-dropout 0.1 --lr-shrink 1 --max-lr 0.001 --t-mult 1 --lr-period-updates 70000`\r\n#### Code\r\n```\r\npython3 $code_dir/fairseq-master/fairseq_cli/train.py \\\r\n    $data \\\r\n    --ddp-backend=no_c10d \\\r\n    --save-dir $nmt_dir/checkpoints/$signature \\\r\n    --max-update 30000 --share-all-embeddings --optimizer adam \\\r\n    --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --weight-decay 0.0 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --min-lr 1e-09 --update-freq 16 --attention-dropout 0.1 --keep-last-epochs 10 \\\r\n    --ddp-backend=no_c10d --max-tokens 3584 \\\r\n    --lr-scheduler cosine --warmup-init-lr 1e-7 --warmup-updates 10000 \\\r\n    --lr-shrink 1 --max-lr 0.001 --lr 1e-7 --min-lr 1e-9 --warmup-init-lr 1e-07 \\\r\n    --t-mult 1 --lr-period-updates 70000 \\\r\n    --arch lightconv_wmt_en_fr_big --save-dir $SAVE \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-dropout 0.1 \\\r\n    --encoder-glu 1 --decoder-glu 1 \\\r\n    --fp16 \\\r\n    --no-progress-bar\r\n```\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI try to drop these args but the bleu score is too low.\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):1.0\r\n - PyTorch Version (e.g., 1.0)1.4\r\n - OS (e.g., Linux):linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version:3.6\r\n - CUDA/cuDNN version:10.0\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2858/comments",
    "author": "SefaZeng",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-11-06T03:25:34Z",
        "body": "--activation-dropout is named --relu-dropout i think\r\nfor the rest, it should be working (possibly fixed yesterday).\r\nlet me know if it still doesnt work after pulling\r\n\r\ni will look into updating the docs"
      },
      {
        "user": "alexeib",
        "created_at": "2020-11-06T03:33:45Z",
        "body": "actually there is no --activation-dropout in the examples"
      }
    ]
  },
  {
    "number": 2840,
    "title": "Can wav2vec2.0 be finetuned with unsupervised data ?",
    "created_at": "2020-11-03T17:50:14Z",
    "closed_at": "2020-11-03T19:22:24Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2840",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2840/comments",
    "author": "MrityunjoyS",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-11-03T19:22:24Z",
        "body": "if you mean, can you continue pre-training on a different dataset, then yes, you can - just be sure to reset the optimizer and lr-scheduler (with the appropriate --reset-* flags)"
      },
      {
        "user": "MrityunjoyS",
        "created_at": "2020-11-03T19:28:34Z",
        "body": "Hi @alexeib , thanks for your quick response. Actually I'm trying to do unsuperviser training using my own audio files over any wav2vec2.0 base-model already provided. Can I do that ?"
      },
      {
        "user": "alexeib",
        "created_at": "2020-11-03T19:34:16Z",
        "body": "yes, thats exactly what i mean"
      },
      {
        "user": "MrityunjoyS",
        "created_at": "2020-11-03T19:44:32Z",
        "body": "So this is the command I'm running actually -->\r\n\r\n```\r\npython3 train.py /path/audio_file/manifest/save_dir/ \\\r\n--save-dir /path/audio_file/manifest/save_dir/checkpoints/ --fp16  --restore-file /path/audio_file/wav2vec_small.pt --task audio_pretraining --criterion wav2vec --arch wav2vec2 \\\r\n--reset-dataloader --reset-lr-scheduler --reset-meters --reset-optimizer \\\r\n--empty-cache-freq 3 --max-epoch 5 --best-checkpoint-metric wer --num-workers 6 \\\r\n--attention-dropout 0.0 --max-tokens 1280000 --seed 2337 --log-format json --log-interval 500 --ddp-backend no_c10d\r\n```\r\nBelow errors I'm getting -->\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/path/fairseq/fairseq/trainer.py\", line 301, in load_checkpoint\r\n>     state[\"model\"], strict=True, model_cfg=self.cfg.model\r\n>   File \"/path/fairseq/fairseq/models/fairseq_model.py\", line 114, in load_state_dict\r\n>     return super().load_state_dict(new_state_dict, strict)\r\n>   File \"/path/tmp/deepspeech-train-venv/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1045, in load_state_dict\r\n>     self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\n> RuntimeError: Error(s) in loading state_dict for Wav2Vec2Model:\r\n> \tUnexpected key(s) in state_dict: \"quantizer.vars\", \"quantizer.weight_proj.weight\", \"quantizer.weight_proj.bias\", \"feature_extractor.conv_layers.6.0.weight\". \r\n> \tsize mismatch for feature_extractor.conv_layers.1.0.weight: copying a param with shape torch.Size([512, 512, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 8]).\r\n> \tsize mismatch for feature_extractor.conv_layers.2.0.weight: copying a param with shape torch.Size([512, 512, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 4]).\r\n> \tsize mismatch for feature_extractor.conv_layers.3.0.weight: copying a param with shape torch.Size([512, 512, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 4]).\r\n> \tsize mismatch for feature_extractor.conv_layers.4.0.weight: copying a param with shape torch.Size([512, 512, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 4]).\r\n> \tsize mismatch for feature_extractor.conv_layers.5.0.weight: copying a param with shape torch.Size([512, 512, 2]) from checkpoint, the shape in current model is torch.Size([512, 512, 1]).\r\n> \tsize mismatch for project_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([768, 512]).\r\n> \tsize mismatch for project_q.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\r\n> \tsize mismatch for final_proj.weight: copying a param with shape torch.Size([256, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\r\n> \tsize mismatch for final_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"train.py\", line 14, in <module>\r\n>     cli_main()\r\n>   File \"/path/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n>     distributed_utils.call_main(cfg, main)\r\n>   File \"/path/fairseq/fairseq/distributed_utils.py\", line 315, in call_main\r\n>     main(cfg, **kwargs)\r\n>   File \"/path/fairseq/fairseq_cli/train.py\", line 115, in main\r\n>     disable_iterator_cache=task.has_sharded_data(\"train\"),\r\n>   File \"/path/fairseq/fairseq/checkpoint_utils.py\", line 196, in load_checkpoint\r\n>     reset_meters=reset_meters,\r\n>   File \"/path/fairseq/fairseq/trainer.py\", line 310, in load_checkpoint\r\n>     \"please ensure that the architectures match.\".format(filename)\r\n> Exception: Cannot load model parameters from checkpoint /path/audio_file/wav2vec_small.pt; please ensure that the architectures match.\r\n\r\nCan you please look into it ?"
      },
      {
        "user": "alexeib",
        "created_at": "2020-11-03T21:06:07Z",
        "body": "you need to specify all the params from the original pre-training run (you can find it in examples) so that fairseq creates the correct model into which it will then load the weights from your checkpoint"
      },
      {
        "user": "MrityunjoyS",
        "created_at": "2020-11-05T17:09:51Z",
        "body": "HI @alexeib , thanks for the points, I'm able to first train the model with unsupervised data over wav2vec_small base model and then finetune it with 10-12min of supervised data using my own lexicon file. \r\nBut after that while I'm trying to infer using the model I'm facing error, can you please have a look ?!\r\n\r\nCode I'm using -->\r\n```\r\npython3 examples/speech_recognition/infer.py /path/audio_file/manifest/save_dir/ --task audio_pretraining --nbest 1 \\\r\n    --path /path/audio_file/manifest/save_dir/checkpoints1/checkpoint_best.pt --gen-subset valid --results-path /path/audio_file/manifest/save_dir/ \\\r\n    --w2l-decoder fairseqlm --lexicon /path/audio_file/lexicon.lst --lm-model /path/audio_file/lm_librispeech_word_transformer.pt \\\r\n    --lm-weight 2 --word-score -1 --sil-weight 0 --criterion ctc --max-tokens 4000000 --labels ltr\r\n```\r\n\r\nError I'm getting -->\r\n\r\n> INFO:__main__:| loading model(s) from /path/audio_file/manifest/save_dir/checkpoints1/checkpoint_best.pt\r\n> Traceback (most recent call last):\r\n>   File \"examples/speech_recognition/infer.py\", line 464, in <module>\r\n>     cli_main()\r\n>   File \"examples/speech_recognition/infer.py\", line 460, in cli_main\r\n>     main(args)\r\n>   File \"examples/speech_recognition/infer.py\", line 285, in main\r\n>     model_state=model_state,\r\n>   File \"examples/speech_recognition/infer.py\", line 209, in load_models_and_criterions\r\n>     model = task.build_model(args)\r\n>   File \"/path/fairseq/fairseq/tasks/fairseq_task.py\", line 548, in build_model\r\n>     model = models.build_model(args, self)\r\n>   File \"/path/fairseq/fairseq/models/__init__.py\", line 56, in build_model\r\n>     return ARCH_MODEL_REGISTRY[cfg.arch].build_model(cfg, task)\r\n> AttributeError: 'NoneType' object has no attribute 'arch'\r\n>   "
      }
    ]
  },
  {
    "number": 2826,
    "title": "Training BERT-Fuse model gets stuck after few epochs with 100% GPU utilization",
    "created_at": "2020-10-30T11:09:46Z",
    "closed_at": "2022-04-19T12:21:23Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2826",
    "body": "Hello hello,\r\n\r\nI am currently experimenting with the BERT-Fuse model (based on bert-nmt repo custom code / architecture). I could succesfully run a training with a small dataset. Now, as I am attempting a training with a 100+ Million-token-dataset, the training gets stuck with 100% GPU utilisation after a few epochs. Tried reducing the batch size, but same issue. Has anyone faced a similar issue? Do you have any suggestions for solving this please?\r\n\r\nThank you in advance.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2826/comments",
    "author": "salmatfq",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2820,
    "title": "Using wav2vec2.0 finetuned model for streaming_asr",
    "created_at": "2020-10-29T12:48:39Z",
    "closed_at": "2020-11-05T17:05:38Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2820",
    "body": "I'm sorry if I'm missing something or this feature is already there or if it's not possible. But is there any api with which if I've my own dataset finetuned by wa2vec2.0 model, it can be used for streamming_asr  ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2820/comments",
    "author": "MrityunjoyS",
    "comments": [
      {
        "user": "srinath-zapr",
        "created_at": "2020-11-19T10:40:21Z",
        "body": "@MrityunjoyS  Did you solve this issue? I am also searching for a way to do this. "
      },
      {
        "user": "magiczixiao",
        "created_at": "2021-12-23T09:31:48Z",
        "body": "@MrityunjoyS @srinath-zapr Do you have any solutions? I also want to use the streaming ASR. Thanks!"
      }
    ]
  },
  {
    "number": 2816,
    "title": "RoBERTa with next sentence prediction",
    "created_at": "2020-10-29T03:19:35Z",
    "closed_at": "2022-05-02T21:22:15Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2816",
    "body": "## Is there any implementation of RoBERTa with both MLM and next sentence prediction?\r\n\r\n#### What is your question?\r\nRecently, I am trying to apply pre-trained language models to a very different domain (i.e. protein sequence). I want to encode some domain knowledge that can be formulated as an NSP task. However, I did not find any implementations with NSP as part of the pre-train task. So I wonder if there is such implementation.\r\n\r\nIf the answer is no, Could you please give me some suggestions about how to modify the original RoBERTa framework.\r\n\r\nthanks a lot!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2816/comments",
    "author": "yuanenming",
    "comments": [
      {
        "user": "ddofer",
        "created_at": "2021-01-28T09:07:57Z",
        "body": "I'm doing the same problem! We will be releasing the model, code (including data loading) and paper soon :)\r\n(Dan Ofer, M. Linial)"
      },
      {
        "user": "yuanenming",
        "created_at": "2021-01-28T09:24:55Z",
        "body": "Hi, @ddofer \r\nLooking forward to your work"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T03:04:29Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2814,
    "title": "Contributing new pre-trained model to fairseq",
    "created_at": "2020-10-28T18:45:12Z",
    "closed_at": "2022-04-19T12:21:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2814",
    "body": "Hi there, quick question that I couldn't answer by searching the docs: \r\n\r\nI trained an EL-EN (Greek to English) and an EN-EL machine translation model using the `transformer_iwslt_de_en` architecture on ~6GB of parallel corpora. Given that the models report a better BLEU score compared to the existing SotA, I would like to share them somehow. I thought that fairseq might offer a huggingface-like way to upload trained models but I couldn't find any, so I would appreciate any guidance.\r\nAlternatively, if there's a straightforward way to convert and upload these as huggingface models that would also be great. \r\n\r\nMany thanks! ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2814/comments",
    "author": "lighteternal",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:38Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2809,
    "title": "Can I get a translation score for a translation not generated by the model?",
    "created_at": "2020-10-28T04:50:45Z",
    "closed_at": "2022-04-19T12:21:20Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2809",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nCurrently translating by using interactive.py prints scores for the hypothesis like H-0 and so on. I want to know if it is possible that given a source sentence and a hypothesis obtained from elsewhere can I use the NMT to assign a score to it. Please note I know there are measures like BLEU etc. which I am not keen on using.\r\n\r\nIf this is possible can I also modify the models to look into its activations?\r\n\r\nCheers,\r\n\r\nTarun\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2809/comments",
    "author": "reachtarunhere",
    "comments": [
      {
        "user": "lorelupo",
        "created_at": "2020-10-28T15:00:03Z",
        "body": " Hi, the `--score-reference` option should allow you to score the reference translation instead of your own hypothesis."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:39Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2791,
    "title": "Inference wav2vec using char Language Model",
    "created_at": "2020-10-25T12:17:39Z",
    "closed_at": "2022-04-19T12:21:17Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2791",
    "body": "How can i use a char LM along with wav2vec .pt models to generate transcripts?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2791/comments",
    "author": "spygaurad",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:47Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2782,
    "title": "Error when trying to train with pipeline parallelism",
    "created_at": "2020-10-23T15:59:08Z",
    "closed_at": "2020-10-26T08:40:55Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2782",
    "body": "Hi guys,\r\n\r\nI was trying to train a transformer model with pipeline parallelism. Is this supposed to work already? \r\n\r\nThe command i tried (following the translation example):\r\n`fairseq-train     data-bin/iwslt14.tokenized.de-en     --arch transformer_iwslt_de_en_pipeline_parallel --share-decoder-input-output-embed     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0     --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000     --dropout 0.3 --weight-decay 0.0001     --criterion label_smoothed_cross_entropy --label-smoothing 0.1     --max-tokens 4096     --eval-bleu     --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}'     --eval-bleu-detok moses     --eval-bleu-remove-bpe     --eval-bleu-print-samples     --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --pipeline-model-parallel --pipeline-encoder-balance '[8]' --pipeline-encoder-devices '[0]' --pipeline-decoder-balance '[1,6,1]' --pipeline-decoder-devices '[0,1,0]' --pipeline-chunks 1 --distributed-world-size 2`\r\n\r\nerror:\r\n```\r\n2020-10-23 17:17:08 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types\r\n2020-10-23 17:17:08 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types\r\n2020-10-23 17:17:08 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de\r\n2020-10-23 17:17:08 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en\r\n2020-10-23 17:17:08 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples\r\nTraceback (most recent call last):\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/tertiary/thies/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/tertiary/thies/fairseq/fairseq/distributed_utils.py\", line 301, in call_main\r\n    cfg.distributed_training.distributed_world_size,\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 247, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 205, in start_processes\r\n    while not context.join():\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 166, in join\r\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\ntorch.multiprocessing.spawn.ProcessRaisedException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/tertiary/thies/fairseq/fairseq/distributed_utils.py\", line 283, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/tertiary/thies/fairseq/fairseq_cli/train.py\", line 74, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/tertiary/thies/fairseq/fairseq/tasks/translation.py\", line 327, in build_model\r\n    model = super().build_model(args)\r\n  File \"/tertiary/thies/fairseq/fairseq/tasks/fairseq_task.py\", line 548, in build_model\r\n    model = models.build_model(args, self)\r\n  File \"/tertiary/thies/fairseq/fairseq/models/__init__.py\", line 56, in build_model\r\n    return ARCH_MODEL_REGISTRY[cfg.arch].build_model(cfg, task)\r\n  File \"/tertiary/thies/fairseq/fairseq/model_parallel/models/pipeline_parallel_transformer/model.py\", line 277, in build_model\r\n    checkpoint=args.pipeline_checkpoint,\r\n  File \"/tertiary/thies/fairseq/fairseq/model_parallel/models/pipeline_parallel_transformer/model.py\", line 57, in __init__\r\n    + [encoder.final_layer_norm]\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 796, in __getattr__\r\n    type(self).__name__, name))\r\ntorch.nn.modules.module.ModuleAttributeError: 'TransformerEncoder' object has no attribute 'embedding_layer'\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2782/comments",
    "author": "thies1006",
    "comments": [
      {
        "user": "shruti-bh",
        "created_at": "2020-10-23T17:28:40Z",
        "body": "For training, a single `Pipe()` module is created for the Transformer encoder-decoder model. So, you need to set `--pipeline-balance` and `--pipeline-devices` in the training command, instead of `--pipeline-encoder-balance`, `--pipeline-encoder-devices`, `--pipeline-decoder-balance`, `--pipeline-decoder-devices`.\r\nFor inference/generation, two `Pipe()` modules are created, one for the encoder and one for the decoder, since the encoder and decoder are called separately during generation. So, in that case, you need to set `--pipeline-encoder-balance`, `--pipeline-encoder-devices`, `--pipeline-decoder-balance`, `--pipeline-decoder-devices` instead."
      },
      {
        "user": "thies1006",
        "created_at": "2020-10-26T08:40:55Z",
        "body": "Awesome, works now.\r\nThank you very much."
      }
    ]
  },
  {
    "number": 2763,
    "title": "architecture mismatch issue between mbart and general transformer",
    "created_at": "2020-10-21T03:51:19Z",
    "closed_at": "2022-04-19T12:21:10Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2763",
    "body": "## ❓ Questions and Help\r\n\r\nI want to load the mbart model and use the pre-trained weights in a new transformer model but I have the mismatch issues in the architecture layers. mbart's state dict doesn't match with regular transformer architecture because they split the attention layer to three individual layers naming attention_k, attention_v, attention_q. Does anybody know how I can fix this issue? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2763/comments",
    "author": "fahimeh-saleh",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:40Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2759,
    "title": "RoBERTa checkpoint trained on NSP objective",
    "created_at": "2020-10-20T11:22:22Z",
    "closed_at": "2020-10-20T11:29:23Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2759",
    "body": "It has been mentioned in the RoBERTa paper that RoBERTa trained with NSP objective does not help in performance. But for my use case, I wanted to try out RoBERTa trained on NSP+MLM instead of the MLM one which is publicly released. Wanted to know if the NSP+MLM variant can be made publicly available. If it already is, can you please point me to it, unable to find it here and in HF model hub.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2759/comments",
    "author": "prajjwal1",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-10-20T11:29:23Z",
        "body": "Sorry, we don’t have these. In any case, the models with NSP in the paper were smaller scale, so would not be comparable to the released models."
      }
    ]
  },
  {
    "number": 2757,
    "title": "The model cannot predict correctly in levenshtein_transformer",
    "created_at": "2020-10-20T03:38:15Z",
    "closed_at": "2022-04-19T12:21:11Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2757",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nHi，I trained a new model in  levenshtein_transformer.\r\n\r\n\r\n\r\n\r\n\r\n#### Code\r\n\r\nThe data is processed by the code below:\r\n\r\npython ./fairseq_cli/preprocess.py \\\r\n$common_params \\\r\n--trainpref $trainpref \\\r\n--validpref $validpref \\\r\n--testpref $testpref \\\r\n--destdir './out/wmt14_be/' \\\r\n--workers 60 \\\r\n--joined-dictionary \\\r\n--thresholdtgt 1 \\ \r\n--thresholdsrc 1 \\ \r\n| tee $OUT/data_bin_$data.log\r\n\r\nAnd the model is trained by:\r\n\r\nDATA_BIN=./out/wmt14_be/\r\nCUDA_VISIBLE_DEVICES=$device nohup python train.py $DATA_BIN \\\r\n  --save-dir $MODELS \\\r\n  --seed 4321 \\\r\n  --ddp-backend=no_c10d \\\r\n  --task translation_lev \\\r\n  --criterion nat_loss \\\r\n  --arch levenshtein_transformer \\\r\n  --noise random_delete \\\r\n  --share-all-embeddings \\\r\n  --optimizer adam --adam-betas '(0.9,0.98)' \\\r\n  --lr 0.0005 --lr-scheduler inverse_sqrt \\\r\n  --min-lr '1e-09' --warmup-updates 10000 \\\r\n  --warmup-init-lr '1e-07' --label-smoothing 0.1 \\\r\n  --dropout 0.3 --weight-decay 0.01 \\\r\n  --decoder-learned-pos \\\r\n  --encoder-learned-pos \\\r\n  --apply-bert-init \\\r\n  --log-format 'simple' --log-interval 100 \\\r\n  --fixed-validation-seed 7 \\ \r\n  --max-tokens 8000 \\\r\n  --save-interval-updates 10000 \\\r\n  --max-epoch 3\\\r\n  --max-update 300000   > $OUT/log$exp.out 2>&1 &\r\n\r\nand get result is：\r\n\r\nCUDA_VISIBLE_DEVICES=$device python ./fairseq_cli/interactive.py ./out/wmt14_be \\\r\n    --task translation_lev \\\r\n    --path ./out/models20201020_111531/checkpoint_last.pt \\\r\n    --iter-decode-max-iter 9 \\ \r\n    --iter-decode-eos-penalty 2 \\ \r\n    --remove-bpe \\\r\n    --print-step \\\r\n    --batch-size 1 \\ \r\n    --beam 1  \r\n\r\nbut result is:\r\n2020-10-20 11:26:32 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\r\nMadam President , on a point of order .\r\nS-0     Madam President , on a point of order .\r\nW-0     0.591   seconds\r\nH-0     -2.8191957473754883     <unk>\r\nD-0     -2.8191957473754883     <unk>\r\nP-0     0.0000 -8.4576 0.0000\r\nI find that cause for extremely serious concern .\r\nS-1     I find that cause for extremely serious concern .\r\nW-1     0.070   seconds\r\nH-1     -2.8055810928344727     <unk>\r\nD-1     -2.8055810928344727     <unk>\r\nP-1     0.0000 -8.4167 0.0000\r\n\r\nThank you very much if you can answer my question.\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):0.9\r\n - PyTorch Version (e.g., 1.0):1.4\r\n - OS (e.g., Linux):centos\r\n - How you installed fairseq (`pip`, source):source\r\n - Build command you used (if compiling from source):pip install --editable ./\r\n - Python version:3. 7\r\n - CUDA/cuDNN version:10.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2757/comments",
    "author": "xiaoshengjun",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2755,
    "title": "Zero-shot generating of pre-trained mBART25 behaves weirdly.",
    "created_at": "2020-10-19T16:19:48Z",
    "closed_at": "2021-06-10T08:22:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2755",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI tried to run zero-shot generating with pre-trained mbart.cc25/model.pt. According to the description of paper, I expected it to generate something similar with source tokens. However, all it can generate are \",\" and \"...\". \r\n\r\nDo you have any idea about this problem?\r\n\r\nThank you.\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI've checked the format of `sample['net_input']['src_tokens']`, `sample['net_input']['prev_output_tokens']`, and `sample['target']`. They are as follows:\r\n* src: `▁risk s </s> [en_XX]`\r\n* prev_output: `[en_XX] ▁risk s </s>`\r\n* target: `▁risk s </s> [en_XX]`\r\n\r\nIt seems like the correct input format that matches the pre-training objective of mbart. But it just could not generate legal sentences. Instead, it would output something like `,,,,,,,,...`.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: master:\r\n - PyTorch Version: 1.6\r\n - OS: Linux\r\n - How you installed fairseq: source\r\n - Build command you used (if compiling from source): pip install -e .\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: V100\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2755/comments",
    "author": "yongchanghao",
    "comments": [
      {
        "user": "kaushal0494",
        "created_at": "2020-12-28T08:00:28Z",
        "body": "Hi @yongchanghao, I am facing a similar problem.  Any suggestion, how did you solve the problem. Thank you!  "
      },
      {
        "user": "yongchanghao",
        "created_at": "2020-12-29T12:30:15Z",
        "body": "Hi @kaushal0494, I didn't solve this problem actually. I pre-trained a new model and didn't find this problem. Though the fine-tuning results are similar."
      },
      {
        "user": "kaushal0494",
        "created_at": "2020-12-29T12:59:36Z",
        "body": "Hi @yongchanghao, Thank you for your reply.  This seems a feasible solution. Are your code and pre-trained models are available openly?  Please share if possible.  "
      }
    ]
  },
  {
    "number": 2750,
    "title": "How to translate a long text with fairest",
    "created_at": "2020-10-18T19:32:50Z",
    "closed_at": "2022-04-19T12:21:07Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2750",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI am trying to back translate a long text from movie review data and it appears that somehow it was trucated and I have tried to set the max_len_a and max_len_b with no success \r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n```\r\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\r\nde2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.de-en.single_model', tokenizer='moses', bpe='fastbpe')\r\n\r\ntext = \"\"\"\r\nEvening is the beautiful story of the flawed love of a mother. The movie split in time, is magically shot, amazingly acted and has a touching script. Vanessa Redgrave plays Anne Grant Lord, a woman sun-setting out of life. Lying in her bed, her mind remembering and misfiring, she recalls her first mistake. Claire Danes plays the young Anne, giving a youthful vitality to dying bed ridden woman. Daughters Nina (Toni Collette) and Constance (Natasha Richardson) try to decipher the real story from the disheartening dementia. Her first mistake revolves around Harris Arden (Patrick Wilson); the man her best friend Lila (Mamie Gummer) deeply loved. The daughters must come to terms with their mother's past, and their futures. The cast is glowing in Evening. The collective acting energy of this movie could have powered the equipment for the production of this entire film. I am so glad to see Claire Danes working again, especially in this role. She is so young, and alive, fully living the joys, mistakes and heartbreak of young Anne's first mistake. This is a true feat when you realize she is playing a woman, dying in bed. When her life overwhelms her, you can feel her desire to crack and her hopeless hope that she won't. Some of her facial expressions grinded on me a little, but over all her performance was so radiant, I was left with that only as a side note. Toni Collette continues to prove that you can be a powerful actress without being a super model. She plays the black sheep of the family; a little lost. Nina finds a great deal of strength in her mother's mistake. Collette delicately avoids creating a cruel character who revels in the mistakes of her mother, instead choosing the wiser path of learning from her mother's mistakes. There is a great deal of infighting between Nina and her sister Constance. Their fights remind me of ones I have with my sister all the time. Mamie Gummer, who plays Anne's youthful best friend, is wonderful. Her character is stuck between her heart and her status in society. Even when she is crying and her heart is breaking, she is incredibly regal and charming. I can't wait to see her act in something else in the future. Vanessa Redgrave's performance is very hard for me to describe. Her talent at making her mental status ambiguous without being wacko or even especially tragic is why it is so powerful. The audience does not know if she is making up the story because she is slipping away or if these events truly happened. Physically and emotionally speaking, Redgrave is acting in a box. Not much physical space and limited emotional range might have been a stunner to a lesser actress but she makes the limitations work for her. I was constantly amazed. The movie is definitely woman-focused but the men in the movie are not just accessories. Patrick Wilson is mesmerizing as Harris. It is no wonder that everyone in the movie is in love with him, I sure was. Buddy Wittenborn is Lila's brother, spiraling out of control. Hugh Dancy spirals Buddy out of control without sending his acting down the drain. Glen Close has my favorite scene in the movie. It reminded me of the famous scene from Monster's Ball. It is terrible and jaw dropping grief. I was utterly stunned. The one acting disappointment was Natasha Richardson. While her fight scenes were memorable, most of her acting reeks of melodrama. It would have suited her to take an acting bath before we had to breathe her stink. It's a good thing she wasn't in charge of the visuals. The visuals of the movie are sparkling. Cinematographer Gyula Pados couldn't make a film richer in color, light so perfectly matched to mood and emotion. The visual concepts of the flash back sequences are powerful and resonating. There were many scenes that could have been stopped, printed, mounted and sold as art. I admit it, I cried. Evening is a powerful movie. Evening is defiantly a chick flick but a really great chick flick. If you want to impress a woman with a movie choice, pick Evening.\r\n\"\"\"\r\nprint(text)\r\nparaphrase = de2en.translate(en2de.translate(text,max_len_a=1, max_len_b=1000), max_len_a=1, max_len_b=1000)\r\nprint(paraphrase)\r\n\r\n```\r\n\r\nBelow shows the original text and back translated text and one can see that the back translated text is much shorter than the original one and clearly the result was trucated somehow:\r\n\r\n*Evening is the beautiful story of the flawed love of a mother. The movie split in time, is magically shot, amazingly acted and has a touching script. Vanessa Redgrave plays Anne Grant Lord, a woman sun-setting out of life. Lying in her bed, her mind remembering and misfiring, she recalls her first mistake. Claire Danes plays the young Anne, giving a youthful vitality to dying bed ridden woman. Daughters Nina (Toni Collette) and Constance (Natasha Richardson) try to decipher the real story from the disheartening dementia. Her first mistake revolves around Harris Arden (Patrick Wilson); the man her best friend Lila (Mamie Gummer) deeply loved. The daughters must come to terms with their mother's past, and their futures. The cast is glowing in Evening. The collective acting energy of this movie could have powered the equipment for the production of this entire film. I am so glad to see Claire Danes working again, especially in this role. She is so young, and alive, fully living the joys, mistakes and heartbreak of young Anne's first mistake. This is a true feat when you realize she is playing a woman, dying in bed. When her life overwhelms her, you can feel her desire to crack and her hopeless hope that she won't. Some of her facial expressions grinded on me a little, but over all her performance was so radiant, I was left with that only as a side note. Toni Collette continues to prove that you can be a powerful actress without being a super model. She plays the black sheep of the family; a little lost. Nina finds a great deal of strength in her mother's mistake. Collette delicately avoids creating a cruel character who revels in the mistakes of her mother, instead choosing the wiser path of learning from her mother's mistakes. There is a great deal of infighting between Nina and her sister Constance. Their fights remind me of ones I have with my sister all the time. Mamie Gummer, who plays Anne's youthful best friend, is wonderful. Her character is stuck between her heart and her status in society. Even when she is crying and her heart is breaking, she is incredibly regal and charming. I can't wait to see her act in something else in the future. Vanessa Redgrave's performance is very hard for me to describe. Her talent at making her mental status ambiguous without being wacko or even especially tragic is why it is so powerful. The audience does not know if she is making up the story because she is slipping away or if these events truly happened. Physically and emotionally speaking, Redgrave is acting in a box. Not much physical space and limited emotional range might have been a stunner to a lesser actress but she makes the limitations work for her. I was constantly amazed. The movie is definitely woman-focused but the men in the movie are not just accessories. Patrick Wilson is mesmerizing as Harris. It is no wonder that everyone in the movie is in love with him, I sure was. Buddy Wittenborn is Lila's brother, spiraling out of control. Hugh Dancy spirals Buddy out of control without sending his acting down the drain. Glen Close has my favorite scene in the movie. It reminded me of the famous scene from Monster's Ball. It is terrible and jaw dropping grief. I was utterly stunned. The one acting disappointment was Natasha Richardson. While her fight scenes were memorable, most of her acting reeks of melodrama. It would have suited her to take an acting bath before we had to breathe her stink. It's a good thing she wasn't in charge of the visuals. The visuals of the movie are sparkling. Cinematographer Gyula Pados couldn't make a film richer in color, light so perfectly matched to mood and emotion. The visual concepts of the flash back sequences are powerful and resonating. There were many scenes that could have been stopped, printed, mounted and sold as art. I admit it, I cried. Evening is a powerful movie. Evening is defiantly a chick flick but a really great chick flick. If you want to impress a woman with a movie choice, pick Evening.*\r\n\r\n*The film is split in time, is magically shot, amazingly acted and has a touching script. Vanessa Redgrave plays Anne Grant Lord, a woman sunset from life. Lying in her bed, her mind remembers and misfires, she remembers her first mistake. Claire Danes plays the young Anne, gives a youthful vitality to bed-ridden woman. Daughters Nina (Toni Collette) and Constance (Natasha Richardson) try to decipher the real story of the disheartening dementia. Her first mistake revolves around Harris Arden (Patrick Wilson); the man deeply loved her best friend Lila (Mamie Gummer). The daughters must come to reconcile with their mother's past, and her future. The cast is glowing in Evening. The collective acting energy of this film could have driven the production of this entire movie.*\r\n\r\nThank you so much!\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):  0.9.0\r\n - PyTorch Version (e.g., 1.0).   1.4.0\r\n - OS (e.g., Linux):            MacOS\r\n - How you installed fairseq (`pip`, source):   pip\r\n - Build command you used (if compiling from source):\r\n - Python version:  3.7\r\n - CUDA/cuDNN version:   10.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2750/comments",
    "author": "junwen-austin",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-10-19T11:31:34Z",
        "body": "You should probably split the input into separate sentences and translate them one by one. Most of these models have a maximum length that they support, due to the number of positional embeddings that were present at training time, so you can’t feed in arbitrarily long inputs."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:53Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2747,
    "title": "Change layer shape in Wav2Vec models",
    "created_at": "2020-10-17T16:15:52Z",
    "closed_at": "2020-10-21T22:52:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2747",
    "body": "Can i change the layer shape of the fine-tuned models?\r\neg. i want to change the final linear layer of  of fine-tuned English model of size torch.Size([36]) to torch.Size([Any other value]).",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2747/comments",
    "author": "spygaurad",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-10-21T05:38:38Z",
        "body": "you can certainly do this by modifying the code to replace the loader layer by a new one and then (optionally) copying part of the weights from the old layer. why would you want to do this? "
      },
      {
        "user": "spygaurad",
        "created_at": "2020-10-21T05:45:03Z",
        "body": "thanks @alexeib for the reply. I am experimenting if i could create a bilingual decoder by further finetuning the english finetuned model on another language by concatenating the dictionary with that of english.\r\n"
      },
      {
        "user": "edunov",
        "created_at": "2020-10-21T22:52:52Z",
        "body": "I'm not sure if there are still any questions, closing the issue, please reopen if you have any other questions "
      },
      {
        "user": "rinoji",
        "created_at": "2023-01-25T03:12:37Z",
        "body": "@edunov @alexeib \r\nI am a Japanese high school student studying speech recognition. Is it possible to fine tune again using only 2 layers of the wav2vec fine tuned model (other layers freezed)? And please tell me how to do it."
      }
    ]
  },
  {
    "number": 2740,
    "title": "Freezing while Finetuning Wav2vec 2.0 Models",
    "created_at": "2020-10-16T08:03:13Z",
    "closed_at": "2020-10-21T05:46:18Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2740",
    "body": "\r\nHow can i freeze the finetuned wav2vec 2.0 .pt model upto certain layers and train on other language so that i can achieve transfer learning ? \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2740/comments",
    "author": "spygaurad",
    "comments": [
      {
        "user": "medabalimi",
        "created_at": "2020-10-16T14:08:05Z",
        "body": "How many hours of labeled data do you have on the target language?"
      },
      {
        "user": "spygaurad",
        "created_at": "2020-10-16T14:29:54Z",
        "body": "@medabalimi I have nearly 150 hours labeled data. I am experimenting with changing the model to make a multilingual decoder.\r\n"
      },
      {
        "user": "medabalimi",
        "created_at": "2020-10-16T14:33:01Z",
        "body": "If you have 150 hours labeled data just try training on it. "
      },
      {
        "user": "spygaurad",
        "created_at": "2020-10-16T14:35:28Z",
        "body": "I dont have the training resource right now. So i am using smaller size of labelled audios. Also i want my model to learn some english styles as well because i am trying to implement the model on bilingual data."
      }
    ]
  },
  {
    "number": 2739,
    "title": "fairseq-preprocess joined dictionary",
    "created_at": "2020-10-16T05:49:27Z",
    "closed_at": "2022-04-19T11:21:34Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2739",
    "body": "## ❓ Questions and Help\r\n\r\n \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\nCould you please tell me, given a corpus, \r\n\r\n- How do we decide whether to use a joined dictionary or two separate dictionaries for source and target? (My corpus size is less than 100k) \r\n\r\n- If I create BPE units jointly on the source and target language, should I use a joined dictionary during fairseq-preprocess? \r\n\r\nThank You",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2739/comments",
    "author": "thilakshiK",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:21:01Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2731,
    "title": "OOM when fine-tune BART for summarization",
    "created_at": "2020-10-14T13:23:18Z",
    "closed_at": "2020-10-15T13:52:08Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2731",
    "body": "\r\n#### What is your question?\r\n\r\nWith my GPU 1080Ti with 12GB memory, it keeps having errors OOM until I decrease the max_tokens to 64. However, it has another error below:\r\n\"AssertionError: sentence at index 2512 of size 101 exceeds max_tokens limit of 64!\"\r\nSo is it possible to fine-tune bart with 12GB memory?  I wonder it cannot have great performance in 64 tokens even if it can run successfully.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):cent os7\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2731/comments",
    "author": "monologue1107",
    "comments": [
      {
        "user": "moussaKam",
        "created_at": "2020-10-15T10:35:04Z",
        "body": "Try with --memory-efficient-fp16 . Otherwise, you can use the base architecture instead of the large one.\r\nAlso you can use --truncate-source to avoid exceeding limit error.  "
      },
      {
        "user": "monologue1107",
        "created_at": "2020-10-15T11:34:31Z",
        "body": "> Try with --memory-efficient-fp16 . Otherwise, you can use the base architecture instead of the large one.\r\n> Also you can use --truncate-source to avoid exceeding limit error.\r\n\r\nThanks for your reply. I used --memory-efficient-fp16 for bart-large model and now train successfully with max_tokens=1024 in two 1080Ti GPU with 12GB memory. Hope for good training results."
      }
    ]
  },
  {
    "number": 2728,
    "title": "Getting Encoder Representation from a trained model",
    "created_at": "2020-10-13T07:29:19Z",
    "closed_at": "2022-04-19T11:21:32Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2728",
    "body": "Once a seq2seq model is trained, I want to extract the encoder representation for test dataset. \r\n\r\nCan anyone please assist me in doing so?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2728/comments",
    "author": "shashankg7",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:58Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:59Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2727,
    "title": "colon-separated list of dataset",
    "created_at": "2020-10-13T06:31:44Z",
    "closed_at": "2020-10-15T10:29:33Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2727",
    "body": "## ❓ Questions and Help\r\n\r\nHi, I am pre-training a model on a large dataset that cannot fit into the CPU memory. So I tried the solution mentioned in #880 by @myleott . I splitted my dataset into 4 splits, and each split is read separately. \r\n\r\nHowever this could not solve the problem as each time a split is loaded the memory usage increases and at some point I get OOM. \r\n\r\nHere's my command:\r\n\r\n```\r\n#!/bin/bash\r\n#SBATCH --job-name=gpu-32node\r\n#SBATCH --partition=gpu_p1\r\n#SBATCH --qos=qos_gpu-t3\r\n#SBATCH --output=x.out\r\n#SBATCH --error=x.err\r\n#SBATCH --nodes=32\r\n#SBATCH --ntasks-per-node=1\r\n#SBATCH --gres=gpu:4\r\n#SBATCH --time=20:00:00\r\n#SBATCH --cpus-per-task=40\r\n#SBATCH --hint=nomultithread\r\n\r\nmodule purge\r\n\r\nset -x\r\n\r\nDATA_PATH='data-bin/data-bin1:data-bin/data-bin2:data-bin/data-bin3:data-bin/data-bin4'\r\nMAX_TOKENS=8192\r\nMAX_UPDATE=190000\r\nSAVE_INTERVAL=5000\r\nLR=0.0008\r\nMAX_EPOCH=32\r\nDISTRIBUTED_WORLD_SIZE=128\r\nSENTENCE_PIECE_MODEL='sentencepiece.model'\r\nVALID_SUBSET='valid'\r\n\r\nsrun fairseq-train $DATA_PATH \\\r\n    --optimizer=adam \\\r\n    --adam-betas='(0.9, 0.999)' \\\r\n    --adam-eps=1e-06 \\\r\n    --arch='bart_base' \\\r\n    --bpe='sentencepiece' \\\r\n    --sentencepiece-vocab $SENTENCE_PIECE_MODEL \\\r\n    --clip-norm=0.1 \\\r\n    --log-interval=10 \\\r\n    --mask=0.3 \\\r\n    --mask-length='span-poisson' \\\r\n    --mask-random=0.1 \\\r\n    --permute-sentences=1 \\\r\n    --poisson-lambda=3.5 \\\r\n    --replace-length=1 \\\r\n    --rotate=0 \\\r\n    --max-update $MAX_UPDATE \\\r\n    --total-num-update $MAX_UPDATE \\\r\n    --save-dir $SAVE_DIR \\\r\n    --save-interval-updates=$SAVE_INTERVAL \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --task='denoising' \\\r\n    --update-freq=2 \\\r\n    --restore-file=$MODEL_PATH \\\r\n    --required-batch-size-multiple 8 \\\r\n    --fp16 \\\r\n    --lr=$LR \\\r\n    --weight-decay=0.01 \\\r\n    --lr-scheduler polynomial_decay \\\r\n    --activation-fn 'gelu' \\\r\n    --pooler-activation-fn 'tanh' \\\r\n    --tensorboard-logdir=$TENSORBOARD_LOGS \\\r\n    --max-tokens=$MAX_TOKENS \\\r\n    --distributed-world-size=$DISTRIBUTED_WORLD_SIZE \\\r\n    --distributed-port 12345 \\\r\n    --dropout 0.1 \\\r\n    --dataset-impl 'mmap' \\\r\n    --max-epoch $MAX_EPOCH \\\r\n    --warmup-updates $((6*$MAX_UPDATE/100)) \\\r\n    --no-epoch-checkpoints \\\r\n    --valid-subset $VALID_SUBSET\r\n```\r\n\r\nAm I doing something wrong?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2727/comments",
    "author": "moussaKam",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-10-14T13:58:18Z",
        "body": "A couple things:\r\n1) have you installed pyarrow? `pip install pyarrow`, it should automatically kick in and improve memory utilization\r\n2) are you using the master version of fairseq? There was a known memory leak with colon-separated datasets, which was fixed 1 or 2 months back."
      },
      {
        "user": "moussaKam",
        "created_at": "2020-10-15T10:29:33Z",
        "body": "Actually my fairseq was not up-to-date, there was this memory leak problem. Now it works. Thank you!"
      }
    ]
  },
  {
    "number": 2721,
    "title": "OOM error when resume pretraining (MBART) from checkpoint_last.pt",
    "created_at": "2020-10-12T02:43:57Z",
    "closed_at": "2022-05-01T02:22:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2721",
    "body": "I was pretraining MBART on my data and it accidentally stopped. Then when I tried to resume the training, it is giving OOM error when `checkpoint_last.pt` is used. Fairseq tries to get back from the OOM error but it can't for a long time. So, I terminated the job. Then I tried to restore from a previous checkpoint and it works! So, my question is:\r\n\r\n1. In what situation does the use of `checkpoint_last.pt` causes OOM error? I am using fp16 during pretraining.\r\n\r\n2. Will there be any difference in optimization settings if I load from an earlier checkpoint? Say the last checkpoint was saved after 40k steps but I am loading from a checkpoint that was saved after 38k steps. Loading from 38k steps would cause me 2k steps to be done again but I assume in terms of learning rates and other optimization settings, things will be the same. Is it right?\r\n\r\n**Environment**\r\n\r\n```\r\nPython 3.6.10 :: Anaconda, Inc.\r\n\r\nName: fairseq\r\nVersion: 0.9.0\r\n\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.4 LTS (Bionic Beaver)\"\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2721/comments",
    "author": "wasiahmad",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "firdota",
        "created_at": "2021-07-22T00:56:46Z",
        "body": "hello，how can we continue pretraining mBART from the released checkpoint? @wasiahmad "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2709,
    "title": "(wav2vec2.0)when I finetune the pretrained model, The performance of pre-trained model connected to Transformer decoder is worse than that of CTC？",
    "created_at": "2020-10-09T02:14:00Z",
    "closed_at": "2022-04-19T12:21:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2709",
    "body": "Hello everyone, I have a question. When I finetune the pretrained model, The performance of pre-trained model connected to Transformer decoder is worse than that of CTC？Generally speaking, the result of accessing the decoder is better than the result of accessing the CTC. I want to know if anyone else has encountered the same problem and how to solve it. Thanks a lot.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2709/comments",
    "author": "zqs01",
    "comments": [
      {
        "user": "phantomcoder1996",
        "created_at": "2020-10-21T14:30:43Z",
        "body": "Yes, I have the same problem the viterbi decoder gets much better results vs kenlm "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2700,
    "title": "How to load pre-train after fine-tuned model mBart for translation",
    "created_at": "2020-10-06T07:41:34Z",
    "closed_at": "2022-05-02T21:22:23Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2700",
    "body": "Hello all!\r\nI have just fine-tuned mBART for translation. But i don't know load model for generating. Can u help me! Thanks a lot!!!\r\n`path_sentencepiece_vocab = os.path.join(BASEDIR, 'sentence.bpe.model')\r\n\r\nbart = BARTModel.from_pretrained(\r\n        BASEDIR,\r\n        checkpoint_file='checkpoint_best.pt',\r\n        bpe='sentencepiece',\r\n        task=\"translation_from_pretrained_bart\",\r\n        sentencepiece_vocab=path_sentencepiece_vocab)\r\nbart.eval()`\r\n\r\nThis is error\r\n`/usr/local/lib/python3.6/dist-packages/fairseq/checkpoint_utils.py in _upgrade_state_dict(state)\r\n    347 \r\n    348     # set any missing default values in the task, model or other registries\r\n--> 349     registry.set_defaults(state[\"args\"], tasks.TASK_REGISTRY[state[\"args\"].task])\r\n    350     registry.set_defaults(state[\"args\"], models.ARCH_MODEL_REGISTRY[state[\"args\"].arch])\r\n    351     for registry_name, REGISTRY in registry.REGISTRIES.items():\r\n\r\nKeyError: 'translation_from_pretrained_bart'`",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2700/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "PYMAQ",
        "created_at": "2021-03-31T08:07:48Z",
        "body": "I have same question！who can solve it？\r\n2021-03-31 07:56:52 | INFO | fairseq_cli.train | model bart_large, criterion LabelSmoothedCrossEntropyCriterion\r\n2021-03-31 07:56:52 | INFO | fairseq_cli.train | num. model params: 416791552 (num. trained: 416791552)\r\n2021-03-31 07:56:58 | INFO | fairseq_cli.train | training on 1 GPUs\r\n2021-03-31 07:56:58 | INFO | fairseq_cli.train | max tokens per GPU = 800 and max sentences per GPU = None\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/tf/pym/Multi-View-Seq2Seq-master/fairseq_multi_view/fairseq_cli/train.py\", line 440, in cli_main\r\n    main(args)\r\n  File \"/tf/pym/Multi-View-Seq2Seq-master/fairseq_multi_view/fairseq_cli/train.py\", line 88, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\r\n  File \"/tf/pym/Multi-View-Seq2Seq-master/fairseq_multi_view/fairseq/checkpoint_utils.py\", line 133, in load_checkpoint\r\n    reset_meters=args.reset_meters,\r\n  File \"/tf/pym/Multi-View-Seq2Seq-master/fairseq_multi_view/fairseq/trainer.py\", line 269, in load_checkpoint\r\n    state = checkpoint_utils.load_checkpoint_to_cpu(filename)\r\n  File \"/tf/pym/Multi-View-Seq2Seq-master/fairseq_multi_view/fairseq/checkpoint_utils.py\", line 172, in load_checkpoint_to_cpu\r\n    state = _upgrade_state_dict(state)\r\n  File \"/tf/pym/Multi-View-Seq2Seq-master/fairseq_multi_view/fairseq/checkpoint_utils.py\", line 354, in _upgrade_state_dict\r\n    registry.set_defaults(state[\"args\"], tasks.TASK_REGISTRY[state[\"args\"].task])\r\nKeyError: 'translation_from_pretrained_bart'\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T02:04:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:53Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2696,
    "title": "Getting NaN loss while finetunning wav2vec2",
    "created_at": "2020-10-05T17:47:41Z",
    "closed_at": "2022-04-19T11:21:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2696",
    "body": "\r\nI am finetunnig wav2vec2 on my own data. I am running this on k80 and as it doesn't support fp16 I switch to fp32 but loss is nan now.\r\n\r\nWhen I was training with fp16 flag got loss scale reached to 0.0001 and training stoped. Then I switched to FP32 but loss became nun this time:\r\nlog:\r\n```2020-10-01 10:25:50 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\r\n2020-10-01 10:25:50 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:25:50 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:25:50 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:25:50 | INFO | train | {\"epoch\": 73, \"train_loss\": \"nan\", \"train_ntokens\": \"15558.8\", \"train_nsentences\": \"427.098\", \"t\r\nrain_nll_loss\": \"nan\", \"train_wps\": \"2443\", \"train_ups\": \"0.16\", \"train_wpb\": \"15558.8\", \"train_bsz\": \"427.1\", \"train_num_updates\": \"\r\n9291\", \"train_lr\": \"1e-08\", \"train_gnorm\": \"nan\", \"train_loss_scale\": null, \"train_train_wall\": \"867\", \"train_wall\": \"0\"}\r\n2020-10-01 10:25:50 | INFO | fairseq.trainer | begin training epoch 74\r\n2020-10-01 10:40:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2020-10-01 10:42:40 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:42:40 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:42:40 | INFO | valid | {\"epoch\": 74, \"valid_loss\": \"nan\", \"valid_ntokens\": \"2570.42\", \"valid_nsentences\": \"71.4286\", \"v\r\nalid_nll_loss\": \"nan\", \"valid_uer\": \"100\", \"valid_wer\": \"100\", \"valid_raw_wer\": \"100\", \"valid_wps\": \"2780.3\", \"valid_wpb\": \"2570.4\", \r\n\"valid_bsz\": \"71.4\", \"valid_num_updates\": \"9454\", \"valid_best_wer\": \"100\"}\r\n```\r\nany suggestions on how to overcome this?\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2696/comments",
    "author": "008karan",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:38Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2683,
    "title": "training speed is too much slowly",
    "created_at": "2020-10-02T02:48:01Z",
    "closed_at": "2020-10-03T00:22:45Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2683",
    "body": "when I training the model just like \"transformer_vaswani_wmt_en_de_big\" ,and my wps is 1706 ,this is too much slow, I have run the training about one week, and now the training steps is 112000, one epoch has 522288 steps, so I have to run about one month to complete one epoch, this is too slow",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2683/comments",
    "author": "dahaogewsh",
    "comments": [
      {
        "user": "neel04",
        "created_at": "2020-10-02T15:55:11Z",
        "body": "@dahaogewsh I am having the same problem - In the earlier versions, I believe there was no such problem but now with the current branch, It's taking 4x more time than previously set `batch_size`. My solution was to set the `batch_size` to a bit higher model to reduce the number of iterations per epoch and train the model in reasonable time steps. "
      },
      {
        "user": "myleott",
        "created_at": "2020-10-03T00:22:45Z",
        "body": "Sorry, we can't help if you don't share the command you ran and more details about your system setup. Please open a new Issue following the provided templates."
      }
    ]
  },
  {
    "number": 2680,
    "title": "How to generate multiple translations for a single sentence?",
    "created_at": "2020-10-01T15:57:28Z",
    "closed_at": "2020-10-03T00:29:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2680",
    "body": "I have `fconv` translation model and I want to get the best `K` translations for a single sentence.\r\n\r\nI'm using this code:\r\n```\r\nfrom fairseq.models.fconv import FConvModel\r\nmodel = FConvModel.from_pretrained(**)\r\nmodel.eval()\r\nmodel.translate(sent)\r\n```\r\nI traced the translate method, but couldn't find what parameter should I provide to get best `K` translation, though `fairseq-interactive` with `--nbest` returns multiple translations.\r\n\r\n - fairseq Version: v0.9.0\r\n - PyTorch Version: 1.6.0\r\n - OS: Linux\r\n - How you installed fairseq: `pip install --editable ./` \r\n - Python version: 3.6.9",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2680/comments",
    "author": "customer101",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-10-03T00:29:43Z",
        "body": "You need to use the `.generate` interface. Try something like this:\r\n\r\n```python\r\n>>> en = model.encode(\"Hello world\")\r\n>>> outputs = model.generate(en, beam=5, nbest=5)\r\n>>> [en2fr.decode(x['tokens']) for x in output]\r\n['Bonjour à tous.', 'Bonjour au monde', 'Bonjour à tous', 'Bonjour monde', 'Bonjour.']\r\n```"
      }
    ]
  },
  {
    "number": 2676,
    "title": "Assertion error for conv.wmt14.en-fr model",
    "created_at": "2020-09-29T18:20:51Z",
    "closed_at": "2020-09-29T21:53:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2676",
    "body": "## 🐛 Bug\r\n\r\nModels conv.wmt14.en-fr, conv.wmt14.en-de and conv.wmt17.en-de fail on assert:\r\nassert isinstance(model.models[0], fairseq.models.transformer.TransformerModel)\r\nAssertionError\r\n\r\n\r\n### To Reproduce\r\n```\r\nmodel = torch.hub.load('pytorch/fairseq', 'conv.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\r\nmodel.eval()\r\nassert isinstance(model.models[0], fairseq.models.transformer.TransformerModel)\r\n```\r\n#### Code sample\r\n\r\nwhen just running the model (comment the assert):\r\n```\r\nmodel = torch.hub.load('pytorch/fairseq', 'conv.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\r\nmodel.eval()\r\ninner = model.models[0]\r\ndtype = torch.LongTensor\r\nx = torch.zeros(1, 1024).type(dtype)\r\ny = torch.zeros(1, 1024).type(dtype)\r\nout = inner(x, torch.IntTensor([1024]), y)\r\n```\r\nI get this error:\r\nFile \"export_fairseq.py\", line 117, in main\r\n    out = inner(x, torch.IntTensor([1024]), y)\r\n  File \"/home/ksenija/anaconda3/envs/cpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ksenija/anaconda3/envs/cpu/lib/python3.7/site-packages/fairseq/models/fairseq_model.py\", line 223, in forward\r\n    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\r\n  File \"/home/ksenija/anaconda3/envs/cpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ksenija/anaconda3/envs/cpu/lib/python3.7/site-packages/fairseq/models/fconv.py\", line 215, in forward\r\n    x = self.embed_tokens(src_tokens) + self.embed_positions(src_tokens)\r\n  File \"/home/ksenija/anaconda3/envs/cpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ksenija/anaconda3/envs/cpu/lib/python3.7/site-packages/fairseq/modules/learned_positional_embedding.py\", line 43, in forward\r\n    return super().forward(positions)\r\n  File \"/home/ksenija/anaconda3/envs/cpu/lib/python3.7/site-packages/torch/nn/modules/sparse.py\", line 126, in forward\r\n    self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n  File \"/home/ksenija/anaconda3/envs/cpu/lib/python3.7/site-packages/torch/nn/functional.py\", line 1853, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nIndexError: index out of range in self\r\n### Expected behavior\r\n\r\nto pass the assert, no errors when running the model\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master):0.9\r\n - PyTorch Version (e.g., 1.0): 1.7\r\n - OS (e.g., Linux): 18.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.7 \r\n - CUDA/cuDNN version: 10.2 / 8.0.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2676/comments",
    "author": "KsenijaS",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-09-29T21:53:10Z",
        "body": "The max supported length for that model is 1022 tokens, since it only has 1024 positional embeddings and two are reserved (one for padding, the other is just unused).\r\n\r\nThis works:\r\n```python\r\nmaxlen = 1022\r\nmodel.models[0](\r\n  torch.zeros(1, maxlen).long(),\r\n  torch.IntTensor([maxlen]),\r\n  torch.zeros(1, maxlen).long()\r\n)\r\n```"
      }
    ]
  },
  {
    "number": 2672,
    "title": "Cannot load models transformer_lm.gbw.adaptive_huge and transformer_lm.wiki103.adaptive",
    "created_at": "2020-09-29T05:59:40Z",
    "closed_at": "2020-09-29T22:07:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2672",
    "body": "## 🐛 Bug\r\nWhen loading models transformer_lm.gbw.adaptive_huge and transformer_lm.wiki103.adaptive using torch.hub.load I got this error:\r\nAttributeError: 'Namespace' object has no attribute 'bpe_codes'\r\n\r\n### To Reproduce\r\nmodel = torch.hub.load('pytorch/fairseq', 'transformer_lm.gbw.adaptive_huge', tokenizer='moses', bpe='fastbpe')\r\n\r\n### Expected behavior\r\nModels to load.\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9\r\n - PyTorch Version (e.g., 1.0) 1.7\r\n - OS (e.g., Linux): 18.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2 /8.0.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2672/comments",
    "author": "KsenijaS",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-09-29T22:07:32Z",
        "body": "The GBW model doesn't use BPE, so it should work if you remove the `bpe='fastbpe'` part.\r\n\r\nI also recommend adding `tokenizer='moses', moses_no_escape=True`, based on looking at the dict.txt file in the model archive"
      }
    ]
  },
  {
    "number": 2671,
    "title": "Retrieval Augmented Generation (RAG) implementation ",
    "created_at": "2020-09-29T02:42:33Z",
    "closed_at": "2022-04-19T11:21:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2671",
    "body": "Hi, \r\n\r\nIs there any chance that we can integrate RAG into fairseq ?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2671/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2667,
    "title": "setup.py error",
    "created_at": "2020-09-28T09:32:56Z",
    "closed_at": "2020-09-30T03:53:25Z",
    "labels": [
      "question",
      "windows"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2667",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nInstalling the setup for fairseq using pip install --editable . gives following error\r\n\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\r\n\\14.27.29110\\include\\yvals.h(12): fatal error C1083: Cannot open include file: '\r\ncrtdbg.h': No such file or directory\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2667/comments",
    "author": "poojanv123",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-09-29T22:01:22Z",
        "body": "Sorry, I don't have access to a windows machine to test this. Hopefully someone from the community can help (or if you figure it out please share the solution back!)"
      },
      {
        "user": "maya-burnard",
        "created_at": "2020-10-31T18:49:21Z",
        "body": "Was there a solution here? All I see is @poojanv123 closing it out of nowhere."
      }
    ]
  },
  {
    "number": 2661,
    "title": "Experiences with multiple GPUs (different models and makes)",
    "created_at": "2020-09-27T15:50:58Z",
    "closed_at": "2022-04-19T11:21:14Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2661",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nDoes anyone have experience with combining multiple GPU's of different models and makes? I here have an Inno3D 1080 Ti and a Gigabyte GeForce AORUS RTX 2080 Ti XTREME 11G, and the amount of hard crashes is pretty staggering when firing up fairseq from ModernMT. The power supply is 1000 Watt and the heat seems to be okay (86 degrees Celcius, while slow-down temp is 91 degrees). When using just one GPU (no matter which one) while the 2nd GPU is still fitted in the motherboard, the system is more stable (I still get occasional hard crashes, though way less).",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2661/comments",
    "author": "LoekvanKooten",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:44Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2655,
    "title": "Issue in fine tuning Wav2Vec2: getting 'ascii' codec can't decode byte ",
    "created_at": "2020-09-25T18:24:44Z",
    "closed_at": "2020-10-05T17:35:06Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2655",
    "body": "## ❓ Questions and Help\r\n\r\nI am fine-tuning  wav2vec2 on my data using ctc. I ran the script with librispeech dara and was running fine. When running on my data is throwing:\r\n```\r\n weight_decay=0.0, wer_args=None, zero_infinity=True, zero_sharding='none')\r\n2020-09-25 18:06:08 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 8513, skipped 0 samples\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 14, in <module>\r\n    cli_main()\r\n  File \"/data/fairseq/fairseq_cli/train.py\", line 343, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/data/fairseq/fairseq/distributed_utils.py\", line 172, in call_main\r\n    args.distributed_world_size,\r\n  File \"/data/wav/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/data/wav/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/data/wav/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 2 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/data/wav/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/data/fairseq/fairseq/distributed_utils.py\", line 154, in distributed_main\r\n    main(args, **kwargs)\r\n  File \"/data/fairseq/fairseq_cli/train.py\", line 65, in main\r\n    task.load_dataset(valid_sub_split, combine=False, epoch=1)\r\n  File \"/data/fairseq/fairseq/tasks/audio_pretraining.py\", line 110, in load_dataset\r\n    for line in f:\r\n  File \"/data/wav/lib/python3.6/encodings/ascii.py\", line 26, in decode\r\n    return codecs.ascii_decode(input, self.errors)[0]\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4924: ordinal not in range(128)\r\n```\r\nits seems not able to read data files. But have followed the same preprocessing steps.\r\n\r\n**train.wrd**:\r\n```\r\nAVENGER\r\nFOUR. THREE. TWO. ONE. SIX. FIVE. ZERO. NINE. EIGHT. SEVEN.\r\nLISTENING MUSIC\r\nYES\r\n```\r\n**train.ltr**:\r\n```\r\nA V E N G E R |\r\nF O U R . | T H R E E . | T W O . | O N E . | S I X . | F I V E . | Z E R O . | N I N E . | E I G H T . | S E V E N . |\r\nL I S T E N I N G | M U S I C |\r\nY E S |\r\n```\r\n**train.tsv**:\r\n```\r\n/data/fairseq/IE/\r\n1143-GR/M1IE041143GENC0005.wav\t21440\r\n1143-GR/M1IE041143DGSQ1143.wav\t69120\r\n1143-GR/M1IE041143GENC0004.wav\t17920\r\n```\r\n**Any suggestions? what could be wrong**",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2655/comments",
    "author": "008karan",
    "comments": [
      {
        "user": "srinath-zapr",
        "created_at": "2020-12-08T08:50:34Z",
        "body": "@008karan I am getting the same error. Can you mention how did you solve this?"
      },
      {
        "user": "008karan",
        "created_at": "2020-12-08T09:16:59Z",
        "body": "you need to check whether your path is correct or not for input and input data formats."
      }
    ]
  },
  {
    "number": 2650,
    "title": "AssertionError: sentence at index 16482046 of size 201 exceeds max_tokens limit of 200!",
    "created_at": "2020-09-24T15:22:16Z",
    "closed_at": "2022-05-01T02:22:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2650",
    "body": "I looked up the solution for the same and came across various fixes like adding `--skip-invalid-size-inputs-valid-test` to the command. It is not working. Is there any other way to approach this? Thanks in advance! :)\r\n\r\nNote: Having a GPU with 4GB RAM, I am not able to allocate a massive number to the `--max-tokens` parameter. On that note, is it possible to train Fairseq from scratch with a 4GB GPU?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2650/comments",
    "author": "priyamehta01",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:46Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "NomadXD",
        "created_at": "2021-07-22T15:36:32Z",
        "body": "@priyamehta01 Did you find any solution ?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T15:20:35Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2647,
    "title": "issue running dataset scripts: No such file or directory: '/train.de'",
    "created_at": "2020-09-22T16:13:02Z",
    "closed_at": "2022-04-19T11:21:13Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2647",
    "body": "Hi, it may sound a bit dumb asking this question but I really tried several workarounds without success. I am trying to reproduce the steps to train on the IWSLT14 De-En dataset but running the prepare-iwslt14.sh script either by \r\n\r\n`cd examples/translation/`\r\n`bash prepare-iwslt14.sh`\r\n`cd ../..`\r\n\r\nor\r\n\r\n`bash examples/translation/prepare-iwslt14.sh`\r\n\r\nI always run into the error FileNotFoundError: [Errno 2] No such file or directory: '/train.de'\r\n\r\nI see that the script generates a train.de archive (not a folder) that appear inside a 'iwslt14.tokenized.de-en' directory, that is created within the folder where you run the bash command. I tried moving the '/train.de' folder to the root folder but it also didn't work. \r\n\r\nCan you please shed some light on what might be the issue? My env configurations are given below, thank you for the help in advance\r\n\r\n#### My environment\r\n\r\n(running inside a kaggle kernel)\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0): 1.5.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): installed from pip\r\n - Build command you used (if compiling from source): bash examples/translation/prepare-iwslt14.sh and bash prepare-iwslt14.sh\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration: NVidia K80\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2647/comments",
    "author": "arthur-telles3",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2645,
    "title": "Will the word segmentation be performed again after converting a txt file into a bin file?",
    "created_at": "2020-09-22T04:22:18Z",
    "closed_at": "2022-04-19T11:21:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2645",
    "body": "I am training the transformer_lm model. \r\nWill the word segmentation be performed again after converting a txt file into a bin file? Because the txt file is already segmented.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2645/comments",
    "author": "ismymajia",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2630,
    "title": "how to monitor a fixed batch during training?",
    "created_at": "2020-09-18T02:49:20Z",
    "closed_at": "2022-04-19T11:21:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2630",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHi! May I ask how to specify the parameters to get the same batch in every epoch during training? Cause I want to monitor how the decoding decisions change during training. Thank you!\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0) 1.2\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2630/comments",
    "author": "haorannlp",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:53Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2629,
    "title": "What is the dimensionality for the pertained vq-wav2vec K-means?",
    "created_at": "2020-09-17T13:34:18Z",
    "closed_at": "2022-04-19T11:21:07Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2629",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nIt seems that 465 frames of raw audio results in 1 feature vector ala:\r\n```\r\n            wav_input_16khz = torch.randn(1, 465)\r\n            z = model.feature_extractor(wav_input_16khz)\r\n```\r\nWhich results in a `z.shape` of `torch.Size([1, 512, 1])`\r\n\r\nIf I then do:\r\n```\r\n_, idxs = model.vector_quantizer.forward_idx(z)\r\n```\r\n\r\nI have a `idxs.shape` of `torch.Size([1, 1, 2])`. So am I to understand that the 465 frames of audio result in a feature vector of 512, which then results in a single embedding (of dimension 2)?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2629/comments",
    "author": "shamoons",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:55Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2626,
    "title": "fairseq-train not utilizing GPU",
    "created_at": "2020-09-16T15:27:39Z",
    "closed_at": "2020-09-16T17:02:59Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2626",
    "body": "When I try to train a model using fairseq-train, it trains on CPU only. And uses up to 2000% on a server. But never uses GPU. Even though other applications are Using GPU successfully. \r\n\r\n    Environment: Conda 4.8.3\r\n    Python: 3.8.3\r\n    Torch: 1.4.0+cu100\r\n    OS: Ubuntu 18.04.1 LTS\r\n\r\nFull command:\r\n\r\n``\r\nCHECKPOINT_DIR=checkpoints_de_en_parallel\r\nnohup fairseq-train \\\r\n    data-bin/wmt18_de_en \\\r\n    --source-lang de --target-lang en \\\r\n    --arch transformer_wmt_en_de_big --share-all-embeddings --log-interval  1 \\\r\n    --dropout 0.3 --weight-decay 0.0 --log-format simple \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --num-workers 1 \\\r\n    --max-tokens 500 --update-freq 2 \\\r\n    --max-update 30000 \\\r\n    --save-dir $CHECKPOINT_DIR &\r\n``\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2626/comments",
    "author": "babangain",
    "comments": [
      {
        "user": "kalyangvs",
        "created_at": "2020-09-16T15:38:15Z",
        "body": "Please check if you have installed torch such that it supports gpu.\r\nIn your python environment, check\r\n```python\r\nimport torch\r\ntorch.cuda.is_available()\r\ntorch.cuda.device_count()\r\n```"
      },
      {
        "user": "babangain",
        "created_at": "2020-09-16T15:39:58Z",
        "body": "    torch.cuda.is_available()  >>> True\r\n    torch.cuda.device_count() >>> 7"
      },
      {
        "user": "kalyangvs",
        "created_at": "2020-09-16T15:47:26Z",
        "body": "Is it supposed to be 7 or 8 GPUs, Can you please try with one GPU using CUDA_VISIBLE_DEVICES=1 with fairseq-train"
      },
      {
        "user": "babangain",
        "created_at": "2020-09-16T17:02:59Z",
        "body": "Yes, It was supposed to be 7 GPU.\r\nand similar fate with  CUDA_VISIBLE_DEVICES=1\r\nI resolved the problem now. \r\n\r\nI think It was a mismatch between CUDA versions. I installed PyTorch 1.2 using pip. And my CUDA version is 10.0.13. But earlier, I installed PyTorch 1.6 for CUDA 10.1 which was unsupported. After installing the correct version, even though torch.cuda.is_available() and other were showing properly, I don't know why fairseq-train was not using CPU. I tried uninstalling torch fairseq. ( from pip)\r\nThen tried to install torch via conda. There was some resolve environment problem..\r\n\r\nSo, I created a new env. \r\nInstalled pytorch 1.2 cudatoolkit 10.0 \r\nthen installed fairseq using pip. \r\nAnd it's using GPU now.\r\n\r\nThank you @gvskalyan anyway."
      }
    ]
  },
  {
    "number": 2625,
    "title": "More details about the MT results of mBART",
    "created_at": "2020-09-16T14:41:00Z",
    "closed_at": "2022-04-19T11:21:29Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2625",
    "body": "## ❓ Questions and Help\r\n#### What is your question?\r\nHello, I want to know more details about the MT experimental settings and results.\r\n1. Can you provide the train updates and batch size when fine-tuning on WMT16 ro-en (bitext-only) and WMT16 eo-en (+back translation) in Table 4?\r\n2.  In Table 3,  which test set of WMT en-de do you use? And also the corresponding train updates and batch size?\r\n\r\nI think those details are important for me to reproduce the MT results of mBART.  Thx~\r\n\r\n@SunbowLiu\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2625/comments",
    "author": "luofuli",
    "comments": [
      {
        "user": "alphadl",
        "created_at": "2020-10-08T12:13:06Z",
        "body": "Looking forward to more details too @SunbowLiu"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:33Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:56Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2621,
    "title": "new batches creation at the beginning of every epoch",
    "created_at": "2020-09-15T16:09:02Z",
    "closed_at": "2022-04-19T11:21:06Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2621",
    "body": "I need to create batches from the dataset at every epoch, not only the initialisation. I need to call ordered_indices of the dataset every epoch to create a certain ordered indices. I am setting shuffle for both batches and the dataset to False because I do not want the dataset to shuffle. I am creating my own custom dataset and task.\r\nSo first how to call ordered_indices at the beginning of every epoch, how to achieve this?\r\nDoes setting shuffle to False create any performance issues? When setting the shuffle to False for both dataset and batches, the iterator for the second epoch seems ti be empty, is the shuffling the responsible for creating data for the epochs?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2621/comments",
    "author": "ChristineBasta",
    "comments": [
      {
        "user": "ChristineBasta",
        "created_at": "2020-09-17T12:38:27Z",
        "body": "@myleott can you help regarding this please?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:36Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2619,
    "title": "Can i find any test data for the transformer lm model or arch?",
    "created_at": "2020-09-15T06:46:32Z",
    "closed_at": "2022-04-19T06:21:31Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2619",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI see that there are many arches, is there any relevant ppl data？such as transformer_lm 、transformer_lm_gpt、fconv_lm and so on.\r\n\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 0.9 master):\r\n - PyTorch Version (e.g., 1.4)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq ( source):\r\n - Build command you used (if compiling from source): follow website command\r\n - Python version: 3.6\r\n - CUDA/cuDNN version:  10.0\r\n - GPU models and configuration: \r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2619/comments",
    "author": "ismymajia",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T13:04:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:21:01Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2613,
    "title": "No results from --profile",
    "created_at": "2020-09-14T15:13:46Z",
    "closed_at": "2022-04-19T11:21:05Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2613",
    "body": "\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nWhen I try to get the profile, the .prof file saves nothing. The source code is not changed. Could you please tell me how to fix this?\r\n\r\n#### Code\r\nTOTAL_UPDATES=10    # Total number of training steps\r\nWARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\r\nPEAK_LR=0.0005          # Peak learning rate, adjust as needed\r\nTOKENS_PER_SAMPLE=512   # Max sequence length\r\nMAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\r\nMAX_SENTENCES=8        # Number of sequences per batch (batch size)\r\nUPDATE_FREQ=32          # Increase the batch size 16x\r\n\r\nDATA_DIR=data-bin/wikitext-103\r\n\r\nnvprof --profile-from-start off -o trace_name.prof -f python ../fairseq/train.py $DATA_DIR\\\r\n    --task masked_lm --criterion masked_lm \\\r\n    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\r\n    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --profile\r\n\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0)1.6\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq: source\r\n - Python version:3.8\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 4xP100\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2613/comments",
    "author": "Rivendile",
    "comments": [
      {
        "user": "Rivendile",
        "created_at": "2020-09-17T08:23:09Z",
        "body": "Besides, when I try to torch.autograd.profiler.load_prof, error occurs:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"prof.py\", line 3, in <module>\r\n>     print(torch.autograd.profiler.load_nvprof('bert.prof'))\r\n>   File \"/home/yihao/anaconda3/envs/bert/lib/python3.8/site-packages/torch/autograd/profiler.py\", line 572, in load_nvprof\r\n>     return EventList(parse_nvprof_trace(path))\r\n>   File \"/home/yihao/anaconda3/envs/bert/lib/python3.8/site-packages/torch/autograd/profiler.py\", line 993, in parse_nvprof_trace\r\n>     evt = FunctionEvent(id=row['marker_id'],\r\n> TypeError: __init__() missing 1 required positional argument: 'node_id'"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:57Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:35Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "lazerliu",
        "created_at": "2022-12-30T04:32:39Z",
        "body": "have you fixed it? I meet the same problem now"
      }
    ]
  },
  {
    "number": 2610,
    "title": "Can i find any test data  for the transformer lm model or arch?",
    "created_at": "2020-09-14T03:30:04Z",
    "closed_at": "2020-09-14T13:05:37Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2610",
    "body": " Can i find any test data  for the transformer lm model or arch?\r\n\r\n --arch ARCH, -a ARCH  model architecture: fconv_lm,\r\n                        fconv_lm_dauphin_wikitext103, fconv_lm_dauphin_gbw,\r\n                        fconv, fconv_iwslt_de_en, fconv_wmt_en_ro,\r\n                        fconv_wmt_en_de, fconv_wmt_en_fr, lstm,\r\n                        lstm_wiseman_iwslt_de_en, lstm_luong_wmt_en_de,\r\n                        fconv_self_att, fconv_self_att_wp,\r\n                        transformer_lm, transformer_lm_big,\r\n                        transformer_lm_wiki103, transformer_lm_gbw,\r\n                        transformer, transformer_iwslt_de_en,\r\n                        transformer_wmt_en_de,\r\n                        transformer_vaswani_wmt_en_de_big,\r\n                        transformer_vaswani_wmt_en_fr_big,\r\n                        transformer_wmt_en_de_big,\r\n                        transformer_wmt_en_de_big_t2t (default: fconv)",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2610/comments",
    "author": "ismymajia",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-14T13:05:37Z",
        "body": "Please follow the issue templates"
      }
    ]
  },
  {
    "number": 2609,
    "title": "wav2vec 2.0 's augmentation",
    "created_at": "2020-09-14T01:57:39Z",
    "closed_at": "2022-04-19T11:21:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2609",
    "body": "Hi, I have question about wav2vec 2.0's augmentation\r\n\r\nin paper, for simillar effect of specaug, wav2vec using chunk masking to cnn encoder output.\r\n\r\nin case of time span, I think it works well because of strong correlation with near time step.\r\n\r\nBut, in case of channel span, I think channel is no correlation with near channel and why channel chunk masking works well?\r\n\r\nIn this network channel have strong correlation with near channel?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2609/comments",
    "author": "zelabean",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:59Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:34Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2605,
    "title": "How to use the api 'torch.load' to load transform_lm？",
    "created_at": "2020-09-11T11:01:37Z",
    "closed_at": "2020-09-11T13:48:27Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2605",
    "body": "i want to load transform_lm.pt by the api torch.load which didnot depent the fairseq.\r\n\r\nhow to do for me ?  ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2605/comments",
    "author": "ismymajia",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-11T13:48:27Z",
        "body": "Please follow the issue templates.  Provide the exact error you are encountering and a reproducible example"
      }
    ]
  },
  {
    "number": 2602,
    "title": "Can not find Docs for libnat_cuda.levenshtein_distance/generate_deletion_labels/generate_insertion_labels   or libnat.suggested_ed2_path",
    "created_at": "2020-09-10T09:53:04Z",
    "closed_at": "2022-04-19T11:21:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2602",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI am reading the code of lev-T (thx for the great job!).\r\n\r\nWhen I come to the codes about libnat or libnat_cuda part, I found that there is no docs for the functions :\r\n\r\nlibnat_cuda.levenshtein_distance/generate_deletion_labels/generate_insertion_labels  \r\nlibnat.suggested_ed2_path\r\n\r\nwhat are the input and output and the meanings of them? \r\nwhat are the main steps for these function ? \r\nThose really make me frustrated, any help? Thanks! \r\n\r\n#### Code\r\n        # in   fairseq/models/nat/levenshtein_utils.py  \r\n         mask_ins_targets, masked_tgt_masks = libnat.generate_insertion_labels(\r\n            out_tokens.int(), libnat.levenshtein_distance(\r\n                in_tokens.int(), out_tokens.int(),\r\n                in_masks.sum(1).int(), out_masks.sum(1).int()\r\n            )\r\n        )\r\n\r\n        word_del_targets = libnat.generate_deletion_labels(\r\n            in_tokens.int(),\r\n            libnat.levenshtein_distance(\r\n                in_tokens.int(), out_tokens.int(),\r\n                in_masks.sum(1).int(), out_masks.sum(1).int()\r\n            )\r\n        )\r\n\r\n>>> a = torch.Tensor([[0,1,2]]).to('cuda').int()\r\n>>> b = torch.Tensor([[0,1,2]]).to('cuda').int();\r\n>>> la = torch.Tensor([3]).to('cuda').int()\r\n>>> lb = torch.Tensor([3]).to('cuda').int()\r\n>>> libnat_cuda.levenshtein_distance(a, b, la, lb)\r\ntensor([[3, 3, 3, 0, 0, 0]], device='cuda:0', dtype=torch.int32)\r\n>>> b = torch.Tensor([[0,1]]).to('cuda').int();\r\n>>> lb = torch.Tensor([2]).to('cuda').int()\r\n>>> libnat_cuda.levenshtein_distance(a, b, la, lb)\r\ntensor([[3, 3, 2, 0, 0]], device='cuda:0', dtype=torch.int32)\r\n>>> b = torch.Tensor([[0,1,3]]).to('cuda').int();\r\n>>> lb = torch.Tensor([3]).to('cuda').int()\r\n>>> libnat_cuda.levenshtein_distance(a, b, la, lb)\r\ntensor([[3, 3, 2, 1, 0, 0]], device='cuda:0', dtype=torch.int32)\r\n\r\n\r\n\r\n#### What have you tried?\r\nread the documents of fairseq\r\ngoogle the doc or codes of libnat/libnat_cuda but have got nothing.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2602/comments",
    "author": "CheungZeeCn",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-10T13:55:34Z",
        "body": "CC @kahne @MultiPath "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:57Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "conan1024hao",
        "created_at": "2023-04-11T02:48:40Z",
        "body": "Have you figured it out?"
      }
    ]
  },
  {
    "number": 2600,
    "title": "what is the paper behind implementation of multilingual translation?",
    "created_at": "2020-09-10T01:03:06Z",
    "closed_at": "2022-04-19T06:21:28Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2600",
    "body": "I am wondering if anyone know the reference paper for multilingual translation model?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2600/comments",
    "author": "sophiapeng0426",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-10T15:07:26Z",
        "body": "CC @pipibjc "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T13:04:34Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:20:57Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2597,
    "title": "How fairseq-generate work? (dimension mismatch in decoder output)",
    "created_at": "2020-09-09T19:13:36Z",
    "closed_at": "2022-04-19T06:21:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2597",
    "body": "#### What is your question?\r\n\r\nI just implemented a customized LSTM model on fairseq and the training process has no bug. Then in the generation process, I could only successfully generate the first batch, then there is a dimension mismatch for my decoder's output and the vocab size, which really confused me. \r\n\r\n\r\n#### Code\r\n\r\n\r\n```\r\ndecoder output dimension: torch.Size([1000, 1, 62992])\r\ntorch.Size([1000, 62992])\r\ndecoder output dimension: torch.Size([1000, 2, 62992])\r\ntorch.Size([1000, 62992])\r\ndecoder output dimension: torch.Size([1000, 3, 62992])\r\ntorch.Size([1000, 62992])\r\ndecoder output dimension: torch.Size([1000, 4, 62992])\r\ntorch.Size([1000, 62992])\r\ndecoder output dimension: torch.Size([965, 5, 62992])\r\ntorch.Size([965, 62992])\r\ndecoder output dimension: torch.Size([935, 6, 62992])\r\ntorch.Size([935, 62992])\r\ndecoder output dimension: torch.Size([675, 7, 62992])\r\ntorch.Size([675, 62992])\r\ndecoder output dimension: torch.Size([415, 8, 62992])\r\ntorch.Size([415, 62992])\r\ndecoder output dimension: torch.Size([370, 9, 62992])\r\ntorch.Size([370, 62992])\r\ndecoder output dimension: torch.Size([315, 10, 62992])\r\ntorch.Size([315, 62992])\r\ndecoder output dimension: torch.Size([125, 11, 62992])\r\ntorch.Size([125, 62992])\r\ndecoder output dimension: torch.Size([115, 12, 62992])\r\n```\r\nThe print results above are from `sequence_generator.py`: \r\nI print out the dimension for my decoder out\r\n```\r\ndecoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)\r\n```\r\nThis batch works properly and then a new batch starts, where my decoder_out has a dimension\r\n```\r\ndecoder output dimension: torch.Size([800, 1, 62993])\r\n```\r\nand an error was thrown in following code because ` lprobs.view(bsz, -1, self.vocab_size)` use a vocab size of 62992, not 62993. I'm really confused why the decoder output dimension's vocab size is incremented by 1. (I map my decoder output to len(dictionary), which should be the vocab size 62992)\r\n\r\n```\r\n# Shape: (batch, cand_size)\r\n            cand_scores, cand_indices, cand_beams = self.search.step(\r\n                step,\r\n                lprobs.view(bsz, -1, self.vocab_size),\r\n                scores.view(bsz, beam_size, -1)[:, :, :step],\r\n            )\r\n```\r\n\r\n#### What's your environment?\r\n\r\nfairseq Version (e.g., 1.0 or master): master\r\nPyTorch Version (e.g., 1.0) 1.6\r\nOS (e.g., Linux): linux\r\nHow you installed fairseq (pip, source): source\r\nBuild command you used (if compiling from source):\r\n```\r\nfairseq-generate /home/steven/Documents/GITHUB/DQN/data/multilingual/azetur_eng\\\r\n--path /home/steven/Documents/GITHUB/DQN/checkpoints/azetur_eng/checkpoint_best.pt \\\r\n--user-dir /home/steven/Documents/GITHUB/DQN/src \\\r\n--beam 5 \\\r\n--remove-bpe \\\r\n```\r\n    Python version: 3.7\r\n    CUDA/cuDNN version: 10.2\r\n    GPU models and configuration: RTX2070, default config\r\n    Any other relevant information:\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2597/comments",
    "author": "steventan0110",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-10T15:05:49Z",
        "body": "It's hard to say without having access to your custom model.  Make sure that you specify the same `task` when training and generating (some tasks add extra entries to the dictionary).  To debug, I would simply set a breakpoint when the dictionaries are loaded in both training and generation and see what entries are different."
      },
      {
        "user": "steventan0110",
        "created_at": "2020-09-10T16:24:05Z",
        "body": "> It's hard to say without having access to your custom model. Make sure that you specify the same `task` when training and generating (some tasks add extra entries to the dictionary). To debug, I would simply set a breakpoint when the dictionaries are loaded in both training and generation and see what entries are different.\r\n\r\nI used the default task of fairseq and it should be consistent for both training and generating.\r\n\r\nfor the model, I use the `len(self.dictionary)` (in decoder class) as the output dimension, and map my final output from RNN to size of (batch_size, target_length, output_dim), as requested by the documentation. I used fairseq-preprocess to handle the text preprocessing so I think the vocab size (len(self.dictionary)) should be constant but somehow it's changed. Should I look into data loading code? I think the default loading should not have a problem in vocab size?\r\n\r\nThank you.\r\n\r\n-----------------------Update-------------------------\r\nI checked the source code for generator and task and I found no issue. In the end I pinpoint the source of my bug: \r\n```\r\ndecoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)\r\n```\r\nthis line of code produce a decoder output of dimension (batch_size, target_len, output_dim+1) in the second batch, which is really weird. When I trained my model, everything is fine and the output of decoder should always be mapped to (batch_size, target_len, output_dim), there's no way to produce an output_dim+1 in my custom model but it seems that somehow the decoder output changes..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T13:04:33Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:21:00Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2593,
    "title": "Inconsistent Sacrebleu score using ./scripts/sacrebleu.sh and score.py",
    "created_at": "2020-09-09T07:01:15Z",
    "closed_at": "2020-09-10T09:02:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2593",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHi! I want to check if I use sacrebleu in the right way.\r\n\r\n#### Code\r\nGenerate ``vanilla.output.detok.txt`` : \r\n``python generate.py ./data-bin/wmt14_en_de --path checkpoints_wmt14en2de_vanilla_transformer/checkpoint_best.pt --batch-size 512 --beam 5  --remove-bpe > vanilla.output.detok.txt``\r\n\r\nThen run \r\n``bash ./scripts/sacrebleu.sh wmt14/full en de vanilla.output.detok.txt\r\n``. \r\nThe output is \r\n``BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.13a+version.1.4.12 = 26.1 57.3/31.8/19.8/12.9 (BP = 1.000 ratio = 1.039 hyp_len = 65117 ref_len = 62688)\r\n``\r\n\r\nBut when I use ``score.py``: \r\nGenerate ``vanilla.output.detok.sys``, ``vanilla.output.detok.sys``: \r\n``grep ^H vanilla.output.detok.txt | cut -f3- > vanilla.output.detok.sys``\r\n``grep ^T vanilla.output.detok.txt | cut -f2- > vanilla.output.detok.ref``\r\n\r\n1) without ``sacrebleu``:  \r\n``python score.py --sys vanilla.output.detok.sys --ref vanilla.output.detok.ref``\r\noutput:\r\n``\r\nNamespace(ignore_case=False, order=4, ref='vanilla.output.detok.ref', sacrebleu=False, sentence_bleu=False, sys='vanilla.output.detok.sys')\r\nBLEU4 = 26.72, 58.1/32.5/20.3/13.3 (BP=1.000, ratio=1.031, syslen=66486, reflen=64506)\r\n``\r\n2) with ``sacrebleu``: \r\n``python score.py --sys vanilla.output.detok.sys --ref vanilla.output.detok.ref --sacrebleu``\r\noutput: \r\n``\r\nNamespace(ignore_case=False, order=4, ref='vanilla.output.detok.ref', sacrebleu=True, sentence_bleu=False, sys='vanilla.output.detok.sys')\r\nWARNING:root:That's 100 lines that end in a tokenized period ('.')\r\nWARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\r\nWARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\r\n<sacrebleu.metrics.bleu.BLEUScore object at 0x7fbbbc1ebcd0>\r\n``. I checked the output in this object, it is ``27.36``.\r\n\r\nSo did I use these commands correctly? Thank you.\r\n\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9\r\n - PyTorch Version (e.g., 1.0): \r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2593/comments",
    "author": "haorannlp",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-09T14:11:34Z",
        "body": "You're close.  One thing that the `./scripts/sacrebleu.sh` script does that the `score.py` script does not is detokenize.  To reproduce the `sacrebleu.sh` script using `score.py` you'll want to make the following change:\r\n\r\n```\r\ngrep ^H vanilla.output.detok.txt | cut -f3- | sacremoses detokenize > vanilla.output.detok.sys\r\ngrep ^T vanilla.output.detok.txt | cut -f2- | sacremoses detokenize > vanilla.output.detok.ref\r\n```\r\n\r\nThis will detokenize both the system outputs and the reference before computing BLEU.  Hope this helps!"
      },
      {
        "user": "haorannlp",
        "created_at": "2020-09-09T15:29:22Z",
        "body": "@lematt1991 Thank you for your clarification. But BLEU4 now turned to be: 22.27 (without ``--sacrebleu``), 25.89 (with ``sacrebleu``). Does the ``--remove-bpe`` parameter in ``generate.py`` already detokenize the output? "
      },
      {
        "user": "haorannlp",
        "created_at": "2020-09-10T09:02:15Z",
        "body": "@lematt1991 \r\n``grep ^H vanilla.output.detok.txt | cut -f3 | sacremoses detokenize > vanilla.output.detok.sys``\r\n``grep ^T vanilla.output.detok.txt | cut -f2 | sacremoses detokenize > vanilla.output.detok.ref``\r\ncan reproduce the results."
      },
      {
        "user": "lorelupo",
        "created_at": "2020-11-05T17:43:03Z",
        "body": "Hello ,\r\n\r\nIt looks to me that this is still an issue for wmt14 en-fr. I follow this procedure:\r\n\r\n1. generate with\r\n    `fairseq-generate ./data-bin/wmt14_en_fr --task translation --path $sdir/$avg_checkpoint  --batch-size 16 --remove-bpe --beam 4 --lenpen 0.6 | tee $sdir/logs/test.log`\r\n2. score with \r\n   ```\r\n   grep ^H $sdir/logs/test.log | cut -f3 | sacremoses detokenize > $sdir/logs/test.detok.sys\r\n   grep ^T $sdir/logs/test.log | cut -f2 | sacremoses detokenize > $sdir/logs/test.detok.ref\r\n   python fairseq_cli/score.py --sys $sdir/logs/test.detok.sys --ref $sdir/logs/test.detok.ref --sacrebleu | tee $sdir/logs/score.log\r\n   ```\r\n3. finally scoring with:\r\n   `bash scripts/sacrebleu.sh wmt14/full $src $tgt $sdir/logs/test.log | tee $sdir/logs/score.log`\r\n\r\nResults:\r\n\r\n2. Scoring with `fairseq_cli/score.py`: \r\n    BLEU = **37.04** 66.6/44.8/32.1/23.5 (BP = 0.956 ratio = 0.957 hyp_len = 80771 ref_len = 84388)\r\n\r\n3. Scoring with `scripts/sacrebleu.sh`:\r\n    BLEU+case.mixed+lang.en-fr+numrefs.1+smooth.exp+test.wmt14/full+tok.13a+version.1.4.14 = **32.3** 60.5/38.3/26.0/17.9 (BP = 1.000 ratio = 1.045 hyp_len = 80771 ref_len = 77306)\r\n\r\nI think that this might be due to the fact that when removing BPE some tokens remains separated by a white-space even after detokenization, although they should not, e.g. \"d' un\" instead of \"d'un\", 'km / h\" instead of \"km/h\".\r\n\r\nAs a concrete example, this commands\r\n````\r\nref=~/.sacrebleu/wmt14/full/en-fr.fr\r\nsys=checkpoints/wmt14/transfo_base/logs/test.log.sorted.detok\r\npaste -d \\\\n $sys $ref >out.txt\r\nhead out.txt\r\ntail out.txt\r\n````\r\n\r\nreturn:\r\n\r\n```\r\nCoup de pinceau spectaculaire au-dessus de Bogota\r\nSpectaculaire saut en \"wingsuit\" au-dessus de Bogota\r\nLe sportif Jhonathan Florez a sauté d' un hélicoptère au-dessus de Bogota, la capitale de la Colombie, jeudi.\r\nLe sportif Jhonathan Florez a sauté jeudi d'un hélicoptère au-dessus de Bogota, la capitale colombienne.\r\nPortant une combinaison d' ailes, il a survolé le célèbre sanctuaire Monserrate à 160 km / h. Le sanctuaire est situé à une altitude de plus de 3 000 mètres et de nombreux spectateurs s' y sont rassemblés pour observer son exploitation.\r\nEquipé d'un wingsuit (une combinaison munie d'ailes), il est passé à 160 km/h au-dessus du célèbre sanctuaire Monserrate, situé à plus de 3 000 mètres d'altitude, où de nombreux badauds s'étaient rassemblés pour observer son exploit.\r\nUne boîte noire dans votre voiture?\r\nUne boîte noire dans votre voiture ?\r\nTandis que les planificateurs routiers américains luttent pour trouver l' argent nécessaire à la mise en place d' un réseau routier en panne, beaucoup commencent à voir une solution dans une petite boîte noire qui correspond parfaitement au tableau de bord de votre voiture.\r\nAlors que les planificateurs du réseau routier des États-Unis ont du mal à trouver l'argent nécessaire pour réparer l'infrastructure autoroutière en décrépitude, nombreux sont ceux qui entrevoient une solution sous forme d'une petite boîte noire qui se fixe au-dessus du tableau de bord de votre voiture.\r\n```\r\n\r\nand\r\n\r\n```\r\nLe conseil scolaire Marguerite-Bourgeoys a créé un centre de recherche qui fournira des outils aux enseignants qui, eux-mêmes, viennent parfois d' ailleurs.\r\nLa commission scolaire Marguerite-Bourgeoys a créé un centre de recherche qui donnera des outils aux professeurs qui, eux aussi parfois, viennent d'ailleurs.\r\nRachida Azdouz de l' Université de Montréal sera le directeur scientifique.\r\nRachida Azdouz, de l'Université de Montréal, en sera la directrice scientifique.\r\nPréparation à la gestion d' une classe dans un contexte nord-américain et québécois.\r\nLa préparation à gérer une classe dans un contexte nord-américain, québécois.\r\n\"Le besoin réel est de mettre en œuvre différentes stratégies éducatives\", résume-t-elle.\r\n\"Des stratégies pédagogiques différentes, c'est ça le véritable besoin \", résume-t-elle.\r\nLa recherche portera sur l' inclusion sous tous ses aspects: linguistique, éducatif, social et culturel.\r\nLes recherches porteront sur l'inclusion sous tous ses angles: linguistique, scolaire, social et culturel.\r\n```\r\n\r\nIs there a way to fix this?\r\n"
      },
      {
        "user": "kurtabela",
        "created_at": "2022-01-25T11:51:49Z",
        "body": "> @lematt1991 Thank you for your clarification. But BLEU4 now turned to be: 22.27 (without `--sacrebleu`), 25.89 (with `sacrebleu`). Does the `--remove-bpe` parameter in `generate.py` already detokenize the output?\r\n\r\nI do have the `--remove-bpe` parameter but I still face this warning. "
      }
    ]
  },
  {
    "number": 2588,
    "title": "need to preprocess (binarize) a huge dataset ~ 200G data. ",
    "created_at": "2020-09-08T20:40:09Z",
    "closed_at": "2020-09-08T21:12:23Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2588",
    "body": "Hi \r\n\r\nI need to binarize a huge dataset ~200G, however my machine is limited. ~128G memory. Is there an arg to do so in the fairseq-preprocess command?\r\n\r\nThanks so much!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2588/comments",
    "author": "kkissmart",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-08T21:12:23Z",
        "body": "Did you try preprocessing and get an out of memory error?  As far as I know, I don't think `fairseq-preprocess` should need to read the entire file in at once.  "
      },
      {
        "user": "kkissmart",
        "created_at": "2020-09-08T21:16:03Z",
        "body": "@lematt1991 of course I tried before I asked. the process was auto-killed, and I have tried multiple times. Can you reopen this issue so that other people can help?"
      }
    ]
  },
  {
    "number": 2584,
    "title": "the output log format do not have timestamp info ",
    "created_at": "2020-09-08T11:49:11Z",
    "closed_at": "2020-09-08T14:51:43Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2584",
    "body": "\r\n| epoch 001:  15000 / 34323 loss=6.628, nll_loss=6.628, ppl=98.88, wps=644, ups=0, wpb=262143.968, bsz=2048.000, num_updates=15001, lr=0.00025819, gnorm=0.224, clip=0.106, oom=0.000, wall=407, train_wall=6043\r\n| epoch 001:  15010 / 34323 loss=6.627, nll_loss=6.627, ppl=98.85, wps=7006, ups=0, wpb=262143.968, bsz=2048.000, num_updates=15011, lr=0.000258104, gnorm=0.224, clip=0.106, oom=0.000, wall=412, train_wall=6047\r\n\r\nNow the output log format is above. Why not have timestamp info in the line header.  ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2584/comments",
    "author": "ismymajia",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-08T14:51:43Z",
        "body": "You should be able to reconstruct this from the `train_wall` times"
      }
    ]
  },
  {
    "number": 2579,
    "title": "--share-all-embeddings requires a joined dictionary",
    "created_at": "2020-09-05T14:25:41Z",
    "closed_at": "2020-09-08T14:45:57Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2579",
    "body": "@edunov @myleott @ngoyal2707 I am trying to train a seq2seq model for translation purposes but am facing a problem when using the GPU for training. This is the command used to do the training:-\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0 fairseq-train \"/content/drive/My Drive/HashPro/New/\" --fp16 --max-sentences 8 --lr 0.02 --clip-norm 0.1  \\\r\n  --optimizer sgd --dropout 0.2  \\\r\n  --arch bart_large --save-dir \"/content/drive/My Drive/HashPro/Checkpoints\"\r\n```\r\n\r\nAnd this is the error:-\r\n```\r\n2020-09-05 14:11:00 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_large', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='/content/drive/My Drive/HashPro/New/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.2, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.02], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=0, max_sentences=8, max_sentences_valid=8, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, momentum=0.0, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=True, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_workers=1, optimizer='sgd', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/My Drive/HashPro/Checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.0, zero_sharding='none')\r\n2020-09-05 14:11:00 | INFO | fairseq.tasks.translation | [input] dictionary: 21936 types\r\n2020-09-05 14:11:00 | INFO | fairseq.tasks.translation | [output] dictionary: 9216 types\r\n2020-09-05 14:11:00 | INFO | fairseq.data.data_utils | loaded 1 examples from: /content/drive/My Drive/HashPro/New/valid.input-output.input\r\n2020-09-05 14:11:00 | INFO | fairseq.data.data_utils | loaded 1 examples from: /content/drive/My Drive/HashPro/New/valid.input-output.output\r\n2020-09-05 14:11:00 | INFO | fairseq.tasks.translation | /content/drive/My Drive/HashPro/New/ valid input-output 1 examples\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/content/fairseq/fairseq_cli/train.py\", line 343, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/content/fairseq/fairseq/distributed_utils.py\", line 187, in call_main\r\n    main(args, **kwargs)\r\n  File \"/content/fairseq/fairseq_cli/train.py\", line 68, in main\r\n    model = task.build_model(args)\r\n  File \"/content/fairseq/fairseq/tasks/translation.py\", line 279, in build_model\r\n    model = super().build_model(args)\r\n  File \"/content/fairseq/fairseq/tasks/fairseq_task.py\", line 248, in build_model\r\n    model = models.build_model(args, self)\r\n  File \"/content/fairseq/fairseq/models/__init__.py\", line 48, in build_model\r\n    return ARCH_MODEL_REGISTRY[args.arch].build_model(args, task)\r\n  File \"/content/fairseq/fairseq/models/transformer.py\", line 198, in build_model\r\n    raise ValueError(\"--share-all-embeddings requires a joined dictionary\")\r\nValueError: --share-all-embeddings requires a joined dictionary\r\n\r\n```\r\nFrom the docs, I can only glean that the \"target_dictionary\" and the \"source_dictionary\" is not the same. Apart from that, I could find no help from the internet. Since the error seems to be related to joined dictionaries, it seems that maybe there was a preprocessing step I missed. However, I have scanned all the arguments and they seem to be correct. Even then, here is the command for reference:-\r\n```\r\n\r\n%%bash\r\nfairseq-preprocess --source-lang input --target-lang output \\\r\n  --trainpref /content/drive/'My Drive'/HashPro/tokenized/hashpro_hashes.bpe --bpe characters --validpref /content/drive/'My Drive'/HashPro/tokenized/hashpro_hashes.bpe \\\r\n  --destdir /content/drive/'My Drive'/HashPro/New/\r\n```\r\nDoes anybody have any idea on how to fix this?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2579/comments",
    "author": "neel04",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-08T14:45:57Z",
        "body": "You'll need to use the `--joined-dictionary` option when running `fairseq-preprocess`."
      },
      {
        "user": "neel04",
        "created_at": "2020-09-10T15:24:48Z",
        "body": "\r\n@lematt1991 mind explaining what `--joined-dictionary` does?"
      },
      {
        "user": "lematt1991",
        "created_at": "2020-09-10T15:28:33Z",
        "body": "Instead of creating separate source and target dictionaries, it creates one dictionary for both source and target"
      }
    ]
  },
  {
    "number": 2571,
    "title": "how to use \"necleus sampling\" instead of \"beam search\" in asr",
    "created_at": "2020-09-04T08:37:28Z",
    "closed_at": "2020-09-11T01:30:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2571",
    "body": "I'm doing speech_recognition task with wav2vec2.0 and I want to use \"necleus sampling\"  instead of \"beam search\" to do decoder.\r\nI don't use wav2letter/binding/python module, can you give some advices on how to do it? thanks @alexeib \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2571/comments",
    "author": "luweishuang",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-10T15:07:52Z",
        "body": "CC @alexeib "
      },
      {
        "user": "alexeib",
        "created_at": "2020-09-10T17:48:22Z",
        "body": "you can implement your own decoder or integrate with some existing one instead of wav2letter. you can use infer.py as a guide on how to get emissions from a ctc model. pull requests are welcome :)\r\n\r\nwe also support seq2seq models (use wav2vec_seq2seq instead of wav2vec_ctc) which can be decoded with beam search with our standard generate.py machinery used for machine translation (you can use --scoring wer to get WER instead of BLEU), but you may have to play around with the decoder architecture and hyper parameters"
      },
      {
        "user": "SamChen",
        "created_at": "2021-05-08T19:24:23Z",
        "body": "@alexeib Can you offer an intuitive example about how to use generate.py? I tried to use it to run wav2vec_seq2seq. But I failed and could not find an example in `fairseq/example`. Thanks :-)"
      },
      {
        "user": "dangvansam",
        "created_at": "2021-09-25T07:59:08Z",
        "body": "> you can implement your own decoder or integrate with some existing one instead of wav2letter. you can use infer.py as a guide on how to get emissions from a ctc model. pull requests are welcome :)\r\n> \r\n> we also support seq2seq models (use wav2vec_seq2seq instead of wav2vec_ctc) which can be decoded with beam search with our standard generate.py machinery used for machine translation (you can use --scoring wer to get WER instead of BLEU), but you may have to play around with the decoder architecture and hyper parameters\r\n\r\n@alexeib i have a question. i use infer.py to inferring wav2vec_seq2seq model (trained with cross entropy loss, autoregressive=True) but get error: _forward() missing 1 required positional argument: 'prev_output_tokens'_\r\nThanks!"
      }
    ]
  },
  {
    "number": 2569,
    "title": "How use dynamicconv_layer for fairsep 0.8.0",
    "created_at": "2020-09-04T04:41:52Z",
    "closed_at": "2020-09-10T15:03:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2569",
    "body": "\r\n#### I can not install dynamicconv_layer for 0.8.0\r\n\r\nI install as follows:\r\ncd fairseq/modules/dynamicconv_layer\r\npython cuda_function_gen.py\r\npython setup.py install\r\n\r\n#### The error is followed.\r\nIn file included from lightconv_cuda_kernel.cu:8:0:\r\nlightconv_cuda.cuh:9:33: fatal error: c10/cuda/CUDAStream.h: No such file or directory\r\ncompilation terminated.\r\nerror: command '/usr/local/cuda/bin/nvcc' failed with exit status 1\r\n\r\nHow to use dynamicconv_layer for fairsep 0.8\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2569/comments",
    "author": "Jxu-Thu",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-09-08T14:41:59Z",
        "body": "Please include the full command that you ran"
      },
      {
        "user": "lematt1991",
        "created_at": "2020-09-10T15:03:52Z",
        "body": "Closing due to inactivity "
      }
    ]
  },
  {
    "number": 2561,
    "title": "ASSERTION ERROR:sentence at index 8241 of size 6922994 exceeds max_tokens limit of 1280000!",
    "created_at": "2020-09-03T02:14:01Z",
    "closed_at": "2020-09-03T02:29:45Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2561",
    "body": "#### What is your question?\r\nI am trying to fine-tune the model using my own audio and label. However, I met this ASSERTION ERROR:**sentence at index 8241 of size 6922994 exceeds max_tokens limit of 1280000!**  Does this mean that there is something in label file not included in dict.txt.ltr? How to solve it?\r\n#### Code\r\nnohup python /data/home/v_liwenxu/fairseq/fairseq_cli/train.py //data3/mirex2020/label \\\r\n--save-dir /data/home/v_liwenxu/data/segmented_sing/s5/models/ --fp16 \\\r\n--post-process letter --valid-subset //data3/mirex2020/label/valid \\\r\n--no-epoch-checkpoints --best-checkpoint-metric wer --num-workers 4 \\\r\n--max-update 80000 --sentence-avg --task audio_pretraining --arch wav2vec_ctc --w2v-path /data/home/v_liwenxu/data/segmented_sing/s5/models/wav2vec_small.pt \\\r\n--labels ltr --apply-mask --mask-selection static --mask-other 0 --mask-length 10 --mask-prob 0.5 --layerdrop 0.1 \\\r\n--mask-channel-selection static --mask-channel-other 0 --mask-channel-length 64 --mask-channel-prob 0.5 --zero-infinity \\\r\n--feature-grad-mult 0.0 --freeze-finetune-updates 10000 --validate-after-updates 10000 --optimizer adam \\\r\n--adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr 2e-05 --lr-scheduler tri_stage --warmup-steps 8000 --hold-steps 32000 \\\r\n--decay-steps 40000 --final-lr-scale 0.05 --final-dropout 0.0 --dropout 0.0 --activation-dropout 0.1 --criterion ctc \\\r\n--attention-dropout 0.0 --max-tokens 1280000 --seed 2337 --log-format json --log-interval 500  --ddp-backend no_c10d  > train_output.txt 2>&1 &\r\n#### ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2561/comments",
    "author": "MorrisXu-Driving",
    "comments": [
      {
        "user": "MorrisXu-Driving",
        "created_at": "2020-09-03T02:29:42Z",
        "body": "I just need to adjust the max-tokens in the training script."
      },
      {
        "user": "ArtemisZGL",
        "created_at": "2020-10-25T09:05:33Z",
        "body": "@MorrisXu-Driving hello, i met this error too. If you just adjust the max_tokens, will it cause the oom error?"
      },
      {
        "user": "MorrisXu-Driving",
        "created_at": "2020-10-25T11:12:36Z",
        "body": "I actually don't know what is the real reason that triggered this error. I just used a smaller dataset (nearly 90%) than the one that triggered this error and everything started working propearly. I guess this may because of some unexpected tokens in labels of your original dataset."
      },
      {
        "user": "ArtemisZGL",
        "created_at": "2020-10-25T11:20:04Z",
        "body": "@MorrisXu-Driving oh, thanks. I just filter some samples which is longer than max tokens. It seems that some samples are strange."
      }
    ]
  },
  {
    "number": 2560,
    "title": "Why does the feature vector increase the dimensionality?",
    "created_at": "2020-09-03T01:17:22Z",
    "closed_at": "2020-09-03T20:15:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2560",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nWhen running the `wav2vec_featurize.py` on a raw audio file, I find that I have `93680` length for an audio file, but that results in a feature vector with a shape of `(512, 583)`. That means I have `298496` data points now, whereas the raw audio only has `93680`. I thought the point of embeddings was to reduce dimensionality, so I'm a bit confused as to why it's expanded?\r\n\r\n#### Code\r\n```\r\npython examples/wav2vec/wav2vec_featurize.py --input data-bin/LibriSpeech --output tmp --model data-bin/models/wav2vec_large.pt --split dev-clean --ext flac --no-copy-labels --use-feat\r\n```\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): `master`\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2560/comments",
    "author": "shamoons",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-09-03T20:15:51Z",
        "body": "the raw waveform is encoded into representations with sample rate of 100hz. librispeech has sample rate of 16khz so 93680/160 = 585.5. you lose a bit because of \"valid\" padding (as opposed to \"same\") but seems to add up..."
      },
      {
        "user": "shamoons",
        "created_at": "2020-09-03T20:57:09Z",
        "body": "Ahhh - that makes sense. Just so I'm clear, the encoded representations are not reduced dimensionality? "
      }
    ]
  },
  {
    "number": 2559,
    "title": "WMT14en2de validation dataset is not newstest2013?",
    "created_at": "2020-09-02T21:48:13Z",
    "closed_at": "2020-09-12T20:04:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2559",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI understand that ``prepare-wmt14en2de.sh`` generates train/valid/test sets automatically, but I do not see ``newstest2013``, which is used as dev set in many papers, is directly used. The valid set is instead generated by splitting the training set? Do I miss something here or I should modify the script to account for ``newtest2013``? \r\nIf we are using ``newstest 2013`` as valid set, ``newstest 2014`` as test set. The best BLEU score on the dev set should be close to the score on the test set. I trained a Transformer base on europarlv7, it can score over 30 BLEU score on the dev set, but only 20 on the test set? This might suggest that actual dev set we use is not newstest 2013? Thank you\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9\r\n - PyTorch Version (e.g., 1.0): 1.2\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2559/comments",
    "author": "haorannlp",
    "comments": [
      {
        "user": "edunov",
        "created_at": "2020-09-12T20:04:08Z",
        "body": "Hi @haorannlp, \r\n\r\nWe used a subset extracted from train as validation set in this setup, that was a fairly common practice back when this script was written. You're right that nowadays, I'd use newstest13 for validation instead. If you'd like, feel free to add an option to the script to use newstest2013 as a valid and send a pull request. \r\n\r\nThanks,\r\nSergey\r\n\r\n"
      }
    ]
  },
  {
    "number": 2558,
    "title": "Dataset not found even though all files are present",
    "created_at": "2020-09-02T16:09:11Z",
    "closed_at": "2020-09-03T11:04:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2558",
    "body": "Hi all!\r\nI was training a seq2seq model for a specific task (In same language) however I am getting this error:-\r\n```\r\nNamespace(activation_fn='gelu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='bart_large', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.1, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='/content/drive/My Drive/HashPro/preprocessed', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=True, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.02], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=0, max_sentences=8, max_sentences_valid=8, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, momentum=0.0, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=1, optimizer='sgd', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, raw_text=False, relu_dropout=0.0, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/My Drive/HashPro/Checkpoints/', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\r\n| [input] dictionary: 21936 types\r\n| [output] dictionary: 9216 types\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 333, in cli_main\r\n    main(args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 48, in main\r\n    task.load_dataset(valid_sub_split, combine=False, epoch=0)\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq/tasks/translation.py\", line 219, in load_dataset\r\n    truncate_source=self.args.truncate_source,\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq/tasks/translation.py\", line 52, in load_langpair_dataset\r\n    raise FileNotFoundError('Dataset not found: {} ({})'.format(split, data_path))\r\nFileNotFoundError: Dataset not found: valid (/content/drive/My Drive/HashPro/preprocessed)\r\n```\r\nIt does report finding the dictionaries, but apparently the dataset is not found. Here are the dataset files :-\r\n> dict.input.txt\r\n> dict.output.txt\r\n> hashpro_hashes.bpe.input\r\n> hashpro_hashes.bpe.output\r\n> preprocess.log\r\n> train.input-output.input.bin\r\n> train.input-output.input.idx\r\n> train.input-output.output.bin\r\n> train.input-output.output.idx\r\n\r\nSince all the files are included, and the path seems to be correct (since it can load up the dictionaries) I don't understand why such a problem is occurring. This is the training command I am using to train the whole model-\r\n\r\n`%%bash`\r\n`fairseq-train '/content/drive/My Drive/HashPro/preprocessed' --max-sentences 8 --fp16 --lr 0.02 --clip-norm 0.1 --optimizer sgd --dropout 0.2 --arch bart_large --save-dir /content/drive/'My Drive'/HashPro/Checkpoints/`\r\n\r\nI am using TPU which has been initialized in the standard way shown in Colab examples. Apparently there have been some changes in the implementations - I can no longer put the `--tpu` flag or `--bf16`. Has the support been disabled for debugging or is there a problem with the way I have installed FairSeq?\r\n ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2558/comments",
    "author": "neel04",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-09-03T11:04:59Z",
        "body": "You need to have valid.input and valid.output when you run preprocess.py, that's what --validpref is looking for. Your model is trying to validate, and it cannot find it. preprocess.py will generate valid.input-output.input.bin, valid.input-output.input.idx etc just like train"
      },
      {
        "user": "neel04",
        "created_at": "2020-09-03T15:03:38Z",
        "body": "@huihuifan Thanks a ton!! I had not put the `--validpref` flag in my preprocessing step and since it didn't give me any warning or error, I thought that it must have used the same argument for `--trainpref` as the path for validpref. Again, appreciate the help!!"
      },
      {
        "user": "Crista23",
        "created_at": "2021-04-18T23:05:20Z",
        "body": "Hi @huihuifan , I have the same problem even though my files are present in the correct format and I am trying to generate translations with --replace_unk:\r\n\r\nTraceback (most recent call last):\r\n  File \"generate.py\", line 192, in <module>\r\n    cli_main()\r\n  File \"generate.py\", line 188, in cli_main\r\n    main(args)\r\n  File \"generate.py\", line 35, in main\r\n    task.load_dataset(args.gen_subset)\r\n  File \"/usr/fairseq/tasks/translation.py\", line 154, in load_dataset\r\n    raise FileNotFoundError('Dataset not found: {} ({})'.format(split, data_path))\r\nFileNotFoundError: Dataset not found: test (/data/test)\r\n\r\nWhat could be causing this? Thanks!\r\n    "
      }
    ]
  },
  {
    "number": 2557,
    "title": "Wav2Vec 2.0 training model on completely different language need some help with parameters and info",
    "created_at": "2020-09-02T07:17:22Z",
    "closed_at": "2020-09-03T11:05:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2557",
    "body": "Hi,\r\nI am training an ASR model on a completely different language from english using Wav2vec 2.0. Just need to know upto which  value of loss/accuracy I should train the model before moving to fine-tuning. \r\n\r\nCan you share info like:\r\n1. Number of epochs trained for pre-training before moving to fine-tuning. \r\n2. Loss/accuracy required before moving on to fine-tuning.\r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2557/comments",
    "author": "amant555",
    "comments": [
      {
        "user": "shamoons",
        "created_at": "2020-09-02T11:17:50Z",
        "body": "Will goo be using a pretrained model as a baseline or are you starting entirely from scratch?"
      },
      {
        "user": "amant555",
        "created_at": "2020-09-02T11:21:10Z",
        "body": "I am starting from scratch."
      },
      {
        "user": "shamoons",
        "created_at": "2020-09-02T11:23:17Z",
        "body": "Why not start from a baseline pretrained model? I assume the vector representations of the audio will be similar enough across languages. You can then use those representations for your downstream ASR task. At least that’s how I think it’s supposed to work."
      },
      {
        "user": "amant555",
        "created_at": "2020-09-02T11:32:38Z",
        "body": "That's a good point. The reason I am training from scratch is because I want to know if training a model from scratch or using the baseline model yield any result change. I am also training a model on top of baseline model in parallel but there is stagnation for quite a while. And I am not sure if for a completely different language it will learn the correct quantized latent audio representation. So you can say that I am running an experiment here for my learning. \r\nAnd starting from pertained model doesn't give complete solution, It do give a boost to pre-training but still need to train on my audio set till it reaches that point where I can start fine-tuning."
      },
      {
        "user": "shamoons",
        "created_at": "2020-09-02T12:46:42Z",
        "body": "Oh wow, that's pretty cool. I'd love to hear more about how your training from scratch works out. Please keep us posted"
      },
      {
        "user": "luweishuang",
        "created_at": "2020-09-03T01:47:29Z",
        "body": "I finetuned from english model \"wav2vec_vox.pt\", using 400+ hours chinese unsupervised audios. running until checkpoint_best.pt keep the same. It ran 17 epochs. Then I finetuned wav2vec_ctc model on chinese supervised audios, 200+ hours and running 14 epochs, finally I got a similarity(little less than wav2letter) cer/ler result with wav2letter \"streaming_convnets\" model which using 1000+ hours audios to training. The 200+ supervised dataset got from 1000+ hours proportionally and I used \"LexiconFreeDecoder\" both in wav2letter and in fairseq's binding/python module."
      },
      {
        "user": "amant555",
        "created_at": "2020-09-03T04:59:38Z",
        "body": "Hi @luweishuang can you let me know what were the loss and accuracy value you got in pre-training( i.e before starting fine tuning). That would really help.\r\n"
      },
      {
        "user": "luweishuang",
        "created_at": "2020-09-03T06:35:48Z",
        "body": "@amant555 the pretraining log maybe was deleted by me, sorry"
      },
      {
        "user": "huihuifan",
        "created_at": "2020-09-03T11:05:57Z",
        "body": "This question is difficult to answer, as it depends on your dataset. I would train the model until it has converged and make sure it is not overfitting, before exploring finetuning. "
      },
      {
        "user": "Leeyouxie",
        "created_at": "2021-06-16T03:37:34Z",
        "body": "> This question is difficult to answer, as it depends on your dataset. I would train the model until it has converged and make sure it is not overfitting, before exploring finetuning.\r\n\r\nHi, I'm trying to train a model from scratch using Arabic audio, but I found it is overfitting. The training set is about 500 hours. "
      },
      {
        "user": "KadoshYa",
        "created_at": "2021-09-01T20:31:03Z",
        "body": "Hey I'm trying to train a model with  baseline pretrained model for Amharic language. I'm just starting up.\r\n\r\nThe dataset I have is only around 100 hours"
      },
      {
        "user": "sangam-kushwaha",
        "created_at": "2023-02-10T17:19:27Z",
        "body": "Hi everyone, I am a B.Tech final year student and I am trying to run  and train the Wav2Vec2.0 model from scratch  and I am getting error. Can someone please tell me steps in order to train the model from scratch that I should follow? That would really help me."
      }
    ]
  },
  {
    "number": 2550,
    "title": "infer.py in speech recognition example",
    "created_at": "2020-09-01T04:35:58Z",
    "closed_at": "2020-09-02T17:12:43Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2550",
    "body": "infer.py in speech recognition example, line 288, return super().build_generator(args).  \r\nBut super() is impossible because it is not a class but a function.  \r\nI have to face line 298 because I don't use wav2letters.   \r\nOr should I set up another w2l_decoder?  \r\n  \r\nThank You.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2550/comments",
    "author": "sooftware",
    "comments": [
      {
        "user": "Lineldor",
        "created_at": "2020-09-18T02:01:05Z",
        "body": "Hi, have you solved this problem?"
      },
      {
        "user": "sooftware",
        "created_at": "2020-09-18T04:12:45Z",
        "body": "yes, Are you in the same situation as me?"
      },
      {
        "user": "Lineldor",
        "created_at": "2020-09-18T04:14:57Z",
        "body": "Yeah, I have encountered the same problem. If I don't use any decoder, can I run infer.py without this error?"
      },
      {
        "user": "sooftware",
        "created_at": "2020-09-18T04:29:26Z",
        "body": "I was modified and used as an old commit on infer.py. infer.py now has to install wav2letters and set --w2ldecoder as viterbi."
      },
      {
        "user": "Lineldor",
        "created_at": "2020-09-18T04:31:13Z",
        "body": "Oh, thanks!"
      }
    ]
  },
  {
    "number": 2549,
    "title": "how to utilise multiple GPUs at the time of inference for MT?",
    "created_at": "2020-09-01T02:57:53Z",
    "closed_at": "2020-09-03T11:07:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2549",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\n I have Nvidia 4 V100 GPUs. I observed that during inference (either `generate.py` or `interactive.py`) only one GPU is utilized and others remain ideal. Is there a possible way to utilize the other three GPUs as well at the inference time? Thank you!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2549/comments",
    "author": "kaushal0494",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-09-03T11:07:44Z",
        "body": "No, currently inference only runs on one GPU. You can pass a shard-id and run generate and interactive on multiple gpus by sharding the dataset, but it won't automatically run on multiple"
      },
      {
        "user": "kaushal0494",
        "created_at": "2020-09-03T13:53:02Z",
        "body": "Thank you @huihuifan "
      },
      {
        "user": "FayZ676",
        "created_at": "2023-02-27T20:34:39Z",
        "body": "@huihuifan @kaushal0494 is this still the case, or is there support for multi gpu inference?"
      }
    ]
  },
  {
    "number": 2539,
    "title": "size mismatch for w2v_encoder while loading pretrained model(wav2vec2.0)",
    "created_at": "2020-08-29T13:43:00Z",
    "closed_at": "2020-08-29T13:46:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2539",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nRuntimeError: Error(s) in loading state_dict for Wav2VecCtc:\r\n        size mismatch for w2v_encoder.proj.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\r\n        size mismatch for w2v_encoder.proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\r\n#### Code\r\npython train.py /data/home/v_liwenxu/data/LibriSpeech/train-pipeline/data/ \\\r\n--save-dir /data/home/v_liwenxu/data/LibriSpeech/train-pipeline/models/ --fp16 \\\r\n--post-process letter --valid-subset /data/home/v_liwenxu/data/LibriSpeech/train-pipeline/data/libris-train \\\r\n--no-epoch-checkpoints --best-checkpoint-metric wer --num-workers 4 \\\r\n--max-update 80000 --sentence-avg --task audio_pretraining --arch wav2vec_ctc --w2v-path /data/home/v_liwenxu/data/LibriSpeech/train-pipeline/models/wav2vec_small_960h.pt \\\r\n--labels ltr --apply-mask --mask-selection static --mask-other 0 --mask-length 10 --mask-prob 0.5 --layerdrop 0.1 \\\r\n--mask-channel-selection static --mask-channel-other 0 --mask-channel-length 64 --mask-channel-prob 0.5 --zero-infinity \\\r\n--feature-grad-mult 0.0 --freeze-finetune-updates 10000 --validate-after-updates 10000 --optimizer adam \\\r\n--adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr 2e-05 --lr-scheduler tri_stage --warmup-steps 8000 --hold-steps 32000 \\\r\n--decay-steps 40000 --final-lr-scale 0.05 --final-dropout 0.0 --dropout 0.0 --activation-dropout 0.1 --criterion ctc \\\r\n--attention-dropout 0.0 --max-tokens 1280000 --seed 2337 --log-format json --log-interval 500 --ddp-backend no_c10d\r\n\r\nI am trying to fine-tune the pre-trained model in wav2vec2.0，wav2vec_small_960h.pt ，specifically. However, while I ran the above code, the following error emerged.:\r\nTraceback (most recent call last):\r\n  File \"/data/home/v_liwenxu/miniconda3/envs/mirex/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/data1/home/v_liwenxu/fairseq/fairseq/distributed_utils.py\", line 156, in distributed_main\r\n    main(args, **kwargs)\r\n  File \"/data1/home/v_liwenxu/fairseq/fairseq_cli/train.py\", line 67, in main\r\n    model = task.build_model(args)\r\n  File \"/data1/home/v_liwenxu/fairseq/fairseq/tasks/fairseq_task.py\", line 248, in build_model\r\n    model = models.build_model(args, self)\r\n  File \"/data1/home/v_liwenxu/fairseq/fairseq/models/__init__.py\", line 48, in build_model\r\n    return ARCH_MODEL_REGISTRY[args.arch].build_model(args, task)\r\n  File \"/data1/home/v_liwenxu/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 168, in build_model\r\n    w2v_encoder = Wav2VecEncoder(args, task.target_dictionary)\r\n  File \"/data1/home/v_liwenxu/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py\", line 345, in __init__\r\n    model.load_state_dict(state[\"model\"], strict=True)\r\n  File \"/data1/home/v_liwenxu/fairseq/fairseq/models/fairseq_model.py\", line 93, in load_state_dict\r\n    return super().load_state_dict(new_state_dict, strict)\r\n  File \"/data/home/v_liwenxu/miniconda3/envs/mirex/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 847, in load_state_dict\r\n    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for Wav2VecCtc:\r\n        size mismatch for w2v_encoder.proj.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\r\n        size mismatch for w2v_encoder.proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\r\n\r\nI really have no idea how to fix this problem, could anyone help?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2539/comments",
    "author": "MorrisXu-Driving",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-08-29T13:46:27Z",
        "body": "the model you are trying to finetune is itself a finetuned model. what you probably want is to download the model with \"no finetuning\", and finetune that instead. or if you want to continue training a model that has already been fine-tuned, then you can just use it as a checkpoint (--restore-file /path/to/model), and dont forget to set the various --reset-x flags as well"
      },
      {
        "user": "zilunpeng",
        "created_at": "2020-10-08T20:58:10Z",
        "body": "Did you manage to solve this problem? I tried to --restore-file and --reset and still get this error....."
      },
      {
        "user": "alexeib",
        "created_at": "2020-10-08T22:51:22Z",
        "body": "actually i guess someone added a --finetune-from-model arg that resets all the relevant things, so maybe you can try using that (if you are finetuning a finetuned model)"
      },
      {
        "user": "tarang-jain",
        "created_at": "2020-10-11T10:11:13Z",
        "body": "Hello @alexeib ! I am trying to fine-tune an already fine-tuned wav2vec 2.0 base model (wav2vec 2.0 base model fine tuned on 100 hours of librispeech). However, I am trying to fine-tune it on a different language and therefore with a different dict.ltr.txt. So the dimensions of the final output layer would of course be different. I want that layer to be replaced with the new dimensions for my new letter targets, but I wish to fine-tune the layers before that by starting from the already fine-tuned model. Can this be done?\r\nAlso, I am facing the same error as above even with the --fine-tune-from-model arg and with the --restore-file arg. \r\nRuntimeError: Error(s) in loading state_dict for Wav2VecCtc:\r\nsize mismatch for w2v_encoder.proj.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\r\nsize mismatch for w2v_encoder.proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\r\n\r\nPlease help me solve these two problems. "
      },
      {
        "user": "alexeib",
        "created_at": "2020-10-11T18:35:26Z",
        "body": "i would suggest finetuning the pretrained (\"no finetuning\") model instead of one that has been already finetuned for english.\r\n\r\nhowever to do what you are trying to do, you can just modify the code in wav2vec2_asr.py to load the weights in non-strict mode - either modify or create an alternative to load_checkpoint_to_cpu where torch.load is called with strict=False flag. for example you can add \"strict\" argument to load_checkpoint_to_cpu defaulted to True and change this:\r\n```\r\n    with PathManager.open(path, \"rb\") as f:\r\n        state = torch.load(\r\n            f, map_location=lambda s, l: default_restore_location(s, \"cpu\"), strict=False\r\n        )\r\n```"
      },
      {
        "user": "spygaurad",
        "created_at": "2020-10-17T10:26:31Z",
        "body": "\r\n> Hello @alexeib ! I am trying to fine-tune an already fine-tuned wav2vec 2.0 base model (wav2vec 2.0 base model fine tuned on 100 hours of librispeech). However, I am trying to fine-tune it on a different language and therefore with a different dict.ltr.txt. So the dimensions of the final output layer would of course be different. I want that layer to be replaced with the new dimensions for my new letter targets, but I wish to fine-tune the layers before that by starting from the already fine-tuned model. Can this be done?\r\n> Also, I am facing the same error as above even with the --fine-tune-from-model arg and with the --restore-file arg.\r\n> RuntimeError: Error(s) in loading state_dict for Wav2VecCtc:\r\n> size mismatch for w2v_encoder.proj.weight: copying a param with shape torch.Size([32, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\r\n> size mismatch for w2v_encoder.proj.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\r\n> \r\n> Please help me solve these two problems.\r\n\r\n@tarang-jain have you solved this issue?"
      }
    ]
  },
  {
    "number": 2538,
    "title": "(wav2vec 2.0)Can you provide detailed hyperparameters for finetune?",
    "created_at": "2020-08-29T11:28:37Z",
    "closed_at": "2020-09-04T02:37:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2538",
    "body": "You guys have done a great job, can you provide detailed hyperparameters for 10h finetune in wav2vec 2.0. I don’t know how to adjust the hyperparameters for 10min, 1h and 10h datasets. Thanks a lot.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2538/comments",
    "author": "zqs01",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-08-31T19:29:14Z",
        "body": "theres a table in the appendix B in the paper that shows the differences between various splits. in general you would just adjust --max-update, and then adjust --warmup-steps, --hold-steps, and --decay steps so that they use 0.1/0.4/0.5 of max-update respectively. you then need to update --mask-prob and --mask-channel-prob. this prob would be mask-length * x where x is the number in the table and mask-length is what you use for --mask-length (10 in the example) or --mask-channel-length.\r\n\r\nso for example, for 10h we see that timestep mask prob should be 0.065, so we set --mask-prob to 0.65. channel mask prob is 0.004, so we set it to 64 * 0.004 = 0.256. then we set --max-updates to 20000 and change --warmup-steps to 20000 * 0.1 = 2000, --hold-steps to 8000 and --decay-steps to 10000.\r\n\r\nyou can adjust the example for other splits following the same procedure.\r\n\r\ndo you think it would be valuable to add examples for every split even though it will make readme much longer?"
      },
      {
        "user": "craigbaker",
        "created_at": "2020-09-01T00:09:52Z",
        "body": "Thank you for the explanation. I was able to figure out the masking parameters by reading the code and appendix B, but not the training schedule. In the readme, I would suggest providing this explanation and just the relevant command line arguments for the 10h example as you have here, with a reference to appendix B as a guide for other dataset sizes."
      },
      {
        "user": "zqs01",
        "created_at": "2020-09-04T02:37:24Z",
        "body": "Thank you @alexeib "
      },
      {
        "user": "Nian-Chen",
        "created_at": "2021-07-04T14:59:03Z",
        "body": "Hi@alexeib\r\nFor 10-min-finetuning experiment：\r\nFollow the wav2vec2.0 paper, the 10min-dataset contains 48 samples.\r\nIs it reasonable for me to set the batch-size to 48? and also what is the learning rate? I have found severe overfitting on this experiment so far.\r\nCan you help me？Thanks a lot!"
      }
    ]
  },
  {
    "number": 2535,
    "title": "how to generate output using generate.py without shuffling?",
    "created_at": "2020-08-29T03:52:49Z",
    "closed_at": "2020-08-29T18:13:28Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2535",
    "body": "@myleott I am trying to use the mBART generative interface to generated output in the Hindi language (from English). I fine-tuning the model with English-Hindi parallel dataset and results are good.\r\n\r\nFor my ongoing work, what I want is: \"can we generate the Hindi output such that the order of sentences should not be change after generation? (i.e., English sentence order and generated Hindi sentence order should be same, there should not be any shuffling)\" How can we achieve the same? Waiting for your response. Thank you!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2535/comments",
    "author": "kaushal0494",
    "comments": [
      {
        "user": "masonreznov",
        "created_at": "2020-08-29T04:35:49Z",
        "body": "You should try `fairseq-interactive`. It generates the output in the same order."
      },
      {
        "user": "kaushal0494",
        "created_at": "2020-08-29T18:13:28Z",
        "body": "Thanks, @masonreznov "
      }
    ]
  },
  {
    "number": 2533,
    "title": "Sharding an already Pre-processed data-bin for training",
    "created_at": "2020-08-28T15:22:22Z",
    "closed_at": "2020-09-07T19:37:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2533",
    "body": "Hi, \r\n\r\nSo I have some CPU and GPU machines, the CPU machines have more RAM. \r\n\r\nI have managed to Preprocess my full training data with mmap indexing and Lazy Loading into 1 data-bin, but when I attempt to do distributed training on the GPU machines I run out of CPU RAM, I believe my only other option is sharding. Must I re-pre-process my data in shards or can I just load in shards? (pre-processing took a week so would prefer if I didn't have to). \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2533/comments",
    "author": "hichiaty",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-28T22:20:11Z",
        "body": "Did you install pyarrow? `pip install pyarrow` might improve the memory situation.\r\n\r\nAlso, how big is the dataset? How much CPU RAM do you have and how many GPUs are sharing that CPU RAM?"
      },
      {
        "user": "hichiaty",
        "created_at": "2020-08-29T01:17:20Z",
        "body": "@myleott Yes I do have pyarrow installed. my dataset is 186GB binarized (after being encoded with Sentencepiece), on my preprocessing machine I have 260GB RAM, no GPUs, On my 4 GPU nodes I have 4 GPUs in each node, with 30GB CPU RAM/Node.  "
      },
      {
        "user": "hichiaty",
        "created_at": "2020-09-07T19:37:59Z",
        "body": "Just sharded the data and re-processed."
      }
    ]
  },
  {
    "number": 2532,
    "title": "weight clipping in RoBERTa",
    "created_at": "2020-08-27T07:58:36Z",
    "closed_at": "2022-04-19T06:21:23Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2532",
    "body": "I loaded the released RoBERTa-large checkpoint, and find the `min(all parameters)` is exactly `-1.0`, and `max(all parameters)` is `1.0`. \r\nDoes RoBERTa train by weight clipping to the range [-1, 1], or there are some post-processings after pretraining?\r\n\r\nI cannot find the related descriptions from the RoBERTa paper and code.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2532/comments",
    "author": "guolinke",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-28T21:47:15Z",
        "body": "Interesting. We didn't clip the weights during pretraining or postprocess the weights. The `roberta.base` model was trained in the exact same way, but weights are in [-1.0449, 1.3037]."
      },
      {
        "user": "guolinke",
        "created_at": "2020-08-29T01:27:34Z",
        "body": "Thanks, @myleott , very interesting. \r\nI try to pre-train the roberta-large with 160G data from scratch. But I find the training is very unstable, the loss often explodes, and I need to decrease the learning rate.\r\nBefore explode, I find some weights become very large. So I check the weight of the released roberta-large, and find its range is [-1, 1]. This indeed confuses me."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T13:04:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:20:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "sh0416",
        "created_at": "2022-06-26T12:33:47Z",
        "body": "Any thought?"
      }
    ]
  },
  {
    "number": 2522,
    "title": "Questions on the date of Wikipedia dumps for pretrained checkpoints (RoBERTa)",
    "created_at": "2020-08-25T01:21:02Z",
    "closed_at": "2022-04-19T06:21:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2522",
    "body": "## ❓ Questions and Help\r\nHi, I'd like to know the date (month, year) of Wikipedia dumps used for the current pretrained checkpoints of RoBERTa-base and large\r\nI am looking for an older version of pretrained checkpoints that were trained on a Wikipedia dump before 2019.\r\nIf available, is there a way to get the older version of pretrained checkpoints (before 2019)?\r\nThanks!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2522/comments",
    "author": "woojeongjin",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T13:04:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:20:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2516,
    "title": "Quantization Noise model convert to torch.qint8",
    "created_at": "2020-08-23T13:14:33Z",
    "closed_at": "2020-11-03T18:35:45Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2516",
    "body": "Hi, I have trained a model using the steps in the Quantization Noise docs (Scalar Quantization) and was just wondering if there is a nice way to convert to a model which actually uses PyTorch torch.qint8 tensors/ops, rather than using fake quantization layers, e.g. equivalent to `torch.quantization.convert` used by PyTorch native quantization-aware training?\r\n\r\nAlso can we assume that the accuracy of a Quantization Noise model in eval model should give the same accuracy as one which has actually been converted to torch.qint8?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2516/comments",
    "author": "david-macleod",
    "comments": [
      {
        "user": "stolam",
        "created_at": "2021-11-04T13:51:11Z",
        "body": "Hi, have you been able to resolve this?"
      }
    ]
  },
  {
    "number": 2514,
    "title": "Wav2Vec 2.0 change dict.ltr.txt ",
    "created_at": "2020-08-23T08:11:09Z",
    "closed_at": "2020-08-25T01:08:41Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2514",
    "body": "#### What is your question?\r\nI am trying to use wav2vec for other languages like french. How can I modify the dict.ltr.txt so that I can accommodate extra characters for Automatic Speech Recognition? \r\n\r\n#### Code\r\nAlso, the dictionary is something like this:\r\n\r\n```\r\n| 94802\r\nE 51860\r\nT 38431\r\nA 33152\r\nO 31495\r\nN 28855\r\nI 28794\r\nH 27187\r\nS 26071\r\nR 23546\r\nD 18289\r\nL 16308\r\nU 12400\r\n...\r\n```\r\nWhat does the second number mean? Is this a word count in the training corpus/data? Is this generated from any script dynamically? Any references would be really helpful. \r\n\r\nThanks in advance!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2514/comments",
    "author": "karanglass34",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-08-25T01:08:41Z",
        "body": "if you are finetuning a pre-trained model on another dataset then yes you can certainly create a new dictionary. i used fairseq's \"preprocess.py\" but you can also create them by hand if you'd like, with one symbol per line. doesnt have to be letters, this also works with e.g. bpe or sentence pieces if you prefer.\r\n\r\nfor preprocess i did something like this\r\n```\r\npython preprocess.py --dataset-impl mmap --trainpref train.txt  --only-source  --thresholdsrc 0\r\n```\r\nand then just deleted the bin and idx files it created\r\n\r\nthe 2nd column is the count of each symbol but its not actually used. you can just put any number youd like there.\r\n"
      },
      {
        "user": "Adportas",
        "created_at": "2020-11-10T21:01:57Z",
        "body": "> if you are finetuning a pre-trained model on another dataset then yes you can certainly create a new dictionary. i used fairseq's \"preprocess.py\" but you can also create them by hand if you'd like, with one symbol per line. doesnt have to be letters, this also works with e.g. bpe or sentence pieces if you prefer.\r\n> \r\n> for preprocess i did something like this\r\n> \r\n> ```\r\n> python preprocess.py --dataset-impl mmap --trainpref train.txt  --only-source  --thresholdsrc 0\r\n> ```\r\n> \r\n> and then just deleted the bin and idx files it created\r\n> \r\n> the 2nd column is the count of each symbol but its not actually used. you can just put any number youd like there.\r\n\r\nIn the first column, the order of the letters is associated with the probability of occurrence or is any one, for example alphabet?"
      }
    ]
  },
  {
    "number": 2501,
    "title": "How can I use Roberta to extract (/regonize) named entities (preferably in czech, but any language would be helpful)?",
    "created_at": "2020-08-20T09:31:02Z",
    "closed_at": "2022-04-19T06:21:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2501",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI am a beginner in python and just learning about fairseq. I want to compare XLMRoberta with some other NER tools but I struggle to find the relevant command for Named Entity recognition of input/txt file. Any advice/tutorial would be much appreciated.\r\nThanks!\r\n#### Code\r\nimport torch\r\n!pip install sentencepiece\r\nxlmr = torch.hub.load('pytorch/fairseq', 'xlmr.large')\r\nxlmr.eval() \r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nSo far I went through the README on XLMRoberta as well as the pdf of the relevant paper, number of ISSUES on github and through Fairseq documentation.\r\n#### What's your environment?\r\nI use Google Colab with\r\n - sentencepiece Version (0.1.91)\r\n - fairseq Version (master I think)\r\n - PyTorch Version 1.6.0+cu101\r\n - OS (e.g., Linux): windows 10\r\n - How you installed fairseq (`pip`, source): !pip install fairseq\r\n - Python version: 3.6.7\r\n - CUDA/cuDNN version:\r\n\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2501/comments",
    "author": "Tomas-Malik",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T13:04:52Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:20:45Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2494,
    "title": "mask-whole-words training tokenize wordpiece",
    "created_at": "2020-08-18T10:14:08Z",
    "closed_at": "2020-08-19T20:04:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2494",
    "body": "Hello, \r\n\r\nI trained a Roberta model with --mask-whole-words. However, the tokenizer seem to do word piece and not whole words. \r\nI loaded the tokenizer and model with the transformers library to fill masks but I want to do this with whole word tokenization. \r\nAny idea why I'm getting word pieces ?\r\n\r\nThank you\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2494/comments",
    "author": "Skylixia",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-19T19:57:46Z",
        "body": "Correct, when training with `--mask-whole-words` it still uses word piece tokenization, but will mask out multiple word pieces when necessary to mask a whole word.\r\n\r\nFor example, suppose your sentence is `\"Testing masking whole words\"`\r\nwhich gets split into word pieces `['Testing', ' mask', 'ing', ' whole', ' words']`.\r\nWhen you train with `--mask-whole-words` it will only replace \"mask\" and \"ing\" together,\r\nso you'll get `['Testing', '<mask>', '<mask>', ' whole', ' words']`"
      }
    ]
  },
  {
    "number": 2489,
    "title": "Size of matrix mismatch error when using pre-trained model(transformer.wmt19.de-en) ",
    "created_at": "2020-08-17T15:44:29Z",
    "closed_at": "2020-08-19T20:04:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2489",
    "body": "Hello \r\n\r\nI'm trying to finetune the provided pretrained model transformer.wmt19.de-en from the paper(Facebook FAIR’s WMT19 News Translation Task Submission).\r\nHowever, I cannot find the correct architecture for this pre-trained model.\r\nAccording to the paper, seems like 'transformer_vaswani_wmt_en_de_big' is used but it doesn't fit with the pre-trained model.\r\nFurthermore, I tried out all sensible other architectures, such as transformer, wmt_en_de_big but it didn't work either.\r\n\r\n\r\nI preprocessed the data usin this command ;\r\n`fairseq-preprocess --source-lang de --target-lang en \\\r\n    --trainpref $TEXT/train   \\\r\n    --validpref $TEXT/valid   \\\r\n    --testpref $TEXT/test     \\\r\n    --destdir data-bin/wmt19.tokenized.de-en \\\r\n    --workers 20  \\\r\n    --joined-dictionary --srcdict ../models/wmt19.de-en.joined-dict.ensemble/dict.de.txt  \\\r\n`\r\n\r\nAnd afterwards finetune like this ;\r\n`fairseq-train \\\r\n    data-bin/wmt19.tokenized.de-en \\\r\n    --restore-file ../models/wmt19.de-en.joined-dict.ensemble/model1.pt \\\r\n    --save-dir checkpoints/finetune_wmt_model1 \\\r\n    --arch transformer_vaswani_wmt_en_de_big  --share-decoder-input-output-embed \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-detok moses \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric   \\\r\n    --max-sentences 100`\r\n\r\nThen it gives me this error ;\r\n\r\n> Traceback (most recent call last):\r\n  File \"/data/s3475743/myver_fairseq/fairseq/fairseq/trainer.py\", line 256, in load_checkpoint\r\n    self.get_model().load_state_dict(\r\n  File \"/data/s3475743/myver_fairseq/fairseq/fairseq/models/fairseq_model.py\", line 93, in load_state_dict\r\n    return super().load_state_dict(new_state_dict, strict)\r\n  File \"/data/s3475743/myver_fairseq/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 846, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for TransformerModel:\r\n\tsize mismatch for encoder.layers.0.fc1.weight: copying a param with shape torch.Size([8192, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n\tsize mismatch for encoder.layers.0.fc1.bias: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n\tsize mismatch for encoder.layers.0.fc2.weight: copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n\tsize mismatch for encoder.layers.1.fc1.weight: copying a param with shape torch.Size([8192, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n\tsize mismatch for encoder.layers.1.fc1.bias: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n\tsize mismatch for encoder.layers.1.fc2.weight: copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n\tsize mismatch for encoder.layers.2.fc1.weight: copying a param with shape torch.Size([8192, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n\tsize mismatch for encoder.layers.2.fc1.bias: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n\tsize mismatch for encoder.layers.2.fc2.weight: copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n\tsize mismatch for encoder.layers.3.fc1.weight: copying a param with shape torch.Size([8192, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n\tsize mismatch for encoder.layers.3.fc1.bias: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n\tsize mismatch for encoder.layers.3.fc2.weight: copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n\tsize mismatch for encoder.layers.4.fc1.weight: copying a param with shape torch.Size([8192, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n\tsize mismatch for encoder.layers.4.fc1.bias: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n\tsize mismatch for encoder.layers.4.fc2.weight: copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n\tsize mismatch for encoder.layers.5.fc1.weight: copying a param with shape torch.Size([8192, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\r\n\tsize mismatch for encoder.layers.5.fc1.bias: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096]).\r\n\tsize mismatch for encoder.layers.5.fc2.weight: copying a param with shape torch.Size([1024, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\r\n\r\n> During handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/data/s3475743/myver_fairseq/venv/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/data/s3475743/myver_fairseq/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/data/s3475743/myver_fairseq/fairseq/fairseq/distributed_utils.py\", line 189, in call_main\r\n    main(args, **kwargs)\r\n  File \"/data/s3475743/myver_fairseq/fairseq/fairseq_cli/train.py\", line 106, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\r\n  File \"/data/s3475743/myver_fairseq/fairseq/fairseq/checkpoint_utils.py\", line 134, in load_checkpoint\r\n    extra_state = trainer.load_checkpoint(\r\n  File \"/data/s3475743/myver_fairseq/fairseq/fairseq/trainer.py\", line 264, in load_checkpoint\r\n    raise Exception(\r\nException: Cannot load model parameters from checkpoint ../models/wmt19.de-en.joined-dict.ensemble/model1.pt; please ensure that the architectures match.\r\n\r\nMaybe the architecture was changed after saving the pretrained model or am I just doing things plain wrong?\r\nI hope you can help me figure out how to load the pretrained models when finetuning because I kind of ran out of ideas whats going wrong.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2489/comments",
    "author": "Sohyo",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-19T20:03:43Z",
        "body": "You can determine the arguments/architecture by loading the model checkpoint and checking the 'args' attribute:\r\n```python\r\n>>> import torch\r\n>>> model = torch.load('wmt19.en-de.joined-dict.ensemble/model1.pt')\r\n>>> model['args']\r\nNamespace(adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_wmt_en_de_big', attention_dropout=0.1, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data=['/private/home/edunov/wmt19/data/old/ende', '/private/home/edunov/wmt19/data/old/ende', '/private/home/edunov/wmt19/data/finetune/nc'], ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:17406', distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.2, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=8192, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, extra_data='', fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source=True, left_pad_target=False, log_format='simple', log_interval=100, lr=[0.0007], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3584, max_update=201800, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, momentum=0.99, no_epoch_checkpoints=False, no_progress_bar=True, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, relu_dropout=0.0, reset_lr_scheduler=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/checkpoint/edunov/20190403/wmt19en2de.btsample5.ffn8192.transformer_wmt_en_de_big_bsz3584_lr0.0007_dr0.2_size_updates200000_seed20_lbsm0.1_size_sa1_upsample2//finetune1', save_interval=1, save_interval_updates=200, seed=2, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='de', task='translation', tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)\r\n```\r\n\r\nWhile it started with `transformer_vaswani_wmt_en_de_big` there are some customizations to other parameter. The main change seems to be `--encoder-ffn-embed-dim=8192`."
      }
    ]
  },
  {
    "number": 2485,
    "title": "How to get '.ltr' file ?",
    "created_at": "2020-08-17T09:20:28Z",
    "closed_at": "2020-08-20T04:21:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2485",
    "body": "python3 train.py /path/ --save-dir /path/model_exportdir1 --fp16 --post-process letter --valid-subset valid --no-epoch-checkpoints --best-checkpoint-metric wer --num-workers 4 --max-update 80000 --sentence-avg --task audio_pretraining --arch wav2vec_ctc --w2v-path /path/wav2vec_small_10m.pt --labels ltr --apply-mask --mask-selection static --mask-other 0 --mask-length 10 --mask-prob 0.5 --layerdrop 0.1 --mask-channel-selection static --mask-channel-other 0 --mask-channel-length 64 --mask-channel-prob 0.5 --zero-infinity --feature-grad-mult 0.0 --freeze-finetune-updates 10000 --validate-after-updates 10000 --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr 2e-05 --lr-scheduler tri_stage --warmup-steps 8000 --hold-steps 32000 --decay-steps 40000 --final-lr-scale 0.05 --final-dropout 0.0 --dropout 0.0 --activation-dropout 0.1 --criterion ctc --attention-dropout 0.0 --max-tokens 1280000 --seed 2337 --log-format json --log-interval 500 --ddp-backend no_c10d\r\n\r\n\r\nRunning this above command and getting below error :-\r\n\r\nFile \"/path/fairseq-master/fairseq/tasks/audio_pretraining.py\", line 110, in load_dataset\r\n    with open(label_path, \"r\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/path/valid.ltr\r\n\r\n\r\nCan anyone suggest what'd this '.ltr' file and how to create it ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2485/comments",
    "author": "MrityunjoyS",
    "comments": [
      {
        "user": "zqs01",
        "created_at": "2020-08-19T14:34:15Z",
        "body": "I also want to solve this question."
      },
      {
        "user": "MrityunjoyS",
        "created_at": "2020-08-19T14:36:57Z",
        "body": "I did one thing, just copying the '.ltr.txt' file created from manifest file to '.ltr' file and was able to finetune using Librispeech model. Although I don't know if it's correct or not"
      },
      {
        "user": "alexeib",
        "created_at": "2020-08-19T21:25:50Z",
        "body": "yeah sorry, you can just rename the files that \"libri_labels.py\" outputs to .wrd and .ltr respectively to use the .ltr as letter targets. i'll update the script when i get a chance"
      },
      {
        "user": "MrityunjoyS",
        "created_at": "2020-08-20T04:21:42Z",
        "body": "Thank you @alexeib "
      }
    ]
  },
  {
    "number": 2481,
    "title": "suggested_ed2_path in libnat",
    "created_at": "2020-08-13T21:13:35Z",
    "closed_at": "2022-04-19T06:21:12Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2481",
    "body": "Congrats on the great work on Levenshtein Transformer. I am really intrigued by the work and trying to understand the paper and the implementation. \r\nCould you help me in understanding what the suggested_ed2_path function is and what does it returns?\r\n\r\nI am looking to understand the description of the learning algorithm in the paper (especially expert policy implementation). Could you please give some heads up in the code to understand the learning algorithm in the appendix of the paper?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2481/comments",
    "author": "rajevv",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-19T20:13:21Z",
        "body": "@MultiPath @kahne "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T13:04:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:20:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "conan1024hao",
        "created_at": "2023-04-11T02:59:19Z",
        "body": "Have you figured it out?"
      }
    ]
  },
  {
    "number": 2474,
    "title": "what is the data_name_or_path parameter for?",
    "created_at": "2020-08-13T01:06:32Z",
    "closed_at": "2020-08-14T19:26:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2474",
    "body": "In the CNN inference code, you set up the bart object with \r\nbart = BARTModel.from_pretrained(\r\n    'checkpoints/',\r\n    checkpoint_file='checkpoint_best.pt',\r\n    data_name_or_path='cnn_dm-bin'\r\n)\r\nwhat is the data_name_or_path parameter for? It is pointing to the cnn_dm-bin directory. I understand that's where the binarized training data is stored. Why would you need them for inference?\r\n\r\nIt matters because it is not clear how to translate this for the XSum case. There is no instruction for creating equivalent binary files for XSum.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2474/comments",
    "author": "bhomass",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-13T17:55:12Z",
        "body": "It's for loading the dictionary files. It's not necessary for that directory to actually contain training data."
      }
    ]
  },
  {
    "number": 2467,
    "title": "what is the score in wav2vec 2.0?",
    "created_at": "2020-08-12T09:57:59Z",
    "closed_at": "2020-08-25T01:18:29Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2467",
    "body": "Hello,\r\n\r\nI was wondering what is the score that the wav2vec 2.0 inference is returning? And if it is possible to use to it get the confidence probability for the predicted sequence?\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2467/comments",
    "author": "natspan",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-08-25T01:18:29Z",
        "body": "the score depends on what kind of inference you are doing. in general it is some combination of emission log prob for that particular timestep, and weighted language model score. in theory you could use relative ranking of scores as a confidence measure (not probability) but i have not tried it\r\n\r\nthis score is computed by the wav2letter decoder"
      }
    ]
  },
  {
    "number": 2465,
    "title": "Virtual data size",
    "created_at": "2020-08-11T20:06:27Z",
    "closed_at": "2022-04-19T06:21:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2465",
    "body": "In the `SampledMultiDataset` code, there is a warning that\r\n\r\n>>If virtual size << real data size, there could be data coverage issue.\r\n\r\nWhat does this mean, and how should the virtual dataset size be chosen?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2465/comments",
    "author": "zphang",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:30Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:20:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2463,
    "title": "Need code to use pretrained model inn wav2vec 2.0",
    "created_at": "2020-08-11T09:35:06Z",
    "closed_at": "2020-08-25T01:20:02Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2463",
    "body": "I'v downloaded  pre-trained  \"Wav2Vec 2.0 Base | No finetuning | Librispeech \"  model. But it's in '.pt' format, and there is no documentation/code of how to run it. Need suggestions regarding that.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2463/comments",
    "author": "MrityunjoyS",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-08-25T01:20:02Z",
        "body": "you cant actually do anything interesting that i know of with a pre-trained model that hasn't been finetuned except use it to fine-tune on some labeled dataset. theres a sample command showing how to do finetuning in the docs"
      }
    ]
  },
  {
    "number": 2453,
    "title": "Output of generate for LevT",
    "created_at": "2020-08-09T11:13:20Z",
    "closed_at": "2020-08-12T23:04:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2453",
    "body": "``T-368\tI think that it should be in the school library because if it is possible, then everyone is happy and adults and children can talk. People want to read “manga” in the library. My friends said to me that “I want to read manga in the library.” Then I thought, “Me too.”\r\nH-368\t-2.274385690689087\tI think that should should in in in school want want school want want to want want\r\nD-368\t-2.274385690689087\tI think that should should in in in school want want school want want to want want\r\nP-368\t0.0000 -0.1273 -0.2680 -0.7848 -0.8135 -1.5424 -1.8979 -2.3356 -3.4206 -3.0096 -3.2159 -3.0636 -3.6891 -3.1828 -3.5199 -4.2248 -3.9852 -4.1323 0.0000``\r\n\r\n\r\nHow to interpret the results of generate as obtained by LevT. What are H, D, P, and the numbers?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2453/comments",
    "author": "rajevv",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-12T23:04:50Z",
        "body": "`H` is the *hypothesis* (i.e., the system output). The first number is the average log probability of the output.\r\n`D` is the hypothesis after detokenization (if `--tokenizer` or `--bpe` are provided)\r\n`P` is the log probability of each token in the hypothesis"
      }
    ]
  },
  {
    "number": 2452,
    "title": "Looking for the best solution to handling CPU memory cost when loading a very large training set",
    "created_at": "2020-08-09T10:56:29Z",
    "closed_at": "2020-08-12T23:06:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2452",
    "body": "Hello,\r\n\r\nI'm trying to train a model with a very large training set and confronting the cpu memory issue (oom). I checked the previous issues and solutions. They are basically the following 3 ways:\r\n\r\n1) lazy load\r\n2) mmap indexed dataset\r\n3) sharding \r\n\r\nI tried lazy load but it looks not so promising to significantly reduce the memory cost; for mmap indexed dataset, it seems that it needs to load all the data first for \"warming up\". The sharing solution looks the most suitable for our case but I can't find the examples or detailed introduction for the feature in fairseq.\r\n\r\nIs my understanding correct? If so, could anyone provide the ways to use the sharding feature in fairseq to facilitate handling large dataset?\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2452/comments",
    "author": "getao",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-12T23:06:24Z",
        "body": "Sharding is the most flexible. You can simply pass a colon-separated list of data-bin paths and each one will be iterated over as a separate \"epoch\".\r\n\r\nSo if you do `fairseq-train data-bin1:data-bin2:data-bin3 (...)` then the first epoch will loop over data-bin1, the second epoch over data-bin2, third epoch over data-bin3, fourth epoch over data-bin1 again, etc."
      }
    ]
  },
  {
    "number": 2450,
    "title": "how to use transformer to decode step by step during training ?",
    "created_at": "2020-08-09T08:05:06Z",
    "closed_at": "2022-04-19T06:21:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2450",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHow do I modify the source code  to allow transformer to decode step by step  during training? Because I want to apply scheduled sampling to Transformer. Thank you!\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2450/comments",
    "author": "haorannlp",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:20:33Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2440,
    "title": "how model from torch.hub.load()  using multi-gpu to translate",
    "created_at": "2020-08-06T13:17:38Z",
    "closed_at": "2022-04-18T06:21:27Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2440",
    "body": "```python\r\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')\r\nen2de.translate('Hello world', beam=5)\r\n# 'Hallo Welt'\r\n```\r\nAs the above code snippet, is there a way to use multi-gpu to translate\r\nenvironment:\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9.0\r\n - PyTorch Version (e.g., 1.0) 1.4\r\n - OS (e.g., Linux): ubuntu 16.04\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 10.2/7603",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2440/comments",
    "author": "HuipengXu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:38Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:57Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2438,
    "title": "How to configure the args of examples/speech_recognition/infer.py to reproduce the results in the wav2vec 2.0 paper?",
    "created_at": "2020-08-06T10:29:39Z",
    "closed_at": "2022-04-18T06:21:26Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2438",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nHow to configure the args of examples/speech_recognition/infer.py to reproduce the results in the wav2vec 2.0 paper?\r\n\r\n#### Code\r\npython examples/speech_recognition/infer.py ./data/librispeech/finetune --task audio_pretraining \\\r\n--nbest 1 --path ./models/wav2vec_small_10m.pt --gen-subset \"dev_other\" --results-path ./decoder_result --w2l-decoder kenlm --lm-model data/librispeech/4-gram.bin --lm-weight 3.23 --word-score -0.26 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 4000000  --post-process letter --lexicon ./libri_lexicon.txt\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI downloaded the model of wav2vec_small_10m.pt and tried to reproduce the results on dev_other of librispeech. \r\nI set --lm-weight 3.23 and --word-score -0.26 according to the Table 7 in the paper.\r\nI got the result WER=22.8056% by the above command. But the WER of dev_other is 15.7 in the wav2vec 2.0 paper.\r\nWhat is wrong with the args? Could you give me a correct args list which can reproduce the results in the paper?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2438/comments",
    "author": "shiyuzh2007",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:39Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:56Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2437,
    "title": "Any way to translate one sentence into more than ones？",
    "created_at": "2020-08-06T07:23:00Z",
    "closed_at": "2022-04-18T06:21:25Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2437",
    "body": "Excuse me.\r\n\r\nI tried \r\n        `en2de.translate(\"A little boy is sitting on a wooden\")`\r\n        # Ein Kleiner Junge sitzt auf dem Holzstuhl.\r\n\r\nIs there any way for me to translate it into more than one sentences?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2437/comments",
    "author": "XChuanLee",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:55Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2435,
    "title": "Anyway to load the pretrained GPT-2 model weight to levenshtein transformer?",
    "created_at": "2020-08-05T22:11:40Z",
    "closed_at": "2020-09-11T21:56:23Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2435",
    "body": "Hi,\r\n\r\nI'm trying to use a small dataset (500 training examples) on levenshtein transformer to do text generation task. However, the loss maintains around 11. I think the dataset is too small, so I'm wondering if there is a way to load the pretrained GPT-2 weights to levenshtein transformer ?  Thanks.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2435/comments",
    "author": "wentinghome",
    "comments": [
      {
        "user": "zodiacR",
        "created_at": "2020-08-06T05:52:10Z",
        "body": "I found a way to load pre-trained BERT models from huggingface API to fairseq transformers by substituting the name of parameters. However, since gpt2 has a different architecture from the standard transformer, my method doesn't work in this scenario. I suggest you use BERT models, or you have to modify the arch of Levenshtein transformer such that you can load gpt-2 for your tasks. "
      },
      {
        "user": "ttzHome",
        "created_at": "2020-09-28T06:31:26Z",
        "body": "> I found a way to load pre-trained BERT models from huggingface API to fairseq transformers by substituting the name of parameters. However, since gpt2 has a different architecture from the standard transformer, my method doesn't work in this scenario. I suggest you use BERT models, or you have to modify the arch of Levenshtein transformer such that you can load gpt-2 for your tasks.\r\n\r\nexcuse me, may I ask how to load the pretrained model in huggingface  to fairseq? I want to use the bert-base-chinese in huggingface and trained it with fairseq?\r\n"
      },
      {
        "user": "zodiacR",
        "created_at": "2020-10-17T04:06:14Z",
        "body": "> > I found a way to load pre-trained BERT models from huggingface API to fairseq transformers by substituting the name of parameters. However, since gpt2 has a different architecture from the standard transformer, my method doesn't work in this scenario. I suggest you use BERT models, or you have to modify the arch of Levenshtein transformer such that you can load gpt-2 for your tasks.\r\n> \r\n> excuse me, may I ask how to load the pretrained model in huggingface to fairseq? I want to use the bert-base-chinese in huggingface and trained it with fairseq?\r\n\r\nHi @ttzHome, here is my solution:\r\n\r\n```\r\ndef __init__(self, args, encoder, decoder):\r\n        super().__init__(encoder, decoder)\r\n        self.args = args\r\n        self.supports_align_args = True\r\n\r\n       pretrained_model = BertModel.from_pretrained(pretrained_model_path)\r\n       \r\n       # encoder\r\n       loaded_state_dict = upgrade_state_dict_with_pretrained_weights(\r\n                state_dict=encoder.state_dict(),\r\n                pretrained_model=pretrained_model,\r\n            )\r\n       encoder.load_state_dict(loaded_state_dict, strict=True)\r\n\r\n      # decoder\r\n      decoder_state_dict = decoder.state_dict()\r\n      for k in decoder_state_dict.keys():\r\n          if k not in loaded_state_dict:\r\n              loaded_state_dict[k] = decoder_state_dict[k]\r\n\r\n       decoder.load_state_dict(loaded_state_dict, strict=True)\r\n\r\ndef upgrade_state_dict_with_pretrained_weights(\r\n    state_dict: Dict[str, Any], pretrained_model: BertModel, \r\n) -> Dict[str, Any]:\r\n\r\n\r\n    pretrained_state_dict = pretrained_model.state_dict()\r\n\r\n    embed_ln = re.compile(\"LayerNorm\\.(weight|bias)\")\r\n    self_attn = re.compile(r\"layer\\.(\\d+)\\.+attention.+((q)uery|(k)ey|(v)alue|(out)put\\.dense|(output)\\.LayerNorm)\\.(weight|bias)\")\r\n    ffns = re.compile(r\"layer\\.(\\d+)\\.(intermediate|output)\\.dense\\.(weight|bias)\")\r\n    ffns_ln_p = re.compile(r\"layer\\.(\\d+)\\.output\\.LayerNorm\\.(weight|bias)\")\r\n        \r\n\r\n    for key in pretrained_state_dict.keys():\r\n        # print(key)\r\n        if \"embeddings\" in key:\r\n            if \"word\" in key:\r\n                new_key = \"embed_tokens.weight\"\r\n            elif \"position\" in key:\r\n                new_key = 'embed_positions.weight'\r\n            elif \"type\" in key:\r\n                new_key = None\r\n            else:\r\n                fs_embed_ln = \"layernorm_embedding.{}\"\r\n                new_key = fs_embed_ln.format(embed_ln.search(key).group(1))\r\n        elif \"attention\" in key:\r\n            # print(k)\r\n            groups = self_attn.search(key).groups()\r\n            fs_self_attn = \"layers.{}.self_attn.{}_proj.{}\"\r\n            fs_self_attn_ln = \"layers.{}.self_attn_layer_norm.{}\"\r\n            if \"query\" in key:\r\n                # print(fs_self_attn.format(groups[0], groups[2], groups[-1]))\r\n                new_key = fs_self_attn.format(groups[0], groups[2], groups[-1])\r\n            elif \"key\" in key:\r\n                # print(fs_self_attn.format(groups[0], groups[3], groups[-1]))\r\n                new_key = fs_self_attn.format(groups[0], groups[3], groups[-1])\r\n            elif \"value\" in key:\r\n                # print(fs_self_attn.format(groups[0], groups[4], groups[-1]))\r\n                new_key = fs_self_attn.format(groups[0], groups[4], groups[-1])\r\n            elif \"output.dense\" in key:\r\n                # print(fs_self_attn.format(groups[0], groups[5], groups[-1]))\r\n                new_key = fs_self_attn.format(groups[0], groups[5], groups[-1])\r\n            else:\r\n                # print(fs_self_attn_ln.format(groups[0], groups[-1]))\r\n                new_key = fs_self_attn_ln.format(groups[0], groups[-1])\r\n        elif \"dense\" in key and \"pooler\" not in key:\r\n            groups = ffns.search(key).groups()\r\n            # print(groups)\r\n            ffns_f1 = \"layers.{}.fc1.{}\"\r\n            ffns_f2 = \"layers.{}.fc2.{}\"\r\n            if \"intermediate\" in key:\r\n                # print(ffns_f1.format(groups[0], groups[-1]))\r\n                new_key = ffns_f1.format(groups[0], groups[-1])\r\n            else:\r\n                # print(ffns_f2.format(groups[0], groups[-1]))\r\n                new_key = ffns_f2.format(groups[0], groups[-1])\r\n        elif \"LayerNorm\" in key:\r\n            # print(key)\r\n            groups = ffns_ln_p.search(key).groups()\r\n            ffns_ln = \"layers.{}.final_layer_norm.{}\"\r\n            # print(groups)\r\n            # print(ffns_ln.format(groups[0], groups[-1]))\r\n            new_key = ffns_ln.format(groups[0], groups[-1])\r\n\r\n        else:\r\n            new_key = None\r\n        \r\n        if new_key is not None:\r\n            # print(model[key].shape)\r\n            # print(new_key, key)\r\n            # print(state_dict[new_key].shape, pretrained_state_dict[key].shape)\r\n            assert new_key in state_dict, (\r\n                \"{} Transformer encoder / decoder \"\r\n                \"state_dict does not contain {}. Cannot \"\r\n                \"load {} from pretrained XLM checkpoint \"\r\n                \"{} into Transformer.\".format(\r\n                    str(state_dict.keys()),\r\n                    new_key, key, pretrained_state_dict)\r\n                )\r\n\r\n            if new_key == \"embed_tokens.weight\":\r\n                size = pretrained_state_dict[key].size(0)\r\n                state_dict[new_key][:size] = pretrained_state_dict[key]\r\n            else:\r\n                assert state_dict[new_key].shape == pretrained_state_dict[key].shape\r\n                state_dict[new_key] = pretrained_state_dict[key]\r\n\r\n    return state_dict\r\n```"
      },
      {
        "user": "ttzHome",
        "created_at": "2020-10-20T02:11:08Z",
        "body": "> > > I found a way to load pre-trained BERT models from huggingface API to fairseq transformers by substituting the name of parameters. However, since gpt2 has a different architecture from the standard transformer, my method doesn't work in this scenario. I suggest you use BERT models, or you have to modify the arch of Levenshtein transformer such that you can load gpt-2 for your tasks.\r\n> > \r\n> > \r\n> > excuse me, may I ask how to load the pretrained model in huggingface to fairseq? I want to use the bert-base-chinese in huggingface and trained it with fairseq?\r\n> \r\n> Hi @ttzHome, here is my solution:\r\n> \r\n> ```\r\n> def __init__(self, args, encoder, decoder):\r\n>         super().__init__(encoder, decoder)\r\n>         self.args = args\r\n>         self.supports_align_args = True\r\n> \r\n>        pretrained_model = BertModel.from_pretrained(pretrained_model_path)\r\n>        \r\n>        # encoder\r\n>        loaded_state_dict = upgrade_state_dict_with_pretrained_weights(\r\n>                 state_dict=encoder.state_dict(),\r\n>                 pretrained_model=pretrained_model,\r\n>             )\r\n>        encoder.load_state_dict(loaded_state_dict, strict=True)\r\n> \r\n>       # decoder\r\n>       decoder_state_dict = decoder.state_dict()\r\n>       for k in decoder_state_dict.keys():\r\n>           if k not in loaded_state_dict:\r\n>               loaded_state_dict[k] = decoder_state_dict[k]\r\n> \r\n>        decoder.load_state_dict(loaded_state_dict, strict=True)\r\n> \r\n> def upgrade_state_dict_with_pretrained_weights(\r\n>     state_dict: Dict[str, Any], pretrained_model: BertModel, \r\n> ) -> Dict[str, Any]:\r\n> \r\n> \r\n>     pretrained_state_dict = pretrained_model.state_dict()\r\n> \r\n>     embed_ln = re.compile(\"LayerNorm\\.(weight|bias)\")\r\n>     self_attn = re.compile(r\"layer\\.(\\d+)\\.+attention.+((q)uery|(k)ey|(v)alue|(out)put\\.dense|(output)\\.LayerNorm)\\.(weight|bias)\")\r\n>     ffns = re.compile(r\"layer\\.(\\d+)\\.(intermediate|output)\\.dense\\.(weight|bias)\")\r\n>     ffns_ln_p = re.compile(r\"layer\\.(\\d+)\\.output\\.LayerNorm\\.(weight|bias)\")\r\n>         \r\n> \r\n>     for key in pretrained_state_dict.keys():\r\n>         # print(key)\r\n>         if \"embeddings\" in key:\r\n>             if \"word\" in key:\r\n>                 new_key = \"embed_tokens.weight\"\r\n>             elif \"position\" in key:\r\n>                 new_key = 'embed_positions.weight'\r\n>             elif \"type\" in key:\r\n>                 new_key = None\r\n>             else:\r\n>                 fs_embed_ln = \"layernorm_embedding.{}\"\r\n>                 new_key = fs_embed_ln.format(embed_ln.search(key).group(1))\r\n>         elif \"attention\" in key:\r\n>             # print(k)\r\n>             groups = self_attn.search(key).groups()\r\n>             fs_self_attn = \"layers.{}.self_attn.{}_proj.{}\"\r\n>             fs_self_attn_ln = \"layers.{}.self_attn_layer_norm.{}\"\r\n>             if \"query\" in key:\r\n>                 # print(fs_self_attn.format(groups[0], groups[2], groups[-1]))\r\n>                 new_key = fs_self_attn.format(groups[0], groups[2], groups[-1])\r\n>             elif \"key\" in key:\r\n>                 # print(fs_self_attn.format(groups[0], groups[3], groups[-1]))\r\n>                 new_key = fs_self_attn.format(groups[0], groups[3], groups[-1])\r\n>             elif \"value\" in key:\r\n>                 # print(fs_self_attn.format(groups[0], groups[4], groups[-1]))\r\n>                 new_key = fs_self_attn.format(groups[0], groups[4], groups[-1])\r\n>             elif \"output.dense\" in key:\r\n>                 # print(fs_self_attn.format(groups[0], groups[5], groups[-1]))\r\n>                 new_key = fs_self_attn.format(groups[0], groups[5], groups[-1])\r\n>             else:\r\n>                 # print(fs_self_attn_ln.format(groups[0], groups[-1]))\r\n>                 new_key = fs_self_attn_ln.format(groups[0], groups[-1])\r\n>         elif \"dense\" in key and \"pooler\" not in key:\r\n>             groups = ffns.search(key).groups()\r\n>             # print(groups)\r\n>             ffns_f1 = \"layers.{}.fc1.{}\"\r\n>             ffns_f2 = \"layers.{}.fc2.{}\"\r\n>             if \"intermediate\" in key:\r\n>                 # print(ffns_f1.format(groups[0], groups[-1]))\r\n>                 new_key = ffns_f1.format(groups[0], groups[-1])\r\n>             else:\r\n>                 # print(ffns_f2.format(groups[0], groups[-1]))\r\n>                 new_key = ffns_f2.format(groups[0], groups[-1])\r\n>         elif \"LayerNorm\" in key:\r\n>             # print(key)\r\n>             groups = ffns_ln_p.search(key).groups()\r\n>             ffns_ln = \"layers.{}.final_layer_norm.{}\"\r\n>             # print(groups)\r\n>             # print(ffns_ln.format(groups[0], groups[-1]))\r\n>             new_key = ffns_ln.format(groups[0], groups[-1])\r\n> \r\n>         else:\r\n>             new_key = None\r\n>         \r\n>         if new_key is not None:\r\n>             # print(model[key].shape)\r\n>             # print(new_key, key)\r\n>             # print(state_dict[new_key].shape, pretrained_state_dict[key].shape)\r\n>             assert new_key in state_dict, (\r\n>                 \"{} Transformer encoder / decoder \"\r\n>                 \"state_dict does not contain {}. Cannot \"\r\n>                 \"load {} from pretrained XLM checkpoint \"\r\n>                 \"{} into Transformer.\".format(\r\n>                     str(state_dict.keys()),\r\n>                     new_key, key, pretrained_state_dict)\r\n>                 )\r\n> \r\n>             if new_key == \"embed_tokens.weight\":\r\n>                 size = pretrained_state_dict[key].size(0)\r\n>                 state_dict[new_key][:size] = pretrained_state_dict[key]\r\n>             else:\r\n>                 assert state_dict[new_key].shape == pretrained_state_dict[key].shape\r\n>                 state_dict[new_key] = pretrained_state_dict[key]\r\n> \r\n>     return state_dict\r\n> ```\r\n\r\nThanks! Let me try......"
      }
    ]
  },
  {
    "number": 2433,
    "title": "fine-tuning on ConvAI dataset",
    "created_at": "2020-08-05T11:56:04Z",
    "closed_at": "2022-04-18T06:21:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2433",
    "body": "I am trying to fine-tune Bart to train a chatbot. I am not able to find the right resource either here or on huggingface. Could you please help me find one? \r\n\r\nThe paper talks about fine-tuning on ConvAI dataset which I am not able to find in the documentation. \r\n\r\nThanks in Advance!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2433/comments",
    "author": "karthikgrama",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:41Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2424,
    "title": "WordPerSecond (wps) tends to increase over time when using fp16",
    "created_at": "2020-08-04T14:40:53Z",
    "closed_at": "2022-04-18T06:21:22Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2424",
    "body": "I am running a training for an MT engine using the transformer architecture using both ``--fp16`` and  ``-- memory_efficient_fp16``\r\n\r\nI noticed in the log that the wps continues to increase from the starting value of 9357 up to 91416 after 11380 updates.\r\n```\r\n| epoch 001:    100 / 12603 loss=14.481, nll_loss=14.336, ppl=20679.11, wps=9357, ups=0, wpb=82837.531, bsz=4065.306, num_updates=98, lr=1.23476e-05, gnorm=2.761, clip=0.000, oom=0.000, loss_scale=16.000, wall=868, train_wall=101\r\n| epoch 001:    200 / 12603 loss=13.597, nll_loss=13.348, ppl=10423.42, wps=17272, ups=0, wpb=82946.934, bsz=4023.030, num_updates=198, lr=2.48451e-05, gnorm=1.896, clip=0.000, oom=0.000, loss_scale=16.000, wall=951, train_wall=182\r\n| epoch 001:    300 / 12603 loss=13.048, nll_loss=12.715, ppl=6722.88, wps=23921, ups=0, wpb=83024.534, bsz=4012.050, num_updates=298, lr=3.73426e-05, gnorm=1.604, clip=0.000, oom=0.000, loss_scale=16.000, wall=1034, train_wall=263\r\n| epoch 001:    400 / 12603 loss=12.685, nll_loss=12.292, ppl=5014.46, wps=29604, ups=0, wpb=83152.887, bsz=4041.078, num_updates=398, lr=4.98401e-05, gnorm=1.519, clip=0.000, oom=0.000, loss_scale=16.000, wall=1118, train_wall=344\r\n| epoch 001:    500 / 12603 loss=12.371, nll_loss=11.926, ppl=3892.33, wps=34456, ups=0, wpb=83130.333, bsz=4054.452, num_updates=498, lr=6.23376e-05, gnorm=1.496, clip=0.000, oom=0.000, loss_scale=16.000, wall=1202, train_wall=424\r\n.....\r\n| epoch 001:  10900 / 12603 loss=5.348, nll_loss=3.858, ppl=14.50, wps=91059, ups=1, wpb=83266.804, bsz=4034.729, num_updates=10880, lr=0.00030317, gnorm=0.626, clip=0.000, oom=0.000, loss_scale=8.000, wall=9949, train_wall=8839\r\n| epoch 001:  11000 / 12603 loss=5.335, nll_loss=3.844, ppl=14.36, wps=91133, ups=1, wpb=83264.020, bsz=4034.256, num_updates=10980, lr=0.000301786, gnorm=0.625, clip=0.000, oom=0.000, loss_scale=8.000, wall=10032, train_wall=8919\r\n| epoch 001:  11100 / 12603 loss=5.323, nll_loss=3.830, ppl=14.22, wps=91207, ups=1, wpb=83264.594, bsz=4033.216, num_updates=11080, lr=0.000300421, gnorm=0.623, clip=0.000, oom=0.000, loss_scale=8.000, wall=10115, train_wall=8999\r\n| epoch 001:  11200 / 12603 loss=5.311, nll_loss=3.817, ppl=14.09, wps=91276, ups=1, wpb=83263.373, bsz=4032.870, num_updates=11180, lr=0.000299074, gnorm=0.621, clip=0.000, oom=0.000, loss_scale=8.000, wall=10199, train_wall=9080\r\n| epoch 001:  11300 / 12603 loss=5.299, nll_loss=3.803, ppl=13.96, wps=91343, ups=1, wpb=83261.330, bsz=4032.307, num_updates=11280, lr=0.000297746, gnorm=0.619, clip=0.000, oom=0.000, loss_scale=8.000, wall=10282, train_wall=9161\r\n| epoch 001:  11400 / 12603 loss=5.287, nll_loss=3.790, ppl=13.83, wps=91416, ups=1, wpb=83262.638, bsz=4031.766, num_updates=11380, lr=0.000296435, gnorm=0.618, clip=0.000, oom=0.000, loss_scale=16.000, wall=10365, train_wall=9241\r\ncheckpoint_1_10000.pt\r\n```\r\n\r\nInstead, without fp16 support the wps is almost constant.\r\n\r\nI would like to know which is the reason for that.\r\nI suspect that with the fp16 the computation of the wps takes into account also the time for loading the models, whereas this does not happen without fp16. If this true, I suggest you to fix it.\r\n\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.7.1\r\n - torch Version (e.g., 1.0)   1.3.0\r\n - OS (e.g., Linux): Ubuntu 18.04.3 LTS\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: GeForce RTX 2080 Ti\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2424/comments",
    "author": "nicolabertoldi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2423,
    "title": "Any other use for the --source/target-lang flags other than finding files?",
    "created_at": "2020-08-04T12:37:34Z",
    "closed_at": "2022-04-18T06:21:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2423",
    "body": "This ties into an earlier issue I still have (fairseq-train Appears to Stall/Stop Just Before Training Starts #2405). I've trained loads of models on my English-Irish dataset yet now it just sits there, not training anything on my slightly modified dataset. My dataset remains the same as the one I normally use - except - I took out 'initial mutations' from my Irish (target) sentences (i.e. the words were stripped of a few characters). That's the only change I made but it seems to be stopping my training.\r\n\r\nI was thinking, do the `--source-lang` & `--target-lang` flags of `fairseq-preprocess` have any use beyond just finding the files? Like, does it somehow tie into how the data is processed? Beyond that I have no idea as to why my training has just completely collapsed.\r\n\r\nJust to note (in case someone has a solution here from my previous issue I mentioned), my English (source) corpus has `BPE` while my Irish (target) corpus does not, it's just plain text.\r\n\r\nThanks\r\n\r\n - fairseq Version: 0.9.0\r\n - PyTorch Version: 1.5.0\r\n - OS: Linux (on a Cluster server)\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: 9.2.148\r\n - GPU models and configuration: NVIDIA Tesla V100 16GB PCIe (Didn't configure, as it's a shared server)",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2423/comments",
    "author": "JustCunn",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2422,
    "title": "Namespace' object has no attribute 'load_lm'",
    "created_at": "2020-08-04T08:56:50Z",
    "closed_at": "2022-04-18T06:21:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2422",
    "body": "`Traceback (most recent call last): File \"train.py\", line 431, in <module> main(args) File \"train.py\", line 77, in main if args.load_lm: AttributeError: 'Namespace' object has no attribute 'load_lm'`\r\n\r\nI want to train a new language model. The script is as follows\r\n\r\n`python train.py $DATA  --task language_modeling --arch transformer_lm \\\r\n  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n  --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \\\r\n  --lr 0.0005 --min-lr 1e-09 \\\r\n  --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n  --max-tokens 4096  --tokens-per-sample 4096  --save-dir $SAVE --update-freq 16`",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2422/comments",
    "author": "genbei",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:45Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:50Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2419,
    "title": "GOTO arch/hyperparam for production",
    "created_at": "2020-08-04T07:28:45Z",
    "closed_at": "2022-04-18T06:21:19Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2419",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI want to use FairSeq for production. Currently I use the default WMT de-en in the translation example repository, but it is an old arch/hyperparams. What are the current goto arch/hyperparams I should use? I care about speed and performance. For compute and data I have access to the A2 machines and about 100M DE-EN sentences.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2419/comments",
    "author": "alrojo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:46Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2417,
    "title": "Roberta CUDA out of memory + very slow training time",
    "created_at": "2020-08-03T13:10:35Z",
    "closed_at": "2022-04-19T12:21:28Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2417",
    "body": "## ❓ Questions and Help\r\n\r\nI am training a Roberta model with the following command:\r\n\r\n```\r\nTOTAL_UPDATES=125000    # Total number of training steps\r\nWARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\r\nPEAK_LR=0.0005          # Peak learning rate, adjust as needed\r\nTOKENS_PER_SAMPLE=512   # Max sequence length\r\nMAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\r\nMAX_SENTENCES=8        # Number of sequences per batch (batch size)\r\nUPDATE_FREQ=32          # Increase the batch size 16x\r\n\r\nDATA_DIR=/data-bin/data\r\n\r\nfairseq-train $DATA_DIR \\ #FP32: f16 not supported\r\n    --task masked_lm --criterion masked_lm \\\r\n    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\r\n    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\r\n    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 \\\r\n    --mask-whole-words \r\n```\r\n\r\nI have the following CUDA 11.0 environment: \r\n2020-08-03 12:33:22 | INFO | fairseq.utils | rank   0: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               \r\n2020-08-03 12:33:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\r\n2020-08-03 12:33:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\r\n\r\nI am training on just 1.9GB data\r\nIn 30 min, I only reached epoch 001:   37 / 3729 \r\nThen after some time I have a CUDA out of memory error:\r\n\r\n2020-08-03 13:07:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 11.17 GiB total capacity; 8.26 GiB already allocated; 64.44 MiB free; 10.74 GiB reserved in total by PyTorch)\r\n\r\nWhat can I do to avoid out of memory issues and complete the training in a reasonable time frame (I would like it to not exceed 1 week) ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2417/comments",
    "author": "Skylixia",
    "comments": [
      {
        "user": "spacewalk01",
        "created_at": "2020-08-04T16:03:20Z",
        "body": "same problem: \r\n\r\n2020-08-05 01:00:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 10.91 GiB total capacity; 3.91 GiB already allocated; 181.50 MiB free; 3.95 GiB reserved in total by PyTorch)"
      },
      {
        "user": "monologue1107",
        "created_at": "2020-11-05T05:45:03Z",
        "body": "the same problem"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:58Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2416,
    "title": "Fairseq Fconv_self_attention implementation",
    "created_at": "2020-08-03T12:41:16Z",
    "closed_at": "2022-04-18T06:21:18Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2416",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n\r\nCan someone point me to the paper from where fconv_self_attention is implemented?\r\n\r\n\r\n\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2416/comments",
    "author": "sukannyapurkayastha",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2415,
    "title": "Minimum GPU size for training Roberta",
    "created_at": "2020-08-03T10:47:24Z",
    "closed_at": "2022-04-19T12:21:25Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2415",
    "body": "I am trying to fine-tune the pre-trained Roberta from fairseq for commonsenseQA data. I am running it on CPU (with 20 cores) but it took almost 12 hours to train the model for one step. If its default training step is 3000 (num of training epochs), then it will take 1500 days to complete the training. So, I decided to use GPU instead but I only have a GPU with 4gb memory and I got out of memory error when I use cuda. My question is what is the recommended(minimum) GPU size for fine-tuning(training) Roberta? And what GPU device would you recommend? Thank you in advance.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2415/comments",
    "author": "spacewalk01",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:34Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:55Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2412,
    "title": "Unable to retrieve the RoBERTa model for RACE dataset",
    "created_at": "2020-08-03T00:10:08Z",
    "closed_at": "2022-04-18T06:21:17Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2412",
    "body": "Hello,\r\nThis may be a dumb question to ask, but when I execute the following code to get the RoBERTa model,\r\n\r\n```python\r\nimport json\r\nimport torch\r\nfrom fairseq.models.roberta import RobertaModel\r\nfrom examples.roberta import commonsense_qa\r\nroberta = RobertaModel.from_pretrained('checkpoints', 'checkpoint_best.pt', 'data/CommonsenseQA')\r\n```\r\nThe following error occurs:\r\n```\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n```\r\n\r\nHow can I fix this error? Thank you,",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2412/comments",
    "author": "h56cho",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:47Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2411,
    "title": "Is fairseq supported on windows?",
    "created_at": "2020-08-02T19:20:17Z",
    "closed_at": "2020-08-07T18:58:46Z",
    "labels": [
      "question",
      "windows"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2411",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nIs fairseq is supported on Windows 10 now? I would like to use it for machine translations and speech recognition on windows machines.  Please help me out. \r\n\r\nThanks\r\nNagaraju\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2411/comments",
    "author": "nag0811",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-07T18:58:46Z",
        "body": "I don't have access to a Windows machine to test with, so am not sure. The github unit tests show some failure on Windows, but I think that's just something related to installation. Please feel free to try it out and let us know how it goes!"
      },
      {
        "user": "detrin",
        "created_at": "2022-01-10T14:54:06Z",
        "body": "Wheels for Windows would be still much appreciated"
      }
    ]
  },
  {
    "number": 2406,
    "title": "How to process HellaSwag dataset with RoBERTa?",
    "created_at": "2020-08-01T22:58:06Z",
    "closed_at": "2022-04-18T06:21:15Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2406",
    "body": "Hello,\r\n`HellaSwag` is a common sense reasoning challenge that is in a multiple-choice format with 4 options.\r\nHow can I process this `HellaSwag` dataset with RoBERTa (I do not want to use the HuggingFace RoBERTa for multiple choice question processing)? Could someone give me an example of how to make the RoBERTa model to predict the answers for `HellaSwag` questions?\r\n\r\nIn particular, I have pre-processed and encoded each `HellaSwag` question in the following format:\r\n```python\r\nencoded_question =  \r\n[\r\n  [encoding for question statement + 1st option + end-token + padding], \r\n  [encoding for question statement + 2nd option + end-token + padding],\r\n  [encoding for question statement + 3rd option + end-token + padding],\r\n  [encoding for question statement + 4th option + end-token + padding]\r\n]\r\n```\r\nnote that the list `encoded_question` is a list of 4 sub-lists, where each sub-list pertains to the question statement and a single multiple-choice option.  For HuggingFace `RobertaForMultipleChoice` model, I would simply do the following to generate a prediction:\r\n\r\n```python\r\n# get the classification scores for each multiple-choice option\r\nclass_scores = myRobertaModel(input_ids = encoded_question)[0]\r\n```\r\n\r\nBut I am not sure how I can do the same with your regular RoBERTa model. \r\n\r\nSo my questions are:\r\n- Did I encode the `HellaSwag` questions in a correct way? is this encoding the right format for your RoBERTa Model? (If no, what is the right format of encoding for this)?\r\n\r\n- Once the encoding is done, how can I use your RoBERTa model to compute classification scores (or predictions)?\r\n\r\nThank you,",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2406/comments",
    "author": "h56cho",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:45Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2405,
    "title": "fairseq-train Appears to Stall/Stop Just Before Training Starts",
    "created_at": "2020-08-01T21:09:42Z",
    "closed_at": "2022-04-19T11:21:27Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2405",
    "body": "#### What is your question?\r\nI am trying to train an NMT model using the `fairseq-train` command-line tool. I've done this successfully many, many times but now it just seems to sit there not doing anything at the point just before it starts training (I'm pasting the entire thing, just so no context is lost):\r\n```\r\nNamespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin-0108_bpe2text/AistNMT', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='en', srcdict=None, target_lang='ga', task='translation', tensorboard_logdir='', testpref='full_corp_mutations_bpe0108/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='full_corp_mutations_bpe0108/train', user_dir=None, validpref='full_corp_mutations_bpe0108/dev', workers=1)\r\n| [en] Dictionary: 8783 types\r\n| [en] full_corp_mutations_bpe0108/train.en: 825193 sents, 30367862 tokens, 0.0% replaced by <unk>\r\n| [en] Dictionary: 8783 types\r\n| [en] full_corp_mutations_bpe0108/dev.en: 4051 sents, 93109 tokens, 0.0% replaced by <unk>\r\n| [en] Dictionary: 8783 types\r\n| [en] full_corp_mutations_bpe0108/test.en: 4012 sents, 92706 tokens, 0.0% replaced by <unk>\r\n| [ga] Dictionary: 258847 types\r\n| [ga] full_corp_mutations_bpe0108/train.ga: 825193 sents, 26534892 tokens, 0.0% replaced by <unk>\r\n| [ga] Dictionary: 258847 types\r\n| [ga] full_corp_mutations_bpe0108/dev.ga: 4051 sents, 86466 tokens, 0.64% replaced by <unk>\r\n| [ga] Dictionary: 258847 types\r\n| [ga] full_corp_mutations_bpe0108/test.ga: 4012 sents, 85868 tokens, 0.601% replaced by <unk>\r\n| Wrote preprocessed data to data-bin-0108_bpe2text/AistNMT\r\n| distributed init (rank 0): tcp://localhost:19372\r\n| distributed init (rank 1): tcp://localhost:19372\r\n| initialized host n359 as rank 1\r\n| initialized host n359 as rank 0\r\nNamespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.1, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin-0108_bpe2text/AistNMT', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19372', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/AistTran_demuted_0108_bpe2text_enga', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0)\r\n| [en] dictionary: 8784 types\r\n| [ga] dictionary: 258848 types\r\n| loaded 4051 examples from: data-bin-0108_bpe2text/AistNMT/valid.en-ga.en\r\n| loaded 4051 examples from: data-bin-0108_bpe2text/AistNMT/valid.en-ga.ga\r\n| data-bin-0108_bpe2text/AistNMT valid en-ga 4051 examples\r\nTransformerModel(\r\n  (encoder): TransformerEncoder(\r\n    (embed_tokens): Embedding(8784, 512, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n  )\r\n  (decoder): TransformerDecoder(\r\n    (embed_tokens): Embedding(258848, 512, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\r\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n  )\r\n)\r\n| model transformer, criterion CrossEntropyCriterion\r\n| num. model params: 313696256 (num. trained: 313696256)\r\n| training on 2 GPUs\r\n| max tokens per GPU = 4096 and max sentences per GPU = None\r\n| no existing checkpoint found checkpoints/AistTran_demuted_0108_bpe2text_enga/checkpoint_last.pt\r\n| loading train data for epoch 0\r\n| loaded 825193 examples from: data-bin-0108_bpe2text/AistNMT/train.en-ga.en\r\n| loaded 825193 examples from: data-bin-0108_bpe2text/AistNMT/train.en-ga.ga\r\n| data-bin-0108_bpe2text/AistNMT train en-ga 825193 examples\r\n```\r\n\r\nI've encountered this issue before but the problem was I was running it on a CPU (to test if I'd set things up correctly). All my files seem to be in their correct places, and a checkpoint folder is even made, but no checkpoints added for an entire perios of 4 hours. I often have at least 20 checkpoints using the same corpus* during that time.\r\n*My usual corpus has BPE applied to both languages, but because of a test I'm doing, my target does not have BPE this time.\r\n\r\nI haven't tried much because I am truly confused as to what is going on. Like mentioned, the only difference is the BPE thing, but nothing else.\r\n\r\nHere's the script I use to train:\r\n```\r\n#!/bin/bash\r\n\r\nTEXT='full_corp_mutations_bpe0108'\r\nfairseq-train data-bin-0108_bpe2text/AistNMT \\\r\n\t--lr 5e-4 \\\r\n\t--lr-scheduler inverse_sqrt \\\r\n\t--optimizer adam\\\r\n  \t--clip-norm 0.1 \\\r\n\t--dropout 0.3 \\\r\n\t--max-tokens 4096 \\\r\n  \t--arch transformer \\\r\n\t--fp16 \\\r\n\t--save-dir checkpoints/AistTran_demuted_0108_bpe2text_enga\r\n```\r\n\r\n - fairseq Version: 0.9.0\r\n - PyTorch Version: 1.5.0\r\n - OS: Linux (on a Cluster server)\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: 9.2.148\r\n - GPU models and configuration: NVIDIA Tesla V100 16GB PCIe (Didn't configure, as it's a shared server)\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2405/comments",
    "author": "JustCunn",
    "comments": [
      {
        "user": "nguyenvulebinh",
        "created_at": "2020-08-03T00:17:49Z",
        "body": "I got the same problem as you. Did you handle this issue?. It seems to work fine in v0.7.2\r\nAnother clue is this it works fine on multiply gpus 2080ti but not V100."
      },
      {
        "user": "JustCunn",
        "created_at": "2020-08-03T13:51:15Z",
        "body": "No, unfortunately I haven’t solved it. It’s weird since I’ve had no problems up until recently but I’m continuing to look into it and find a solution. Thanks also for telling me about v0.7.2. \r\nI’ll post if I come across anything new."
      },
      {
        "user": "babangain",
        "created_at": "2020-09-17T05:02:22Z",
        "body": "Got any solution? @JustCunn "
      },
      {
        "user": "JustCunn",
        "created_at": "2020-10-06T22:18:51Z",
        "body": "@babangain Apologies, I've not checked in on this issue for a while. No, no solution yet but I'm coming back to it now and hope to find some solution. If I do, I'll make sure to let you know"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:35Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2404,
    "title": " overflow detected, setting loss scale to: 1024.0 but model is sitll being trained.",
    "created_at": "2020-08-01T10:46:57Z",
    "closed_at": "2020-08-07T18:57:31Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2404",
    "body": "Hi,\r\nWhen I train RoBERTa, I have many the warnings like, \r\n2020-08-01 12:39:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0\r\n2020-08-01 12:39:39 | INFO | train_inner | epoch 001:   3615 / 6547 loss=0.439, nll_loss=0.001, accuracy=87.9, wps=82565, ups=2.66, wpb=31075.5, bsz=64, num_updates=3600, lr=9e-06, gnorm=3.952, loss_scale=1399, train_wall=38, wall=1353\r\n\r\nIs it fine?\r\nMy command is \r\nWARMUP_UPDATES=4000      # 6 percent of the number of updates\r\nLR=1e-05                # Peak LR for polynomial LR scheduler.\r\nHEAD_NAME=fariseq_head_pre     # Custom name for the classification head.\r\nNUM_CLASSES=2           # Number of classes for the classification task.\r\nMAX_SENTENCES=8         # Batch size.\r\nROBERTA_PATH=fairseq/roberta.base/model.pt\r\nSAVE_DIR=roberta_class_base_pre\r\nfairseq-train fairseq/fairseq_data-bin \\\r\n    --restore-file $ROBERTA_PATH \\\r\n    --max-positions 512 \\\r\n    --max-sentences $MAX_SENTENCES \\\r\n    --max-tokens 4400 \\\r\n    --task sentence_prediction \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --init-token 0 --separator-token 2 \\\r\n    --arch roberta_base \\\r\n    --criterion sentence_prediction \\\r\n    --classification-head-name $HEAD_NAME \\\r\n    --num-classes $NUM_CLASSES \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\r\n    --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --warmup-updates $WARMUP_UPDATES   \\\r\n    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\r\n    --max-epoch 40 \\\r\n    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\r\n    --shorten-method \"truncate\" \\\r\n    --find-unused-parameters \\\r\n    --update-freq 4 \\\r\n    --save-dir $SAVE_DIR\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2404/comments",
    "author": "Marvinmw",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-07T18:57:31Z",
        "body": "Yes, that's expected when training with mixed precision (`--fp16`). It's just a notice, you can ignore it as long as you don't see the scale get too small... any scale >= 0.1 is probably fine."
      },
      {
        "user": "edosyhptra",
        "created_at": "2021-06-18T03:57:15Z",
        "body": "> Yes, that's expected when training with mixed precision (`--fp16`). It's just a notice, you can ignore it as long as you don't see the scale get too small... any scale >= 0.1 is probably fine.\r\n\r\nThanks for the answer. I got the same warning in training. However, What is actually happening when it detects an overflow?"
      }
    ]
  },
  {
    "number": 2401,
    "title": "Quit training without error at the very begining",
    "created_at": "2020-07-31T09:41:16Z",
    "closed_at": "2020-07-31T11:30:26Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2401",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI'm running a customized model on IWSLT14-de-en dataset with the following settings. Training process ends at the beginning without any error. Sometimes the training process \r\n``\r\n2020-08-01 01:36:18 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:17632\r\n2020-08-01 01:36:18 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:17632\r\n2020-08-01 01:36:18 | INFO | fairseq.distributed_utils | initialized host statnlp-super as rank 3\r\n2020-08-01 01:36:18 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:17632\r\n2020-08-01 01:36:18 | INFO | fairseq.distributed_utils | initialized host statnlp-super as rank 2\r\n2020-08-01 01:36:18 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:17632\r\n2020-08-01 01:36:18 | INFO | fairseq.distributed_utils | initialized host statnlp-super as rank 1\r\n2020-08-01 01:36:18 | INFO | fairseq.distributed_utils | initialized host statnlp-super as rank 0\r\n2020-08-01 01:36:21 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='oracle_transformer_iwslt_de_en', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, broadcast_buffers=False, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='oracle_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', dataset_impl=None, ddp_backend='c10d', decay_k=1.0, decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:17632', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gumbel_noise=0.5, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=100, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid=8192, max_update=1400, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', oracle_search_beam_size=4, patience=-1, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bleu_gumbel_noise=True, use_bmuf=False, use_epoch_numbers_decay=True, use_greed_gumbel_noise=False, use_neighborhood_beam_search=False, use_old_adam=False, use_sentence_level_oracles=True, use_word_level_oracles=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001)\r\n2020-08-01 01:36:21 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types\r\n2020-08-01 01:36:21 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types\r\n2020-08-01 01:36:21 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de\r\n2020-08-01 01:36:21 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en\r\n2020-08-01 01:36:21 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples\r\n2020-08-01 01:36:22 | INFO | fairseq_cli.train | TransformerModel(\r\n  (encoder): TransformerEncoder(\r\n    (embed_tokens): Embedding(8848, 512, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerEncoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n  )\r\n  (decoder): TransformerDecoder(\r\n    (embed_tokens): Embedding(6632, 512, padding_idx=1)\r\n    (embed_positions): SinusoidalPositionalEmbedding()\r\n    (layers): ModuleList(\r\n      (0): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (1): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (2): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (3): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (4): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n      (5): TransformerDecoderLayer(\r\n        (self_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (encoder_attn): MultiheadAttention(\r\n          (k_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (v_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (q_proj): Linear(in_features=512, out_features=512, bias=True)\r\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\r\n        )\r\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n        (fc1): Linear(in_features=512, out_features=1024, bias=True)\r\n        (fc2): Linear(in_features=1024, out_features=512, bias=True)\r\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n      )\r\n    )\r\n  )\r\n)\r\n2020-08-01 01:36:22 | INFO | fairseq_cli.train | model oracle_transformer_iwslt_de_en, criterion OracleLabelSmoothedCrossEntropyCriterion\r\n2020-08-01 01:36:22 | INFO | fairseq_cli.train | num. model params: 42864640 (num. trained: 42864640)\r\n2020-08-01 01:36:22 | INFO | fairseq_cli.train | training on 4 GPUs\r\n2020-08-01 01:36:22 | INFO | fairseq_cli.train | max tokens per GPU = 8192 and max sentences per GPU = None\r\n2020-08-01 01:36:22 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\r\n2020-08-01 01:36:23 | INFO | fairseq.trainer | loaded checkpoint checkpoints/checkpoint_last.pt (epoch 10 @ 1400 updates)\r\n2020-08-01 01:36:23 | INFO | fairseq.trainer | loading train data for epoch 10\r\n2020-08-01 01:36:23 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de\r\n2020-08-01 01:36:23 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en\r\n2020-08-01 01:36:23 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples\r\n2020-08-01 01:36:24 | INFO | fairseq_cli.train | done training in 0.0 seconds\r\n\r\nProcess finished with exit code 0\r\n\r\n``\r\n\r\n#### Code\r\n``data-bin/iwslt14.tokenized.de-en\r\n--lr\r\n0.001\r\n--optimizer\r\nadam\r\n--lr-scheduler\r\ninverse_sqrt\r\n--criterion\r\noracle_label_smoothed_cross_entropy\r\n--label-smoothing\r\n0.1\r\n--weight-decay\r\n0.0001\r\n--clip-norm\r\n0.0\r\n--dropout\r\n0.3\r\n--max-tokens\r\n8192\r\n--arch\r\noracle_transformer_iwslt_de_en\r\n--save-dir\r\ncheckpoints/\r\n--use-sentence-level-oracles\r\n--use-epoch-numbers-decay\r\n--decay-k\r\n1\r\n--use-bleu-gumbel-noise\r\n--gumbel-noise\r\n0.5\r\n--max-update\r\n1400``\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9\r\n - PyTorch Version (e.g., 1.0): 1.2\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: quadro RTX 8000\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2401/comments",
    "author": "haorannlp",
    "comments": [
      {
        "user": "haorannlp",
        "created_at": "2020-07-31T11:31:07Z",
        "body": "Create a new checkpoint folder."
      },
      {
        "user": "olesyaksyon",
        "created_at": "2020-12-16T14:48:40Z",
        "body": "I have the same issue with wav2vec model. I removed and created checkpoint folder, but it didn't help.\r\n\r\nCommand:\r\npython train.py /path/to/manifest--save-dir models --num-workers 2 --fp16 --max-update 400000 \\\r\n--save-interval 1 --no-epoch-checkpoints --arch wav2vec --task audio_pretraining --min-lr 1e-06 \\\r\n--optimizer adam --lr 1e-05 --lr-scheduler fixed \\\r\n--conv-feature-layers \"[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1), (512, 1, 1)]\" \\\r\n--conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]\" \\\r\n--activation gelu --offset auto --skip-connections-agg --residual-scale 0.5 \\\r\n--vq-type gumbel --vq-groups 2 --vq-depth 2 \\\r\n--combine-groups --vq-vars 320 --vq-temp \"(2,0.5,0.999995)\" --prediction-steps 12 --warmup-updates 1000 \\\r\n--criterion wav2vec --num-negatives 10 --max-sample-size 150000 \\\r\n--max-tokens 300000 --cross-sample-negatives 0 --update-freq 1 --seed 2 --skip-invalid-size-inputs-valid-test"
      },
      {
        "user": "Jarlene",
        "created_at": "2021-07-22T09:35:07Z",
        "body": "I have the same issue too。how to solve it?"
      },
      {
        "user": "Jarlene",
        "created_at": "2021-07-22T09:35:07Z",
        "body": "I have the same issue too。how to solve it?\r\nfairseq-train  xxxxxxx \\\r\n        --save-dir xxxxxx \\\r\n        --log-format tqdm \\\r\n        --reset-meters \\\r\n        --reset-lr-scheduler \\\r\n        --seed 4420 \\\r\n        --no-epoch-checkpoints \\\r\n        --enable-padding \\\r\n        --num-workers 8 \\\r\n        --sample-rate 8000 \\\r\n        --max-epoch 100000000 \\\r\n        --save-interval 10 \\\r\n        --arch wav2vec2 \\\r\n        --task audio_pretraining \\\r\n        --min-lr 1e-06 \\\r\n        --final-dim 256 \\\r\n        --encoder-layerdrop 0.05 \\\r\n        --dropout-input  0.1 \\\r\n        --dropout-features 0.1 \\\r\n        --feature-grad-mult 0.1 \\\r\n        --optimizer adam \\\r\n        --lr 0.005 \\\r\n        --lr-scheduler cosine \\\r\n        --warmup-updates 500 \\\r\n        --warmup-init-lr 1e-07 \\\r\n        --criterion wav2vec \\\r\n        --num-negatives 10 \\\r\n        --max-sample-size 8000 \\\r\n        --min-sample-size 8000 \\\r\n        --max-tokens 80000 "
      },
      {
        "user": "haorannlp",
        "created_at": "2021-07-22T09:57:55Z",
        "body": "> I have the same issue too。how to solve it?\r\n> fairseq-train xxxxxxx\r\n> --save-dir xxxxxx\r\n> --log-format tqdm\r\n> --reset-meters\r\n> --reset-lr-scheduler\r\n> --seed 4420\r\n> --no-epoch-checkpoints\r\n> --enable-padding\r\n> --num-workers 8\r\n> --sample-rate 8000\r\n> --max-epoch 100000000\r\n> --save-interval 10\r\n> --arch wav2vec2\r\n> --task audio_pretraining\r\n> --min-lr 1e-06\r\n> --final-dim 256\r\n> --encoder-layerdrop 0.05\r\n> --dropout-input 0.1\r\n> --dropout-features 0.1\r\n> --feature-grad-mult 0.1\r\n> --optimizer adam\r\n> --lr 0.005\r\n> --lr-scheduler cosine\r\n> --warmup-updates 500\r\n> --warmup-init-lr 1e-07\r\n> --criterion wav2vec\r\n> --num-negatives 10\r\n> --max-sample-size 8000\r\n> --min-sample-size 8000\r\n> --max-tokens 80000\r\n\r\nPerhaps you need to check if `save-dir` has already contained some checkpoints that tell the program the training has finished. "
      },
      {
        "user": "Jarlene",
        "created_at": "2021-07-22T12:01:23Z",
        "body": "> > I have the same issue too。how to solve it?\r\n> > fairseq-train xxxxxxx\r\n> > --save-dir xxxxxx\r\n> > --log-format tqdm\r\n> > --reset-meters\r\n> > --reset-lr-scheduler\r\n> > --seed 4420\r\n> > --no-epoch-checkpoints\r\n> > --enable-padding\r\n> > --num-workers 8\r\n> > --sample-rate 8000\r\n> > --max-epoch 100000000\r\n> > --save-interval 10\r\n> > --arch wav2vec2\r\n> > --task audio_pretraining\r\n> > --min-lr 1e-06\r\n> > --final-dim 256\r\n> > --encoder-layerdrop 0.05\r\n> > --dropout-input 0.1\r\n> > --dropout-features 0.1\r\n> > --feature-grad-mult 0.1\r\n> > --optimizer adam\r\n> > --lr 0.005\r\n> > --lr-scheduler cosine\r\n> > --warmup-updates 500\r\n> > --warmup-init-lr 1e-07\r\n> > --criterion wav2vec\r\n> > --num-negatives 10\r\n> > --max-sample-size 8000\r\n> > --min-sample-size 8000\r\n> > --max-tokens 80000\r\n> \r\n> Perhaps you need to check if `save-dir` has already contained some checkpoints that tell the program the training has finished.\r\n\r\nmy `save-dir` contained nothing，i donot know what should to do"
      },
      {
        "user": "onlyonewater",
        "created_at": "2024-06-21T14:15:38Z",
        "body": "I have a similar problem, does someone have any idea about this?"
      }
    ]
  },
  {
    "number": 2396,
    "title": "Get current epoch number in the model's forward pass",
    "created_at": "2020-07-29T13:48:50Z",
    "closed_at": "2020-07-30T06:36:16Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2396",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nWould it be possible to get current epoch number during model forwarding?\r\nThe reason I ask is that I want to freeze part of the model in the first K epoch of training. \r\n\r\n#### Code\r\nI am thinking of achieving the goal as below, the only problem remain is to get the epoch number.\r\n```python\r\nclass TransformerEncoder():\r\n  def forward(input):\r\n    if current_epoch < 5:\r\n       with torch.no_grad():\r\n           x=model_operation_1(input)\r\n    else:\r\n        x=model_operation_1(input)\r\n    x2=model_operation_2(x, input)\r\n    return x2\r\n```\r\n\r\n\r\n#### What have you tried?\r\n\r\nI understand that we can get the epoch number during training loops using 'epoch_itr.epoch'. \r\nBut I want to get this number in the model forwarding.\r\n\r\nOne possible solution is to pass current epoch number as parameters all the way from trainer->criterion->model.\r\nBut I am wondering is there any better solutions? So that I don't have to change the input for all models/criterions.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version:  master\r\n - PyTorch Version: 1.5\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2396/comments",
    "author": "LividWo",
    "comments": [
      {
        "user": "FadedCosine",
        "created_at": "2022-01-02T09:19:14Z",
        "body": "So, how did you solve this problem?"
      }
    ]
  },
  {
    "number": 2390,
    "title": "RobertaClassificationHead explanation",
    "created_at": "2020-07-28T15:11:05Z",
    "closed_at": "2022-04-18T06:21:11Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2390",
    "body": "I was just trying to understand the RobertaClassificationHead. I noticed that for the original BERT, we used to extract the [CLS ]representations and feed to output layer for classification task. However, here in the roberta modeling for classification, you add a linear layer on top of the pooled output.\r\n\r\n```\r\nclass RobertaClassificationHead(nn.Module):\r\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\r\n\r\n    def __init__(self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout, q_noise=0, qn_block_size=8):\r\n        super().__init__()\r\n        self.dense = nn.Linear(input_dim, inner_dim)\r\n        self.activation_fn = utils.get_activation_fn(activation_fn)\r\n        self.dropout = nn.Dropout(p=pooler_dropout)\r\n        self.out_proj = apply_quant_noise_(\r\n            nn.Linear(inner_dim, num_classes), q_noise, qn_block_size\r\n        )\r\n\r\n    def forward(self, features, **kwargs):\r\n        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\r\n        x = self.dropout(x)\r\n        x = self.dense(x)\r\n        x = self.activation_fn(x)\r\n        x = self.dropout(x)\r\n        x = self.out_proj(x)\r\n        return x\r\n```\r\n\r\nI was wondering why do you decide to add one more linear layer? Or do I understand it incorrectly?\r\n\r\nAlso it might be a dump question, but I thought [CLS] was used as the aggregate representation of the sentence because of the **NSP** task. Is it still that good for classification if we dropped the **NSP** task?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2390/comments",
    "author": "graced03",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:55Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:40Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2389,
    "title": "LayerDrop multiple GPU command",
    "created_at": "2020-07-28T13:54:17Z",
    "closed_at": "2020-07-30T01:53:39Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2389",
    "body": "I followed the layerdrop docs from @huihuifan on 8 GPUS for to replicate `layerdrop_wmt_en_de_12_6` and ended up with the following command:\r\n\r\n```bash\r\nfairseq-train \\\r\n    data-bin/wmt16_en_de_bpe32k \\\r\n    --arch transformer_vaswani_wmt_en_de_big --share-all-embeddings \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\r\n    --dropout 0.3 --weight-decay 0.0 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 3584 \\\r\n    --fp16 \\\r\n    --encoder-layerdrop 0.2 --decoder-layerdrop 0.2 \\\r\n    --distributed-world-size 8 | tee fairseq_logs_layerdrop.txt\r\n```\r\nThis produced an error, suggesting that \"some parameters aren't contributing to the loss, use `--find-unused-parameters`, so I did that and now the command is running well.\r\n\r\nI'm wondering whether adding `--find-unused-parameters` is expected/ok, or if it might cause unindented side effects, like the model being pruned permanently after step 0.\r\nThanks in advance!\r\n\r\nEnv:\r\n\r\n- torch 1.5.1\r\n- fairseq master 108bb256 (Jul 28 AM)\r\n- cuda 10.1\r\n- 8 v100 GPUS\r\n- apex installed from source",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2389/comments",
    "author": "sshleifer",
    "comments": [
      {
        "user": "sshleifer",
        "created_at": "2020-07-28T14:18:38Z",
        "body": "Here are the metrics after 3 epochs, epoch 1 was the best:\r\n```bash\r\n2020-07-28 13:52:03 | INFO | valid | epoch 001 | loss 11.758 | nll_loss 11.098 | ppl 2191.47 | wps 341069 | wpb 16782.6 | bsz 600 | num_updates 5462\r\n2020-07-28 13:52:12 | INFO | train | epoch 001 | loss 11.125 | nll_loss 10.465 | ppl 1413.16 | wps 197521 | ups 7.85 | wpb 25150.9 | bsz 822.5 | num_updates 5462 | lr 0.000427882 | gnorm 1.858 | loss_scale 4 | train_wall 674 | wall 735\r\n\r\n2020-07-28 14:03:36 | INFO | valid | epoch 002 | loss 12.235 | nll_loss 11.592 | ppl 3086.26 | wps 292097 | wpb 16782.6 | bsz 600 | num_updates 10931 | best_loss 11.758\r\n2020-07-28 14:04:02 | INFO | train | epoch 002 | loss 10.919 | nll_loss 10.233 | ppl 1203.28 | wps 193520 | ups 7.69 | wpb 25151.1 | bsz 822.5 | num_updates 10931 | lr 0.000302461 | gnorm 5.111 | loss_scale 0 | train_wall 668 | wall 1446\r\n\r\n2020-07-28 14:15:27 | INFO | valid | epoch 003 | loss 12.160 | nll_loss 11.544 | ppl 2985.29 | wps 337505 | wpb 16782.6 | bsz 600 | num_updates 16400 | best_loss 11.758\r\n2020-07-28 14:15:56 | INFO | train | epoch 003 | loss 10.634 | nll_loss 9.901 | ppl 955.76 | wps 192786 | ups 7.66 | wpb 25151.8 | bsz 822.6 | num_updates 16400 | lr 0.000 | gnorm 3.609 | loss_scale 0 | train_wall 668 | wall 2160\r\n\r\n```\r\n\r\n**Update:** Trained for 14 epochs and didn't get much better. Now trying to add `--update_freq 16 and --lr .001`"
      },
      {
        "user": "sshleifer",
        "created_at": "2020-07-29T16:59:01Z",
        "body": "Adding `--lr 0.001 --update-freq 16 --encoder-layers 12 --decoder-layers 6` helps a lot.\r\n\r\nFull command:\r\n\r\n```bash\r\nexport save_dir=ckpt/layerdrop_v2\r\nfairseq-train \\\r\n    data-bin/wmt16_en_de_bpe32k \\\r\n    --arch transformer_vaswani_wmt_en_de_big --share-all-embeddings \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 0.001 --update-freq 16 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\r\n    --dropout 0.3 --weight-decay 0.0 --task translation \\\r\n    --decoder-layerdrop 0.3 --encoder-layerdrop 0.3 --encoder-layers 12 --decoder-layers 6 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 3584 \\\r\n    --fp16 \\\r\n    --save-interval 1 --save-interval-updates 5000 --keep-interval-updates=-1  --tensorboard-logdir tboard \\\r\n    --distributed-world-size 8 --ddp-backend no_c10d --save-dir $save_dir | tee $save_dir/logs.txt\r\n```\r\n\r\n"
      },
      {
        "user": "songyouwei",
        "created_at": "2020-08-24T07:41:24Z",
        "body": "add `--ddp-backend no_c10d` works for me"
      },
      {
        "user": "sunmeng01",
        "created_at": "2021-01-25T11:33:53Z",
        "body": "@sshleifer Hi, Can you get the bleu score 30.2 reported in the paper using the above script? After averaging 10 latest checkpoints,   the best bleu score I got is only 29.64"
      },
      {
        "user": "sshleifer",
        "created_at": "2021-01-28T15:35:34Z",
        "body": "I don't remember the exact numbers I got, but I would have been satisfied with 29.64."
      },
      {
        "user": "sunmeng01",
        "created_at": "2021-01-29T08:11:10Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 2385,
    "title": "Worker node does not stop and still holds nearly 100% GPU utility after I kill the master node. (Vice versa)",
    "created_at": "2020-07-28T07:34:39Z",
    "closed_at": "2022-04-18T06:21:08Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2385",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\nI try to search for a similar question but cannot find one.   \r\nIf there is any, please drop me a link.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI train a language model across two machines using distributed training.   \r\nConnecting to my machine always need the password, so I build an SSH tunnel between the two machines to avoid this.   \r\nThe order I do is:   \r\n1. Establish the SSH tunnel.   (run `ssh -L 1234:127.0.0.1:1234 username@master_ip` on the worker node machine)  \r\n2. Start the master node.  (the 1st machine with 4 GPUs)   (`bash train.sh 0`)      \r\n3. Start the worker node.  (the 2nd machine with 4GPUs)    (`bash train.sh 1`)    \r\n(I show `train.sh` below)    \r\n\r\nThis master node prints the log as the following:  (This looks normal)\r\n```\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | rank   2: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | rank   3: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | rank   4: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | rank   5: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | rank   6: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | rank   7: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-PCIE-16GB\r\n2020-07-28 06:31:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************\r\n2020-07-28 06:31:03 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)\r\n2020-07-28 06:31:03 | INFO | fairseq_cli.train | max tokens per GPU = 8000 and max sentences per GPU = None\r\n2020-07-28 06:31:03 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\r\n2020-07-28 06:31:03 | INFO | fairseq.optim.adam | using FusedAdam\r\n2020-07-28 06:31:05 | INFO | fairseq.trainer | loaded checkpoint /mnt/distributedcheckpoints/transformer_lm/1blm-3L-1024ED-4096FD-16H-shared-lr0.0007-SEED1-DROPOUT0.0-CRITERIONcross_entropy/checkpoint_last.pt (epoch 26 @ 8037 updates)\r\n2020-07-28 06:31:05 | INFO | fairseq.trainer | loading train data for epoch 26\r\n2020-07-28 06:31:07 | INFO | fairseq.data.data_utils | loaded 30312927 examples from: /mnt/distributed/bin/train\r\n2020-07-28 06:31:38 | INFO | fairseq_cli.train | begin training epoch 27\r\n2020-07-28 06:35:10 | INFO | train_inner | epoch 027:     63 / 2679 loss=6.756, ppl=108.12, wps=128008, ups=0.26, wpb=495820, bsz=11144.6, num_updates=8100, lr=0.000695666, gnorm=0.278, loss_scale=None, train_wall=333, wall=0\r\n2020-07-28 06:40:41 | INFO | train_inner | epoch 027:    163 / 2679 loss=6.746, ppl=107.33, wps=149680, ups=0.3, wpb=495997, bsz=11268.7, num_updates=8200, lr=0.000691411, gnorm=0.193, loss_scale=None, train_wall=331, wall=0\r\n2020-07-28 06:46:13 | INFO | train_inner | epoch 027:    263 / 2679 loss=6.741, ppl=106.99, wps=149629, ups=0.3, wpb=496625, bsz=11276.5, num_updates=8300, lr=0.000687233, gnorm=0.174, loss_scale=None, train_wall=332, wall=0\r\n2020-07-28 06:51:45 | INFO | train_inner | epoch 027:    363 / 2679 loss=6.743, ppl=107.15, wps=149657, ups=0.3, wpb=496325, bsz=11348.9, num_updates=8400, lr=0.00068313, gnorm=0.208, loss_scale=None, train_wall=332, wall=0\r\n2020-07-28 06:57:17 | INFO | train_inner | epoch 027:    463 / 2679 loss=6.732, ppl=106.29, wps=149645, ups=0.3, wpb=496588, bsz=11502.2, num_updates=8500, lr=0.0006791, gnorm=0.211, loss_scale=None, train_wall=332, wall=0\r\n2020-07-28 07:02:48 | INFO | train_inner | epoch 027:    563 / 2679 loss=6.747, ppl=107.38, wps=149546, ups=0.3, wpb=496178, bsz=11190.1, num_updates=8600, lr=0.00067514, gnorm=0.201, loss_scale=None, train_wall=332, wall=0\r\n2020-07-28 07:08:20 | INFO | train_inner | epoch 027:    663 / 2679 loss=6.741, ppl=107, wps=149615, ups=0.3, wpb=496211, bsz=11222.3, num_updates=8700, lr=0.000671249, gnorm=0.203, loss_scale=None, train_wall=332, wall=0\r\n2020-07-28 07:13:52 | INFO | train_inner | epoch 027:    763 / 2679 loss=6.751, ppl=107.68, wps=149689, ups=0.3, wpb=496526, bsz=11231, num_updates=8800, lr=0.000667424, gnorm=0.184, loss_scale=None, train_wall=332, wall=0\r\n```\r\nAnd the worker node only prints:  \r\n```\r\n*****************************************\r\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal per\r\nformance in your application as needed.\r\n*****************************************\r\n2020-07-28 06:30:58 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://\r\n2020-07-28 06:30:58 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000007 as rank 4\r\n2020-07-28 06:30:58 | INFO | fairseq.distributed_utils | distributed init (rank 7): env://\r\n2020-07-28 06:30:58 | INFO | fairseq.distributed_utils | distributed init (rank 6): env://\r\n2020-07-28 06:30:58 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000007 as rank 7\r\n2020-07-28 06:30:58 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://\r\n2020-07-28 06:30:58 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000007 as rank 6\r\n2020-07-28 06:30:58 | INFO | fairseq.distributed_utils | initialized host 5b1a1be8c7a14bc9aa7a607f04a78f16000007 as rank 5\r\n2020-07-28 06:36:36 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the numbe\r\nr of workers (--num-workers) may help.\r\n2020-07-28 06:36:36 | INFO | fairseq.data.iterators | Data loading buffer is empty or nearly empty. This may indicate a data loading bottleneck, and increasing the numbe\r\nr of workers (--num-workers) may help.\r\n```\r\n\r\nAnd after I kill the training program on the master node, the process on the worker node is still running and holding nearly 100% GPU utility.   \r\nBesides, if I kill the worker node, the master node is also running and holding nearly 100% GPU utility.    \r\nI am confused about this. Could anyone please provide me with a short explanation?  \r\n\r\n#### Code\r\n\r\nThe training script is as the following:   \r\n```\r\nMASTER_IP=\"127.0.0.1\"\r\nMASTER_PORT=1234\r\nTOTAL_NODES_NUM=2\r\nNODE_RANK=$1\r\n\r\nDATA_DIR=/mnt/distributed/bin\r\n\r\narch=transformer_lm\r\nseed=1\r\nlr=0.0007\r\ndecoder_layers=3\r\ndecoder_embed_dim=1024\r\ndecoder_ffn_embed_dim=4096\r\ndecoder_attention_heads=16\r\ndropout=0.0\r\ncriterion=cross_entropy\r\nlabel_smoothing=0.0\r\n\r\nSAVE_DIR=/mnt/distributed/checkpoints/${arch}/1blm-${decoder_layers}L-${decoder_embed_dim}ED-${decoder_ffn_embed_dim}FD-${decoder_attention_heads}H-shared-lr${lr}-SEED${seed}-DROPOUT${dropout}-CRITERION${criterion}\r\n\r\nRESTORE_FILE=/mnt/distributed/restore-checkpoint/checkpoint23.pt\r\n\r\nRESTORE_FILE=/mnt/distributed-xiangxin-${SERVER_PORT}/sampling-distill/restore-checkpoint/checkpoint23.pt\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 \\\r\n    --nnodes=${TOTAL_NODES_NUM} --node_rank=${NODE_RANK} --master_addr=${MASTER_IP} \\\r\n    --master_port=${MASTER_PORT} \\\r\n    $(which fairseq-train) ${DATA_DIR} \\\r\n    --task language_modeling \\\r\n    --share-decoder-input-output-embed \\\r\n    --arch ${arch} \\\r\n    --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 8000 \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n    --decoder-layers ${decoder_layers} \\\r\n    --decoder-embed-dim ${decoder_embed_dim} \\\r\n    --decoder-ffn-embed-dim ${decoder_ffn_embed_dim} \\\r\n    --decoder-attention-heads ${decoder_attention_heads} \\\r\n    --lr ${lr} --min-lr 1e-09 --weight-decay 0.0 \\\r\n    --criterion ${criterion} --dropout ${dropout} \\\r\n    --save-dir ${SAVE_DIR} \\\r\n    --max-epoch 200 --max-tokens 8000 --update-freq 8 \\\r\n    --no-progress-bar --seed ${seed} \\\r\n    --sample-break-mode eos \\\r\n    --num-workers 20 \\\r\n    --tensorboard-logdir ${SAVE_DIR} \\\r\n    --distributed-backend nccl \\\r\n    --distributed-no-spawn \\\r\n\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.4.0\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):\r\n - Python version:  3.6\r\n - CUDA/cuDNN version: CUDA10.1\r\n - GPU models and configuration:  V100\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2385/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:57Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2383,
    "title": "[QuantNoise] Details of int4 quantization in image classification task",
    "created_at": "2020-07-28T03:19:47Z",
    "closed_at": "2022-04-18T06:21:09Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2383",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?       \r\nThanks for open-sourcing such a great project!    \r\n    \r\nI'm tring to reproduce QuantNoise for int4-mobilenetv2 in ImageNet according to the paper and this project. But I'm confused about some details in the paper. \r\n1. ***Table1*** in the paper shows that training int4-EfficientNet-B3 in ImageNet-1k with QAT and Quant-Noise gets 59.4% and 67.8% top-1 accuracy respectively. In such a experiment(Table1), \r\n    1.  ***Section8.3*** say that activations are quantized with Hitogram(L2) scheme in scalar quantization.      \r\n        **What about weights? Are weights all quantized with Histogram(L2) scheme as well?** \r\n    2. As I know, origin QAT quantized activations with MovingAverageMinMax scheme and fold BN into Conv to apply FakeBN.      \r\n        **Which scheme is used for QAT in the experiment, Histogram(L2) or MovingAverageMinMax?**      \r\n        **Does QAT use FakeBN in the experiment, or just use normal BN? Does the normal BN freeze statistic parameters(running_mean and running_varience)?**\r\n    3. **Are all weights and activations symmetricly quantized into int4, including depthwise convolutions, downsample convolutions, SE modules, the first convolution as well as the last fully connection layer?**\r\n2. ***Table9*** compares the results of Histogram and Channel scheme.     \r\n    **Are weights and activations both quantized with Histogram scheme or Channel scheme here?**\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2383/comments",
    "author": "hey-yahei",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "MohammedHAlali",
        "created_at": "2022-06-01T21:01:27Z",
        "body": "Hi @hey-yahei, did you figure out how to run the image classification task on MobileNet?"
      }
    ]
  },
  {
    "number": 2382,
    "title": "Training an LM with adaptive inputs with TPUs seems pretty slow",
    "created_at": "2020-07-28T03:14:47Z",
    "closed_at": "2022-04-18T06:21:07Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2382",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n\r\n#### I am running codes for training an LM with adaptive inputs on TPU V3-8\r\n\r\nThe training is pretty slow. And I checked the utilization of TPU Matrix Units, it was 0.000%.\r\n\r\nDo you have any ideas on how to improve the utilization/efficiency when training adaptive inputs LM with TPUs?\r\n\r\n#### Code\r\n\r\n```\r\npython /usr/share/torch-xla-1.5/tpu-examples/deps/fairseq/train.py data-bin/wikitext-103 \\\r\n    --task language_modeling \\\r\n    --save-dir checkpoints/transformer_wikitext-103 \\\r\n    --arch transformer_lm_wiki103 \\\r\n    --max-update 286000 --max-lr 1.0 --t-mult 2 --lr-period-updates 270000 --lr-scheduler cosine --lr-shrink 0.75 \\\r\n    --warmup-updates 16000 --warmup-init-lr 1e-07 --min-lr 1e-09  --lr 0.0001 --clip-norm 0.1 \\\r\n    --update-freq 10 --tokens-per-sample 512 \\\r\n    --sample-break-mode none --skip-invalid-size-inputs-valid-test --ddp-backend=no_c10d --no-epoch-checkpoints \\\r\n    --optimizer nag --criterion adaptive_loss \\\r\n    --num_cores=8 --metrics_debug \\\r\n    --input_shapes 16x512 \\\r\n    --log_steps=30 \\\r\n    --log-format=simple  \r\n```\r\n\r\n#### What have you tried?\r\n\r\nWhen I am running tutorials about MT and Roberta, they work well. \r\n\r\n#### What's your environment?\r\n\r\n - PyTorch Version: pytorch-xla-1.5\r\n - TPU: V3-8\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2382/comments",
    "author": "jiaaoc",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:58Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2371,
    "title": "Need help speech recognition using fairseq in python",
    "created_at": "2020-07-24T19:25:22Z",
    "closed_at": "2022-04-18T06:21:05Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2371",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHi,\r\n  Will you please provide speech recognition example in python?\r\n\r\nThanks\r\nNagaraju\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2371/comments",
    "author": "nag0811",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:35Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2370,
    "title": "Need help on training our own data for machine translation using python",
    "created_at": "2020-07-24T17:04:19Z",
    "closed_at": "2022-04-18T06:21:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2370",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI posted the question already and asked to look at the example. But my problem is, I wants to implement it in python. Will you please provide me python example for machine translation (if possible, please guide to train our own data).\r\n\r\nThanks\r\nNagaraju\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2370/comments",
    "author": "nag0811",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:34Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2368,
    "title": "which architecture should be selected for finetuning transformer_lm.wmt19.en",
    "created_at": "2020-07-24T12:23:14Z",
    "closed_at": "2022-04-18T06:21:02Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2368",
    "body": "Hello\r\n\r\nI am trying to finetune transformer_lm.wmt19.en on different domain data. \r\n\r\n`fairseq-train --task language_modeling \\\r\n  data-bin/wmt_data \\\r\n  --restore-file data-bin/wmt19.en/model.pt \\\r\n  --save-dir checkpoints/finetune_wmt19 \\\r\n  --arch transformer_lm --share-decoder-input-output-embed \\\r\n  --dropout 0.1  \\\r\n  --optimizer adam --adam-betas '(0.9, 0.98)'  --weight-decay 0.01 --clip-norm 0.0\\\r\n  --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\r\n  --tokens-per-sample 512 --sample-break-mode none \\\r\n  --max-sentences 16  --update-freq 16 \\\r\n  --fp16 \\\r\n  --max-update 500`\r\n\r\nHowever, seems like **transformer_lm** is not the proper architecture.\r\n\r\n> \"please ensure that the architectures match.\".format(filename)\r\nException: Cannot load model parameters from checkpoint data-bin/wmt19.en/model.pt; please ensure that the architectures match.\r\n\r\nI also tried with other architectures but I couldn't find the proper architecture.\r\nCould you help me with this? \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2368/comments",
    "author": "Sohyo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:34Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:32Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2359,
    "title": "What the subword model is in XLM-R ?",
    "created_at": "2020-07-22T07:38:24Z",
    "closed_at": "2022-04-18T06:20:59Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2359",
    "body": "Hi,\r\n\r\nI am wondering what subword model is used in XLM-R.\r\nIn the paper, it writes `using Sentence Piece (Kudo and Richardson, 2018) with a unigram language model`, but the subword model file I downloaded is named as `sentencepiece.bpe.model`.\r\n\r\nSo what the subword model is actually used in XLM-R? Does the `bpe` shown in the filename mean the subword model is a bee model trained by spm_train?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2359/comments",
    "author": "MGithubGA",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:38Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T06:20:29Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "mani-rai",
        "created_at": "2022-07-03T14:25:24Z",
        "body": "XLM-R seems a mess. The code doesn't seems to follow the paper."
      }
    ]
  },
  {
    "number": 2339,
    "title": "does fairseq-train support finetune with \"bart_base\"",
    "created_at": "2020-07-17T07:44:09Z",
    "closed_at": "2020-07-17T16:20:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2339",
    "body": "## ❓ Questions and Help\r\nWhen I set the parameter a**rch** as \"**bart_base**\", I have the following errors\r\n\r\nfairseq-train: error: argument --arch/-a: invalid choice: 'bart_base' (choose from 'transformer', 'transformer_iwslt_de_en', 'transformer_wmt_en_de', 'transformer_vaswani_wmt_en_de_big', 'transformer_vaswani_wmt_en_fr_big', 'transformer_wmt_en_de_big', 'transformer_wmt_en_de_big_t2t', 'transformer_align', 'transformer_wmt_en_de_big_align', 'levenshtein_transformer', 'levenshtein_transformer_wmt_en_de', 'levenshtein_transformer_vaswani_wmt_en_de_big', 'levenshtein_transformer_wmt_en_de_big', 'nonautoregressive_transformer', 'nonautoregressive_transformer_wmt_en_de', 'cmlm_transformer', 'cmlm_transformer_wmt_en_de', 'lightconv', 'lightconv_iwslt_de_en', 'lightconv_wmt_en_de', 'lightconv_wmt_en_de_big', 'lightconv_wmt_en_fr_big', 'lightconv_wmt_zh_en_big', 'lightconv_lm', 'lightconv_lm_gbw', 'fconv', 'fconv_iwslt_de_en', 'fconv_wmt_en_ro', 'fconv_wmt_en_de', 'fconv_wmt_en_fr', 'fconv_lm', 'fconv_lm_dauphin_wikitext103', 'fconv_lm_dauphin_gbw', 'lstm', 'lstm_wiseman_iwslt_de_en', 'lstm_luong_wmt_en_de', 'transformer_from_pretrained_xlm', 'masked_lm', 'bert_base', 'bert_large', 'xlm_base', 'iterative_nonautoregressive_transformer', 'iterative_nonautoregressive_transformer_wmt_en_de', 'insertion_transformer', 'wav2vec', 'fconv_self_att', 'fconv_self_att_wp', 'roberta', 'roberta_base', 'roberta_large', 'xlm', 'multilingual_transformer', 'multilingual_transformer_iwslt_de_en', 'transformer_lm', 'transformer_lm_big', 'transformer_lm_baevski_wiki103', 'transformer_lm_wiki103', 'transformer_lm_baevski_gbw', 'transformer_lm_gbw', 'transformer_lm_gpt', 'transformer_lm_gpt2_small', 'transformer_lm_gpt2_medium', 'transformer_lm_gpt2_big', 'bart_large')\r\n\r\n\r\n>>> sorted(a)\r\n['bart_large', 'bert_base', 'bert_large', 'cmlm_transformer', 'cmlm_transformer_wmt_en_de', 'fconv', 'fconv_iwslt_de_en', 'fconv_lm', 'fconv_lm_dauphin_gbw', 'fconv_lm_dauphin_wikitext103', 'fconv_self_att', 'fconv_self_att_wp', 'fconv_wmt_en_de', 'fconv_wmt_en_fr', 'fconv_wmt_en_ro', 'insertion_transformer', 'iterative_nonautoregressive_transformer', 'iterative_nonautoregressive_transformer_wmt_en_de', 'levenshtein_transformer', 'levenshtein_transformer_vaswani_wmt_en_de_big', 'levenshtein_transformer_wmt_en_de', 'levenshtein_transformer_wmt_en_de_big', 'lightconv', 'lightconv_iwslt_de_en', 'lightconv_lm', 'lightconv_lm_gbw', 'lightconv_wmt_en_de', 'lightconv_wmt_en_de_big', 'lightconv_wmt_en_fr_big', 'lightconv_wmt_zh_en_big', 'lstm', 'lstm_luong_wmt_en_de', 'lstm_wiseman_iwslt_de_en', 'masked_lm', 'multilingual_transformer', 'multilingual_transformer_iwslt_de_en', 'nonautoregressive_transformer', 'nonautoregressive_transformer_wmt_en_de', 'roberta', 'roberta_base', 'roberta_large', 'transformer', 'transformer_align', 'transformer_from_pretrained_xlm', 'transformer_iwslt_de_en', 'transformer_lm', 'transformer_lm_baevski_gbw', 'transformer_lm_baevski_wiki103', 'transformer_lm_big', 'transformer_lm_gbw', 'transformer_lm_gpt', 'transformer_lm_gpt2_big', 'transformer_lm_gpt2_medium', 'transformer_lm_gpt2_small', 'transformer_lm_wiki103', 'transformer_vaswani_wmt_en_de_big', 'transformer_vaswani_wmt_en_fr_big', 'transformer_wmt_en_de', 'transformer_wmt_en_de_big', 'transformer_wmt_en_de_big_align', 'transformer_wmt_en_de_big_t2t', 'wav2vec', 'xlm', 'xlm_base']\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2339/comments",
    "author": "songwang41",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-07-17T16:20:48Z",
        "body": "There's `bert_base` and `bart_large`, but no `bart_base`."
      },
      {
        "user": "songwang41",
        "created_at": "2020-07-18T22:25:07Z",
        "body": "The author is wrong or did not further check before closing this.  @myleott \r\n\r\nI found the newer version of fairseq instead of the pip version support this."
      },
      {
        "user": "myleott",
        "created_at": "2020-07-24T14:54:42Z",
        "body": "Ah, right, the `bart_base` architecture was added later. Using the latest version should fix it"
      },
      {
        "user": "yuye2133",
        "created_at": "2020-10-14T12:45:27Z",
        "body": "> The author is wrong or did not further check before closing this. @myleott\r\n> \r\n> I found the newer version of fairseq instead of the pip version support this.\r\n\r\nHow to install the latest version，I installed fairseq-0.9.0 with PIP and still reported the error"
      }
    ]
  },
  {
    "number": 2332,
    "title": "V3-8 update-freq 4 and V3-32 training having very different behaviors than GPU",
    "created_at": "2020-07-16T15:01:22Z",
    "closed_at": "2020-07-21T16:09:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2332",
    "body": "I am running a transformer based mbart-base model.\r\n\r\nI used args.tpu\r\n\r\nI initially trained on V3-32, the loss doesnt go down. then I switch to V3-8 with update freq 4, and loss looks better but still very off from gpu logs.\r\n\r\nTPU log:\r\n2020-07-17 04:00:55 | INFO | train_inner | epoch 001:    100 / 2952000 loss=16.137, ppl=72076.5, wps=0, ups=0, wpb=1928, bsz=2, num_updates=100, lr=1e-06, gnorm=4.43, train_wall=13, wall=191\r\n2020-07-17 04:00:55 | INFO | root | NOTE: XLA compilation detected; too many of these can lead to slow training, but we expect a few in the beginning\r\n2020-07-17 04:01:26 | INFO | train_inner | epoch 001:    200 / 2952000 loss=14.498, ppl=23144.6, wps=60.1, ups=0.03, wpb=1877, bsz=2, num_updates=200, lr=2e-06, gnorm=2.725, train_wall=11, wall=222\r\n2020-07-17 04:01:26 | INFO | root | NOTE: XLA compilation detected; too many of these can lead to slow training, but we expect a few in the beginning\r\n2020-07-17 04:01:57 | INFO | train_inner | epoch 001:    300 / 2952000 loss=13.286, ppl=9988.85, wps=64.3, ups=0.03, wpb=2001, bsz=2, num_updates=300, lr=3e-06, gnorm=2.416, train_wall=11, wall=253\r\n2020-07-17 04:02:28 | INFO | train_inner | epoch 001:    400 / 2952000 loss=11.931, ppl=3903.91, wps=61, ups=0.03, wpb=1900, bsz=2, num_updates=400, lr=4e-06, gnorm=5.17, train_wall=11, wall=284\r\n2020-07-17 04:02:59 | INFO | train_inner | epoch 001:    500 / 2952000 loss=13.251, ppl=9747.4, wps=61, ups=0.03, wpb=1888, bsz=2, num_updates=500, lr=5e-06, gnorm=2.323, train_wall=11, wall=315\r\n2020-07-17 04:03:30 | INFO | train_inner | epoch 001:    600 / 2952000 loss=12.786, ppl=7062.4, wps=61.1, ups=0.03, wpb=1891, bsz=2, num_updates=600, lr=6e-06, gnorm=2.155, train_wall=11, wall=346\r\n2020-07-17 04:04:01 | INFO | train_inner | epoch 001:    700 / 2952000 loss=12.686, ppl=6590.07, wps=60.1, ups=0.03, wpb=1859, bsz=2, num_updates=700, lr=7e-06, gnorm=2.015, train_wall=11, wall=377\r\n2020-07-17 04:04:32 | INFO | train_inner | epoch 001:    800 / 2952000 loss=12.712, ppl=6707.93, wps=63.9, ups=0.03, wpb=1988, bsz=2, num_updates=800, lr=8e-06, gnorm=3.707, train_wall=11, wall=408\r\n2020-07-17 04:05:03 | INFO | train_inner | epoch 001:    900 / 2952000 loss=11.645, ppl=3203.19, wps=64.6, ups=0.03, wpb=1991, bsz=2, num_updates=900, lr=9e-06, gnorm=4.098, train_wall=11, wall=439\r\n2020-07-17 04:05:34 | INFO | train_inner | epoch 001:   1000 / 2952000 loss=11.528, ppl=2953.08, wps=60.8, ups=0.03, wpb=1881, bsz=2, num_updates=1000, lr=1e-05, gnorm=1.523, train_wall=11, wall=470\r\n2020-07-17 04:06:06 | INFO | train_inner | epoch 001:   1100 / 2952000 loss=10.505, ppl=1453.23, wps=62.5, ups=0.03, wpb=1971, bsz=2, num_updates=1100, lr=1.1e-05, gnorm=4.227, train_wall=12, wall=501\r\n2020-07-17 04:06:36 | INFO | train_inner | epoch 001:   1200 / 2952000 loss=10.752, ppl=1725, wps=64.8, ups=0.03, wpb=1997, bsz=2, num_updates=1200, lr=1.2e-05, gnorm=2.649, train_wall=11, wall=532\r\n2020-07-17 04:07:07 | INFO | train_inner | epoch 001:   1300 / 2952000 loss=11.373, ppl=2652.16, wps=62, ups=0.03, wpb=1912, bsz=2, num_updates=1300, lr=1.3e-05, gnorm=1.555, train_wall=11, wall=563\r\n2020-07-17 04:07:38 | INFO | train_inner | epoch 001:   1400 / 2952000 loss=9.646, ppl=801.3, wps=62.4, ups=0.03, wpb=1932, bsz=2, num_updates=1400, lr=1.4e-05, gnorm=2.147, train_wall=11, wall=594\r\n2020-07-17 04:08:09 | INFO | train_inner | epoch 001:   1500 / 2952000 loss=11.189, ppl=2334.9, wps=59.7, ups=0.03, wpb=1845, bsz=2, num_updates=1500, lr=1.5e-05, gnorm=1.252, train_wall=11, wall=625\r\n2020-07-17 04:08:40 | INFO | train_inner | epoch 001:   1600 / 2952000 loss=11.251, ppl=2437.86, wps=56.3, ups=0.03, wpb=1743, bsz=2, num_updates=1600, lr=1.6e-05, gnorm=1.251, train_wall=11, wall=656\r\n2020-07-17 04:09:12 | INFO | train_inner | epoch 001:   1700 / 2952000 loss=8.878, ppl=470.61, wps=64.1, ups=0.03, wpb=2011, bsz=2, num_updates=1700, lr=1.7e-05, gnorm=5.188, train_wall=12, wall=687\r\n2020-07-17 04:09:43 | INFO | train_inner | epoch 001:   1800 / 2952000 loss=10.918, ppl=1935.07, wps=59.9, ups=0.03, wpb=1859, bsz=2, num_updates=1800, lr=1.8e-05, gnorm=5.364, train_wall=11, wall=718\r\n2020-07-17 04:10:13 | INFO | train_inner | epoch 001:   1900 / 2952000 loss=10.954, ppl=1984.24, wps=65.4, ups=0.03, wpb=2017, bsz=2, num_updates=1900, lr=1.9e-05, gnorm=2.008, train_wall=11, wall=749\r\n2020-07-17 04:10:44 | INFO | train_inner | epoch 001:   2000 / 2952000 loss=11.478, ppl=2851.54, wps=63.6, ups=0.03, wpb=1964, bsz=2, num_updates=2000, lr=2e-05, gnorm=2.558, train_wall=11, wall=780\r\n2020-07-17 04:11:15 | INFO | train_inner | epoch 001:   2100 / 2952000 loss=9.209, ppl=591.91, wps=64.8, ups=0.03, wpb=2003, bsz=2, num_updates=2100, lr=2.1e-05, gnorm=4.174, train_wall=11, wall=811\r\n2020-07-17 04:11:46 | INFO | train_inner | epoch 001:   2200 / 2952000 loss=10.191, ppl=1169.15, wps=64.8, ups=0.03, wpb=1999, bsz=2, num_updates=2200, lr=2.2e-05, gnorm=1.466, train_wall=11, wall=842\r\n2020-07-17 04:12:17 | INFO | train_inner | epoch 001:   2300 / 2952000 loss=8.639, ppl=398.56, wps=62.9, ups=0.03, wpb=1942, bsz=2, num_updates=2300, lr=2.3e-05, gnorm=1.759, train_wall=11, wall=873\r\n\r\nGPU log:\r\n2020-07-17 04:08:14 | INFO | train_inner | epoch 001:    100 / 2952087 loss=16.348, ppl=83437.5, wps=8640.2, ups=4.66, wpb=1854, bsz=2, num_updates=100, lr=1e-06, gnorm=4.215, loss_scale=128, train_wall=22, wall=120\r\n2020-07-17 04:08:36 | INFO | train_inner | epoch 001:    200 / 2952087 loss=14.736, ppl=27279.9, wps=8800.8, ups=4.69, wpb=1878.2, bsz=2, num_updates=200, lr=2e-06, gnorm=2.849, loss_scale=128, train_wall=21, wall=141\r\n2020-07-17 04:08:57 | INFO | train_inner | epoch 001:    300 / 2952087 loss=13.761, ppl=13882.4, wps=8737.9, ups=4.68, wpb=1868.8, bsz=2, num_updates=300, lr=3e-06, gnorm=2.349, loss_scale=128, train_wall=21, wall=163\r\n2020-07-17 04:09:19 | INFO | train_inner | epoch 001:    400 / 2952087 loss=13.199, ppl=9403.66, wps=8783.3, ups=4.66, wpb=1883.9, bsz=2, num_updates=400, lr=4e-06, gnorm=2.206, loss_scale=128, train_wall=21, wall=184\r\n2020-07-17 04:09:40 | INFO | train_inner | epoch 001:    500 / 2952087 loss=12.839, ppl=7328.79, wps=8732.4, ups=4.66, wpb=1873.4, bsz=2, num_updates=500, lr=5e-06, gnorm=2.079, loss_scale=128, train_wall=21, wall=205\r\n2020-07-17 04:10:02 | INFO | train_inner | epoch 001:    600 / 2952087 loss=12.426, ppl=5504.27, wps=8684.1, ups=4.66, wpb=1864.6, bsz=2, num_updates=600, lr=6e-06, gnorm=1.973, loss_scale=128, train_wall=21, wall=227\r\n2020-07-17 04:10:23 | INFO | train_inner | epoch 001:    700 / 2952087 loss=11.955, ppl=3969.9, wps=8706.6, ups=4.66, wpb=1869.2, bsz=2, num_updates=700, lr=7e-06, gnorm=1.82, loss_scale=128, train_wall=21, wall=248\r\n2020-07-17 04:10:44 | INFO | train_inner | epoch 001:    800 / 2952087 loss=11.481, ppl=2858.64, wps=8703.8, ups=4.65, wpb=1871.2, bsz=2, num_updates=800, lr=8e-06, gnorm=1.606, loss_scale=128, train_wall=21, wall=270\r\n2020-07-17 04:11:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\r\n2020-07-17 04:11:06 | INFO | train_inner | epoch 001:    901 / 2952087 loss=11.042, ppl=2108.45, wps=8672.3, ups=4.6, wpb=1884.5, bsz=2, num_updates=900, lr=9e-06, gnorm=1.521, loss_scale=110, train_wall=22, wall=292\r\n2020-07-17 04:11:28 | INFO | train_inner | epoch 001:   1001 / 2952087 loss=10.691, ppl=1653.52, wps=8731.1, ups=4.64, wpb=1881.1, bsz=2, num_updates=1000, lr=1e-05, gnorm=1.324, loss_scale=64, train_wall=21, wall=313\r\n2020-07-17 04:11:49 | INFO | train_inner | epoch 001:   1101 / 2952087 loss=10.477, ppl=1425.28, wps=8661.6, ups=4.64, wpb=1865.9, bsz=2, num_updates=1100, lr=1.1e-05, gnorm=1.29, loss_scale=64, train_wall=21, wall=335\r\n2020-07-17 04:12:11 | INFO | train_inner | epoch 001:   1201 / 2952087 loss=10.289, ppl=1251.5, wps=8674.2, ups=4.64, wpb=1869, bsz=2, num_updates=1200, lr=1.2e-05, gnorm=1.272, loss_scale=64, train_wall=21, wall=356\r\n2020-07-17 04:12:32 | INFO | train_inner | epoch 001:   1301 / 2952087 loss=10.146, ppl=1132.95, wps=8659.2, ups=4.64, wpb=1865.1, bsz=2, num_updates=1300, lr=1.3e-05, gnorm=1.263, loss_scale=64, train_wall=21, wall=378\r\n2020-07-17 04:12:54 | INFO | train_inner | epoch 001:   1401 / 2952087 loss=10.017, ppl=1035.98, wps=8641.4, ups=4.64, wpb=1862.1, bsz=2, num_updates=1400, lr=1.4e-05, gnorm=1.248, loss_scale=64, train_wall=21, wall=399\r\n2020-07-17 04:13:16 | INFO | train_inner | epoch 001:   1501 / 2952087 loss=9.973, ppl=1004.81, wps=8698.6, ups=4.64, wpb=1875.2, bsz=2, num_updates=1500, lr=1.5e-05, gnorm=1.287, loss_scale=64, train_wall=21, wall=421\r\n2020-07-17 04:13:37 | INFO | train_inner | epoch 001:   1601 / 2952087 loss=9.916, ppl=965.88, wps=8645.5, ups=4.64, wpb=1863.9, bsz=2, num_updates=1600, lr=1.6e-05, gnorm=1.31, loss_scale=64, train_wall=21, wall=442\r\n2020-07-17 04:13:59 | INFO | train_inner | epoch 001:   1701 / 2952087 loss=9.782, ppl=880.15, wps=8619.2, ups=4.63, wpb=1860.5, bsz=2, num_updates=1700, lr=1.7e-05, gnorm=1.31, loss_scale=64, train_wall=21, wall=464\r\n2020-07-17 04:14:20 | INFO | train_inner | epoch 001:   1801 / 2952087 loss=9.826, ppl=907.57, wps=8653.1, ups=4.64, wpb=1863.2, bsz=2, num_updates=1800, lr=1.8e-05, gnorm=1.391, loss_scale=64, train_wall=21, wall=486\r\n2020-07-17 04:14:42 | INFO | train_inner | epoch 001:   1901 / 2952087 loss=9.69, ppl=826.16, wps=8686.3, ups=4.64, wpb=1871.7, bsz=2, num_updates=1900, lr=1.9e-05, gnorm=1.407, loss_scale=64, train_wall=21, wall=507\r\n2020-07-17 04:15:03 | INFO | train_inner | epoch 001:   2001 / 2952087 loss=9.64, ppl=797.68, wps=8702.3, ups=4.64, wpb=1875.5, bsz=2, num_updates=2000, lr=2e-05, gnorm=1.389, loss_scale=64, train_wall=21, wall=529\r\n2020-07-17 04:15:25 | INFO | train_inner | epoch 001:   2101 / 2952087 loss=9.503, ppl=725.43, wps=8631.3, ups=4.64, wpb=1860.1, bsz=2, num_updates=2100, lr=2.1e-05, gnorm=1.434, loss_scale=64, train_wall=21, wall=550\r\n2020-07-17 04:15:46 | INFO | train_inner | epoch 001:   2201 / 2952087 loss=9.366, ppl=660.07, wps=8743.7, ups=4.64, wpb=1883.9, bsz=2, num_updates=2200, lr=2.2e-05, gnorm=1.426, loss_scale=64, train_wall=21, wall=572\r\n2020-07-17 04:16:08 | INFO | train_inner | epoch 001:   2301 / 2952087 loss=9.4, ppl=675.6, wps=8639.4, ups=4.64, wpb=1861.4, bsz=2, num_updates=2300, lr=2.3e-05, gnorm=1.379, loss_scale=64, train_wall=21, wall=593\r\n\r\n\r\nHave you seen anything similar? @myleott \r\nAny thoughts?\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2332/comments",
    "author": "kkissmart",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-07-16T20:09:52Z",
        "body": "What training command did you run? Are you using bfloat16 by any chance? If so, I highly recommend trying without it."
      },
      {
        "user": "kkissmart",
        "created_at": "2020-07-16T20:23:58Z",
        "body": "i tried both with and without\r\n\r\nhere is the command:\r\n\r\npython train.py /home/xwu/data-bin --encoder-normalize-before --decoder-normalize-before --arch mbart_base --layernorm-embedding --task multilingual_denoising --criterion cross_entropy --dataset-impl mmap  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' --lr-scheduler polynomial_decay --lr 1e-04 --min-lr -1 --warmup-updates 5000 --total-num-update 500000 --dropout 0.0 --attention-dropout 0.0 --weight-decay 0.0 --max-tokens 4104  --seed 222 --log-format simple --log-interval 100  --add-lang-token --no-whole-word-mask-langs JA  --mask 0.35 --permute-sentences 1.0 --mask-length span-poisson --replace-length 1 --rotate 0.0 --max-source-positions 1026 --max-target-positions 1026 --tokens-per-sample 1026 --sample-break-mode complete  --save-interval-updates 5000 --skip-invalid-size-inputs-valid-test --langs EN,JA  --multilang-sampling-alpha 0.7 --distributed-world-size 8  --tpu  --no-bos  --save-dir $HOME  --num-buckets 2 --no-input-eos --share-all-embeddings  --max-update 500000"
      },
      {
        "user": "myleott",
        "created_at": "2020-07-16T21:05:57Z",
        "body": "Hmm, I've never tried mbart with TPUs. One observation: it doesn't seem like those two logs you shared are really comparable. The bsz on the GPU log is bsz=2, but on the TPU log is bsz=128. The learning rates also seem to be different.\r\n\r\n"
      },
      {
        "user": "kkissmart",
        "created_at": "2020-07-16T21:42:05Z",
        "body": "@myleott @taylanbil \r\nI updated the logs to make sure they are apple2apple comparison. \r\n\r\nThanks!"
      }
    ]
  },
  {
    "number": 2329,
    "title": "Checkpoint file saved each node when running distributed training for 2 nodes. Is this right?",
    "created_at": "2020-07-15T07:47:35Z",
    "closed_at": "2020-07-17T18:08:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2329",
    "body": "#### What is your question?\r\n- While running the distributed training in 2 nodes(2 gpu server), the checkpoint files are stored in each node(Master node, Other node separately). And the number of checkpoint files are different. Is it working properly?\r\n\r\n#### Code\r\n```\r\n[Node1 - Master (4GPUs)]\r\nexport MKL_THREADING_LAYER=GNU\r\nexport NCCL_DEBUG=info\r\nexport NCCL_SOCKET_IFNAME=bond1\r\ntime CUDA_VISIBLE_DEVICES=4,5,6,7 \\\r\n\tpython -m torch.distributed.launch --nproc_per_node=1 --nnodes=2 --node_rank=0 --master_addr=\"master_node_ip\" --master_port=9000 \\\r\n\t${FAIRSEQ_TRAIN} \\\r\n        ${DATA_DIR} \\\r\n        --arch transformer \\\r\n\t--distributed-init-method=\"tcp://master_node_ip:9000\" \\\r\n\t--optimizer adam \\\r\n        ...(training parameters)\r\n```\r\n```\r\n[Node2 - Other (8GPUs), Docker]\r\nexport MKL_THREADING_LAYER=GNU\r\nexport NCCL_SOCKET_IFNAME=eth0,docker0\r\nexport NCCL_DEBUG=info\r\ntime CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\n\tpython -m torch.distributed.launch --nproc_per_node=1 --nnodes=2 --node_rank=1 --master_addr=\"master_node_ip\" --master_port=9000 \\\r\n\t${FAIRSEQ_TRAIN} \\\r\n        ${DATA_DIR} \\\r\n        --arch transformer \\\r\n\t--distributed-init-method=\"tcp://master_node_ip:9000\" \\\r\n\t--optimizer adam \\\r\n        ...(training parameters)\r\n```\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version : 0.9.0\r\n - PyTorch Version : 1.4.0\r\n - OS (e.g., Linux):  Ubuntu\r\n - How you installed fairseq (`pip`, source): `pip install fairseq`\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: Master - Titan XP, Other - Tesla v100\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2329/comments",
    "author": "Yeom",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-07-16T20:08:07Z",
        "body": "Hmm, that doesn't seem right. In the training log there should be a line: \"training on 12 devices (GPUs/TPUs)\", do you see 12 or a different number?"
      },
      {
        "user": "Yeom",
        "created_at": "2020-07-17T07:28:15Z",
        "body": "```\r\n| distributed init (rank 4): tcp://master_ip:9000\r\n| initialized host <2nd node server> as rank 4\r\n| distributed init (rank 7): tcp://master_ip:9000\r\n| initialized host <2nd node server> as rank 7\r\n| distributed init (rank 3): tcp://master_ip:9000\r\n| initialized host <2nd node server> as rank 3\r\n| distributed init (rank 0): tcp://master_ip:9000\r\n| initialized host <2nd node server> as rank 0\r\n| distributed init (rank 5): tcp://master_ip:9000\r\n| initialized host <2nd node server> as rank 5\r\n| distributed init (rank 6): tcp://master_ip:9000\r\n| initialized host <2nd node server> as rank 6\r\n| distributed init (rank 2): tcp://master_ip:9000\r\n| initialized host <2nd node server> as rank 2\r\n| distributed init (rank 1): tcp://master_ip:9000\r\n| initialized host <2nd node server> as rank 1\r\n<2nd node server>:5359:5359 [0] NCCL INFO Bootstrap : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5359:5359 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n<2nd node server>:5359:5359 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n<2nd node server>:5359:5359 [0] NCCL INFO NET/Socket : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\nNCCL version 2.4.8+cuda10.1\r\n<2nd node server>:5365:5365 [6] NCCL INFO Bootstrap : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5365:5365 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n<2nd node server>:5365:5365 [6] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n<2nd node server>:5365:5365 [6] NCCL INFO NET/Socket : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5359:5409 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,ffffffff\r\n<2nd node server>:5365:5410 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffffffff\r\n<2nd node server>:5360:5360 [1] NCCL INFO Bootstrap : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5360:5360 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n<2nd node server>:5360:5360 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n<2nd node server>:5360:5360 [1] NCCL INFO NET/Socket : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5363:5363 [4] NCCL INFO Bootstrap : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5363:5363 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n<2nd node server>:5363:5363 [4] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n<2nd node server>:5363:5363 [4] NCCL INFO NET/Socket : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5363:5412 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffffffff\r\n<2nd node server>:5360:5411 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,ffffffff\r\n<2nd node server>:5366:5366 [7] NCCL INFO Bootstrap : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5366:5366 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n<2nd node server>:5366:5366 [7] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n<2nd node server>:5366:5366 [7] NCCL INFO NET/Socket : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5364:5364 [5] NCCL INFO Bootstrap : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5364:5364 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n<2nd node server>:5364:5364 [5] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n<2nd node server>:5364:5364 [5] NCCL INFO NET/Socket : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5362:5362 [3] NCCL INFO Bootstrap : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5362:5362 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n<2nd node server>:5362:5362 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n<2nd node server>:5362:5362 [3] NCCL INFO NET/Socket : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5366:5413 [7] NCCL INFO Setting affinity for GPU 7 to ffffffff,ffffffff\r\n<2nd node server>:5364:5414 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffffffff\r\n<2nd node server>:5361:5361 [2] NCCL INFO Bootstrap : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5361:5361 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n<2nd node server>:5361:5361 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n<2nd node server>:5361:5361 [2] NCCL INFO NET/Socket : Using [0]eth0:node2_ip<0> [1]docker0:172.17.0.1<0>\r\n<2nd node server>:5362:5415 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff\r\n<2nd node server>:5361:5416 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,ffffffff\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 00 :    0   1   2   3   7   5   6   4\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 01 :    0   2   6   7   4   5   1   3\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 02 :    0   3   1   5   4   7   6   2\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 03 :    0   3   2   1   5   6   7   4\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 04 :    0   4   6   5   7   3   2   1\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 05 :    0   4   7   6   5   1   2   3\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 06 :    0   1   2   3   7   5   6   4\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 07 :    0   2   6   7   4   5   1   3\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 08 :    0   3   1   5   4   7   6   2\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 09 :    0   3   2   1   5   6   7   4\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 10 :    0   4   6   5   7   3   2   1\r\n<2nd node server>:5359:5409 [0] NCCL INFO Channel 11 :    0   4   7   6   5   1   2   3\r\n<2nd node server>:5363:5412 [4] NCCL INFO Ring 00 : 4[4] -> 0[0] via P2P/IPC\r\n...\r\n<2nd node server>:5360:5411 [1] NCCL INFO Ring 11 : 1[1] -> 2[2] via P2P/IPC\r\n<2nd node server>:5359:5409 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled\r\n<2nd node server>:5363:5412 [4] NCCL INFO comm 0x7fe6b00025f0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 - Init COMPLETE\r\n<2nd node server>:5366:5413 [7] NCCL INFO comm 0x7fcfcc0025f0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 - Init COMPLETE\r\n<2nd node server>:5365:5410 [6] NCCL INFO comm 0x7f7cfc0025f0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 - Init COMPLETE\r\n<2nd node server>:5361:5416 [2] NCCL INFO comm 0x7f3c980025f0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE\r\n<2nd node server>:5364:5414 [5] NCCL INFO comm 0x7f51d80025f0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 - Init COMPLETE\r\n<2nd node server>:5359:5409 [0] NCCL INFO comm 0x7f9a480025f0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\n<2nd node server>:5362:5415 [3] NCCL INFO comm 0x7fea000025f0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE\r\n<2nd node server>:5360:5411 [1] NCCL INFO comm 0x7f370c0025f0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\n<2nd node server>:5359:5359 [0] NCCL INFO Launch mode Parallel\r\n...\r\n...\r\n| model transformer, criterion LabelSmoothedCrossEntropyCriterion\r\n| num. model params: 56934400 (num. trained: 56934400)\r\n| training on 8 GPUs\r\n| max tokens per GPU = 4000 and max sentences per GPU = None\r\n```\r\n- According to above log, i think the training done in single node. Not distributed training.\r\n\r\nQuestion)\r\n- Is there any problem in my execution script??\r\n\r\nThank you.\r\nYeom."
      },
      {
        "user": "myleott",
        "created_at": "2020-07-17T18:08:43Z",
        "body": "Unfortunately torch.distributed.launch doesn't seem to work with uneven numbers of GPUs.\r\n\r\nFairseq can do it though, just run:\r\n```\r\n[Node1 - Master (4GPUs)]\r\nCUDA_VISIBLE_DEVICES=4,5,6,7 python ${FAIRSEQ_TRAIN} \\\r\n    (...) \\\r\n    --distributed-init-method=\"tcp://master_node_ip:9000\" \\\r\n    --distributed-world-size 12 \\\r\n    --distributed-rank 0\r\n```\r\n\r\nThe second node should set `--distributed-rank` equal to the starting global rank on the node. In this case, node 2 will be ranks 4-11, so we set `--distributed-rank=4`:\r\n```\r\n[Node2 - Other (8GPUs), Docker]\r\npython ${FAIRSEQ_TRAIN} \\\r\n    (...) \\\r\n    --distributed-init-method=\"tcp://master_node_ip:9000\" \\\r\n    --distributed-world-size 12 \\\r\n    --distributed-rank 4\r\n```"
      }
    ]
  },
  {
    "number": 2328,
    "title": "How to Fine-tune Models in Torch Hub with Python instead of CLI?",
    "created_at": "2020-07-15T04:46:48Z",
    "closed_at": "2021-08-03T11:40:54Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2328",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2328/comments",
    "author": "leloykun",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-07-16T20:11:11Z",
        "body": "This is not supported. We currently require use of the CLI tools to launch training. We are working on a project to make the Trainer work as a standalone component (more like a library), but it isn't ready yet."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:41Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      }
    ]
  },
  {
    "number": 2317,
    "title": "How to obtain [CLS] embedding result from a RoBERTa model?",
    "created_at": "2020-07-11T11:22:38Z",
    "closed_at": "2020-07-16T20:13:20Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2317",
    "body": "I was using RoBERTa to obtains sentences embedding for further application. However, when the input has a huge number of sentences (e.g., 100k or 10m), it cannot be accommodated in the memory.\r\n\r\nFor example:\r\nIf we have only one sentence `'How are you'`, the tokens are:\r\n`[   0, 6179,   32,   47,    2]` and the embedding result size is `[1, 5, 768]`\r\nBut when there are 100k sentences with tokens length 200, the embedding result size would come to `[100k, 200, 768]`\r\n\r\nI noticed that many resources indicates that `[CLS]` represents the summary of the sentence (i.e., pooled embedding result of one sentence). Namely, I can reduce the original size from `[100k, 200, 768]` to `[100k, 1, 768]`\r\n\r\nHowever I am not sure how to obtain this [CLS] embedding result.\r\n**Is it equivalent to [:, 0, :] from [100k, 200, 768]?**\r\n\r\nThank you in advance.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2317/comments",
    "author": "robertauser",
    "comments": [
      {
        "user": "shamanez",
        "created_at": "2020-07-13T04:04:03Z",
        "body": "Yes, That is how you extract the first token."
      },
      {
        "user": "myleott",
        "created_at": "2020-07-16T20:13:20Z",
        "body": "Yep, `[:, 0, :]` will get you the first token, but unless you've finetune the model it won't mean anything. This is because RoBERTa isn't trained with a next sentence prediction objective, so the `[CLS]` token has a meaningless representation until you finetune it."
      }
    ]
  },
  {
    "number": 2312,
    "title": "Possible training of transformer model with model parallelism?",
    "created_at": "2020-07-09T14:53:40Z",
    "closed_at": "2020-07-10T09:45:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2312",
    "body": "I tried out to train the transformer_lm model with model parallelism and it worked out fine. Is it also possible to run the transformer seq2seq model with model parallelism (code in the same directory fairseq/model_parallel/models)?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2312/comments",
    "author": "thies1006",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-07-09T19:46:08Z",
        "body": "Yes, it should work, we just don't have an \"example\" for it. Please try it out and let us know how it goes! :)"
      },
      {
        "user": "thies1006",
        "created_at": "2020-07-10T09:45:32Z",
        "body": "Thanks! \r\n\r\nI was a bit unsettled yesterday, because the code seems to lack the block with @register_model_architecture. So I added this block like in the transformer_lm example (using base_architecture instead of base_lm_architecture). I also had to match the vocabulary size with the number of GPUs (has to be divisible). I am running now the translation script from the examples (German-English) adding the relevant options (distributed-world-size, model-parallel-size, memory-efficient-fp16, num-workers). I did not change the criterion. I see the GPUs working and translations from the evaluation step look fine, so I guess it is working. \r\n\r\nJust in case there is something wrong with what I did please comment.\r\nAmazing work guys. :)"
      }
    ]
  },
  {
    "number": 2301,
    "title": "Question about how to install libnat correctly on a cpu-only machine",
    "created_at": "2020-07-04T04:27:16Z",
    "closed_at": "2020-07-04T04:39:55Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2301",
    "body": "Hi,\r\n\r\nI hope to run lev_transformer on a cpu-only machine. After I install libnat on my machine, I met the error of \"cannot import name 'libnat_cuda'... fall back to cpu version\". How to solve it? Thanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2301/comments",
    "author": "dmortem",
    "comments": [
      {
        "user": "nomadlx",
        "created_at": "2020-07-23T06:52:29Z",
        "body": "I met to the same problem, and the answer of the same problem in history does not apply to me. \r\nHow did you solve it?"
      }
    ]
  },
  {
    "number": 2300,
    "title": "Model crashed while getting embeddings",
    "created_at": "2020-07-03T23:20:42Z",
    "closed_at": "2020-07-15T20:42:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2300",
    "body": "I trained a wav2vec model and want to get its embeddings out. The example code on the readme crashed with the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \".\\pretraining.py\", line 14, in <module>\r\n    _, idxs = model.vector_quantizer.forward_idx(z)\r\nAttributeError: 'NoneType' object has no attribute 'forward_idx'\r\n```\r\nThe code I used to produce the above error,\r\n```\r\nimport os\r\nimport torch\r\nimport librosa\r\nfrom fairseq.models.wav2vec import Wav2VecModel\r\n\r\ncp = torch.load('path-to/checkpoint_best.pt')#'/path/to/vq-wav2vec.pt'\r\nmodel = Wav2VecModel.build_model(cp['args'], task=None)\r\nmodel.load_state_dict(cp['model'])\r\nmodel.eval()\r\n\r\nfile = os.listdir(\"path-to/wav-files/\")[0]\r\nx, fs = librosa.load(\"path-to/wav-files/\" + file, sr = 44100) \r\nx = torch.tensor(x).view(1, -1)\r\nz = model.feature_extractor(x)\r\n_, idxs = model.vector_quantizer.forward_idx(z)\r\nprint(idxs.shape) \r\n```\r\nI tried printing out the attributes of the object model... using ```print(model.__dict__)```\r\n```\r\n{'training': False, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_backward_hooks': OrderedDict(), '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('feature_extractor', ConvFeatureExtractionModel(\r\n  (conv_layers): ModuleList(\r\n    (0): Sequential(\r\n      (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\r\n      (1): Dropout(p=0.0, inplace=False)\r\n      (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (3): ReLU()\r\n    )\r\n    (1): Sequential(\r\n      (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)\r\n      (1): Dropout(p=0.0, inplace=False)\r\n      (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (3): ReLU()\r\n    )\r\n    (2): Sequential(\r\n      (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\r\n      (1): Dropout(p=0.0, inplace=False)\r\n      (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (3): ReLU()\r\n    )\r\n    (3): Sequential(\r\n      (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\r\n      (1): Dropout(p=0.0, inplace=False)\r\n      (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (3): ReLU()\r\n    )\r\n    (4): Sequential(\r\n      (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\r\n      (1): Dropout(p=0.0, inplace=False)\r\n      (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (3): ReLU()\r\n    )\r\n    (5): Sequential(\r\n      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\r\n      (1): Dropout(p=0.0, inplace=False)\r\n      (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (3): ReLU()\r\n    )\r\n    (6): Sequential(\r\n      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\r\n      (1): Dropout(p=0.0, inplace=False)\r\n      (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (3): ReLU()\r\n    )\r\n  )\r\n)), ('feature_aggregator', ConvAggegator(\r\n  (conv_layers): Sequential(\r\n    (0): Sequential(\r\n      (0): ReplicationPad1d((1, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (1): Sequential(\r\n      (0): ReplicationPad1d((2, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (2): Sequential(\r\n      (0): ReplicationPad1d((3, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (3): Sequential(\r\n      (0): ReplicationPad1d((4, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (4): Sequential(\r\n      (0): ReplicationPad1d((5, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (5): Sequential(\r\n      (0): ReplicationPad1d((6, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (6): Sequential(\r\n      (0): ReplicationPad1d((7, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (7): Sequential(\r\n      (0): ReplicationPad1d((8, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (8): Sequential(\r\n      (0): ReplicationPad1d((9, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (9): Sequential(\r\n      (0): ReplicationPad1d((10, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (10): Sequential(\r\n      (0): ReplicationPad1d((11, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n    (11): Sequential(\r\n      (0): ReplicationPad1d((12, 0))\r\n      (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))\r\n      (2): Dropout(p=0.0, inplace=False)\r\n      (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\r\n      (4): ReLU()\r\n    )\r\n  )\r\n  (residual_proj): ModuleList(\r\n    (0): None\r\n    (1): None\r\n    (2): None\r\n    (3): None\r\n    (4): None\r\n    (5): None\r\n    (6): None\r\n    (7): None\r\n    (8): None\r\n    (9): None\r\n    (10): None\r\n    (11): None\r\n  )\r\n)), ('wav2vec_predictions', Wav2VecPredictionsModel(\r\n  (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))\r\n  (dropout): Dropout(p=0.0, inplace=False)\r\n)), ('dropout_feats', Dropout(p=0.0, inplace=False)), ('dropout_agg', Dropout(p=0.0, inplace=False))]), '_is_generation_fast': False, 'prediction_steps': 12, 'vector_quantizer': None, 'project_features': None}\r\n```\r\nThe attribute 'vector_quantizer' does seem to have the value of None. Am I loading the model incorrectly? \r\n\r\nI trained the wav2vec model on Windows 10, python = 3.7, torch=1.5.1. I followed the steps mentioned in the readme. The model trained for 33 epochs. I stopped the training using a keyboard interrupt. \r\n\r\nI installed fairseq from source.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2300/comments",
    "author": "anuragkumar95",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-07-06T13:14:39Z",
        "body": "CC @alexeib "
      },
      {
        "user": "alexeib",
        "created_at": "2020-07-15T20:42:29Z",
        "body": "you trained a wav2vec model that does not have a quantizer (vq-wav2vec has quantizier). look at \"wav2vec_featurize.py\" for an example of how to extract features from wav2vec. it will look something like this:\r\n```  with torch.no_grad():\r\n         z = self.model.feature_extractor(x)\r\n         if isinstance(z, tuple):\r\n             z = z[0]\r\n         c = self.model.feature_aggregator(z)\r\n      return z, c\r\n```"
      }
    ]
  },
  {
    "number": 2299,
    "title": "preprocessing data with dictionary of \"transformer_lm.wmt19.en\": too many tokens are replaced by <unk>",
    "created_at": "2020-07-03T14:13:58Z",
    "closed_at": "2020-07-16T20:21:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2299",
    "body": "\r\nHello\r\n\r\nI'm trying to pre-process data using the given dictionary of wmt19en.\r\nWhile doing this I noticed that wmt19en dictionary has **42022 types** and because of this, many of tokens are replaced by <unk>. Then this resulting in very high perplexity. How I can prevent replacing too many tokens to <unk>?\r\n\r\n\r\nFor instance, when I pre-process without using the given dictionary from wmt19en or using a dictionary from adaptive_lm_wiki103.v2, there are **267744 types**.  But then I can't use this because the amount of dictionary is different from transformer_lm.wmt19.en.\r\n\r\n\r\nTo see easy, I just try to pre-proecess wiki-103 dataset with the dictionary from wmt19.en, below ;\r\n\r\n> `fairseq-preprocess     --only-source     --trainpref examples/language_model/wikitext-103/wiki.train.tokens     --validpref examples/language_model/wikitext-103/wiki.valid.tokens     --testpref examples/language_model/wikitext-103/wiki.test.tokens     --destdir data-bin/wikitext-for-wmt19 --srcdict data-bin/wmt19.en/dict.txt   --workers 20\r\n`\r\n\r\nThen, more than 17% of tokens are replaced to <unk>\r\n\r\n> `2020-07-03 16:07:53 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/wikitext-for-wmt19', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer='nag', padding_factor=8, profile=False, quantization_config_path=None, seed=1, source_lang=None, srcdict='data-bin/wmt19.en/dict.txt', target_lang=None, task='translation', tensorboard_logdir='', testpref='examples/language_model/wikitext-103/wiki.test.tokens', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='examples/language_model/wikitext-103/wiki.train.tokens', user_dir=None, validpref='examples/language_model/wikitext-103/wiki.valid.tokens', workers=20)\r\n2020-07-03 16:07:53 | INFO | fairseq_cli.preprocess | [None] Dictionary: 42022 types\r\n2020-07-03 16:09:04 | INFO | fairseq_cli.preprocess | [None] examples/language_model/wikitext-103/wiki.train.tokens: 1801350 sents, 103227021 tokens, 17.6% replaced by <unk>\r\n2020-07-03 16:09:04 | INFO | fairseq_cli.preprocess | [None] Dictionary: 42022 types\r\n2020-07-03 16:09:05 | INFO | fairseq_cli.preprocess | [None] examples/language_model/wikitext-103/wiki.valid.tokens: 3760 sents, 217646 tokens, 17.2% replaced by <unk>\r\n2020-07-03 16:09:05 | INFO | fairseq_cli.preprocess | [None] Dictionary: 42022 types\r\n2020-07-03 16:09:06 | INFO | fairseq_cli.preprocess | [None] examples/language_model/wikitext-103/wiki.test.tokens: 4358 sents, 245569 tokens, 17.6% replaced by <unk>\r\n2020-07-03 16:09:06 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wikitext-for-wmt19`\r\n\r\nTo sum up, \r\n\r\n1. Is there any way to prevent replacing too many tokens to <unk> ?\r\n\r\n2. Also, is there any way to not add <unk> for using 'fairseq-eval-lm' ?\r\n\r\n\r\nThank you in advance! \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2299/comments",
    "author": "Sohyo",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-07-16T20:21:05Z",
        "body": "That model has a byte-pair encoded (BPE) vocabulary, so you need to first apply BPE and then tokenize.\r\n\r\nFor example:\r\n\r\n```python\r\nen_lm = torch.hub.load('pytorch/fairseq', 'transformer_lm.wmt19.en')\r\n\r\n# if you try to encode raw text you'll get unknowns\r\n>>> en_lm.tgt_dict.string(en_lm.binarize(\"Hello world\"))\r\n'<unk> world'\r\n\r\n# but if you first apply BPE\r\n>>> en_lm.bpe.encode(\"Hello world\")\r\n'H@@ ello world'\r\n\r\n# then it will work\r\n>>> en_lm.tgt_dict.string(en_lm.binarize(\"H@@ ello world\"))\r\n'H@@ ello world'\r\n\r\n# you can remove the BPE by replacing \"@@ \" with \"\" or calling bpe.decode\r\n>>> en_lm.bpe.decode(\"H@@ ello world\")\r\n'Hello world'\r\n```"
      }
    ]
  },
  {
    "number": 2294,
    "title": "why I got different size of sents for training on wmt17_en_zh",
    "created_at": "2020-07-02T10:45:01Z",
    "closed_at": "2020-07-16T21:15:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2294",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nWhen I preprocess the wmt17_en_zh data using the following command:\r\n\r\nfairseq-preprocess --source-lang en --target-lang zh  --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test   --destdir data-bin --workers 3 --tokenizer space --thresholdtgt 0 --task translation\r\n\r\nAnd I got the results:\r\n2020-07-02 18:38:26 | INFO | fairseq_cli.preprocess | [en] Dictionary: 80128 types\r\n2020-07-02 18:38:31 | INFO | fairseq_cli.preprocess | [en] /home/anbo/nmt/data/wmt17_en_zh/train.en: 227062 sents, 5960840 tokens, 0.0% replaced by <unk>\r\n2020-07-02 18:38:31 | INFO | fairseq_cli.preprocess | [en] Dictionary: 80128 types\r\n2020-07-02 18:38:32 | INFO | fairseq_cli.preprocess | [en] /home/anbo/nmt/data/wmt17_en_zh/valid.en: 2002 sents, 60441 tokens, 3.32% replaced by <unk>\r\n2020-07-02 18:38:32 | INFO | fairseq_cli.preprocess | [en] Dictionary: 80128 types\r\n2020-07-02 18:38:33 | INFO | fairseq_cli.preprocess | [en] /home/anbo/nmt/data/wmt17_en_zh/test.en: 2001 sents, 55915 tokens, 3.42% replaced by <unk>\r\n2020-07-02 18:38:33 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 79312 types\r\n2020-07-02 18:38:39 | INFO | fairseq_cli.preprocess | [zh] /home/anbo/nmt/data/wmt17_en_zh/train.zh: 185743 sents, 4564331 tokens, 0.00532% replaced by <unk>\r\n2020-07-02 18:38:39 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 79312 types\r\n2020-07-02 18:38:40 | INFO | fairseq_cli.preprocess | [zh] /home/anbo/nmt/data/wmt17_en_zh/valid.zh: 2002 sents, 52303 tokens, 5.95% replaced by <unk>\r\n2020-07-02 18:38:40 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 79312 types\r\n2020-07-02 18:38:41 | INFO | fairseq_cli.preprocess | [zh] /home/anbo/nmt/data/wmt17_en_zh/test.zh: 2001 sents, 49145 tokens, 6.89% replaced by <unk>\r\n2020-07-02 18:38:41 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\r\n\r\nwhich means that I got different size of sents for English and Chinese.\r\n\r\nI have segment the Chinese data using jieba, and the number of data as follows:\r\n2001 test.en\r\n    2001 test.zh\r\n  227062 train.en\r\n  227062 train.zh\r\n    2002 valid.en\r\n    2002 valid.zh\r\n\r\nSo any suggestion?\r\n\r\nThe last version of fairseq.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2294/comments",
    "author": "anbo724",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-07-02T13:36:53Z",
        "body": "Is there a specific example you are following?  Please provide the original data source where you downloaded the data and any kind of pre-processing you did to the data prior to calling fairseq-preprocess."
      },
      {
        "user": "myleott",
        "created_at": "2020-07-16T21:14:30Z",
        "body": "This isn't something we've seen before. Is it possible you have empty lines in your train.zh file? Please double check the data and reopen the Issue if it seems like a fairseq bug."
      },
      {
        "user": "afaq-ahmad",
        "created_at": "2021-01-04T10:08:49Z",
        "body": "So I also face the same issue, but after removing '\\r' from the text my problem solve"
      }
    ]
  },
  {
    "number": 2291,
    "title": "what is the length of GLUE dataset such as SST during fine-tuning ?",
    "created_at": "2020-07-01T06:35:35Z",
    "closed_at": "2020-07-01T13:33:25Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2291",
    "body": "The input sequence length during pretraining is 512, however, what is the length of SST task during fine-tuning ? Is it 512? While the average length of SST dataset is 19, is it a waist of computation resource ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2291/comments",
    "author": "lshowway",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-07-01T13:33:25Z",
        "body": "I'm not sure I follow.  Please follow the issue templates. "
      }
    ]
  },
  {
    "number": 2285,
    "title": "libcudart.so.10.1: cannot open shared object file: No such file or directory",
    "created_at": "2020-06-30T00:22:20Z",
    "closed_at": "2020-06-30T20:28:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2285",
    "body": "I just upgraded CUDA version from 10.1 to 10.2 since apex installation keeps encounter bugs for no reason. But I cannot figure out where torch.hub.load calling for libcudart.so.10.1 and raising the bug. Any insight to reinstall or build dependencies is appreciated. \r\n#### Code\r\n\r\nimport torch\r\n\r\ntorch.hub.list('pytorch/fairseq')  # [..., 'lightconv.glu.wmt17.zh-en', ... ]\r\n\r\nzh2en = torch.hub.load('pytorch/fairseq', 'lightconv.glu.wmt17.zh-en', tokenizer='moses', bpe='subword_nmt')\r\n\r\nassert isinstance(zh2en.models[0], fairseq.models.lightconv.LightConvModel)\r\n\r\nzh2en.translate('你好 世界')\r\n\r\n#### What have you tried?\r\nnvcc --version\r\n10.2\r\ntorch.version.cuda\r\n10.2\r\n#### What's your environment?\r\n - fairseq Version (master):\r\n - PyTorch Version (1.5.1)\r\n - OS (Linux):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version:10.2",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2285/comments",
    "author": "liyy201912",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-06-30T13:40:32Z",
        "body": "Can you reproduce this in pytorch alone?  Something like:\r\n\r\n```Python\r\nimport torch\r\nx = torch.rand(5, 5).cuda()\r\ntorch.mm(x, x)\r\n```\r\n\r\nOr is this only happening when trying to use fairseq?  How did you install pytorch?  If you want to use CUDA 10.2 I think you need to explicitly specify when installing: `conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\r\n`"
      },
      {
        "user": "liyy201912",
        "created_at": "2020-06-30T17:20:35Z",
        "body": "> Can you reproduce this in pytorch alone? Something like:\r\n> \r\n> ```python\r\n> import torch\r\n> x = torch.rand(5, 5).cuda()\r\n> torch.mm(x, x)\r\n> ```\r\n> \r\n> Or is this only happening when trying to use fairseq? How did you install pytorch? If you want to use CUDA 10.2 I think you need to explicitly specify when installing: `conda install pytorch torchvision cudatoolkit=10.2 -c pytorch `\r\n\r\nThank you for your quick reply. Yes, that definitely works. This error only occurs when using some fairseq methods (fairseq-generate, and torch.hub.load). Surprisingly even fairseq-train works well. I'm wondering if some fairseq function is fixed to call libcudart.so.10.1 during installation, since cuda 10.1 does not exist anymore in my system. "
      },
      {
        "user": "myleott",
        "created_at": "2020-06-30T18:30:25Z",
        "body": "Probably some of the fairseq components need to be recompiled. Try `torch.hub.load(..., force_reload=True)`. Alternatively you may need to clone the fairseq source and run `pip install --editable .`."
      },
      {
        "user": "liyy201912",
        "created_at": "2020-06-30T18:37:28Z",
        "body": "> Probably some of the fairseq components need to be recompiled. Try `torch.hub.load(..., force_reload=True)`. Alternatively you may need to clone the fairseq source and run `pip install --editable .`.\r\n\r\nThanks, I'll have a try. "
      },
      {
        "user": "liyy201912",
        "created_at": "2020-06-30T20:28:51Z",
        "body": "Fixed, thanks. "
      }
    ]
  },
  {
    "number": 2278,
    "title": "Roberta: Outputs after BPE encoding step and Pre-processing (Binarize) steps",
    "created_at": "2020-06-28T05:42:56Z",
    "closed_at": "2022-04-18T05:21:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2278",
    "body": "In the fine-tuning steps of Roberta and BART, we first encode raw text into tokens using **roberta.multiprocessing_bpe_encoder**. Then we run **fairseq-preprocess** to binarize the data. Finally, we feed the tokens to the model using the binarized file in the training process. \r\n\r\nBut, when I check the tokens I get for a given sentence using BPE encoding and binarized files are not similar to each other. Why is that?   \r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2278/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-06-30T13:11:41Z",
        "body": "Can you post some examples of the inconsistencies?  Thanks!"
      },
      {
        "user": "shamanez",
        "created_at": "2020-06-30T23:54:23Z",
        "body": "Hi @lematt1991 ,\r\n\r\nThank you for your help.  Please go through the following example to understand the issue.\r\n\r\n\r\n**The input Sentence** \r\n\r\n\"What?!?? Why are people saying this is \"mind blowing?\" Just face it the ending is on of the worst endings in the history of cinematography! 4 left and the whole world has ended! Not to mention the character 9 was a idiot the whole time he got everyone killed. 1 was right the whole time, if he sacrificed 9 then non of this would have happened. People giving there lives for a stupid cause and for what?... to make it rain? I admit the movie had it's parts, and the whole concept was fascinating. But a lot of it was clichés one after another. And did anybody else get this feeling that this is a lot like \"Lord of The Rings?\" Characters died for stupid reasons, there was almost no character development, and honestly ask yourself is it good to have only four guys left in the world; its pointless and stupid. It was one of the shortest movies I've ever seen, and thank god! How is robots turning against humans creative in any way! It's been done like a hundred times! This movie is really stupid, go see a movie that's worth watching like Star Trek, The Hangover, or Inglorious Basterds, those were good movies!\"\r\n\r\n\r\n**Tokens I get after running  examples.roberta.multiprocessing_bpe_encoder**\r\n\r\n'2061 12248 3548 4162 389 661 2282 428 318 366 10155 19280 1701 2329 1986 340 262 7464 318 319 286 262 5290 38168 287 262 2106 286 13483 45501 0 604 1364 290 262 2187 995 468 4444 0 1892 284 3068 262 2095 860 373 257 22324 262 2187 640 339 1392 2506 2923 13 352 373 826 262 2187 640 11 611 339 27445 860 788 1729 286 428 561 423 3022 13 4380 3501 612 3160 329 257 8531 2728 290 329 644 30 986 284 787 340 6290 30 314 9159 262 3807 550 340 338 3354 11 290 262 2187 3721 373 13899 13 887 257 1256 286 340 373 35478 20954 530 706 1194 13 843 750 9599 2073 651 428 4203 326 428 318 257 1256 588 366 22438 286 383 26028 1701 26813 3724 329 8531 3840 11 612 373 2048 645 2095 2478 11 290 12698 1265 3511 318 340 922 284 423 691 1440 3730 1364 287 262 995 26 663 27158 290 8531 13 632 373 530 286 262 35581 6918 314 1053 1683 1775 11 290 5875 5770 0 1374 318 14193 6225 1028 5384 7325 287 597 835 0 632 338 587 1760 588 257 3470 1661 0 770 3807 318 1107 8531 11 467 766 257 3807 326 338 2861 4964 588 2907 12338 11 383 24300 2502 11 393 17589 4685 699 347 1603 9310 11 883 547 922 6918 0'\r\n\r\n**The output I get in the training phase (after fairseq-preprocess )**\r\n\r\n**After  print(sample['net_input']['src_tokens'])**\r\n\r\ntensor([[    0,  2264, 17516, 28749,  2612,    32,    82,   584,    42,    16,\r\n            22, 28583, 13897,  1917,  1801,   652,    24,     5,  3558,    16,\r\n            15,     9,     5,  2373, 41677,    11,     5,   750,     9, 18535,\r\n         34226,   328,   204,   314,     8,     5,  1086,   232,    34,  1249,\r\n           328,  1491,     7,  4521,     5,  2048,   361,    21,    10, 30603,\r\n             5,  1086,    86,    37,   300,   961,   848,     4,   112,    21,\r\n           235,     5,  1086,    86,     6,   114,    37, 26936,   361,   172,\r\n           786,     9,    42,    74,    33,  1102,     4,  1806,  1311,    89,\r\n          1074,    13,    10, 12103,  1303,     8,    13,    99,   116,   734,\r\n             7,   146,    24,  1895,   116,    38,  8109,     5,  1569,    56,\r\n            24,    18,  1667,     6,     8,     5,  1086,  4286,    21, 12509,\r\n             4,   125,    10,   319,     9,    24,    21, 43848,  5739,    65,\r\n            71,   277,     4,   178,   222,  5670,  1493,   120,    42,  2157,\r\n            14,    42,    16,    10,   319,   101,    22, 37565,     9,    20,\r\n         32106,  1917, 44217,   962,    13, 12103,  2188,     6,    89,    21,\r\n           818,   117,  2048,   709,     6,     8, 10728,  1394,  2512,    16,\r\n            24,   205,     7,    33,   129,   237,  1669,   314,    11,     5,\r\n           232,   131,    63, 31321,     8, 12103,     4,    85,    21,    65,\r\n             9,     5, 28617,  4133,    38,   348,   655,   450,     6,     8,\r\n          3392,  9069,   328,  1336,    16, 12129,  3408,   136,  5868,  3904,\r\n            11,   143,   169,   328,    85,    18,    57,   626,   101,    10,\r\n          6317,   498,   328,   152,  1569,    16,   269, 12103,     6,   213,\r\n           192,    10,  1569,    14,    18,   966,  2494,   101,  2141, 20351,\r\n             6,    20, 12403,  2137,     6,    50, 11996, 10159,  6514,   163,\r\n          8831, 11622,     6,   167,    58,   205,  4133,   328,     2]],\r\n       device='cuda:0')\r\n\r\n\r\nSo does fairseq-preprocess again assign tokens to its own dictionary?\r\n\r\n\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:52Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:45Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2271,
    "title": "Loading only encoder part of a pretrained enc-dec transformer model in fairseq ",
    "created_at": "2020-06-25T15:03:10Z",
    "closed_at": "2020-06-30T12:53:24Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2271",
    "body": "## ❓ Questions and Help\r\nI am new to fairseq and wanted to ask you how I can load an encoder part so that I can use it in another model. \r\n\r\nfrom fairseq.models.transformer import TransformerModel\r\nen2de = TransformerModel.from_pretrained(\"checkpoint/wmt16.en-de.joined-dict.transformer\",\r\n                                         checkpoint_file='model.pt',\r\n                                         data_name_or_path='checkpoint/wmt16.en-de.joined-dict.transformer',\r\n                                         bpe='subword_nmt',\r\n                                         bpe_codes='checkpoint/wmt16.en-de.joined-dict.transformer/bpecodes')\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2271/comments",
    "author": "semerekiros",
    "comments": [
      {
        "user": "mingruimingrui",
        "created_at": "2020-06-26T02:38:16Z",
        "body": "Hey @semerekiros perhaps you can load the entire model before extracting only the encoder?\r\nI think it would more more difficult to just load the encoder since fairseq saves model states (namespaces play a key part and there will be complexity involved when renaming them to only the portions involving the encoder). "
      },
      {
        "user": "lematt1991",
        "created_at": "2020-06-30T12:53:24Z",
        "body": "Yes, I would follow @mingruimingrui's advice."
      },
      {
        "user": "semerekiros",
        "created_at": "2020-06-30T22:04:16Z",
        "body": "Solved it. Thanks\r\n"
      }
    ]
  },
  {
    "number": 2269,
    "title": "How can I feed a binarized class label file to BART training?",
    "created_at": "2020-06-25T04:09:30Z",
    "closed_at": "2020-07-01T00:05:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2269",
    "body": "Is there any way that I can feed a label file to the training mechanism, Farrelly with source and target files.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2269/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "Vsanku01",
        "created_at": "2020-06-28T21:04:01Z",
        "body": "Could you be more specific, please!\r\n"
      },
      {
        "user": "shamanez",
        "created_at": "2020-06-30T02:51:17Z",
        "body": "@Vsanku01  Thank you for the interest.\r\n\r\nBasically I want to feed a class label for the source text. I am thinking about whether I can feed a class label, while feeding source and target text (similar to text generation or translation task) in the training time."
      },
      {
        "user": "lematt1991",
        "created_at": "2020-06-30T12:47:52Z",
        "body": "I think the easiest way would be to build this into your vocabulary.  For example, find a unique token (ex: `__class_label_0__`, `__class__label_1__`, ..., `__class_label_n__`) and prepend these special tokens on to the beginning (or end) of your sequences before calling `fairseq-preprocess`.  "
      },
      {
        "user": "shamanez",
        "created_at": "2020-06-30T23:26:07Z",
        "body": "Thank you very much."
      },
      {
        "user": "shamanez",
        "created_at": "2020-07-19T04:47:17Z",
        "body": "@lematt1991 \r\n\r\nHow can I create a unique token as you mentioned above?\r\n\r\nWhat if I append a token  like **\"__class_label_0__\"** to the text and then do the tokenization.\r\n\r\n"
      },
      {
        "user": "lematt1991",
        "created_at": "2020-07-19T14:15:34Z",
        "body": "> What if I append a token like \"class_label_0\" to the text and then do the tokenization.\r\n\r\nYep, that's exactly what I meant."
      },
      {
        "user": "shamanez",
        "created_at": "2020-08-24T09:03:49Z",
        "body": "Thanks a lot."
      }
    ]
  },
  {
    "number": 2263,
    "title": "phrase table",
    "created_at": "2020-06-23T04:19:45Z",
    "closed_at": "2022-04-19T11:21:26Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2263",
    "body": "Hi friends，\r\n\r\nI'd like to know how the tool handles the phrase table.\r\n\r\nthanks。",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2263/comments",
    "author": "sdlmw",
    "comments": [
      {
        "user": "sth4k",
        "created_at": "2020-10-06T10:36:17Z",
        "body": "hi i would also like to know"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T12:04:37Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T11:20:53Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2260,
    "title": "loss=nan in training after adding \"--max-source/target-positions 2k\" to my working model",
    "created_at": "2020-06-22T09:58:01Z",
    "closed_at": "2022-04-18T05:21:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2260",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues. DONE\r\n2. search the docs. DONE\r\n\r\n#### What is your question?\r\n\r\nI have a working transformer model, for a document level seq2seq simple task. I built a baseline to begin, with generic hiperparameters taken from a NMT example, which works quite well (I need to add BPE and more data, and tune training hyperparameters to improve it, but the model trains and generated somehow the expected output).\r\n\r\nBut as it is a document to document task, I need to increase the max seq length from 1024 to at least 2-3K (max length of the valid/test dataset), and ideally to 4k.\r\n\r\nI understand, from the documentation and some answers here, that that this is achieved using --max-source-positions and --max-target-positions in the default --task translation.\r\n\r\nBut when I try to train this model, my loss becomes in \"nan\" from the 1st batch.\r\n\r\nI don't know if i'm doing something wrong, I didn't understood how to use --max-target-positions correctly, I have to tune the hyperparameters of the transformer after increasing the length, or what's causing this nan loss problem.\r\n\r\nThaks for taking the time to help on this.\r\n\r\nGorka\r\n\r\n#### Code\r\n\r\nTHIS IS WORKING CORRECTLY:\r\n\r\n```\r\nfairseq-preprocess --source-lang src --target-lang trg --trainpref data/train_ARRAU_seq --validpref data/dev_ARRAU\r\n --testpref data/test_ARRAU --destdir data-bin/text2cor.seq --workers 8\r\n\r\nfairseq-train data-bin/text2cor.seq --arch transformer_iwslt_de_en --optimizer adam --adam-betas '(0.9, 0.98)' \r\n--clip-norm 0.1 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 \r\n--weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --no-epoch-checkpoints \r\n--num-workers 8 --max-epoch 10 --save-dir checkpoints/transformer_text2cor_seq \r\n--skip-invalid-size-inputs-valid-test --max-tokens 4096 --update-freq 8\r\n\r\nfairseq-generate data-bin/text2cor.seq --skip-invalid-size-inputs-valid-test \r\n--path checkpoints/transformer_text2cor_seq/checkpoint_best.pt --beam 5\r\n```\r\n\r\nTHIS IS WHERE I GOT THE NAN LOSS PROBLEM:\r\n\r\n`fairseq-train data-bin/text2cor.seq --arch transformer_iwslt_de_en --optimizer adam --adam-betas '(0.9, 0.98)' \r\n--clip-norm 0.1 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 \r\n--weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --no-epoch-checkpoints \r\n--num-workers 8 --max-epoch 10 --save-dir checkpoints/transformer_text2cor_seq \r\n--skip-invalid-size-inputs-valid-test --max-tokens 4096 --update-freq 8 \r\n--max-source-positions 2048 --max-target-positions 2048`\r\n\r\n...\r\n| [src] dictionary: 18264 types\r\n[trg] dictionary: 1728 types\r\n| loaded 18 examples from: data-bin/text2cor.seq/valid.src-trg.src\r\n| loaded 18 examples from: data-bin/text2cor.seq/valid.src-trg.trg\r\n| data-bin/text2cor.seq valid src-trg 18 examples`\r\n...\r\n| model transformer_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion                                                                                           | num. model params: 42663936 (num. trained: 42663936)                                                                                                                  | training on 1 GPUs                                                                                                                                                    | max tokens per GPU = 4096 and max sentences per GPU = None                                                                                                            | no existing checkpoint found checkpoints/transformer_text2cor_seq2k/checkpoint_last.pt                                                                                | loading train data for epoch 0                                                                                                                                        | loaded 609781 examples from: data-bin/text2cor.seq/train.src-trg.src                                                                                                  | loaded 609781 examples from: data-bin/text2cor.seq/train.src-trg.trg                                                                                                  | data-bin/text2cor.seq train src-trg 609781 examples\r\n| WARNING: 11644 samples have invalid sizes and will be skipped, max_positions=(2048, 2048), first few sample ids=[96983, 96942, 96984, 97067, 97027, 96943, 96944, 96902, 96858, 96945]                                                                                                                                                        | NOTICE: your device may support faster training with --fp16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \r\n| epoch 001:   0%| | 1/12560 [00:01<5:52:58,  1.69s/it, **loss=nan, nll_loss=nan, ppl=nan**, wps=3122, ups=0, wpb=27607.000, bsz=35.000, num_updates=1, lr=1.25e-07, gnorm=n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \r\n| epoch 001:   0%| | 2/12560 [00:02<5:16:01,  1.51s/it, **loss=nan, nll_loss=nan**, ppl=nan, wps=24736, ups=1, wpb=27385.500, bsz=38.500, num_updates=2, lr=2.5e-07, gnorm=n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\n| epoch 001:   0%| | 3/12560 [00:03<4:49:35,  1.38s/it, l**oss=nan, nll_loss=nan, ppl=nan**, wps=25190, ups=1, wpb=27569.000, bsz=39.333, num_updates=3, lr=3.75e-07, gnorm=n\r\n\r\n#### What have you tried?\r\n\r\nI tried training with an smaller --lr, different --max-tokens and --update-freq, and --fp16 settings, but I get the same issue.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: 0.9.0\r\n - PyTorch Version: 1.6\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): I would have to ask it\r\n - Build command you used (if compiling from source): I would have to ask it\r\n - Python version: 3.6.8\r\n - CUDA/cuDNN version: I would have to ask it\r\n - GPU models and configuration: 1 x Rtx2080Ti\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2260/comments",
    "author": "GorkaUrbizu",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-06-24T19:31:00Z",
        "body": "can you look at the actual input and target samples you are using and see if they make sense? if everything works with fewer max-positions and stops working when you increase it, its very likely a data problem"
      },
      {
        "user": "GorkaUrbizu",
        "created_at": "2020-06-27T08:01:47Z",
        "body": "i tried to train with 1025 max-positions, and i already got the loss=nan problem, so next, I will check the data as you suggested, to see if that the source of the problem.\r\n\r\nThanks."
      },
      {
        "user": "GorkaUrbizu",
        "created_at": "2020-07-14T09:21:13Z",
        "body": "Hi,\r\n\r\nI filtered the dataset into sequences shorter than 1024 (1020 to avoid problems), and it works properly.\r\n\r\nThen, I added a single instance of length 1025 on target (source sequences are a bit shorter at this task (coreference)). And when I trained the system with **--max-source-positions 2048--max-target-positions 2048** , loss converted into nan when the long sequence is taken for training. \r\n\r\nSo now we know that that single training instance is causing the problem. I open it, and there is nothing strange about it. it looks well formed, the vocab used in source is shared with the rest of training data as I am augmenting the training set with diferent sequences of diferent lengths from the same documents, so the tokens are repeated. about the target sequence, its a representation of the coreference anotation, created by my code, which should'nt give any problem.\r\n\r\nHere it's the source sequence:\r\n\r\n`but I do n't he 's wearing like an apron with huge pockets . But I do n't think you see the apron at first . I do n't know if that 's important or not . And uh and then he gets down out of the tree . and he dumps ail pears into the basket and the basket 's full . and one of the pears drops down to the floor . and he picks it up . and he takes kerchief off . and he wipes it off . and places it in in the basket which is very full . That 's why it fell off in the first place . And um then he climbs back up the ladder . and he and he starts picking pears again . And then while he 's up in the ladder . let 's see is it while he 's up in the ladder . Or or before . Um anyway . a guy comes by leading a goat . And the goat 's aaarrr but and they do n't talk to each other . they do n't I do n't think they even look at each other . and the guy walks by . And you watch the goat . disappearing all the way . and then then you 're back to the man in the tree . And he 's up there in the tree . And then definitely when he 's up there . a kid comes by on a bicycle . From the direction where the goat man left . okay . And Um the bicycle 's way too big for the kid . I 'm giving you all these details . I do n't know if you want them . Um the the reason I 'm giving you the details is cause I do n't know what the point of the movie was . Okay . So maybe you can see something that I did n't . Okay . Um g a kid comes by on a bicycle . he stops . he gets off bike . um the movie was in color . And the movie had sound track . It 's important . And then the mo the whole movie started with a a cock crowing . And then you see . Anyway . I just remembered that . Anyway . so um the kid on the bicycle . gets off the bicycle . I think he he picks up the b-asket of pears and he puts it on blcycle . on the front . And he rides off . And you see him . riding off and the next scene you see him . like at a at a distance . a pan shot . And he 's riding on a lane . riding on a bicycle and they cro you know . Cross paths . And he kind of watches her go by . And because he 's watching her . he when he turns around hat flies off . Okay . And then he bumps into a rock . And u m falls down . and the pears all spray all over the place . and and then you see him fall down . and he gets up . and brushes himself off . and he goes you know and then you see three other boys about age . He 's like maybe what . ten or something . Twelve . I do n't know . And and you look at them . and and they see him . and they come up . and without saying anything . there 's no speech in the whole movie. . without saying anything . they um help him put the pears back in the basket . Only they do n't wipe them off first . Just pick them up . and pour them in . And theyhelp him get bike up . and one kid takes the rock that he tripped on and he throws it off to the side of the road . and and they set him upright . and they and then he you know they I don1they do n't say thank you . he just splits . And then they walk the three boys walk down the road and they see the kid 's hat . And so one of them whistles . And the kid on the bike turns around . and then he brings hat to him . and the g guy on the bike gives the other kids gives the kid that returns hat three pears to share with buddies . So and then he splits . With hat . with pears . everything . And um I just remembered another detail . Before he meets the girl . One of the pears . 1t 's a bumpy road little dirt road . one of the pears flies out . Whatever . And Um then you do n't see the kid with the bike with the pears anymore . And the three kids that are walking . one of them has a what do you call those little um paddleball . You know . with a ball on a uh chong chong chong chong . So they 're walking along . and they brush off pears . and they start eating it . Then they walk by the man who was picking the pears . Who looks like a Mexican-American if that 's important . Um and he 's ge he 's getting down out of the tree and fucking with the ladder . And he sees and and there 's one full basket of pears there . and and an empty basket . And the other full basket is gone .`\r\n\r\nhere the target sequence:\r\n\r\n`_ (0) _ _ (1) _ _ _ (2 _ _ (3 2) | 3) _ _ (0) _ _ _ (4) _ (2 2) _ _ _ (0) _ _ _ _ (5) _ _ _ _ _ _ _ _ _ (1) _ _ _ _ (6 6) _ _ (1) _ (7 7) | (8) _ (9 9) _ (9 9) _ _ _ _ (10 _ (8 10) | 8) _ _ _ (11 11) _ _ (1) _ (10) _ _ _ (1) _ (12) _ _ _ (1) _ (10) _ _ _ _ (10) _ _ (9 _ _ _ _ 9) _ (13) _ _ (10) _ _ _ (14 _ 14) _ _ _ _ (1) _ _ _ (15 15) _ _ (1) _ (1) _ _ (16) _ _ _ _ _ (1) _ _ _ (15 15) _ _ _ _ _ (17) _ (1) _ _ _ (15 15) _ _ _ _ _ _ _ _ (18 18) _ _ _ (19 19) _ _ (19 19) _ _ _ _ (20) _ _ _ _ (21 21) _ (20) _ _ (0) _ _ _ (20) _ _ _ (22 22) _ _ (18 18) _ _ _ _ (23) _ (19 19) _ _ (24 _ 24) _ _ _ _ (25) _ _ _ (1 _ _ (6 1) | 6) _ _ (1) _ _ (26 _ (6 26) | 6) _ _ _ _ _ (1) _ _ (26) _ (27 27) _ _ _ (28 28) _ _ (29 _ _ (18 (19) 18) 29) _ _ _ _ _ (28 28) _ _ _ _ _ (27 27) _ (0) _ _ (30) (31 (32 31) | 32) _ (0) _ _ _ _ (30) _ (32) _ _ _ (33 _ (0) _ _ (30) (32 33) | 32) _ _ (0) _ _ _ _ (34 _ _ (35 34) | 35) _ _ _ _ _ _ (30) _ _ (36 _ (0) _ 36) _ _ _ _ _ (27 27) _ _ _ (28 28) _ (27) _ _ (27) _ _ (28) _ _ (35 35) _ _ (37) _ _ (35 35) _ (38 38) _ (39) _ _ _ _ _ (35 _ _ _ 35) _ _ _ (40 40) _ _ _ _ (41) _ _ _ _ (0) _ _ (42) _ _ _ _ _ (27 _ _ (28 27) | 28) _ _ _ (28 28) _ (0) _ (27) (27) _ _ (9 _ _ 9) | (16) _ (27) _ (9) _ (28) _ _ (43 43) _ _ (27) _ _ _ _ (44) _ (27) _ _ _ _ (45 _ 45) (46) _ (27) _ _ _ _ _ (47 47) _ _ (48 48) _ _ (27) _ _ _ (49 49) _ _ _ (28 28) _ (50) _ (51) _ _ _ (52) _ _ (27) _ _ _ (53) _ _ _ _ _ (27) _ _ (53) _ (27) _ (27) _ _ (54) _ _ _ _ _ _ _ (27) _ _ (55 55) _ _ _ _ _ _ _ _ (9 9) _ _ _ _ (56 56) _ _ _ _ (57) _ (27) _ _ _ _ (27) _ _ _ _ _ (27) _ _ _ (27) _ (58) _ _ _ (59) _ (60 _ _ _ 60) | (61) _ (27) _ _ _ _ _ (62 | (63) _ 62) | (64) _ (65) _ (0) _ _ _ _ _ _ (66) _ _ (60) _ _ _ (60) _ (27) _ _ (60) _ _ _ _ _ _ (67) _ (68) _ (69 69) _ (35 _ 35) _ _ _ (70) _ (60) _ _ (27) _ (9 9) _ _ (9 9) _ _ (60) _ _ _ (9) _ _ _ _ _ (9) _ _ _ _ (9) _ _ _ _ (27) _ (28) _ _ _ (71 71) _ (55 _ _ (27) _ 55) _ (71) _ (55) _ _ (72 _ _ (49 72) | 49) _ _ _ (60) _ (27) _ _ _ (60) _ _ (27) (73) _ (60) (0) _ _ _ _ _ _ _ (27) _ _ _ _ _ (60) _ (60 _ 60) _ _ (49 49) _ (60) _ (54 | (27 _ 27) 54) _ _ _ (74 _ 74) | (60) _ _ _ (27 _ _ (28 27) | 28) _ _ _ _ _ (74) _ (54) _ (27) _ _ (27 _ _ _ (28 27) | 28) _ (60 _ 60) _ (74 _ _ _ 74) | (54) (75 75) _ _ _ (76) _ _ _ _ (27) _ _ _ (54) _ _ (9) _ (77) _ _ _ (0) _ _ (78 78) _ _ (27) _ (53 53) _ (79 _ (9 79) | 9) _ (49) _ (80 _ 80) (81 _ 81) _ (79 _ (9 79) | 9) _ _ _ (82) _ _ _ _ (83) _ _ _ (27 _ _ (28 28) _ (9 27) | 9) _ _ _ (60 _ _ _ _ 60) _ (84 _ 84) | (60) _ _ _ _ (85) _ (86 _ _ 86) _ (87) _ _ _ (88 _ _ (89 _ _ _ _ 88) | 89) _ _ (60) _ _ _ _ _ (60) _ _ (75) _ _ (60) _ _ (75) _ _ (60) _ _ (1 _ _ _ _ (9 1) | 9) _ (1) _ _ (90 90) _ (91) _ _ _ _ _ (1) _ _ (1) _ _ _ _ _ (6 6) _ _ _ (15 15) _ _ (1) _ _ _ (92) _ (93 | (94 _ _ _ 93) | (16) (95) _ _ _ (96 _ 94) | 96) _ _ (9 _ _ 9) _ _ _`\r\n\r\ndoes anyone find something suspicious in the data? or may it be something else the cause of my issue?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2252,
    "title": "Finetuned BART Base on custom dataset , while loading, KeyError: 'bart_base",
    "created_at": "2020-06-18T04:41:00Z",
    "closed_at": "2022-05-02T10:22:12Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2252",
    "body": "## ❓ Questions and Help\r\n\r\n####  Getting error here...\r\n`/usr/local/lib/python3.6/dist-packages/fairseq/checkpoint_utils.py in _upgrade_state_dict(state)\r\n    348     # set any missing default values in the task, model or other registries\r\n    349     registry.set_defaults(state[\"args\"], tasks.TASK_REGISTRY[state[\"args\"].task])\r\n--> 350     registry.set_defaults(state[\"args\"], models.ARCH_MODEL_REGISTRY[state[\"args\"].arch])\r\n    351     for registry_name, REGISTRY in registry.REGISTRIES.items():\r\n    352         choice = getattr(state[\"args\"], registry_name, None)\r\n\r\n**KeyError: 'bart_base'`**\r\n\r\n#### My Code \r\n`import torch\r\nfrom fairseq.models.bart import BARTModel\r\n\r\nbart = BARTModel.from_pretrained('drive/My Drive/bart.base/',\r\n    checkpoint_file='checkpoint_best.pt'\r\n)`\r\n#### What's your environment?\r\n\r\n - fairseq Version (0.9.0):\r\n - PyTorch Version (1.5.0+cu101)\r\n - OS (Google Colab):\r\n - How you installed fairseq (`pip`):\r\n - Python version: 3.6\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2252/comments",
    "author": "ShoubhikBanerjee",
    "comments": [
      {
        "user": "ShoubhikBanerjee",
        "created_at": "2020-06-18T13:57:27Z",
        "body": "While trying with bart large,  it gave  this error.  : \r\n\r\n`Error(s) in loading state_dict for BARTModel:\r\n\tUnexpected key(s) in state_dict: \"decoder.output_projection.weight\". `\r\n\r\nI need help please :("
      },
      {
        "user": "MHDBST",
        "created_at": "2020-10-19T21:16:41Z",
        "body": "I got the same error. Did you find the solution?"
      },
      {
        "user": "nlper27149",
        "created_at": "2021-01-20T02:39:08Z",
        "body": "update fairseq to 0.10.2 help me out this error"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T03:04:41Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "EricLina",
        "created_at": "2022-01-13T15:58:02Z",
        "body": "I met the similar question when I use fair-generate on the **transformer_monotonic_iwslt_de_en** , the detailed tracebacks are showed below :\r\n`(torch17) lc2022@Chens-MacBook-Air main_sh % ./generate_infinite.sh\r\nTraceback (most recent call last):\r\n  File \"/Users/lc2022/miniconda/envs/torch17/bin/fairseq-generate\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())\r\n  File \"/Users/lc2022/Desktop/PycharmProj/fairseq/fairseq_cli/generate.py\", line 379, in cli_main\r\n    main(args)\r\n  File \"/Users/lc2022/Desktop/PycharmProj/fairseq/fairseq_cli/generate.py\", line 41, in main\r\n    return _main(args, sys.stdout)\r\n  File \"/Users/lc2022/Desktop/PycharmProj/fairseq/fairseq_cli/generate.py\", line 94, in _main\r\n    num_shards=args.checkpoint_shard_count,\r\n  File \"/Users/lc2022/Desktop/PycharmProj/fairseq/fairseq/checkpoint_utils.py\", line 256, in load_model_ensemble\r\n    num_shards,\r\n  File \"/Users/lc2022/Desktop/PycharmProj/fairseq/fairseq/checkpoint_utils.py\", line 279, in load_model_ensemble_and_task\r\n    state = load_checkpoint_to_cpu(filename, arg_overrides)\r\n  File \"/Users/lc2022/Desktop/PycharmProj/fairseq/fairseq/checkpoint_utils.py\", line 232, in load_checkpoint_to_cpu\r\n    state = _upgrade_state_dict(state)\r\n  File \"/Users/lc2022/Desktop/PycharmProj/fairseq/fairseq/checkpoint_utils.py\", line 435, in _upgrade_state_dict\r\n    registry.set_defaults(state[\"args\"], models.ARCH_MODEL_REGISTRY[state[\"args\"].arch])\r\nKeyError: 'transformer_monotonic_iwslt_de_en'\r\n`\r\n  \r\n\r\n\r\non branch v0.10.2 \r\nI trained a model using codes below:\r\n`fairseq-train \\\r\n    data-bin/wmt15_en_de_32k \\\r\n    --simul-type infinite_lookback \\\r\n    --user-dir $FAIRSEQ/example/simultaneous_translation \\\r\n    --mass-preservation \\\r\n    --criterion latency_augmented_label_smoothed_cross_entropy \\\r\n    --latency-weight-avg  0.1 \\\r\n    --max-update 50000 \\\r\n    --arch transformer_monotonic_iwslt_de_en save_dir_key=lambda \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n    --lr-scheduler 'inverse_sqrt' \\\r\n    --warmup-init-lr 1e-7  --warmup-updates 4000 \\\r\n    --lr 5e-4 --min-lr 1e-9 --clip-norm 0.0 --weight-decay 0.0001\\\r\n    --dropout 0.3 \\\r\n    --label-smoothing 0.1\\\r\n    --max-tokens 3584`\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T13:21:27Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2242,
    "title": "bart.base tokenizer same as bart.large?",
    "created_at": "2020-06-15T13:58:30Z",
    "closed_at": "2020-06-15T14:50:11Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2242",
    "body": "Thanks for releasing bart.base!\r\nWhen I download open it, I see that the embedding shape implies a vocab size of 51,201, whereas `bart.large` had embedding size 50,264.\r\n\r\n\r\nThe first 50,260 lines of the `dict.txt` files are identical, but then they diverge.\r\n`bart.large/dict.txt` ends in `madeupword0002 0`\r\nwhereas `bart.base/dict.txt` continues to `madeupword0938 0`.\r\n\r\n\r\nI'm assuming that these extra entries are identical, the same tokenizer can be used for both models, and that the bart.base embeddings can be truncated to 50,264. Is that correct? Thanks! ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2242/comments",
    "author": "sshleifer",
    "comments": [
      {
        "user": "ngoyal2707",
        "created_at": "2020-06-15T14:50:11Z",
        "body": "Hey, yes, those extra symbols are just dummy tokens for making the matrix more efficient on GPU. you can delete those tokens and adjust the embed_tokens matrix manually."
      },
      {
        "user": "sshleifer",
        "created_at": "2020-06-15T17:04:31Z",
        "body": "Maybe I should leave them? What is the efficiency gain?"
      }
    ]
  },
  {
    "number": 2240,
    "title": "How to set batch size when fine-tunning BART?",
    "created_at": "2020-06-14T14:20:55Z",
    "closed_at": "2020-06-17T08:59:47Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2240",
    "body": "#### What is your question?\r\nWhen I fine-tune BART.large on my server, OOM issue occurs. So I intend to reduce batch_size to enable training. So I would like to know how to set batch size when fine-tunning BART. Thanks!!\r\n#### What's your environment?\r\n\r\n - fairseq Version (0.7.2)\r\n - PyTorch Version (1.5.0)\r\n - OS (Linux):\r\n - How you installed fairseq: pip\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2240/comments",
    "author": "JJJJane",
    "comments": [
      {
        "user": "shamanez",
        "created_at": "2020-06-16T12:24:41Z",
        "body": "You have to change the **--max-tokens** parameter.  Basically, it says the number of maximum tokens that can consist of a batch of training data. The default is 2048. \r\n\r\nBut remember that, BART has dynamic batching, which means it can select examples of different sequence lengths.  "
      },
      {
        "user": "JJJJane",
        "created_at": "2020-06-17T08:59:47Z",
        "body": "Okay, Thanks!"
      }
    ]
  },
  {
    "number": 2224,
    "title": "'fairseq-train' does not appear to train",
    "created_at": "2020-06-08T16:52:25Z",
    "closed_at": "2020-08-01T20:03:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2224",
    "body": "#### What is your question?\r\nI have been using Fairseq for a while now to train NMT models without a problem on Colab. Recently, I have moved to a High-End computer network cluster and have been using Fairseq there. This could be a problem with the cluster, but I just wanted to ask to see if anyone experienced the issue here.\r\nSo, I initiate `fairseq-train` like normal and have the following settings:\r\n```\r\n#!/bin/bash\r\n\r\nfairseq-train data-bin-full_corp/MayNMT \\\r\n    --cpu --arch transformer\\\r\n    --optimizer adam --clip-norm 0.1 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt\\\r\n    --dropout 0.2\\\r\n    --max-tokens 4096 --save-dir checkpoints/full_tran\r\n```\r\nAbout the `--cpu` flag, I'm using a CPU because as of right now, I only have access to the cpu version of PyTorch on the cluster. Whether that is a problem, I don't know.\r\nThe following is the output:\r\n\r\n```\r\n| model transformer, criterion CrossEntropyCriterion\r\n| num. model params: 59244544 (num. trained: 59244544)\r\n| training on 1 GPUs\r\n| max tokens per GPU = 4096 and max sentences per GPU = None\r\n| no existing checkpoint found checkpoints/full_tran/checkpoint_last.pt\r\n| loading train data for epoch 0\r\n| loaded 813393 examples from: data-bin-full_corp/MayNMT/train.en-ga.en\r\n| loaded 813393 examples from: data-bin-full_corp/MayNMT/train.en-ga.ga\r\n| data-bin-full_corp/MayNMT train en-ga 813393 examples\r\n```\r\nAnd it will run on the specified amount of time (30mins in this case for a test) and it won't go past that. It doesn't throw an error, it just sits there and doesn't train anything. No checkpoints are logged.\r\n\r\n - fairseq Version: 0.9.0\r\n - PyTorch Version: 1.5.0 (cpu)\r\n - OS: Linux\r\n - How you installed fairseq: Unfortunately, I can't install libraries on the cluster myself\r\n - Python version: 3.7.7\r\n - Any other relevant information: If the CPU model is relevant - Intel Xeon Gold 6148 (Skylake)\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2224/comments",
    "author": "JustCunn",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-06-08T17:36:12Z",
        "body": "I suspect it's just too slow to train on CPU, so you're not even getting to the first log output. Please try `--log-format simple --log-interval 1`.\r\n\r\nUnfortunately, training on CPU is really only practical for unit tests (and that's the only reason we support CPU training)."
      },
      {
        "user": "JustCunn",
        "created_at": "2020-06-08T17:40:54Z",
        "body": "Ok thanks. Was just making sure it wasn't a deeper issue that we'd have to sort out, but knowing that everything is more than likely is working is good. I'll definitely be moving to the GPU version as soon as."
      }
    ]
  },
  {
    "number": 2223,
    "title": "mBART: Fine-tuning on an unseen language",
    "created_at": "2020-06-08T16:46:24Z",
    "closed_at": "2022-04-18T01:21:05Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2223",
    "body": "## ❓ Questions and Help\r\n#### What is your question?\r\nIn the paper \"Multilingual Denoising Pre-training for Neural Machine Translation\", there is mention of fine-tuning on languages that are not in the pre-trained model. The fine-tuning example in the mBART README is on languages that are in the pre-trained model (en and ro). Is it possible to fine-tune on an unseen language either on the source-side or target-side? The pre-trained model will not contain an embedding vector for the unseen language symbol. Is there anything special that needs to be done to support fine-tuning on an unseen language?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.5\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Nvidia RTX 2070\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2223/comments",
    "author": "ddaspit",
    "comments": [
      {
        "user": "fahimeh-saleh",
        "created_at": "2020-07-26T23:48:10Z",
        "body": "I have the same issue! any help? "
      },
      {
        "user": "sumanthd17",
        "created_at": "2021-01-06T09:11:51Z",
        "body": "Any update on this?"
      },
      {
        "user": "fnangle",
        "created_at": "2021-05-25T07:49:13Z",
        "body": "Need help too！"
      },
      {
        "user": "mahmoudtarek1871997",
        "created_at": "2021-11-16T10:49:26Z",
        "body": "same issue. help!!!!"
      },
      {
        "user": "evelynkyl",
        "created_at": "2021-11-19T23:14:31Z",
        "body": "same issue! appreciate any help on this!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:09Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T01:20:35Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2217,
    "title": "hello,how to fine tune question-answering dataset(such as SQuad1.0 or SQuad2.0) using bart-large model?thank you ver much!",
    "created_at": "2020-06-07T07:24:56Z",
    "closed_at": "2022-04-18T05:21:03Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2217",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2217/comments",
    "author": "ChengchengDu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:33Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2216,
    "title": "ZeroDivisionError: division by zero",
    "created_at": "2020-06-06T01:39:29Z",
    "closed_at": "2020-08-07T10:23:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2216",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI am getting zero divison error. I have 48500000 sentences in my corpus and I am trying to build a language model. \r\n#### Code\r\n \r\nThis is the training command.\r\n>  /data/ahnaf/fairseq_folder/fairseq$ python3 train.py /data/ahnaf/fairseq_folder/models/dest --save-dir /data/ahnaf/fairseq_folder/models/decoder/convlm_models/word_14B \r\n--task=language_modeling --arch=fconv_lm --memory-efficient-fp16 \r\n--max-epoch=48 --optimizer=nag --lr=0.5 --lr-scheduler=fixed --decoder-embed-dim=128 --clip-norm=0.1 \r\n--decoder-layers='[(512, 5)] + [(128, 1, 0), (128, 5, 0), (512, 1, 3)] * 3 + [(512, 1, 0), (512, 5, 0), (1024, 1, 3)] * 3 + [(512, 1, 0), (512, 5, 0), (1024, 1, 3)] * 6 + [(1024, 1, 0), (1024, 5, 0), (2048, 1, 3)]' --dropout=0.1 --weight-decay=1e-07 \r\n--max-tokens=1024 --tokens-per-sample=1024 --sample-break-mode=none --criterion=adaptive_loss --adaptive-softmax-cutoff='100,500,2000' --seed=42 --log-format=json --log-interval=100 --save-interval-updates=10000 --keep-interval-updates=10 --ddp-backend=\"no_c10d\" --distributed-world-size=4 > /data/ahnaf/fairseq_folder/models/decoder/convlm_models/word_14B/train.log\r\n\r\nLog output:\r\n\r\n> /data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py:391: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps\r\n  \"ntokens not found in Criterion logging outputs, cannot log wpb or wps\"\r\n/data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py:391: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps\r\n  \"ntokens not found in Criterion logging outputs, cannot log wpb or wps\"\r\n/data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py:400: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz\r\n  \"nsentences not found in Criterion logging outputs, cannot log bsz\"\r\n/data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py:400: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz\r\n  \"nsentences not found in Criterion logging outputs, cannot log bsz\"\r\n/data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py:391: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps\r\n  \"ntokens not found in Criterion logging outputs, cannot log wpb or wps\"\r\n/data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py:400: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz\r\n  \"nsentences not found in Criterion logging outputs, cannot log bsz\"\r\n/data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py:391: UserWarning: ntokens not found in Criterion logging outputs, cannot log wpb or wps\r\n  \"ntokens not found in Criterion logging outputs, cannot log wpb or wps\"\r\n/data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py:400: UserWarning: nsentences not found in Criterion logging outputs, cannot log bsz\r\n  \"nsentences not found in Criterion logging outputs, cannot log bsz\"\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/data/ahnaf/fairseq_folder/fairseq/fairseq_cli/train.py\", line 365, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/shafkat/anaconda3/envs/fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/shafkat/anaconda3/envs/fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n> -- Process 2 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/shafkat/anaconda3/envs/fairseq/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/data/ahnaf/fairseq_folder/fairseq/fairseq_cli/train.py\", line 333, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/data/ahnaf/fairseq_folder/fairseq/fairseq_cli/train.py\", line 120, in main\r\n    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\r\n  File \"/home/shafkat/anaconda3/envs/fairseq/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data/ahnaf/fairseq_folder/fairseq/fairseq_cli/train.py\", line 206, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/shafkat/anaconda3/envs/fairseq/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data/ahnaf/fairseq_folder/fairseq/fairseq/trainer.py\", line 536, in train_step\r\n    logging_outputs, sample_size, grad_norm,\r\n  File \"/data/ahnaf/fairseq_folder/fairseq/fairseq/trainer.py\", line 893, in _reduce_and_log_stats\r\n    self.task.reduce_metrics(logging_outputs, self.get_criterion())\r\n  File \"/data/ahnaf/fairseq_folder/fairseq/fairseq/tasks/fairseq_task.py\", line 406, in reduce_metrics\r\n    criterion.__class__.reduce_metrics(logging_outputs)\r\n  File \"/data/ahnaf/fairseq_folder/fairseq/fairseq/criterions/adaptive_loss.py\", line 87, in reduce_metrics\r\n    metrics.log_scalar('loss', loss_sum / sample_size / math.log(2), sample_size, round=3)\r\nZeroDivisionError: division by zero\r\n \r\n\r\n#### What have you tried?\r\nChanging different token per sample , max token and number of GPU for training.\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., master):\r\n - PyTorch Version (e.g., 1.4)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:3.6\r\n - CUDA/cuDNN version:10.0\r\n - GPU models and configuration: nvidia GTX 1080Ti 8 GPUs, each one 11GB\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2216/comments",
    "author": "samin9796",
    "comments": [
      {
        "user": "munael",
        "created_at": "2020-06-10T08:45:18Z",
        "body": "`reduce_metrics` and numerous other functions expect logging_outputs to have type `List[dict]` which is a list of dicts from logging variable to log value. That's why the `sum(...)` expressions aren't raising exceptions but returning `0`, unexpectedly.\r\n\r\nBut _actually_ the logging_outputs are being put in a `List[Dict[str, dict]]`. It looks like:\r\n\r\n```py\r\n    [\r\n\t\t{\r\n\t\t\t\"<lang>\": { \"log_var\": log_val },\r\n\t\t\t\"<lang2>\" { ... },\r\n\t\t},\r\n\t\t{ ... },\r\n\t\t...\r\n\t]\r\n```\r\n\r\nUnless there's something obvious in the CLI we're missing, the current internal APIs just don't agree with one another.\r\n\r\nAt multiple points, there are aggregation functions that try to aggregate the different metrics over the logging objects. If a particular metric is not reported in one of the log objects, it defaults to `0`. Because the structure of the logs is different from expected, the aggregation function iterates of the list and just sums `0` in all metrics."
      },
      {
        "user": "Bachstelze",
        "created_at": "2020-06-16T09:47:15Z",
        "body": "Do you know since which commit this internal API is broken?"
      }
    ]
  },
  {
    "number": 2210,
    "title": "Visualizing attn maps",
    "created_at": "2020-06-04T13:59:03Z",
    "closed_at": "2022-04-18T05:21:01Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2210",
    "body": "## ❓ Questions and Help\r\n\r\nTask: Machine Translation.\r\n\r\nCan someone point me where in the code I can print/visualize proper enc/dec attn maps?\r\n\r\nI am trying to look at attn, and attn_weights in:\r\nfairseq/modules/multihead_attention.py\r\n\r\nbut i am not getting proper attn maps (with diagonal alignment)",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2210/comments",
    "author": "AhmedTElthakeb",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:31Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2201,
    "title": "What do the metrics wps, ups and wpb mean in the training logger ?",
    "created_at": "2020-06-01T17:13:17Z",
    "closed_at": "2020-06-02T12:32:58Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2201",
    "body": "In the following dictionary from the training log output:\r\n\r\n<pre>{&quot;epoch&quot;: 27, &quot;update&quot;: 26.267, &quot;loss&quot;: &quot;8.206&quot;, &quot;nll_loss&quot;: &quot;7.049&quot;, &quot;ppl&quot;: &quot;132.47&quot;, &quot;wps&quot;: &quot;1195.4&quot;, &quot;ups&quot;: &quot;1.62&quot;, &quot;wpb&quot;: &quot;738.1&quot;, &quot;bsz&quot;: &quot;46.4&quot;, &quot;num_updates&quot;: &quot;33700&quot;, &quot;lr&quot;: &quot;0.00017226&quot;, &quot;gnorm&quot;: &quot;1.833&quot;, &quot;clip&quot;: &quot;1&quot;, &quot;train_wall&quot;: &quot;61&quot;, &quot;wall&quot;: &quot;30542&quot;}</pre>\r\n\r\nI assume the following from looking at the code and other issues:\r\n**bsz** = batch size \r\n**gnorm** = L2 norm of the gradients\r\n**clip** = gradient clipping threshold\r\n**train_wall** = time taken for one training step\r\n**wall** = total time spent training, validating, saving checkpoints (so far)\r\n**wps** = ?\r\n**ups** = ?\r\n**wpb** = ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2201/comments",
    "author": "shahbazsyed",
    "comments": [
      {
        "user": "kalyangvs",
        "created_at": "2020-06-02T08:13:15Z",
        "body": "wps - Words Per Second\r\nups - Updates Per Second\r\nwpb - Words Per Batch"
      },
      {
        "user": "shahbazsyed",
        "created_at": "2020-06-02T12:32:58Z",
        "body": "Thanks!"
      },
      {
        "user": "benjamin3344",
        "created_at": "2021-04-17T01:39:21Z",
        "body": "Anyone know what nvo, stp is short for? And what does the \"words\" mean in wps and wpb..  @gvskalyan @shahbazsyed \r\n"
      }
    ]
  },
  {
    "number": 2197,
    "title": "vq-wav2vec roberta data cropping",
    "created_at": "2020-05-29T08:48:02Z",
    "closed_at": "2020-06-01T04:05:13Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2197",
    "body": "## What is your question?\r\nI noticed in the paper \r\n\r\n> The batch size is 10, and we crop a random section of 150k frames for each example (approximately 9.3 seconds for 16kHz sampling rate).\r\n\r\nIs this specific to the vqwav2vec training or does it also apply to **roberta** training? i.e. were inputs cropped to a standard length prior to tokenisation for roberta training or were full files from the librispeech dataset trained on regardless of length?\r\n\r\nIf the inputs were cropped, does that mean the files < 9.3 seconds were dropped from training, or padded to that fixed length?\r\n\r\nMany thanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2197/comments",
    "author": "david-macleod",
    "comments": [
      {
        "user": "david-macleod",
        "created_at": "2020-05-31T18:39:24Z",
        "body": "cc @alexeib "
      },
      {
        "user": "alexeib",
        "created_at": "2020-06-01T04:05:13Z",
        "body": "it is specific to audio pre-training only. samples are not cropped for roberta training"
      }
    ]
  },
  {
    "number": 2192,
    "title": "Gradient Normalization when using max-sentences",
    "created_at": "2020-05-27T16:42:25Z",
    "closed_at": "2020-05-27T23:21:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2192",
    "body": "Does fairseq normalize gradient with the number of tokens in a batch when I specify just the `max-sentence ` without `--sentence-avg` and `max-token`? \r\n\r\nIf no, is there a way I can do that?  ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2192/comments",
    "author": "ajesujoba",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-05-27T23:21:13Z",
        "body": "Yes, by default it will normalize by the number of tokens, even when you specify the batch size with `--max-sentence`."
      }
    ]
  },
  {
    "number": 2167,
    "title": "size mismatch when loading mbart pretrained model",
    "created_at": "2020-05-21T13:44:36Z",
    "closed_at": "2022-04-18T03:21:28Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2167",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI've trained a mbart model for English and Pashto from scratch. When I fine-tuning it for en2ps translation model. I got the size mismatch for the size of dictionary(pretrained model: 32002, when fine-tuning: 32004). But I use the same dictionary with size 31997.\r\n#### Error\r\n`2020-05-21 09:26:34 | INFO | fairseq_cli.train | model mbart_large, criterion LabelSmoothedCrossEntropyCriterion\r\n2020-05-21 09:26:34 | INFO | fairseq_cli.train | num. model params: 167926272 (num. trained: 167926272)\r\n2020-05-21 09:26:38 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\r\n2020-05-21 09:26:38 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\r\n2020-05-21 09:26:38 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\r\n2020-05-21 09:26:38 | INFO | fairseq_cli.train | max tokens per GPU = 1024 and max sentences per GPU = None\r\nTraceback (most recent call last):\r\n  File \"/home/zyyt/train/tools/fairseq/fairseq/trainer.py\", line 238, in load_checkpoint\r\n    state[\"model\"], strict=True, args=self.args\r\n  File \"/home/zyyt/train/tools/fairseq/fairseq/models/fairseq_model.py\", line 93, in load_state_dict\r\n    return super().load_state_dict(new_state_dict, strict)\r\n  File \"/home/zyyt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 847, in load_state_dict\r\n    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for BARTModel:\r\n        size mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([32002, 768]) from checkpoint, the shape in current model is torch.Size([32004, 768]).\r\n        size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([32002, 768]) from checkpoint, the shape in current model is torch.Size([32004, 768]).\r\n        size mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([32002, 768]) from checkpoint, the shape in current model is torch.Size([32004, 768]).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/zyyt/train/tools/fairseq/train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/home/zyyt/train/tools/fairseq/fairseq_cli/train.py\", line 382, in cli_main\r\n    main(args)\r\n  File \"/home/zyyt/train/tools/fairseq/fairseq_cli/train.py\", line 104, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\r\n  File \"/home/zyyt/train/tools/fairseq/fairseq/checkpoint_utils.py\", line 137, in load_checkpoint\r\n    reset_meters=args.reset_meters,\r\n  File \"/home/zyyt/train/tools/fairseq/fairseq/trainer.py\", line 247, in load_checkpoint\r\n    \"please ensure that the architectures match.\".format(filename)\r\nException: Cannot load model parameters from checkpoint checkpoints/checkpoint_best.pt; please ensure that the architectures match.`\r\n\r\n#### My preprocess command when training mbart from scratch：\r\n`   \r\npython preprocess.py \r\n      --task multilingual_denoising \r\n      --only-source \r\n      --trainpref $DATA/train.bpe\r\n      --validpref $DATA/valid.bpe\r\n      --testpref $DATA/test.bpe \r\n      --destdir $DATA_DIR \r\n      --workers 20 \r\n      --source-lang $lg \r\n      --srcdict $DATA/dict.txt \r\n`\r\n#### My preprocess command when fine-tuning:\r\n`\r\npython preprocess.py \r\n--source-lang ps_AF --target-lang en_XX \r\n--trainpref data/train.bpe.clean --validpref data/valid.bpe --testpref data/test.bpe \r\n--destdir data-bin-psen --joined-dictionary \r\n--workers 8 --srcdict data/dict.txt --thresholdtgt 0 --thresholdsrc 0 \r\n`\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):master\r\n - PyTorch Version (e.g., 1.0) 1.5\r\n - OS (e.g., Linux): ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source):  pip install --editable .\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: 8 TITAN RTX\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2167/comments",
    "author": "beichao1314",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:58Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2161,
    "title": "[Quant-noise] How to properly load the quantized model?",
    "created_at": "2020-05-20T14:50:47Z",
    "closed_at": "2020-07-17T15:49:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2161",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nHi, I successfully quantized your pretrained roberta-base model during 3 epochs of fine tuning. I saved the quantized model after each epoch. Now, I would like to load the model with the classic procedure (RobertaModel.from_pretrained) but I receive the error when trying to load the state_dict. Error is copy-pasted under the Code section. What should I do to properly load the model? Thanks.\r\n\r\n#### Code\r\n\r\nfrom fairseq.models.roberta import RobertaModel\r\nroberta = RobertaModel.from_pretrained(\r\n       \targs.modeldir,\r\n       \tcheckpoint_file = 'checkpoint' + args.epoch + '.pt',\r\n        data_name_or_path = args.datadir\r\n    )\r\n\r\nError:\r\n\r\nRuntimeError: Error(s) in loading state_dict for RobertaModel:\r\n\tMissing key(s) in state_dict: \"decoder.sentence_encoder.embed_tokens.weight\", \"decoder.sentence_encoder.layers.0.self_attn.k_proj.weight\", \"decoder.sentence_encoder.layers.0.self_attn.v_proj.weight\", \"decoder.sentence_encoder.layers.0.self_attn.q_proj.weight\", \"decoder.sentence_encoder.layers.0.self_attn.out_proj.weight\", \"decoder.sentence_encoder.layers.0.fc1.weight\", \"decoder.sentence_encoder.layers.0.fc2.weight\", \"decoder.sentence_encoder.layers.1.self_attn.k_proj.weight\", \"decoder.sentence_encoder.layers.1.self_attn.v_proj.weight\",\r\n...\r\n\"decoder.sentence_encoder.layers.11.self_attn.out_proj.weight\", \"decoder.sentence_encoder.layers.11.fc1.weight\", \"decoder.sentence_encoder.layers.11.fc2.weight\".\r\n\tUnexpected key(s) in state_dict: \"decoder.sentence_encoder.embed_tokens.centroids\", \"decoder.sentence_encoder.embed_tokens.assignments\", \"decoder.sentence_encoder.embed_tokens.counts\", \"decoder.sentence_encoder.layers.0.self_attn.k_proj.centroids\", \"decoder.sentence_encoder.layers.0.self_attn.k_proj.assignments\", \"decoder.sentence_encoder.layers.0.self_attn.k_proj.counts\", \"decoder.sentence_encoder.layers.0.self_attn.v_proj.centroids\", \"decoder.sentence_encoder.layers.0.self_attn.v_proj.assignments\", \"decoder.sentence_encoder.layers.0.self_attn.v_proj.counts\", \"decoder.sentence_encoder.layers.0.self_attn.q_proj.centroids\", \"decoder.sentence_encoder.layers.0.self_attn.q_proj.assignments\", \"decoder.sentence_encoder.layers.0.self_attn.q_proj.counts\", \"decoder.sentence_encoder.layers.0.self_attn.out_proj.centroids\", \"decoder.sentence_encoder.layers.0.self_attn.out_proj.assignments\", \"decoder.sentence_encoder.layers.0.self_attn.out_proj.counts\", \"decoder.sentence_encoder.layers.0.fc1.centroids\", \"decoder.sentence_encoder.layers.0.fc1.assignments\",\r\n...\r\n\"decoder.sentence_encoder.layers.11.fc1.assignments\", \"decoder.sentence_encoder.layers.11.fc1.counts\", \"decoder.sentence_encoder.layers.11.fc2.centroids\", \"decoder.sentence_encoder.layers.11.fc2.assignments\", \"decoder.sentence_encoder.layers.11.fc2.counts\".\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (master)\r\n - PyTorch Version (1.5)\r\n - OS (Linux)\r\n - How you installed fairseq (source)\r\n - Python version: 3.7.7\r\n - CUDA version: 10.1\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2161/comments",
    "author": "bdropuljic",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-05-22T20:49:20Z",
        "body": "Thanks for opening this issue - I will look into this issue. We did not previously use the RobertaModel.from_pretrained method to load the Roberta Model, so likely there is some incompatibility in the state dict that you are seeing. "
      },
      {
        "user": "bdropuljic",
        "created_at": "2020-05-23T10:49:12Z",
        "body": "Thanks for helping me. It doesn't have to be the RobertaModel.from_pretrained method necessarily. I started with it because I successfully loaded a non-quantized fine tuned model with it before. I also tried several other approaches but always failed with the same error during the load_state_dict phase. If you can recommend some other approach for loading the quantized fine tuned model, I would be very happy to use it :) "
      },
      {
        "user": "jahutwb",
        "created_at": "2020-07-09T11:30:35Z",
        "body": "I've done it. I mean load quantized model, but I doubt it was done properly, but it worked.\r\nLook here #2232"
      },
      {
        "user": "bdropuljic",
        "created_at": "2020-07-17T15:49:12Z",
        "body": "Thanks @jahutwb ! I also succeeded by simulating the quantization process during the model loading, which changed the model architecture :) It's not an elegant solution but it works.."
      }
    ]
  },
  {
    "number": 2154,
    "title": "RuntimeError: index out of range: Tried to access index 300 out of table with 299",
    "created_at": "2020-05-19T14:21:18Z",
    "closed_at": "2022-04-18T03:21:25Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2154",
    "body": "[rebosting #2123]\r\n**Task:**\r\nInference for librispeech\r\n\r\n**Command:**\r\npython examples/speech_recognition/infer.py /home/ahmed/projects/speech_data/librispeech_final --task speech_recognition --max-tokens 250000 --nbest 1 --path model_lib/checkpoint_best_run1.pt --beam 20 --results-path results_dir --batch-size 20 --gen-subset test-clean --user-dir examples/speech_recognition/\r\n\r\n**Error:**\r\nRuntimeError: index out of range: Tried to access index 300 out of table with 299 rows. at /opt/conda/conda-bld/pytorch_1579022027550/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418\r\n\r\nWhile debugging i notice that the dimension of active_bbsz_idx suddenly changes from 320 (16 x 20) to 300 (15 x 20) and this is when the error happens.\r\nIn: fairseq/fairseq/sequence_generator.py\r\n\r\nIt seems that this breaks when this condition is satisfied:\r\n>> if len(finalized_sents) > 0:\r\n\r\nThanks in advance!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2154/comments",
    "author": "AhmedTElthakeb",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2136,
    "title": "How to get the probabilities of the predictions for WSC",
    "created_at": "2020-05-14T22:29:46Z",
    "closed_at": "2022-04-18T03:21:20Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2136",
    "body": "Hi, i am looking to extract the probabilities of my predictions, to understand what is my model learning. How do i get them for a roberta model trained on WSC data",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2136/comments",
    "author": "srininyu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:56Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2126,
    "title": "why should i binarize the source and target for the Translation task in Fairseq?",
    "created_at": "2020-05-13T10:04:30Z",
    "closed_at": "2020-05-13T16:40:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2126",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\nwhy should i binarize the source and target for the Translation task in Fairseq?  can i use the raw sentence? if so,  how should i do it\r\n\r\nthank you!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2126/comments",
    "author": "lyzKF",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-05-13T16:40:02Z",
        "body": "The `fairseq-preprocess` command generates the dictionary file and by default binarizes the dataset, which makes it faster to load and can sometimes make it take less space on disk too. It's highly recommended to binarize the data.\r\n\r\nIf you prefer, you can instead use raw text by passing `--dataset-impl=raw` to both `fairseq-preprocess` and `fairseq-train`. You still need to run `fairseq-preprocess`, but it will use the raw text."
      },
      {
        "user": "lyzKF",
        "created_at": "2020-05-14T06:53:21Z",
        "body": "Thank you!!!\r\nif each line of my source and target dateset  is ids (not raw words), can i still set ```dataset-impl=cache``` to binarize the source and target ? \r\nthe ids are the index of word in Dictionary."
      },
      {
        "user": "dhar7",
        "created_at": "2021-01-25T03:56:04Z",
        "body": "> The `fairseq-preprocess` command generates the dictionary file and by default binarizes the dataset, which makes it faster to load and can sometimes make it take less space on disk too. It's highly recommended to binarize the data.\r\n> \r\n> If you prefer, you can instead use raw text by passing `--dataset-impl=raw` to both `fairseq-preprocess` and `fairseq-train`. You still need to run `fairseq-preprocess`, but it will use the raw text.\r\n\r\nThank you for your cleear explanation . But every time I use 'fairseq-preprocess' it gives me a error like:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'wmt14_en_de/train.bert.en'\r\nBut before this eroor , dictionary for both languages used to be created successfully .\r\nMay I request  a solution for this problem ?"
      },
      {
        "user": "lalopark",
        "created_at": "2022-02-20T21:35:26Z",
        "body": "What exactly is it binarizing the data into? key and value (as in dictionary)? "
      }
    ]
  },
  {
    "number": 2125,
    "title": "Fine-tune BART for translation task on my own dataset",
    "created_at": "2020-05-13T04:46:05Z",
    "closed_at": "2022-04-18T03:21:23Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2125",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n#### Can I fine-tune BART model for translation task on my custom dataset using the same (preprocessing and training) instructions as given for CNN/DM ? Will the same BPE processing and binarizing dataset intructions work here as well or those were specific only for CNN/DM ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2125/comments",
    "author": "rathorevipul28",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-05-16T15:06:02Z",
        "body": "@ngoyal2707 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:53Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "karin0018",
        "created_at": "2022-05-12T13:05:06Z",
        "body": "> ## ❓ Questions and Help\r\n> #### Can I fine-tune BART model for translation task on my custom dataset using the same (preprocessing and training) instructions as given for CNN/DM ? Will the same BPE processing and binarizing dataset intructions work here as well or those were specific only for CNN/DM ?\r\n\r\nhi, I have the same question, did you solve it?"
      }
    ]
  },
  {
    "number": 2116,
    "title": "how to use roberta to do speech_recognition task",
    "created_at": "2020-05-11T02:33:26Z",
    "closed_at": "2022-04-18T03:21:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2116",
    "body": "I saw the paper <EFFECTIVENESS OF SELF-SUPERVISED PRE-TRAINING FOR SPEECH RECOGNITION> and I want to reproduce the paper results using fairseq. I used \"example/speech_recognition\" module and prepared the data like librispeech . Then I need to train the roberta model with ctc_loss as criterion. I used the command `python train.py $DIR_FOR_PREPROCESSED_DATA --save-dir examples/speech_recognition/saved_models --max-epoch 30 --task speech_recognition --arch roberta_base --optimizer adadelta --lr 1.0 --adadelta-eps 1e-8 --adadelta-rho 0.95 --clip-norm 10.0 --max-tokens 10000 --log-format json --log-interval 1 --criterion ctc_loss --user-dir examples/speech_recognition/ --max-positions 6144` and got many bugs. I had used vq-wav2vec feature instead of fbank feature in examples/speech_recognition/data/asr_dataset.py. Is there any wrong with my reproduces?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2116/comments",
    "author": "luweishuang",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:30Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2114,
    "title": "Is `eval-bleu` supported when training Levenshtein transformer?",
    "created_at": "2020-05-09T03:50:36Z",
    "closed_at": "2022-04-18T03:21:15Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2114",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI was training a Levenshtein transformer and turned on the `eval-bleu` option, when the framework tried to save the first checkpoint and evaluate on the valid dataset, the error below was thrown:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq==0.9.0', 'console_scripts', 'fairseq-train')()\r\n  File \"/opt/conda/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py\", line 317, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 3 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py\", line 286, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/opt/conda/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py\", line 96, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/opt/conda/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py\", line 196, in train\r\n    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/opt/conda/lib/python3.7/site-packages/fairseq-0.9.0-py3.7-linux-x86_64.egg/fairseq_cli/train.py\", line 264, in validate\r\n    valid_losses.append(stats[args.best_checkpoint_metric])\r\nKeyError: 'bleu'\r\n``` \r\n\r\nIs it because `eval-bleu` not supported by `translation_lev` task, or I made some wrong configs? My training command is\r\n\r\n```\r\nfairseq-train $data_dir \\\r\n  --task translation_lev \\\r\n  --arch levenshtein_transformer \\\r\n  --noise random_delete \\\r\n  --share-all-embeddings \\\r\n  --save-dir $save_dir \\\r\n  --max-tokens 4096 \\\r\n  --optimizer adam \\\r\n    --adam-betas \"(0.9,0.98)\" \\\r\n  --clip-norm 5.0 \\\r\n  --lr-scheduler inverse_sqrt \\\r\n  --lr 0.${lr} \\\r\n  --min-lr 1e-20 \\\r\n  --criterion nat_loss \\\r\n  --label-smoothing 0.1 \\\r\n  --log-format json \\\r\n  --encoder-normalize-before \\\r\n  --decoder-normalize-before \\\r\n  --dropout 0.3 \\\r\n  --max-update 10000000 \\\r\n  --max-epoch 30 \\\r\n  --keep-interval-updates 200 \\\r\n  --save-interval-updates 1000 \\\r\n  --log-interval 100 \\\r\n  --fp16 \\\r\n  --warmup-updates 10000 \\\r\n  --warmup-init-lr 1e-7 \\\r\n  --update-freq 8 \\\r\n  --weight-decay 0.01 \\\r\n  --decoder-learned-pos \\\r\n  --encoder-learned-pos \\\r\n  --apply-bert-init \\\r\n  --eval-bleu \\\r\n  --eval-bleu-remove-bpe \\\r\n  --eval-bleu-detok space \\\r\n  --eval-bleu-args '{\"beam\": 5, \"lenpen\": 4}' \\\r\n  --best-checkpoint-metric bleu \\\r\n  --maximize-best-checkpoint-metric \\\r\n  --ddp-backend=no_c10d 2>&1 | tee ${log_file}\r\n```\r\n\r\nThank you.\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: master with commit version `fd76cb5b41f5d434ddf9d351cb3a26ba123a179c`\r\n - PyTorch Version (e.g., 1.0): 1.4\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): `pip install --editable .` then `python setup.py build_ext --inplace`\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Tesla V100\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2114/comments",
    "author": "TingxunShi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:44Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2111,
    "title": "About the function of lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs) in sequence_generator.py line 270",
    "created_at": "2020-05-09T03:31:12Z",
    "closed_at": "2022-04-18T03:21:14Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2111",
    "body": "## ❓ Questions and Help\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nThe  line 270 of  sequence_generator.py \r\nlprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs),\r\nI cannot understand the function of the code.\r\nSeemly lprobs always equal to themselves.\r\n#### Code\r\n\r\n            lprobs, avg_attn_scores = self.model.forward_decoder(\r\n                tokens[:, : step + 1], encoder_outs, self.temperature\r\n            )\r\n            lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)\r\n\r\n            lprobs[:, self.pad] = -math.inf  # never select pad\r\n            lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: latest\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2111/comments",
    "author": "yadongxi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:43Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2105,
    "title": "batch size with multiple gpus",
    "created_at": "2020-05-08T05:40:18Z",
    "closed_at": "2020-05-10T13:15:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2105",
    "body": "A quick question.\r\nIf batch size is 100 for fairseq-train, and 2 GPUs are used. \r\n(1) Does that mean each GPU gets a batch of 100? (It looks so.)\r\n(2) Does the model update parameters after processing 100 instances or 200? (It seems to be 200).\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2105/comments",
    "author": "ylmeng",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-05-10T13:15:16Z",
        "body": "(1) Yes\r\n(2) 200. Each GPU will get a batch of 100 instances (200 instances total for 2 GPUs), do a forward/backward pass, sum the gradients, then update the parameters"
      },
      {
        "user": "longkhanh-fam",
        "created_at": "2024-08-25T11:08:58Z",
        "body": "Hi @myleott , i encounter the problem that, if im traning no problem with 1 GPU, however, if i increase the GPU to more than 1, the sample list is empty. Should I increase the batch size? Thank you for your help\r\n"
      }
    ]
  },
  {
    "number": 2104,
    "title": "AttributeError while using evaluation snippet for MNLI",
    "created_at": "2020-05-08T02:52:58Z",
    "closed_at": "2022-04-18T03:21:13Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2104",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n#### What is your question?\r\nI modified the code of evaluation snippet for GLUE task to evaluate MNLI task. However, an error occurred\r\nTraceback (most recent call last):\r\n  File \"mnli_eva.py\", line 23, in <module>\r\n    prediction_label = label_fn(prediction)\r\n  File \"mnli_eva.py\", line 11, in <lambda>\r\n    [label + roberta.task.label_dictionary.nspecial]\r\n  File \"/net/setagaya/disk6/ztc/nlifile/fairseq/fairseq/data/dictionary.py\", line 83, in string\r\n    sent = \" \".join(token_string(i) for i in tensor if i.item() not in extra_symbols_to_ignore)\r\n  File \"/net/setagaya/disk6/ztc/nlifile/fairseq/fairseq/data/dictionary.py\", line 83, in <genexpr>\r\n    sent = \" \".join(token_string(i) for i in tensor if i.item() not in extra_symbols_to_ignore)\r\nAttributeError: 'int' object has no attribute 'item'\r\n\r\nThank you for your reply!\r\n#### Code\r\nfrom fairseq.models.roberta import RobertaModel\r\n\r\nroberta = RobertaModel.from_pretrained(\r\n            'checkpoints/',\r\n            checkpoint_file='checkpoint_best.pt',\r\n            data_name_or_path='MNLI-bin'\r\n             )\r\n\r\nlabel_fn = lambda label: roberta.task.label_dictionary.string(\r\n            [label + roberta.task.label_dictionary.nspecial]\r\n            )\r\nncorrect, nsamples = 0, 0\r\nroberta.cuda()\r\nroberta.eval()\r\nwith open('glue_data/MNLI/dev_matched.tsv') as fin:\r\n    fin.readline()\r\n    for index, line in enumerate(fin):\r\n        tokens = line.strip().split('\\t')\r\n        sent1, sent2, target = tokens[8], tokens[9], tokens[15]\r\n        tokens = roberta.encode(sent1, sent2)\r\n        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\r\n        prediction_label = label_fn(prediction)\r\n        ncorrect += int(prediction_label == target)\r\n        nsamples += 1\r\nprint('| Accuracy: ', float(ncorrect)/float(nsamples))\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (master):\r\n - PyTorch Version (1.4.0)\r\n - OS (Linux):\r\n - How you installed fairseq (`source):\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2104/comments",
    "author": "ztcintokyo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:33Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2102,
    "title": "Help: Understanding why multiprocessing_bpe_encoder.py adds sentences to dataset and how to proceed with my classification task",
    "created_at": "2020-05-07T20:19:47Z",
    "closed_at": "2020-05-09T22:54:51Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2102",
    "body": "## What is your question?\r\nI am following the steps in the custom_classification.md regarding fine-tuning RoBERTa on my classification task. I noticed that when I run multiprocessing_bpe_encoder.py on my train, val, test datasets, it is processing more lines than in the original dataset. This is shown below when I preprocess train, val, test.\r\n#### Code\r\n```\r\n!python /content/fairseq_repo/examples/roberta/multiprocessing_bpe_encoder.py \\\r\n      --encoder-json '/tmp/model-0/content/pretraining_output/tk-vocab.json' \\\r\n      --vocab-bpe '/tmp/model-0/content/pretraining_output/tk-merges.txt' \\\r\n      --inputs \"/content/data/train_texts.tsv\" \\\r\n      --outputs \"/content/data/train.input0.bpe\" \\\r\n      --workers 60 \\\r\n      --keep-empty\r\n\r\n!python /content/fairseq_repo/examples/roberta/multiprocessing_bpe_encoder.py \\\r\n      --encoder-json '/tmp/model-0/content/pretraining_output/tk-vocab.json' \\\r\n      --vocab-bpe '/tmp/model-0/content/pretraining_output/tk-merges.txt' \\\r\n      --inputs \"/content/data/train_labels.tsv\" \\\r\n      --outputs \"/content/data/train.label\" \\\r\n      --workers 60 \\\r\n      --keep-empty\r\n```\r\n\r\n```\r\n2020-05-07 20:05:41 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, bpe='gpt2', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/reddit-bin/input0', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer='nag', padding_factor=8, seed=1, source_lang=None, srcdict='dict.txt', target_lang=None, task='translation', tensorboard_logdir='', testpref='/content/data/test.input0.bpe', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='/content/data/train.input0.bpe', user_dir=None, validpref='/content/data/val.input0.bpe', workers=60)\r\n2020-05-07 20:05:41 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50263 types\r\n2020-05-07 20:05:46 | INFO | fairseq_cli.preprocess | [None] /content/data/train.input0.bpe: 17486 sents, 530589 tokens, 1.02% replaced by <unk>\r\n2020-05-07 20:05:46 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50263 types\r\n2020-05-07 20:05:48 | INFO | fairseq_cli.preprocess | [None] /content/data/val.input0.bpe: 3892 sents, 116555 tokens, 1.08% replaced by <unk>\r\n2020-05-07 20:05:48 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50263 types\r\n2020-05-07 20:05:51 | INFO | fairseq_cli.preprocess | [None] /content/data/test.input0.bpe: 3562 sents, 106116 tokens, 0.999% replaced by <unk>\r\n2020-05-07 20:05:51 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/reddit-bin/input0\r\n2020-05-07 20:05:53 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, bpe='gpt2', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/reddit-bin/label', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer='nag', padding_factor=8, seed=1, source_lang=None, srcdict=None, target_lang=None, task='translation', tensorboard_logdir='', testpref='/content/data/test.label', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref='/content/data/train.label', user_dir=None, validpref='/content/data/val.label', workers=60)\r\n2020-05-07 20:05:54 | INFO | fairseq_cli.preprocess | [None] Dictionary: 7 types\r\n2020-05-07 20:05:54 | INFO | fairseq_cli.preprocess | [None] /content/data/train.label: 9560 sents, 19120 tokens, 0.0% replaced by <unk>\r\n2020-05-07 20:05:54 | INFO | fairseq_cli.preprocess | [None] Dictionary: 7 types\r\n2020-05-07 20:05:55 | INFO | fairseq_cli.preprocess | [None] /content/data/val.label: 2048 sents, 4096 tokens, 0.0% replaced by <unk>\r\n2020-05-07 20:05:55 | INFO | fairseq_cli.preprocess | [None] Dictionary: 7 types\r\n2020-05-07 20:05:55 | INFO | fairseq_cli.preprocess | [None] /content/data/test.label: 2048 sents, 4096 tokens, 0.0% replaced by <unk>\r\n2020-05-07 20:05:55 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/reddit-bin/label   \r\n```\r\n\r\nAs you can see above, train.input0.bpe has 17486 sents, while train.label has 9560 sents. This is causing issues with training the model since the two datasets are not equal length.\r\n```\r\nTOTAL_UPDATES=125000    # Total number of training steps\r\nWARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\r\nPEAK_LR=1e-05         # Peak learning rate, adjust as needed\r\nTOKENS_PER_SAMPLE=512   # Max sequence length\r\nNUM_CLASSES=2\r\nMAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\r\nMAX_SENTENCES=16        # Number of sequences per batch (batch size)\r\nUPDATE_FREQ=16          # Increase the batch size 16x\r\nROBERTA_PATH=r'/content/drive/My\\Drive/Colab\\Notebooks/NLU/checkpoint_best.pt'    \r\n\r\n!python /usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py \\\r\n    --fp16 /content/reddit-bin/ \\\r\n    --restore-file $ROBERTA_PATH \\\r\n    --num-classes $NUM_CLASSES \\\r\n    --max-positions $MAX_POSITIONS \\\r\n    --task sentence_prediction \\\r\n    --criterion sentence_prediction \\\r\n    --arch roberta_base \\\r\n    --max-sentences-valid 2048 \\\r\n    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\r\n    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.1 \\\r\n    --max-sentences $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\r\n    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1 \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --num-workers\t1 \\\r\n    --me 8 \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\r\n    --truncate-sequence \\\r\n    --find-unused-parameters \\\r\n    --update-freq 4 \\\r\n    --save-dir DRIVE\r\n```\r\n\r\n\r\n```\r\nNamespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9,0.98)', adam_eps=1e-06, add_prev_output_tokens=False, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='/content/reddit-bin/', dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, init_token=None, keep_interval_updates=-1, keep_last_epochs=-1, log_format='simple', log_interval=1, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=8, max_positions=512, max_sentences=16, max_sentences_valid=2048, max_tokens=None, max_tokens_valid=None, max_update=125000, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_shuffle=False, num_classes=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, regression_target=False, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/content/drive/MyDrive/ColabNotebooks/NLU/checkpoint_best.pt', save_dir='DRIVE', save_interval=1, save_interval_updates=0, save_predictions=None, seed=1, sentence_avg=False, separator_token=None, skip_invalid_size_inputs_valid_test=True, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=125000, train_subset='train', truncate_sequence=True, update_freq=[4], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=10000, weight_decay=0.1)\r\n| [input] dictionary: 50265 types\r\n| [label] dictionary: 9 types\r\n| loaded 3892 examples from: /content/reddit-bin/input0/valid\r\n| loaded 2048 examples from: /content/reddit-bin/label/valid\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 337, in <module>\r\n    cli_main()\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 333, in cli_main\r\n    main(args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 48, in main\r\n    task.load_dataset(valid_sub_split, combine=False, epoch=0)\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq/tasks/sentence_prediction.py\", line 189, in load_dataset\r\n    sizes=[src_tokens.sizes],\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq/data/nested_dictionary_dataset.py\", line 60, in __init__\r\n    assert len(v) == len(first), 'dataset lengths must match'\r\nAssertionError: dataset lengths must match\r\n```\r\n\r\n## Environment\r\nI'm working on Google Colab.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2102/comments",
    "author": "rm3887",
    "comments": [
      {
        "user": "curranmaguire",
        "created_at": "2024-04-18T15:42:50Z",
        "body": "can you post the fix please \r\n"
      }
    ]
  },
  {
    "number": 2094,
    "title": "xlmr encode and sentence piece encoder off by one",
    "created_at": "2020-05-05T10:03:47Z",
    "closed_at": "2020-05-06T03:03:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2094",
    "body": "There seems to be an inconsistency in encoding returned by hub_interface and bpe model. The sentencepiece encodings seem to be off by one. Here are the code snippets:\r\n    \r\n    import torch\r\n    xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.large')\r\n    xlmr.encode('Hello World!')\r\n    tensor([0, 35378, 6661,38,2])\r\n\r\n    import sentencepiece as spm\r\n    sp = spm.SentencePieceProcessor()\r\n    sp.Load(\"./xlmr.large/sentencepiece.bpe.model\")\r\n    sp.EncodeAsIds('Hello World!')\r\n    [35377, 6660, 37]\r\n\r\nAm I missing something here? \r\n\r\nParticularly, I'm looking to fine-tune XLM-R for a binary sentence classification task. Which encoder should I use?\r\n\r\nI'm using torch 1.4.0 and sentencepiece 0.1.86",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2094/comments",
    "author": "ShubhC",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-05-06T03:03:36Z",
        "body": "Unfortunately this is because we have a secondary fairseq dictionary which adds some special tokens.\r\n\r\nTry this:\r\n```python\r\n# (...)\r\nsp_toks = ' '.join(sp.EncodeAsPieces('Hello world!'))\r\n# '▁Hello ▁world !'\r\n\r\nfrom fairseq.data import Dictionary\r\nfs_dict = Dictionary.load('./xlmr.large/dict.txt')\r\nfs_dict.encode_line(sp_toks)\r\n# tensor([35378,  8999,    38,     2], dtype=torch.int32)\r\n```"
      }
    ]
  },
  {
    "number": 2071,
    "title": "Has anyone tried this quantization method on Regression model?",
    "created_at": "2020-04-28T06:40:41Z",
    "closed_at": "2022-04-18T05:21:07Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2071",
    "body": "Has anyone tried this quantization method on regression model? I can only find experiments are done on classification problems in the paper. I'm wondering if the quantization method can also be used in regression problems without much performance decrease.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2071/comments",
    "author": "JiachuanDENG",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-04-30T11:23:02Z",
        "body": "@huihuifan have you tried regression tasks or have an intuition about if they’d work similarly to classification tasks?"
      },
      {
        "user": "huihuifan",
        "created_at": "2020-04-30T11:41:57Z",
        "body": "Thanks for the question! We did not try regression tasks, but I would expect the quantization method to work. int8 quantization in particular has been used in various regression tasks, and our method is a straightforward modification to simulate this at training time so the model is more robust to the effect of quantization at testing"
      },
      {
        "user": "JiachuanDENG",
        "created_at": "2020-05-01T11:12:05Z",
        "body": "> Thanks for the question! We did not try regression tasks, but I would expect the quantization method to work. int8 quantization in particular has been used in various regression tasks, and our method is a straightforward modification to simulate this at training time so the model is more robust to the effect of quantization at testing\r\n\r\n@huihuifan  Thanks for your reply, it's interesting that most of the papers talking about quantization methods are focusing on classification tasks rather than regression tasks. Do you have any paper lists about quantization methods that they have done experiments on regression tasks? That would be a great help!"
      },
      {
        "user": "huihuifan",
        "created_at": "2020-05-04T16:17:00Z",
        "body": "@pierrestock would you have some references?"
      },
      {
        "user": "rameshkunasi",
        "created_at": "2020-06-22T07:26:22Z",
        "body": "If any example code available please share"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:31Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2069,
    "title": "different language speech can transfer learning in wav2vec or vq-wav2vec?",
    "created_at": "2020-04-27T09:33:04Z",
    "closed_at": "2020-04-27T13:16:56Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2069",
    "body": "I want to train a wav2vec or  vq-wav2vec in chinese speech, and only english pretrained models are supported.  I want to know can I directly finetuning the english pretrained wav2vec model or vq-wav2vec model in chinese speech or I need to train from scratch using chiniese speech",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2069/comments",
    "author": "luweishuang",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-04-27T13:16:56Z",
        "body": "I would recommend training from scratch using Chinese speech"
      },
      {
        "user": "yardenkarny",
        "created_at": "2020-09-09T14:39:11Z",
        "body": "Hi,\r\n\r\nI read about your work of using a pre-trained model and fine-tune it to other low resource languages.\r\nIs there any news of multilingual pre-trained wav2vec models that can be downloaded or will be in the near future?\r\nthanks,\r\n\r\nYarden\r\n\r\n"
      }
    ]
  },
  {
    "number": 2066,
    "title": "How to set multiple reference files for fairseq-preprocess and fairseq-train command-line tools",
    "created_at": "2020-04-26T11:58:10Z",
    "closed_at": "2020-04-26T16:28:11Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2066",
    "body": "## ❓ Questions and Help\r\n\r\nI have one-on-one Chinese to English translation training files, where the source language is Chinese and the target language is English.Such as:\r\n- train.cn\r\n- train.en\r\n\r\nAnd I have one Chinese source file and corresponding four English files for the validation set and test set, e.g., \r\n- valid.cn\r\n- valid.en0\r\n- valid.en1\r\n- valid.en2\r\n- valid.en3\r\n\r\nThe question is that how to set the arguments to add the multiple reference files in the preprocess and training command-line tools?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2066/comments",
    "author": "XinzeZhang",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-04-26T16:28:11Z",
        "body": "This isn't possible, you need to have the same number of files for each language.  Either `valid.cn` and `valid.en` or `valid.cn0`, `valid.cn1`, ..., `valid.en0`, `valid.en1`, ..."
      }
    ]
  },
  {
    "number": 2054,
    "title": "Using roberta for fake news detection",
    "created_at": "2020-04-24T06:20:09Z",
    "closed_at": "2020-04-24T13:29:00Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2054",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2054/comments",
    "author": "aeh1307",
    "comments": [
      {
        "user": "aeh1307",
        "created_at": "2020-04-24T06:22:03Z",
        "body": "Hi, we want to use roberta for fake news detection. Is it possible to use roberta for prediction only, and not having to train the model ourselves? If that is the case, which pretrained model do you recommend for classifying whether a text is fake news or not? "
      },
      {
        "user": "lematt1991",
        "created_at": "2020-04-24T13:29:00Z",
        "body": "> Is it possible to use roberta for prediction only, and not having to train the model ourselves?\r\n\r\nI'm not sure, but I would be surprised if the model works for your task out of the box.  Usually some kind of finetuning is required."
      }
    ]
  },
  {
    "number": 2049,
    "title": "I got AttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'.",
    "created_at": "2020-04-23T01:28:14Z",
    "closed_at": "2020-04-23T04:44:24Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2049",
    "body": "#### What have you tried?\r\n!time fairseq-train \\\r\n    data-bin/iwslt14.tokenized.de-en \\\r\n    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 4096 \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-detok moses \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\r\n\r\n2020-04-23 00:55:17 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples\r\nepoch 001 | loss 9.592 | nll_loss 9.072 | ppl 538.049 | wps 9749.7 | ups 2.72 | wpb 3586.8 | bsz 145.5 | num_updates 1101 | lr 0.000137625 | gnorm 1.687 | clip 0 | oom 0 | train_wall 401 | wall 411\r\nepoch 001 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]2020-04-23 01:02:06 | INFO | fairseq.tasks.translation | example hypothesis: so this is so you know, you can see this is this is.\r\n2020-04-23 01:02:06 | INFO | fairseq.tasks.translation | example reference: that's about that big.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq_cli/train.py\", line 307, in cli_main\r\n    main(args)\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq_cli/train.py\", line 105, in main\r\n    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq_cli/train.py\", line 242, in validate\r\n    trainer.valid_step(sample)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq/trainer.py\", line 437, in valid_step\r\n    sample, self.model, self.criterion\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq/tasks/translation.py\", line 269, in valid_step\r\n    bleu = self._inference_with_bleu(self.sequence_generator, sample, model)\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq/tasks/translation.py\", line 356, in _inference_with_bleu\r\n    tokenize = sacrebleu.DEFAULT_TOKENIZER if not self.args.eval_tokenized_bleu else 'none'\r\nAttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'\r\n\r\n#### What's your environment?\r\n!pip install fairseq\r\nSuccessfully installed fairseq-0.9.0 mecab-python3-0.996.5 portalocker-1.7.0 sacrebleu-1.4.7\r\nPython 3.6.9\r\ntorch 1.4.0\r\nGoogle Colab GPU",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2049/comments",
    "author": "pdc-kaminaga",
    "comments": [
      {
        "user": "pdc-kaminaga",
        "created_at": "2020-04-23T02:43:15Z",
        "body": "Train.py was the same.\r\n\r\n!time python train.py \\\r\n    examples/translation/data-bin/iwslt14.tokenized.de-en \\\r\n    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 4096 \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-detok moses \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric\r\n    \r\n2020-04-23 02:26:59 | INFO | fairseq_cli.train | model transformer_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion\r\n2020-04-23 02:26:59 | INFO | fairseq_cli.train | num. model params: 39469056 (num. trained: 39469056)\r\n2020-04-23 02:27:02 | INFO | fairseq_cli.train | training on 1 GPUs\r\n2020-04-23 02:27:02 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None\r\n2020-04-23 02:27:02 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt\r\n2020-04-23 02:27:02 | INFO | fairseq.trainer | loading train data for epoch 0\r\n2020-04-23 02:27:02 | INFO | fairseq.data.data_utils | loaded 160239 examples from: examples/translation/data-bin/iwslt14.tokenized.de-en/train.de-en.de\r\n2020-04-23 02:27:02 | INFO | fairseq.data.data_utils | loaded 160239 examples from: examples/translation/data-bin/iwslt14.tokenized.de-en/train.de-en.en\r\n2020-04-23 02:27:02 | INFO | fairseq.tasks.translation | examples/translation/data-bin/iwslt14.tokenized.de-en train de-en 160239 examples\r\nepoch 001 | loss 9.592 | nll_loss 9.072 | ppl 538.049 | wps 9659.3 | ups 2.69 | wpb 3586.8 | bsz 145.5 | num_updates 1101 | lr 0.000137625 | gnorm 1.687 | clip 0 | oom 0 | train_wall 405 | wall 410\r\nepoch 001 | valid on 'valid' subset:   0% 0/62 [00:00<?, ?it/s]2020-04-23 02:33:54 | INFO | fairseq.tasks.translation | example hypothesis: so this is so you know, you can see this is this is.\r\n2020-04-23 02:33:54 | INFO | fairseq.tasks.translation | example reference: that's about that big.\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq_cli/train.py\", line 307, in cli_main\r\n    main(args)\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq_cli/train.py\", line 105, in main\r\n    valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq_cli/train.py\", line 242, in validate\r\n    trainer.valid_step(sample)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq/trainer.py\", line 437, in valid_step\r\n    sample, self.model, self.criterion\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq/tasks/translation.py\", line 269, in valid_step\r\n    bleu = self._inference_with_bleu(self.sequence_generator, sample, model)\r\n  File \"/content/drive/My Drive/Colab Notebooks/BERT/BERTによる翻訳/fairseq/fairseq/tasks/translation.py\", line 356, in _inference_with_bleu\r\n    tokenize = sacrebleu.DEFAULT_TOKENIZER if not self.args.eval_tokenized_bleu else 'none'\r\nAttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'"
      },
      {
        "user": "pdc-kaminaga",
        "created_at": "2020-04-23T04:44:24Z",
        "body": "The latest code download solved it.\r\nThank you."
      },
      {
        "user": "irugina",
        "created_at": "2020-04-27T08:01:11Z",
        "body": "Hello! I'm running into the same issue and cannot afford to try the most recent fairseq version. Does anyone know exactly where this problem comes from? Thank you! "
      },
      {
        "user": "myleott",
        "created_at": "2020-05-01T12:27:09Z",
        "body": "@irugina, it's due to an upstream change in sacrebleu. Here's the fix: adff51b4a67c5000aabbe0e00a7bc4b28e855794"
      }
    ]
  },
  {
    "number": 2036,
    "title": "fairseq-generate shuffles test data?",
    "created_at": "2020-04-21T16:04:07Z",
    "closed_at": "2020-04-21T18:25:12Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2036",
    "body": "The results generated from fairseq-generate are shuffled. I am not sure if it is caused by fairseq-preprocess or fairseq-generate, but sometimes we want the results to be in the original order. I suspect it is a way to make batches in optimal shapes.\r\nAlthough it is not hard to order them based on the 'H-XXX' prefix of each line, I wonder if there is any feature/mechanism to avoid the shuffling of test data.\r\n\r\nThe results from fairseq-interactive are in original order.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2036/comments",
    "author": "ylmeng",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-04-21T18:25:12Z",
        "body": "Yes, you should sort the results after the fact based on the `H-XXX` prefix.  The order is not preserved when using generate, because we sort sequences by length to minimize padding and increase efficiency.  This is not the case for interactive, since you don't have all the sequences up front to sort.  "
      },
      {
        "user": "salvacarrion",
        "created_at": "2021-12-28T03:16:54Z",
        "body": "To get the translations sorted in natural order you can run this command:\r\n\r\n```bash\r\ngrep ^S generate-test.txt | LC_ALL=C sort -V | cut -f2- > src.txt\r\ngrep ^T generate-test.txt | LC_ALL=C sort -V | cut -f2- > ref.txt\r\ngrep ^H generate-test.txt | LC_ALL=C sort -V | cut -f3- > hyp.txt\r\n```"
      }
    ]
  },
  {
    "number": 2033,
    "title": "Replicate result from a multi GPU training with less GPUs",
    "created_at": "2020-04-20T21:31:57Z",
    "closed_at": "2020-04-21T12:57:19Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2033",
    "body": "Hi:\r\n  \r\n   When I am reimplementing the baseline result from some paper I am always confused by the batch size issue.\r\n\r\n   For example, if a paper says the training is done with max-token 4000, trained with 8 GPUs, then I assume the effect batch size is 4000x8 =32000\r\n\r\n  If I only have 2 GPU, then if I want to get to as closer performance as possible, I should set the update frequency to 4?\r\n  So that my effective batch size would be 4000x2x4=32000?\r\n  The gradient would be computed per mini-batch (4000) in each GPUs and then accumulate delta for 2GPU x 4 times to make the network update? Is this how the gradient computation and updating work?\r\n \r\n  If not what is the best way of replicating the publish result's batch size with a lesser number of GPUs considering all other things are equal?\r\n\r\nBTW:  if I do training with 1 GPU and then 4GPU, would that affect the performance? It is important because I sometimes can have 1 GPU available, sometimes I have more. If the result of an experiment is very inconsistent due to the GPU numbers, then I have to redo some experiments, as the result difference could be caused by effect batch size changes. \r\n  \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2033/comments",
    "author": "ever4244",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-04-21T12:57:19Z",
        "body": "> For example, if a paper says the training is done with max-token 4000, trained with 8 GPUs, then I assume the effect batch size is 4000x8 =32000\r\n> If I only have 2 GPU, then if I want to get to as closer performance as possible, I should set the update frequency to 4?\r\n\r\nYes, your understanding is correct.\r\n\r\n> if I do training with 1 GPU and then 4GPU, would that affect the performance\r\n\r\nIt should only affect the runtime performance.  It should not affect the quality of the results."
      }
    ]
  },
  {
    "number": 2015,
    "title": "How to save model output from fairseq-generate?",
    "created_at": "2020-04-14T21:18:38Z",
    "closed_at": "2020-04-15T17:15:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2015",
    "body": "I just follow the tutorial and stuck on this command:\r\n```\r\nfairseq-generate data-bin/iwslt14.tokenized.de-en \\\r\n    --path checkpoints/fconv/checkpoint_best.pt \\\r\n    --batch-size 128 --beam 5\r\n```\r\nHow can I save model output on test part of my data? I spent a solid amount of time, but didn't find the answer. I found `--results-path` argument, but for some reason, it doesn't work for me and save data in a strange format, like `H- ...`.  Is there just to save the model output (predictions) on particular data?\r\nSorry, if this question is obvious, but I didn't find anything in docs.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2015/comments",
    "author": "skurzhanskyi",
    "comments": [
      {
        "user": "Alex-Fabbri",
        "created_at": "2020-04-15T14:21:48Z",
        "body": "You can just grep what's in your --results-path file to get the output. Otherwise afaik there isn't a way to get just the outputs. \r\n\r\ngrep ^T output.txt | cut -f2-  > target.txt\r\ngrep ^H output.txt | cut -f3-  > hypotheses.txt"
      },
      {
        "user": "myleott",
        "created_at": "2020-04-15T17:15:44Z",
        "body": "Yep, @Alex-Fabbri is right!"
      },
      {
        "user": "skurzhanskyi",
        "created_at": "2020-04-15T17:24:03Z",
        "body": "Thanks for the swift answer "
      },
      {
        "user": "NikhilPr95",
        "created_at": "2020-07-22T05:17:58Z",
        "body": "It would be great if there was a way to specify an output file on the command line. Currently I am facing issues because the console I am printing to does not have the necessary fonts."
      },
      {
        "user": "Jiahao004",
        "created_at": "2021-09-09T06:49:48Z",
        "body": "how could I save the output to shards?"
      }
    ]
  },
  {
    "number": 2013,
    "title": "Training crashes after a few steps with some worker encounter OOM ?",
    "created_at": "2020-04-14T12:42:16Z",
    "closed_at": "2022-04-18T02:21:23Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2013",
    "body": "hi, team\r\nI met an error when training on my 8GPUs machine. It is weird that the same process has been located on different GPU devices, I am confused about why this happens.\r\n```text\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage ===============================================================|\r\n|    0      3504      C   ...guimin.gm/miniconda3/envs/pt/bin/python 14023MiB |\r\n|    0      3505      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    0      3506      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    0      3507      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    1      3504      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    1      3505      C   ...guimin.gm/miniconda3/envs/pt/bin/python 14023MiB |\r\n|    1      3506      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    1      3507      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    2      3504      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    2      3505      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    2      3506      C   ...guimin.gm/miniconda3/envs/pt/bin/python 13881MiB |\r\n|    2      3507      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    3      3504      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    3      3505      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    3      3506      C   ...guimin.gm/miniconda3/envs/pt/bin/python   745MiB |\r\n|    3      3507      C   ...guimin.gm/miniconda3/envs/pt/bin/python 13851MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nAfter about half of the first epoch, the training crashed, here is the reported error:\r\n```text\r\nTraceback (most recent call last):\r\n  File \"../../train.py\", line 344, in <module>\r\n    cli_main()\r\n  File \"../../train.py\", line 336, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/guimin.gm/miniconda3/envs/pt/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/guimin.gm/miniconda3/envs/pt/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 3 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/gruntdata/guimin.gm/bart/fairseq-master/fairseq/distributed_utils.py\", line 177, in all_gather_list\r\n    result.append(pickle.loads(bytes(out_buffer[2 : size + 2].tolist())))\r\n_pickle.UnpicklingError: could not find MARK\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/guimin.gm/miniconda3/envs/pt/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/gruntdata/guimin.gm/bart/fairseq-master/train.py\", line 303, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/gruntdata/guimin.gm/bart/fairseq-master/train.py\", line 86, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/gruntdata/guimin.gm/bart/fairseq-master/train.py\", line 127, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/gruntdata/guimin.gm/bart/fairseq-master/fairseq/trainer.py\", line 397, in train_step\r\n    [logging_outputs, sample_sizes, ooms, self._prev_grad_norm]\r\n  File \"/gruntdata/guimin.gm/bart/fairseq-master/fairseq/distributed_utils.py\", line 181, in all_gather_list\r\n    'Unable to unpickle data from other workers. all_gather_list requires all '\r\nException: Unable to unpickle data from other workers. all_gather_list requires all workers to enter the function together, so this error usually indicates that the workers have fallen out of sync somehow. Workers can fall out of sync if one of them runs out of memory, or if there are other conditions in your training script that can cause one worker to finish an epoch while other workers are still iterating over their portions of the data.\r\n```\r\nThe task is about summarization and my command is:\r\n```shell\r\nTOTAL_NUM_UPDATES=10000\r\nWARMUP_UPDATES=500\r\nLR=3e-05\r\nMAX_TOKENS=1024\r\nUPDATE_FREQ=4\r\nBART_PATH=/gruntdata/guimin.gm/bart/fairseq-master/bart-pt/bart.large.cnn/finetuned_best.pt\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 python ../../train.py /home/guimin.gm/cnndm/cnndm-data/finished_files/cnn_dm-bin \\\r\n    --restore-file $BART_PATH --seed 111\\\r\n    --max-tokens $MAX_TOKENS \\\r\n    --save-dir ../../log/align3/ \\\r\n    --task translation\\\r\n    --source-lang source --target-lang target \\\r\n    --truncate-source\\\r\n    --layernorm-embedding \\\r\n    --share-all-embeddings \\\r\n    --share-decoder-input-output-embed \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --arch bart_large \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --label-smoothing 0.1 \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --weight-decay 0.01 --optimizer adam --adam-betas \"(0.9, 0.999)\" --adam-eps 1e-08 \\\r\n    --clip-norm 0.1 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --update-freq $UPDATE_FREQ \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --find-unused-parameters;\r\n```\r\n\r\nPyTorch Version 1.2\r\nCUDA/cuDNN version: 9.2\r\nPython version: Python 3.6\r\nGPU models and configuration: P100\r\n\r\nThanks a lot !\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2013/comments",
    "author": "gm0616",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:26:12Z",
        "body": "Can you reduce the batch size? \r\n"
      },
      {
        "user": "gm0616",
        "created_at": "2020-04-16T11:18:51Z",
        "body": "I`ve tried to reduce the UPDATE_FREQ from 4 to 3 and set the max-sentences to 1. However, it still encounters the same problem. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2007,
    "title": "for Summarization, How Cased/Uncased processing matter ?",
    "created_at": "2020-04-14T03:33:36Z",
    "closed_at": "2020-04-14T13:30:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2007",
    "body": "## ❓ How Cased/Uncased processing matter for Summarization Task?\r\n\r\nHi,  I noticed the fairseq use Cased for summarization. What's more, UniLM also use Cased data. While many previous SOTA works (like PointerGenerator, BertSumAbs and so on) use **uncased** instead.\r\n\r\nSo I'm curious why use **cased** instead of uncased? Is it enhance the performance notably, or just for better display?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2007/comments",
    "author": "fseasy",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:30:23Z",
        "body": "It depends on the original dataset used. For example, the pointer generator paper you are referring to uses the uncased version of cnn-dailymail. Cased is more realistic, but uncased can reduce the vocabulary size. "
      },
      {
        "user": "fseasy",
        "created_at": "2020-04-15T07:35:43Z",
        "body": "Sorry, but may be you missed my key points.\r\nthe PGN use the uncased CNNDM, while the BART use the cased CNNDM. So I'm wondering why use Cased? and how it maters? \r\nbecause BART is merged to the fairseq, so I have to asked here. "
      }
    ]
  },
  {
    "number": 2004,
    "title": "Explanation of Extra Embeddings after generation from Dict",
    "created_at": "2020-04-13T02:44:27Z",
    "closed_at": "2020-04-13T12:12:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2004",
    "body": "### What is your question?\r\n\r\nHi! I am trying to extract word embeddings and do some analysis on a transformer model I trained. Compared to the srcdict used to generate the emeddings the 'encoder.embed_tokens.weight' seems to have 4 more tokens. Can someone confirm if these extra or special tokens are at the end, beginning or somewhere else. Also, is the order of the srcdict maintained when initializing the embedding matrix.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2004/comments",
    "author": "reachtarunhere",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-13T12:12:36Z",
        "body": "Hello, the source dict models special tokens in the dictionary, such as unk and start of sentence. The order is maintained and they are always appended at the beginning. You can see the tokens added if you check the dictionary.py file. "
      },
      {
        "user": "reachtarunhere",
        "created_at": "2020-04-25T14:34:59Z",
        "body": "Thanks I got my thing to work :)"
      },
      {
        "user": "huihuifan",
        "created_at": "2020-04-26T20:01:52Z",
        "body": "fantastic!\r\n"
      }
    ]
  },
  {
    "number": 1990,
    "title": "Pre-processing Script",
    "created_at": "2020-04-10T16:48:48Z",
    "closed_at": "2020-04-13T12:15:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1990",
    "body": "I know this isn’t really an issue, more just a question. I’m relatively new to all this, but in the docs, for the example pre-processing, a script called ‘prepare-iwslt14.sh’ is called. I’ve looked into it but I want to ask, is it necessary to have shell scripts like these, and if so why?\r\n\r\nI’d appreciate it if anyone could help me with this.\r\n\r\n\r\nFor info:\r\n - fairseq Version: 0.9\r\n - PyTorch Version: 1.4.0\r\n - OS: Ubuntu (Linux)",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1990/comments",
    "author": "JustCunn",
    "comments": [
      {
        "user": "erip",
        "created_at": "2020-04-11T12:12:03Z",
        "body": "What's necessary for good performance is to tokenize and byte-pair encoder your data. Because `fairseq-preprocess` doesn't perform this for you, something will have to. The most convenient way to do this in an experiment is with scripts like `prepare-iwslt14.sh`."
      },
      {
        "user": "JustCunn",
        "created_at": "2020-04-11T13:38:24Z",
        "body": "Yeah, It’s just different coming from something like OpenNMT-py that does have some options to do pre-processing like that. Thanks very much for clearing that up!"
      },
      {
        "user": "erip",
        "created_at": "2020-04-11T14:13:27Z",
        "body": "PRs are welcome. 😄 "
      },
      {
        "user": "huihuifan",
        "created_at": "2020-04-13T12:15:18Z",
        "body": "Thanks @erip for answering! Yes, usually we preprocess the data with a separate script, though you can link it together with your training. The prepare-iwslt script also downloads data, I believe. "
      },
      {
        "user": "JustCunn",
        "created_at": "2020-04-14T12:15:32Z",
        "body": "Just while I have this thread, instead of opening a new one because I think the two issues might be related, but after training, when using Fairseq-generate, I enter `—batch-size 128` But it tells me that `--max-sentences/--batch-size cannot be larger than --buffer-size' print(args) `. It only works with a batch size of 0. I don’t know if this is affecting my translation quality or not. Could it have been pre-processed wrong?"
      },
      {
        "user": "myleott",
        "created_at": "2020-04-15T17:13:07Z",
        "body": "You're probably using `fairseq-interactive`, right? In that case, `--buffer-size` controls how many lines are read before generation happens. So just set `buffer-size=2000` and it should be fine. It will read 2000 lines and construct batches of size 128 out of it."
      },
      {
        "user": "JustCunn",
        "created_at": "2020-04-15T18:26:42Z",
        "body": "Yeah it must’ve been an issue like that. Whatever it was, it’s gone now, so that’s all good. Thanks for that though!"
      }
    ]
  },
  {
    "number": 1988,
    "title": "CamembertForTokenClassification model always predict the same label for a multi-classification problem",
    "created_at": "2020-04-09T14:21:33Z",
    "closed_at": "2020-04-22T15:03:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1988",
    "body": "Hi every one ! \r\n\r\nI'm working with the pretrained CamembertForTokenClassification  model for NER task and I don't undersatnd why I got the same logits and so the same labels batch after batch. The model doesn't learn and I don't know why. Do you have a clue ? \r\n\r\n**Code** : \r\n\r\n\r\n```\r\ndef train(train_dataloader,validation_dataloader,num_epochs:int):\r\n    \"\"\"\"\r\n    Train phase. Train + Validation steps. Print train loss and flat accuracy epochs after epochs.\r\n    ===================================================\r\n    params: \r\n        - train_dataloader : Data Loader of tensors train inputs, train labels and train masks.\r\n        - num_epochs : Number of epochs for the train phase\r\n        - num_batch : Size of the train batches\r\n        \r\n    returns : \r\n            /\r\n    \"\"\"\r\n    # Function to calculate the accuracy of our predictions vs labels\r\n\r\n    def flat_accuracy(preds, labels):\r\n        pred_flat = np.argmax(preds, axis=2).flatten()\r\n        labels_flat = labels.flatten()\r\n        return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n    \r\n    # Camembert fine-tuning parameters\r\n    param_optimizer = list(model.named_parameters())\r\n    no_decay = ['bias', 'gamma', 'beta']\r\n    optimizer_grouped_parameters = [\r\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\r\n         'weight_decay_rate': 0.01},\r\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\r\n         'weight_decay_rate': 0.0}\r\n    ]\r\n\r\n    optimizer = AdamW(model.parameters(), lr=1e-3, correct_bias=False)\r\n\r\n\r\n\r\n    # Store our loss and accuracy for plotting\r\n    train_loss_set = []\r\n\r\n    # Number of training epochs \r\n    epochs = num_epochs\r\n\r\n    # Camembert training loop\r\n    for _ in trange(epochs, desc=\"Epoch\"):  \r\n\r\n        ## TRAINING\r\n\r\n        # Set our model to training mode\r\n        model.train()  \r\n        # Tracking variables\r\n        tr_loss = 0\r\n        nb_tr_examples, nb_tr_steps = 0, 0\r\n      # Train the data for one epoch\r\n        for step, batch in enumerate(train_dataloader):\r\n\r\n            # Add batch to GPU\r\n            batch = tuple(t.to(device) for t in batch)\r\n            # Unpack the inputs from our dataloader\r\n            b_input_ids, b_input_mask, b_labels = batch\r\n            # Clear out the gradients (by default they accumulate)\r\n            optimizer.zero_grad()\r\n            # Forward pass\r\n            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\r\n            train_loss_set.append(loss.item())    \r\n            # Backward pass\r\n            loss.backward()\r\n            # Update parameters and take a step using the computed gradient\r\n            optimizer.step()\r\n            # Update tracking variables\r\n            tr_loss += loss.item()\r\n            nb_tr_examples += b_input_ids.size(0)\r\n            nb_tr_steps += 1\r\n            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\r\n\r\n      ## VALIDATION\r\n\r\n      # Put model in evaluation mode\r\n        model.eval()\r\n      # Tracking variables \r\n        eval_loss, eval_accuracy = 0, 0\r\n        nb_eval_steps, nb_eval_examples = 0, 0\r\n        # Evaluate data for one epoch\r\n        for batch in validation_dataloader:\r\n            # Add batch to GPU\r\n            batch = tuple(t.to(device) for t in batch)\r\n            # Unpack the inputs from our dataloader\r\n            b_input_ids, b_input_mask, b_labels = batch   \r\n            # print('val_labels:',b_labels)     \r\n            # Telling the model not to compute or store gradients, saving memory and speeding up validation\r\n            with torch.no_grad():\r\n              # Forward pass, calculate logit predictions\r\n              logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\r\n            # Move logits and labels to CPU\r\n            logits = logits.detach().cpu().numpy()\r\n            print('logits :',logits)\r\n            print('labels predicted:',np.argmax(logits, axis=2).flatten())\r\n            label_ids = b_labels.to('cpu').numpy()\r\n            # print('rela labels:',label_ids)\r\n            tmp_eval_accuracy = flat_accuracy(logits, label_ids) \r\n            eval_accuracy += tmp_eval_accuracy\r\n            nb_eval_steps += 1\r\n        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\r\n\r\n\r\n    # plot training performance\r\n    plt.figure(figsize=(15,8))\r\n    plt.title(\"Training loss\")\r\n    plt.xlabel(\"Batch\")\r\n    plt.ylabel(\"Loss\")\r\n    plt.plot(train_loss_set)\r\n    plt.show()\r\n\r\n```\r\n\r\n**After few batches,  here's what I got during a validation phase for a specific batch:** \r\n\r\n```\r\n\r\npredicted logits : [[[-1.0357289   1.2099022  -0.30169785 -0.13911007 -1.0021709 ]\r\n  [-1.0357288   1.2099023  -0.30169785 -0.13911007 -1.0021709 ]\r\n  [-1.0357289   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]\r\n  ...\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]]\r\n\r\n [[-1.0357288   1.2099022  -0.30169785 -0.13911007 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169782 -0.13911004 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169782 -0.13911003 -1.0021709 ]\r\n  ...\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911004 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]]\r\n\r\n [[-1.0357288   1.2099022  -0.30169785 -0.13911004 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911004 -1.0021709 ]\r\n  [-1.0357288   1.209902   -0.30169785 -0.13911004 -1.0021709 ]\r\n  ...\r\n  [-1.0357288   1.2099023  -0.30169782 -0.13911006 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911004 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911004 -1.0021709 ]]\r\n\r\n ...\r\n\r\n [[-1.0357288   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911007 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911007 -1.0021709 ]\r\n  ...\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911007 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169782 -0.13911004 -1.0021708 ]\r\n  [-1.0357288   1.2099022  -0.30169782 -0.13911004 -1.0021709 ]]\r\n\r\n [[-1.0357288   1.2099022  -0.30169785 -0.13911004 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169782 -0.13911004 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911004 -1.0021709 ]\r\n  ...\r\n  [-1.0357288   1.2099022  -0.30169782 -0.13911006 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911003 -1.0021709 ]\r\n  [-1.0357288   1.2099023  -0.30169785 -0.13911006 -1.0021709 ]]\r\n\r\n [[-1.0357288   1.2099023  -0.30169785 -0.13911006 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911007 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911007 -1.0021709 ]\r\n  ...\r\n  [-1.0357289   1.2099022  -0.30169785 -0.13911007 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]\r\n  [-1.0357288   1.2099022  -0.30169785 -0.13911006 -1.0021709 ]]]\r\nlabels predicted: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\r\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\r\nreal validation labels: tensor([[1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n         0],\r\n        [2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n         0],\r\n        [1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3,\r\n         2],\r\n        [2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 1, 1,\r\n         1],\r\n        [4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 0, 0, 0,\r\n         0],\r\n        [4, 4, 4, 4, 4, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n         1],\r\n        [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\r\n         2],\r\n        [2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 4, 4, 1, 1, 0, 0, 0, 0, 0,\r\n         0],\r\n        [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\r\n         0],\r\n        [1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\r\n         0]], device='cuda:0')\r\n\r\n```\r\n\r\n\r\n**Model** : \r\n\r\n`model = CamembertForTokenClassification.from_pretrained('camembert base',num_labels=num_labels).cuda()`\r\n\r\n\r\n**Model config :** \r\n\r\n```\r\n\r\nCamembertConfig {\r\n  \"_num_labels\": 5,\r\n  \"architectures\": [\r\n    \"CamembertForMaskedLM\"\r\n  ],\r\n  \"attention_probs_dropout_prob\": 0.1,\r\n  \"bad_words_ids\": null,\r\n  \"bos_token_id\": 5,\r\n  \"decoder_start_token_id\": null,\r\n  \"do_sample\": false,\r\n  \"early_stopping\": false,\r\n  \"eos_token_id\": 6,\r\n  \"finetuning_task\": null,\r\n  \"hidden_act\": \"gelu\",\r\n  \"hidden_dropout_prob\": 0.1,\r\n  \"hidden_size\": 768,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\",\r\n    \"2\": \"LABEL_2\",\r\n    \"3\": \"LABEL_3\",\r\n    \"4\": \"LABEL_4\"\r\n  },\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 3072,\r\n  \"is_decoder\": false,\r\n  \"is_encoder_decoder\": false,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1,\r\n    \"LABEL_2\": 2,\r\n    \"LABEL_3\": 3,\r\n    \"LABEL_4\": 4\r\n  },\r\n  \"layer_norm_eps\": 1e-05,\r\n  \"length_penalty\": 1.0,\r\n  \"max_length\": 20,\r\n  \"max_position_embeddings\": 514,\r\n  \"min_length\": 0,\r\n  \"model_type\": \"camembert\",\r\n  \"no_repeat_ngram_size\": 0,\r\n  \"num_attention_heads\": 12,\r\n  \"num_beams\": 1,\r\n  \"num_hidden_layers\": 12,\r\n  \"num_return_sequences\": 1,\r\n  \"output_attentions\": false,\r\n  \"output_hidden_states\": false,\r\n  \"output_past\": true,\r\n  \"pad_token_id\": 1,\r\n  \"prefix\": null,\r\n  \"pruned_heads\": {},\r\n  \"repetition_penalty\": 1.0,\r\n  \"task_specific_params\": null,\r\n  \"temperature\": 1.0,\r\n  \"top_k\": 50,\r\n  \"top_p\": 1.0,\r\n  \"torchscript\": false,\r\n  \"type_vocab_size\": 1,\r\n  \"use_bfloat16\": false,\r\n  \"vocab_size\": 32005\r\n}\r\n```\r\n> \r\n\r\n**What I tried :** \r\n\r\nI tried to tweak several hyperparameter like the hidden_dropout_prob ratio, the learning rate of the sizes of the batches and padding length but nothing worked...\r\n\r\n**My environment**`\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version : 1.4.0\r\n - OS (e.g., Linux): Windows 10\r\n - Python version: 3.6.9\r\n - transformers: 2.8.0\r\n - GPU models and configuration:\r\n_CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\r\n\r\nThank you for your help :)\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1988/comments",
    "author": "vicoof",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-13T12:16:06Z",
        "body": "@louismartin "
      },
      {
        "user": "louismartin",
        "created_at": "2020-04-14T08:11:23Z",
        "body": "Hi, sorry for the delay. \r\nAs I mentioned in my email, you should check if the weights of the model change during training.\r\nDo you see the training loss decreasing or is it constant ?\r\n"
      },
      {
        "user": "vicoof",
        "created_at": "2020-04-14T09:32:54Z",
        "body": "Hi no worry. \r\nThe weights are actually changing during training and the loss is decreasing untill a certain point. "
      },
      {
        "user": "louismartin",
        "created_at": "2020-04-14T12:10:30Z",
        "body": "And you confirm that the logits are exactly the same at the beginning of training and at the end of training when the weights have changed? "
      },
      {
        "user": "louismartin",
        "created_at": "2020-04-14T12:11:51Z",
        "body": "You should try to follow the path from the input embeddings to the output embeddings and try to see what happens. If the weights are different, then the output should be different."
      },
      {
        "user": "vicoof",
        "created_at": "2020-04-14T12:34:08Z",
        "body": "> And you confirm that the logits are exactly the same at the beginning of training and at the end of training when the weights have changed?\r\n\r\nYes the logits are evolving but converge towards a positive value for only one class and negative values for all the others. So the same class si predicted when using the argmax of the logits."
      },
      {
        "user": "louismartin",
        "created_at": "2020-04-14T12:37:50Z",
        "body": "Then I think it might be related to your training data or the way you feed the data/labels. I can't help you more on this...\r\nMake sure that you don't always feed the same label during training. Maybe one label is way more frequent than the others. You could investigate what happens when you remove this label from training."
      },
      {
        "user": "vicoof",
        "created_at": "2020-04-15T07:03:09Z",
        "body": "Indeed, one label is way more present than the others but when I remove it, the model predicts another one all the time...\r\nThanks anyway, "
      },
      {
        "user": "louismartin",
        "created_at": "2020-04-15T08:12:52Z",
        "body": "Maybe you could try on the same datasets that we used in the paper.\r\nIf you take the french part of the XNLI dataset, you can finetune using these instructions (MNLI part) .\r\nYou could also create a very simple dummy synthetic dataset to check that the training procedure works well. For instance a dataset where the label is the number of words or something like this."
      },
      {
        "user": "vicoof",
        "created_at": "2020-04-22T15:03:38Z",
        "body": "Hi,\r\nLooks like I messed with the labeling phase. I didn't understand well the mask and the padding goal. In fact, I added the label 0 for 'Unused label' which should be the label for the padded input. I didn't know that Bert was taking care of this already.\r\nAnyway, it's working fine now..."
      },
      {
        "user": "vicoof",
        "created_at": "2020-04-22T15:08:27Z",
        "body": "Hi,\r\nLooks like I messed up with the labeling phase. I didn't understand well the mask and the padding goal. In fact, I added the id label 0 for 'Unused input' which should be the label for the padding inputs. By doing this, I added a label which was not considered for my initial NER task ( like PERS, LOC or ORG) . So the algorithm abviously didn't work well. I didn't know that Bert was already taking care of this .\r\nAnyway, it's working fine now...\r\nSorry for the trouble and thank you for your help."
      },
      {
        "user": "ameya-parab",
        "created_at": "2020-12-04T01:14:20Z",
        "body": "> Hi,\r\n> Looks like I messed up with the labeling phase. I didn't understand well the mask and the padding goal. In fact, I added the id label 0 for 'Unused input' which should be the label for the padding inputs. By doing this, I added a label which was not considered for my initial NER task ( like PERS, LOC or ORG) . So the algorithm abviously didn't work well. I didn't know that Bert was already taking care of this .\r\n> Anyway, it's working fine now...\r\n> Sorry for the trouble and thank you for your help.\r\n\r\nHello,\r\n\r\nCan you please elaborate.on your fix? I'm facing a similar issue with CamemBertForSequenceClassification, wherein the model is predicting the same class over and over again for the entire dataset. The exact same dataset (English) version works perfectly with the Bert model (BertForSequenceClassification), but the translated French text predicts the same class for all of the examples with CamemBert.\r\n"
      }
    ]
  },
  {
    "number": 1967,
    "title": "Huge Difference between train wall time and wall time for En-Fr Transformer experiment",
    "created_at": "2020-04-05T11:03:18Z",
    "closed_at": "2020-04-21T15:38:00Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1967",
    "body": "2020-04-05 18:38:34 | INFO | fairseq_cli.train | model transformer_vaswani_wmt_en_fr_big, criterion LabelSmoothedCrossEntropyCriterion\r\n2020-04-05 18:38:34 | INFO | fairseq_cli.train | num. model params: 225086464 (num. trained: 225086464)\r\n2020-04-05 18:38:41 | INFO | fairseq_cli.train | training on 1 GPUs\r\n2020-04-05 18:38:41 | INFO | fairseq_cli.train | max tokens per GPU = 3584 and max sentences per GPU = None\r\n2020-04-05 18:38:41 | INFO | fairseq.trainer | no existing checkpoint found speed_test/checkpoint_last.pt\r\n2020-04-05 18:38:41 | INFO | fairseq.trainer | loading train data for epoch 0\r\n2020-04-05 18:38:51 | INFO | fairseq.data.data_utils | loaded 35762532 examples from: data-bin/wmt14_en_fr_share/train.en-fr.en\r\n2020-04-05 18:39:03 | INFO | fairseq.data.data_utils | loaded 35762532 examples from: data-bin/wmt14_en_fr_share/train.en-fr.fr\r\n2020-04-05 18:39:03 | INFO | fairseq.tasks.translation | data-bin/wmt14_en_fr_share train en-fr 35762532 examples\r\n\r\n\r\n2020-04-05 18:45:14 | INFO | fairseq.optim.adam | using FusedAdam\r\n2020-04-05 18:47:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\r\n2020-04-05 18:49:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\r\n2020-04-05 18:51:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\r\n2020-04-05 18:54:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\r\n2020-04-05 18:56:16 | INFO | train | {\"epoch\": 1, \"update\": 0.0, \"loss\": \"15.505\", \"nll_loss\": \"15.506\", \"loss_random\": \"15.5414\", \"loss_max\": \"15.5772\", \"ppl\": \"46523.6\", \"wps\": \"0\", \"ups\": \"0\", \"wpb\": \"52208\", \"bsz\": \"1080\", \"num_updates\": \"1\", \"lr\": \"2.24975e-07\", \"gnorm\": \"6.971\", \"clip\": \"0\", \"oom\": \"0\", \"loss_scale\": \"26\", \"train_wall\": \"60\", \"wall\": \"1055\"}\r\n2020-04-05 18:58:21 | INFO | train | {\"epoch\": 1, \"update\": 0.0, \"loss\": \"15.507\", \"nll_loss\": \"15.508\", \"loss_random\": \"15.5454\", \"loss_max\": \"15.5776\", \"ppl\": \"46598.1\", \"wps\": \"433.4\", \"ups\": \"0.01\", \"wpb\": \"53188\", \"bsz\": \"1200\", \"num_updates\": \"2\", \"lr\": \"3.4995e-07\", \"gnorm\": \"7.153\", \"clip\": \"0\", \"oom\": \"0\", \"loss_scale\": \"23\", \"train_wall\": \"68\", \"wall\": \"1180\"}\r\n2020-04-05 19:00:54 | INFO | train | {\"epoch\": 1, \"update\": 0.0, \"loss\": \"15.508\", \"nll_loss\": \"15.509\", \"loss_random\": \"15.5429\", \"loss_max\": \"15.5786\", \"ppl\": \"46633\", \"wps\": \"388.7\", \"ups\": \"0.01\", \"wpb\": \"53381.3\", \"bsz\": \"1312\", \"num_updates\": \"3\", \"lr\": \"4.74925e-07\", \"gnorm\": \"7.254\", \"clip\": \"0\", \"oom\": \"0\", \"loss_scale\": \"21\", \"train_wall\": \"77\", \"wall\": \"1333\"}\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1967/comments",
    "author": "gaopengcuhk",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-04-21T15:38:00Z",
        "body": "`\"wall\"` is the total runtime from the start of training up to the point the message is logged.  `\"train_wall\"` is only the time spent training."
      }
    ]
  },
  {
    "number": 1966,
    "title": "RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED",
    "created_at": "2020-04-05T09:42:34Z",
    "closed_at": "2020-04-09T03:39:42Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1966",
    "body": "## ❓ Questions and Help\r\n#### What is your question?\r\nI am trying to train transformer network for speech recognition. I am trying to run on 4 GPUs\r\nBefore it starts to train, i get this error:\r\nRuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\r\n\r\n#### Code/Command line\r\npython train.py $PATH/librispeech_final/ --save-dir model_save --max-epoch 80 --task speech_recognition --arch vggtransformer_2 --optimizer adadelta --lr 1.0 --adadelta-eps 1e-8 --adadelta-rho 0.95 --clip-norm 10.0  --max-tokens 5000 --log-format json --log-interval 1 --criterion cross_entropy_acc --user-dir examples/speech_recognition/ --num-workers 4 --fp16\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: master\r\n - PyTorch Version: 1.4.0\r\n - OS (e.g., Linux):\r\nLinux version 4.15.0-91-generic (buildd@lgw01-amd64-013) (gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1)) #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020\r\n- Python 3.8.2\r\n - CUDA/cuDNN version: Cuda compilation tools, release 10.0, V10.0.130\r\n - GPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti (UUID: GPU-cc00908a-78fe-231a-b153-5292607abcef)\r\nGPU 1: GeForce RTX 2080 Ti (UUID: GPU-ab293498-6452-3089-ff04-eeb1e53a8c65)\r\nGPU 2: GeForce RTX 2080 Ti (UUID: GPU-494342ab-818b-ebfb-b128-8eb2cac5308a)\r\nGPU 3: GeForce RTX 2080 Ti (UUID: GPU-215eb25d-e368-8517-4ec1-8a14cb0cac3e)\r\n\r\n#### ERROR \r\n\r\n2020-04-05 09:21:22 | INFO | fairseq_cli.train | model vggtransformer_2, criterion CrossEntropyWithAccCriterion\r\n2020-04-05 09:21:22 | INFO | fairseq_cli.train | num. model params: 315190057 (num. trained: 315190057)\r\n2020-04-05 09:21:22 | INFO | fairseq_cli.train | training on 4 GPUs\r\n2020-04-05 09:21:22 | INFO | fairseq_cli.train | max tokens per GPU = 5000 and max sentences per GPU = None\r\n2020-04-05 09:21:22 | INFO | fairseq.trainer | no existing checkpoint found model_save/checkpoint_last.pt\r\n2020-04-05 09:21:22 | INFO | fairseq.trainer | loading train data for epoch 1\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq_cli/train.py\", line 336, in cli_main\r\n    torch.multiprocessing.spawn(\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq_cli/train.py\", line 308, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq_cli/train.py\", line 100, in main\r\n    valid_losses = train(args, trainer, task, epoch_itr, max_update)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq_cli/train.py\", line 171, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq/trainer.py\", line 380, in train_step\r\n    raise e\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq/trainer.py\", line 352, in train_step\r\n    loss, sample_size_i, logging_output = self.task.train_step(\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq/tasks/fairseq_task.py\", line 337, in train_step\r\n    loss, sample_size, logging_output = criterion(model, sample)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/examples/speech_recognition/criterions/cross_entropy_acc.py\", line 92, in forward\r\n    net_output = model(**sample[\"net_input\"])\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 447, in forward\r\n    output = self.module(*inputs[0], **kwargs[0])\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq/models/fairseq_model.py\", line 267, in forward\r\n    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/examples/speech_recognition/models/vggtransformer.py\", line 331, in forward\r\n    x = self.conv_layers[layer_idx](x)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ahmed/projects/asr_quant/fairseq/fairseq/modules/vggblock.py\", line 115, in forward\r\n    x = self.layers[i](x)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 345, in forward\r\n    return self.conv2d_forward(input, self.weight)\r\n  File \"/root/anaconda3/envs/my_env/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 341, in conv2d_forward\r\n    return F.conv2d(input, weight, self.bias, self.stride,\r\nRuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1966/comments",
    "author": "AhmedTElthakeb",
    "comments": [
      {
        "user": "HiroshigeAoki",
        "created_at": "2020-09-10T03:29:59Z",
        "body": "Hello,\r\nI encounter the same issue. How did you solve it?"
      }
    ]
  },
  {
    "number": 1965,
    "title": "Convert list of sentences to IndexedRawTextDataset",
    "created_at": "2020-04-05T08:00:21Z",
    "closed_at": "2020-04-06T13:03:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1965",
    "body": "Does fairseq currently have a method to convert a list of sentences/string to `IndexedRawTextDataset `? From the code,  `IndexedRawTextDataset ` reads the sentences from a file.\r\n\r\nI actually want to create `monolingual dataset` from the list of sentences, is there a way to get that done directly without going through the `IndexedRawTextDataset ` route?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1965/comments",
    "author": "ajesujoba",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-04-06T13:03:09Z",
        "body": "You can use `fairseq-preprocess --only-source` to binarize the text (similar to what's used for binarizing language modeling text) and then in the code use `data_utils.load_indexed_dataset` to get the appropriate IndexedDataset instance."
      },
      {
        "user": "ajesujoba",
        "created_at": "2020-04-06T13:18:31Z",
        "body": "Hi, the explanation you gave implies that I have the text/sentences in a file, which is not what is happening in my case. A different process is generating the list of sentences and the sentences are not written to file. "
      },
      {
        "user": "myleott",
        "created_at": "2020-04-07T22:23:42Z",
        "body": "Ah, you can create a MonolingualDataset with any object that implements the `torch.utils.data.Dataset` interface. So `MonolingualDataset(your_list_of_tensors, ...)` should work. No need to use IndexedRawTextDataset (which is focused on files)."
      }
    ]
  },
  {
    "number": 1964,
    "title": "How can i add bleu validation in NAT task?",
    "created_at": "2020-04-04T13:08:17Z",
    "closed_at": "2022-04-18T03:21:12Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1964",
    "body": "When train nat models, i have a question about how to eval the valid set correctly. \r\ni think average last 5 checkpoint may not be very nice\r\nSo i add \r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\r\n    --eval-bleu-detok moses \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-print-samples \\\r\n    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\r\nit didn't work... \r\nand now i have a question about how to add bleu-eval in NAT task?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1964/comments",
    "author": "sfxjh",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-04-21T15:52:10Z",
        "body": "What do you mean it didn't work?  Please follow the issue template.  Provide a complete command and the full error that you are experiencing"
      },
      {
        "user": "risingdhxs",
        "created_at": "2020-05-08T01:44:05Z",
        "body": "I think what @sfxjh meant is that, when training the NAT models using fairseq, adding the options he mentioned didn't add the BLEU plot to tensorboard (or as an evaluation metric) as expected. I have encountered this issue as well. I trained a NAT model on IWSLT dataset using the following script:\r\n\r\npython train.py \"data-bin/iwslt14_joined\" \\\r\n    --arch nonautoregressive_transformer_iwslt_en_de \\\r\n    --optimizer adam \\\r\n    --adam-betas '(0.9,0.98)' \\\r\n    --criterion nat_loss \\\r\n    --task translation_lev \\\r\n    --label-smoothing 0.1 \\\r\n    --noise full_mask \\\r\n    --lr-scheduler inverse_sqrt \\\r\n    --warmup-init-lr '1e-07' \\\r\n    --lr 0.0005 \\\r\n    --min-lr '1e-09' \\\r\n    --warmup-updates 10000 \\\r\n    --dropout 0.3 \\\r\n    --weight-decay 0.01 \\\r\n    --decoder-learned-pos \\\r\n    --encoder-learned-pos \\\r\n    --pred-length-offset \\\r\n    --length-loss-factor 0.1 \\\r\n    --apply-bert-init \\\r\n    --max-tokens 8000 \\\r\n    --max-epoch 200 \\\r\n    --share-all-embeddings \\\r\n    --fixed-validation-seed 7 \\\r\n    --save-dir ./NATresults/IWSLT/checkpoints/NATtest_d3L5/ \\\r\n    --tensorboard-logdir ./NATresults/IWSLT/tensorboard/NATtest_d3L5/ \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 4, \"lenpen\": 0.3}' \\\r\n    --eval-bleu-remove-bpe \\\r\n    --eval-bleu-detok moses \\\r\n    --ddp-backend=no_c10d \\\r\n\r\nWhen I trained a regular Transformer with similar scripts, by adding the options with eval-bleu, I get a BLEU plot when I plot tensorboard. But not the case with NAT models: I still get best_loss, loss, and ppl plots, but not the BLEU plot.\r\n\r\nFairseq version: 0.9.0\r\npython: 3.7.4\r\npytorch: 1.2.0\r\nCUDA: 10.2\r\n\r\nLet me know if there's any essential config info I missed.\r\n\r\nThanks in advance."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:34Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1955,
    "title": "wav2vec training time & meaning of the log output.",
    "created_at": "2020-04-02T08:05:42Z",
    "closed_at": "2022-04-18T02:21:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1955",
    "body": "Hello,\r\n\r\nI running wav2vec on ~10.000 hours of audio on 6 GPU's with the aim of trying it with wav2letter++\r\nI read in the papers that the 2 models were trained on 40.000 and 400.000 steps.\r\n\r\nWhen should or can I stop training ? When I reach a certain loss on an epoch ? When I reach 400.000 num_updates ? when i reach 400.000 / number of cards ?\r\n\r\nWhat is the meaning of loss, wps, ups, wpb, etc ?\r\n\r\n81989/88743 [10:51:46<53:53,  2.09it/s, loss=5.72711e-14, wps=8.90638e+19, ups=2.11, wpb=4.2243e+19, bsz=5.26836e+06, num_updates=348000, lr=0.00020708, gnorm=0, clip=0, loss_scale=4, train_wall=47, wall=166952]\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1955/comments",
    "author": "joazoa",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:57Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1954,
    "title": "More translation models of mBART",
    "created_at": "2020-04-02T01:37:13Z",
    "closed_at": "2022-04-18T05:21:08Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1954",
    "body": "Hi Fairseq team,\r\n Are you going to publish more translation models listed in (Liu et al., arXiv 2020)? \r\n\r\nThanks,\r\nJudy",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1954/comments",
    "author": "ruofanhu",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-13T12:23:03Z",
        "body": "@ngoyal2707 "
      },
      {
        "user": "SunbowLiu",
        "created_at": "2020-06-24T07:57:56Z",
        "body": "Hi @ngoyal2707, any updates on releasing more pretrained models?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:30Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1950,
    "title": "Why does dict.string(tensor) return string with spaces between characters???",
    "created_at": "2020-03-31T19:57:59Z",
    "closed_at": "2020-05-30T00:16:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1950",
    "body": "Why does dict.string(tensor) return sentence/string with spaces between characters in fairseq. I have a dictionary e.g. src_dict and a tensor representing the sentence and using `.string()` method returns the following:\r\n`\r\nt h r e e e l e m e n t a r y s c h o o l s , L a k e T r a v i s E l e m e n t a r y S c h o o l , L a @ @ k e @ @ w a y E l e m e n t a r y S c h o o l , s e r v e s e c t i o n s o f L a @ @ k e @ @ w a y ..\r\n`\r\nInstead of:\r\n`\r\nthree elementary schools , Lake Travis Elementary School , La@@ ke@@ way Elementary School , serve sections of La@@ ke@@ way .\r\n`\r\nwhich is actually a sentence in the dataset.\r\n\r\nAm I using a wrong method? Is there a different method?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1950/comments",
    "author": "ajesujoba",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-04-21T15:41:00Z",
        "body": "This should be the correct method.  Can you provide a reproducible example?"
      }
    ]
  },
  {
    "number": 1945,
    "title": "AssertionError: assert high > 1",
    "created_at": "2020-03-31T07:58:23Z",
    "closed_at": "2020-04-21T07:46:20Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1945",
    "body": "i use aishell_v2 dataset to training wav2vec model,  commamnd line: python train.py examples/wav2vec --save-dir examples/wav2vec/aishell2_models --num-workers 6 --fp16 --max-update 400000 --save-interval 1 --arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam --max-lr 0.005 --lr-scheduler cosine --conv-feature-layers \"[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]\" --conv-aggregator-layers \"[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]\" --skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion binary_cross_entropy --num-negatives 10 --max-sample-size 150000 --max-tokens 1500000 --skip-invalid-size-inputs-valid-test\r\n\r\nand it always report assertionError when running to \"epoch 001:   3%| | 1291/47874 [20:33<12:10:38,  1.06it/s, loss=0.724954, wps=8.37628e+18, ups=1.07, wTraceback (most recent call last):\"",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1945/comments",
    "author": "luweishuang",
    "comments": [
      {
        "user": "caesar-one",
        "created_at": "2020-04-01T16:19:00Z",
        "body": "@luweishuang with `python train.py...` which file are you executing with command line? Where is the file located in the repo?"
      },
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:32:58Z",
        "body": "Hello, can you provide additional detail here? What assertion error is being hit? "
      }
    ]
  },
  {
    "number": 1944,
    "title": "Use pre-trained BART to generate ",
    "created_at": "2020-03-31T06:14:51Z",
    "closed_at": "2020-04-02T04:20:05Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1944",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\n\r\nHi\r\nI am currently running an experiment for a seq2seq generation task where i have my own source target and fine-tune BART and get great results\r\n\r\nI want to see how are the results with just pre-trained BART without fine-tuning on my dataset. Is there a way for BART to generate the next set of tokens like GPT2 by just providing the prefix and let it finish a sequence\r\n\r\nI tested with four inputs\r\n\r\nMilton scrunched his eyes and moodily turned back to his computer like a\r\nI was crying like a\r\nIt was raining like cats and\r\nThe wind stampeded through me like a\r\nShe was walking like a\r\n~                                         \r\n\r\n\r\nONLY \r\n\r\nMilton scrunched his eyes and moodily turned back to his computer like a\r\nI was crying like a **baby.**\r\nIt was raining like cats and\r\nThe wind stampeded through me like a\r\nShe was walking like a **little girl.**\r\n\r\nThe other sentences did not generate any more. How to ensure the remaining sentences finish without generating an eos ? \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nEnvironment\r\n- fairseq Version (e.g., 1.0 or master): 0.9.0\r\n- PyTorch Version (e.g., 1.0) 1.3.1\r\n- OS (e.g., Linux): Linux\r\n- How you installed fairseq (pip, source):\r\n- Build command you used (if compiling from source):\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU models and configuration: RTX 2080 ( 4 gpus)\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1944/comments",
    "author": "tuhinjubcse",
    "comments": [
      {
        "user": "tuhinjubcse",
        "created_at": "2020-04-02T03:26:33Z",
        "body": "@myleott  any tricks or pointers ?"
      },
      {
        "user": "akholy",
        "created_at": "2020-09-14T17:15:50Z",
        "body": "@tuhinjubcse did you find a way to use BART to generate the next set of tokens like GPT2?"
      },
      {
        "user": "tuhinjubcse",
        "created_at": "2020-09-15T00:48:55Z",
        "body": "@akholy  yes I did. Since BART is a seq2seq model with an autoregressive decoder, you can\r\nsay your test.source will contain the format\r\n\r\nMy name is Tuhin and\\tMy name is Tuhin and I am a PhD\r\nHere \"My name is Tuhin\" is the prefix you are passing to pre-trained BART , for the decoder to generate tokens autoregressively like GPT2\r\n\r\nIdeally best situation is to use a finetuned BART model on any generation task to generate set of next tokens . but should you have to generate using pre-trained BART this is way. I somehow could not generate in batches . It worked for me for single utterance ( source , prefix)  , I did not have lot of time to make it work for batches but hope this helps\r\n\r\n\r\n**Inference Code**\r\n\r\n```\r\nimport torch\r\nfrom fairseq.models.bart import BARTModel\r\nimport os\r\nimport time\r\nimport numpy as np\r\nos.environ['CUDA_VISIBLE_DEVICES']=\"1\"\r\n#np.random.seed(42)\r\n\r\n#extract bart.large.zip \r\nbart = BARTModel.from_pretrained('bart.large', checkpoint_file='model.pt',task='translation',data_name_or_path='your data')\r\n\r\nbart.cuda()\r\nbart.eval()\r\n\r\ntorch.manual_seed(4)\r\nnp.random.seed(4)\r\n\r\ncount = 1\r\nbsz = 1\r\nprint(\"done\")\r\nt = 0.7\r\nfor val in [5]\r\n    with open('data/test.source') as source, open('data/test.target', 'w') as fout:\r\n        line = source.readline().strip()\r\n        sline = line.split('\\t')[0]\r\n        pline = line.split('\\t')[1]\r\n        slines = [sline]\r\n        plines = [pline]\r\n        for sline in source:\r\n            if count % bsz == 0:\r\n                with torch.no_grad():\r\n                    hypotheses_batch = bart.sample(slines, plines, sampling=True, sampling_topk=val  ,temperature=t ,lenpen=2.0, max_len_b=30, min_len=length, no_repeat_ngram_size=3)\r\n                for hypothesis in hypotheses_batch:\r\n                    fout.write(hypothesis.replace('\\n','') + '\\n')\r\n                    fout.flush()\r\n                slines = []\r\n\r\n            slines.append(sline.strip().split('\\t')[0])\r\n            plines.append(sline.strip().split('\\t')[1])\r\n            count += 1\r\n        \r\n        if slines != []:\r\n            print(\"here\",slines)\r\n            hypotheses_batch = bart.sample(slines, plines, sampling=True,   sampling_topk=val  ,temperature=t ,lenpen=2.0, max_len_b=30, min_len=20, no_repeat_ngram_size=3)\r\n            for hypothesis in hypotheses_batch:\r\n                fout.write(hypothesis.replace('\\n','') + '\\n')\r\n                fout.flush()\r\n\r\n```\r\n\r\nYou would need to change the signatures of hub_interface.py inside fairseq/models.bart as well\r\n\r\n\r\n```\r\ndef sample(self, sentences: List[str], prefixes: List[str], beam: int = 1, verbose: bool = False, **kwargs) -> str:\r\n        input = [self.encode(sentence) for sentence in sentences]\r\n        prefix = [self.encode(sentence)[:-1] for sentence in prefixes]\r\n        hypos = self.generate(input, prefix, beam, verbose, **kwargs)\r\n        return [self.decode(x['tokens']) for x in hypos]\r\n```\r\n\r\n```\r\ndef generate(self, tokens: List[torch.LongTensor], prefix: List[torch.LongTensor], beam: int = 5, verbose: bool = False, **kwargs) -> torch.LongTensor:\r\n        sample = self._build_sample(tokens)\r\n        prefix_sample = self._build_sample(prefix)\r\n\r\n\r\n        # build generator using current args as well as any kwargs\r\n        gen_args = copy.copy(self.args)\r\n        gen_args.beam = beam\r\n        for k, v in kwargs.items():\r\n            setattr(gen_args, k, v)\r\n        generator = self.task.build_generator(gen_args)\r\n        translations = self.task.inference_step(\r\n            generator,\r\n            [self.model],\r\n            sample,\r\n            prefix_tokens=prefix_sample['net_input']['src_tokens'],\r\n        )\r\n       \r\n\r\n        if verbose:\r\n            src_str_with_unk = self.string(tokens)\r\n            print('S\\t{}'.format(src_str_with_unk))\r\n\r\n        def getarg(name, default):\r\n            return getattr(gen_args, name, getattr(self.args, name, default))\r\n\r\n        # Process top predictions\r\n        hypos = [x[0] for x in translations]\r\n        hypos = [v for _, v in sorted(zip(sample['id'].tolist(), hypos))]\r\n        return hypos\r\n```\r\n"
      }
    ]
  },
  {
    "number": 1939,
    "title": "Question on Scaling Machine Translation Evaluation remove-bpe command",
    "created_at": "2020-03-30T14:14:16Z",
    "closed_at": "2020-04-02T11:19:24Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1939",
    "body": "fairseq-generate \\\r\n    data-bin/wmt16_en_de_bpe32k \\\r\n    --path checkpoint.avg5.pt \\\r\n    --beam 4 --lenpen 0.6 --remove-bpe\r\n\r\nWhen I drop the --remove-bpe, the BLEU score will increase a lot. Can you give any explanation? \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1939/comments",
    "author": "gaopengcuhk",
    "comments": [
      {
        "user": "LividWo",
        "created_at": "2020-03-31T08:19:31Z",
        "body": "same here. But I think this is expected? Since the model is trained to generate subwords instead of words."
      },
      {
        "user": "myleott",
        "created_at": "2020-04-02T11:19:24Z",
        "body": "Yes, it will generate subwords, for example: `He@@ llo Worl@@ d`. If you don’t remove the BPE continuation markers, this would give you an incorrect (larger) BLEU score."
      }
    ]
  },
  {
    "number": 1937,
    "title": "How to generate translation using multiple GPUs?",
    "created_at": "2020-03-30T13:01:05Z",
    "closed_at": "2020-04-13T12:25:21Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1937",
    "body": "Hi, \r\nRight now I use --num-shards and --shard-id to generate multiple files by multiple gpus, and then concat the results to model multiple gpu decoding.\r\nIs there any more convenient way?\r\nThank you very much!\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1937/comments",
    "author": "SunbowLiu",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-13T12:25:21Z",
        "body": "Unfortunately, this is the current solution. If you have an improvement, please make a pull request!"
      },
      {
        "user": "munael",
        "created_at": "2020-08-13T07:09:58Z",
        "body": "It would be great something like a world size parameter could be set that automatically set the num-shards and such flags appropriately then concat the results automatically on output (and remove the shards). I'll look into but am not that versed in the codebase so could take a while."
      },
      {
        "user": "mahdiabavisani",
        "created_at": "2021-07-28T20:16:26Z",
        "body": "how do you concat the results from the multiple generated predictions to keep the order similar to the original file? "
      }
    ]
  },
  {
    "number": 1931,
    "title": "wav2vec : can we use pretrained wav2vec model to extract features from audio sampled with less sampling rates (12K)?",
    "created_at": "2020-03-28T04:40:56Z",
    "closed_at": "2020-03-29T02:40:27Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1931",
    "body": "I have set of audios that are sampled at 12K **not 16K**. Since wa2vec has retained on 16K audio, is that ok if I use wav2vec to extract features from 12K audios?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1931/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "sgxthsat2019",
        "created_at": "2020-04-21T09:42:44Z",
        "body": "Hi，\r\ncan you share me how you deal with this problem? I just have 8k audios and the wav2vec model doesn't work well.\r\nThanks a lot!"
      },
      {
        "user": "sgxthsat2019",
        "created_at": "2020-04-21T09:43:53Z",
        "body": "@alexeib "
      },
      {
        "user": "alexeib",
        "created_at": "2020-04-23T18:09:45Z",
        "body": "you will need to resample your 12k/8k audio to 16k before you can use the wav2vec model. this is not something we've tested so its not clear how well it will work, though. let us know!"
      },
      {
        "user": "sgxthsat2019",
        "created_at": "2020-04-27T16:53:07Z",
        "body": "> you will need to resample your 12k/8k audio to 16k before you can use the wav2vec model. this is not something we've tested so its not clear how well it will work, though. let us know!\r\n\r\nI use Pushto, not English data, to train an ASR model. The 8k audio with wav2vec feature doesn't work well. I'm checking the reason for the bad performance. \r\nCan you give me some advisement on finetuning the wav2vec model using NOT-English data and/or NOT-16k audio data? In my case, I use 8k Pushto data.\r\nBy the way, wav2vec model work very well on English data! I'm so appreciate your work!\r\nThanks a lot~ @alexeib "
      },
      {
        "user": "alexeib",
        "created_at": "2020-04-27T17:45:52Z",
        "body": "the feature encoder is designed for 16k sample rate. if you pre-train your model on 8k data you may want to change the feature encoder architecture such that strides and receptive field remains the same as it would be for 16k"
      },
      {
        "user": "sgxthsat2019",
        "created_at": "2020-04-29T08:59:14Z",
        "body": "> the feature encoder is designed for 16k sample rate. if you pre-train your model on 8k data you may want to change the feature encoder architecture such that strides and receptive field remains the same as it would be for 16k\r\n\r\nOK, I will have a try. Thank you so much~"
      },
      {
        "user": "m-wiesner",
        "created_at": "2020-06-17T14:48:32Z",
        "body": "As far as I can tell, the resampling is done with \r\n\r\nF.interpolate(x.view(1, 1, -1), scale_factor=factor).squeeze()\r\n\r\nwith the default interpolation mode ('nearest'). Maybe this doesn't ultimately matter, but it seems less than ideal. Switching it to one of the other interpolation options would probably be better. The best would be to use torchaudio which does sincinterpolation or librosa maybe?\r\n"
      },
      {
        "user": "dcshapiro",
        "created_at": "2021-12-21T04:13:07Z",
        "body": "Tried on 8k audio upsampled to 16k... It did not work well at all. The model hears the sounds and guesses words with those sounds, but it is almost 100% wrong on which words it selects. Oh well. I guess retraining from scratch is the way to go if you want to inference on 8k audio...."
      }
    ]
  },
  {
    "number": 1928,
    "title": "what tools is used for sentence segmentation in Roberta",
    "created_at": "2020-03-28T01:22:21Z",
    "closed_at": "2022-04-18T02:21:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1928",
    "body": "Hi,\r\n\r\nIn DOC-SENTENCES and FULL-SENTENCES training format, what tools is used to segment sentences on multilingual training data?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1928/comments",
    "author": "tuyaao",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:33:06Z",
        "body": "@myleott "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:54Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1925,
    "title": "multilingual_denoising task errors",
    "created_at": "2020-03-27T12:20:37Z",
    "closed_at": "2020-04-18T21:11:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1925",
    "body": "I'm trying to run the 'multilingual_denoising' task for pretraining mbart, and I get this error:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/fairseq/train.py\", line 11, in <module>\r\n    cli_main()\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 321, in cli_main\r\n    main(args)\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 59, in main\r\n    task.load_dataset(valid_sub_split, combine=False, epoch=1)\r\n  File \"/home/ubuntu/fairseq/fairseq/tasks/multilingual_denoising.py\", line 158, in load_dataset\r\n    eos=None if not self.args.add_lang_token else self.source_dictionary.index('[{}]'.format(language)),\r\n  File \"/home/ubuntu/fairseq/fairseq/data/denoising_dataset.py\", line 133, in __init__\r\n    raise (f'if using subwords, use replace-length=1 or 0')\r\nTypeError: exceptions must derive from BaseException",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1925/comments",
    "author": "moyid",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:35:04Z",
        "body": "Can you provide more detail please? What command are you running, what dataset? @yinhanliu "
      },
      {
        "user": "yinhanliu",
        "created_at": "2020-04-14T20:06:56Z",
        "body": "@ngoyal2707 can you take a look? I won't have time until this weekend. "
      },
      {
        "user": "ngoyal2707",
        "created_at": "2020-04-18T21:11:03Z",
        "body": "@moyid can you please try giving `replace-length` as `span-poisson`"
      }
    ]
  },
  {
    "number": 1921,
    "title": "Bilingual Translation in Fairseq",
    "created_at": "2020-03-26T22:09:38Z",
    "closed_at": "2020-03-29T23:26:06Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1921",
    "body": "I am trying to train a Multilingual MT between two languages, English(En) and German(De). It is expected to be a Bilingual (Bidrectional) training. \r\n\r\nCan I do it without using the following command: `task multilingual_translation --lang-pairs de-en,en-de`? This requires that I have two copies of the training data? Or can I achieve the training without the multiple copies ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1921/comments",
    "author": "ajesujoba",
    "comments": [
      {
        "user": "Alex-Fabbri",
        "created_at": "2020-03-29T21:31:05Z",
        "body": "I think you can just preprocess the data in one direction and follow the naming conventions and create symbolic links for the other direction, to avoid duplicating the data. "
      }
    ]
  },
  {
    "number": 1919,
    "title": "BART extract_features: memory reduction options + validity of running batch mode",
    "created_at": "2020-03-26T18:14:12Z",
    "closed_at": "2022-04-18T02:21:20Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1919",
    "body": "I'm trying to use BART as an encoder of another system using `bart.extract_features`. But I'm running into a couple issues that I'd like advice trying to find workarounds for.\r\n\r\n1) BART (bart-large) takes up a lot of memory which makes me end up getting OOM (even in eval mode) when I try to train a separate (very lightweight) model at the same time. \r\nI am using `bart.eval() bart.half() bart.cuda()` and limiting the maximum sequence length to encode (to 200), but it's still an issue. The only function I am calling is bart.extract_features.\r\nSo questions:\r\n* Is there anything else I can do memory saving wise?\r\n* I thought about trying to rip out parts of the model that I shouldn't need - like the entire decoder. But when I experimented with this it seems like the extract_features code actually accesses a number of different decoder functions/properties. Any suggestions in this regard?\r\n\r\nI'm just using one GPU with 11GB (RTX 2080) but my batch size is tiny (literally 2 at the moment) so I'm not sure parallelising would do much.\r\n\r\n2) I have tried running `bart.extract_features` on each sample individually, and also on a batch of samples. I've noticed that if I then compare the stacked output of individually running a set of samples and of running a batch, `torch.eq(a,b)` is false but if I print out the output the tensors are the same.\r\nThis is because at some precision (~8 digits, based on a couple examples) the tensor element values _are_ different (and aren't appearing in print by default). \r\n\r\nCan you shed any light on why this is, and whether I need to worry about it? It seems quite small but I also don't know why it is and haven't checked how much variance there is in the differences.\r\n\r\nCode snippets for single sample vs. batch are:\r\n```\r\nbatch = collate_tokens([bart.encode(s) for s in text], pad_idx=1)\r\nb_t = bart.extract_features(batch)\r\ns_t = torch.stack([bart.extract_features(x).squeeze(0) for x in batch])\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: bart-large\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1919/comments",
    "author": "seraphinatarrant",
    "comments": [
      {
        "user": "tuhinjubcse",
        "created_at": "2020-03-26T19:10:31Z",
        "body": "@myleott Can you please help us here . We are stuck on this "
      },
      {
        "user": "seraphinatarrant",
        "created_at": "2020-03-31T17:45:19Z",
        "body": "ok I worked out basically what the major issue is - that `extract_features` was accumulating gradients, whereas if I run other inference code the generation is wrapping in `no_grad` so it's not a problem.\r\n\r\nNo idea if this was intentional or not - it surprised me because I think of extract_features as decoding instead of training but I may be coming from a different set of use cases. Perhaps it should be more clear or an explicit flag though?\r\nIt's now less important to reduce model size (as now it only takes 2.5 gb) though it would still be good to know if there is a way. "
      },
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:38:00Z",
        "body": "@ngoyal2707 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:52Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1913,
    "title": "How can I save a scripted module?",
    "created_at": "2020-03-26T00:16:10Z",
    "closed_at": "2022-04-28T22:22:04Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1913",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI'd like to embed a fairseq model in a java application, but when I try to load a pretrained model using the java runtime, I see the following error:\r\n\r\n```\r\n14393: Error: com.facebook.jni.CppException: `torch::jit::load()` received a file from `torch.save()`, but `torch::jit::load()` can only load files produced by `torch.jit.save()` (load at /Users/erippeth/Code/pytorch/torch/csrc/jit/serialization/import.cpp:287)\r\n```\r\n\r\nHow can I save my fairseq models using `torch.jit.save` instead of `torch.save`?\r\n\r\n#### What have you tried?\r\n\r\nN/A\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): 1.5.0.dev20200324\r\n - OS (e.g., Linux): OS X\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source): n/a\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: n/a\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1913/comments",
    "author": "erip",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:39Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:35Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1909,
    "title": "How to change vocabulary size in transformer model? ",
    "created_at": "2020-03-25T18:19:53Z",
    "closed_at": "2020-03-31T19:42:53Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1909",
    "body": "Hi, I am using Transformer model to translate English to Spanish. \r\n\r\nFirst, I trained transformer in Europarl data.  The vocabulary size is 7376 for english and 9216 for Spanish. Then, I finetuned a smaller dataset on this model, the vocabulary size is 6447 types for english and 8783 in Spanish.\r\n\r\nThe problem is, the vocabulary size didn't match, so I can't continue training extra steps in this smaller dataset, using my pretrained model. \r\n\r\nI have tried using fairseq-proprocess --tgtdict and --srcdict, to preprocess my smaller dataset based on Europarl vocabulary, which will give me the same dimension of vocabulary. However, words that in my smaller dataset has many terminologies, which does not contained in Europral vocabulary.  Using -tgtdict and --srcdict will sacrifice these words into unknown.  The bleu score is obviously poor. \r\n\r\nThe script I am using is as follows: it haves the dimension mismatch problem. \r\nfairseq-train \\\r\n    data-bin/smaller_dataset \\\r\n    --restore-file checkpoints/Europarl/checkpoint_best.pt \\             # vocabulary size: (7376, 9216)\r\n    --arch transformer --dropout 0.1 \\                                                  # vocabulary size:  (6447, 8783) \r\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\r\n    --lr 8e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\r\n    --dropout 0.3 --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n    --max-tokens 4096 \\\r\n    --fp16 \\\r\n    --save-dir checkpoints/result\r\n\r\nthanks. \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1909/comments",
    "author": "leixxli",
    "comments": [
      {
        "user": "villmow",
        "created_at": "2020-03-30T21:39:46Z",
        "body": "Use byte-pair-encoding before  preprocessing your dataset with fairseq (e.g. subword-nmt)."
      },
      {
        "user": "leixxli",
        "created_at": "2020-03-31T19:42:53Z",
        "body": "Thanks for the reply. @villmow \r\n\r\nI can either change the BPE_TOKENS in subword-nmt stage, or preprocessing the combined dataset, then using this dictionary to generate individual dictionary for each dataset. \r\n\r\n"
      }
    ]
  },
  {
    "number": 1906,
    "title": "Using RoBERTa to fill sub-word masks",
    "created_at": "2020-03-25T14:51:26Z",
    "closed_at": "2020-04-14T13:39:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1906",
    "body": "Quick question: How does RoBERTa as a masked language model handle rare word mask filling?\r\n\r\nContext: In original BERT, if you want to predict a token that consists of multiple word pieces, you need multiple masks; e.g. \r\n```\r\nHe works diligently .\r\nHe works [MASK] [MASK] [MASK] .\r\nHe works di ##ligent ##ly .\r\n```\r\nIs the same true for RoBERTa? I know RoBERTa uses BPE instead of word pieces, but do we still need to know how many sub-words constitute a complete word?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1906/comments",
    "author": "chrisjbryant",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:39:51Z",
        "body": "Yes, it should be the same as original BERT, though RoBERTa and BERT use different subwords schemes. "
      }
    ]
  },
  {
    "number": 1905,
    "title": "Is the pretrained-model unidirectional？",
    "created_at": "2020-03-25T12:12:58Z",
    "closed_at": "2022-04-18T02:21:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1905",
    "body": "## ❓ Questions and Help\r\n Whether i can use the pretrained-model A-B to translate A to B and i can use it to translate B to A or not?  Thanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1905/comments",
    "author": "Okay686",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:40:13Z",
        "body": "This is too vague, which pretrained model are you referring to?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:52Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1900,
    "title": "vq-wav2vec RoBERTa extract all features or just final layer?",
    "created_at": "2020-03-24T09:27:32Z",
    "closed_at": "2020-04-15T01:48:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1900",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nTo get the results achieved in the vq-wav2vec paper were the features extracted from all layers of the RoBERTa model or just the final layer i.e. was return_all_hidden set to True or False? `roberta.extract_features(tokens, return_all_hiddens=True)`\r\n\r\nMany thanks\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1900/comments",
    "author": "david-macleod",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-04-14T13:40:52Z",
        "body": "@alexeib "
      },
      {
        "user": "alexeib",
        "created_at": "2020-04-15T01:48:20Z",
        "body": "we only use features from the final layer"
      }
    ]
  },
  {
    "number": 1879,
    "title": "Production with fairseq, translation",
    "created_at": "2020-03-22T12:37:07Z",
    "closed_at": "2020-03-23T12:07:17Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1879",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI want to port my trained model to production. It seems the CLI is not a good option as I want to avoid having to reload my model. So I am testing the `from_pretrained` python functions provided in hub_utils, but I cannot seem to make it work.\r\n\r\n#### Code\r\nGiven a model trained with sentencepiece, I execute the following file `run.py` inside the fairseq root\r\n\r\n```\r\nfrom fairseq.models.transformer import TransformerModel\r\nde2en = TransformerModel.from_pretrained(\r\n  'checkpoints/transformer/',\r\n  checkpoint_file='checkpoint_best.pt',\r\n  data_name_or_path='data-bin/de-en/',\r\n  bpe='sentencepiece',\r\n  bpe_codes='examples/translation/de-en/sentencepiece.bpe.model'\r\n)\r\nprint(de2en.translate('du bist ein ferd'))\r\n```\r\n\r\nthis results in the following error\r\n```\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 7, in <module>\r\n    bpe_codes='examples/translation/de-en/sentencepiece.bpe.model'\r\n  File \"/home/ubuntu/fairseq/fairseq/models/fairseq_model.py\", line 221, in from_pretrained\r\n    return hub_utils.GeneratorHubInterface(x[\"args\"], x[\"task\"], x[\"models\"])\r\n  File \"/home/ubuntu/fairseq/fairseq/hub_utils.py\", line 112, in __init__\r\n    self.bpe = encoders.build_bpe(args)\r\n  File \"/home/ubuntu/fairseq/fairseq/registry.py\", line 41, in build_x\r\n    return builder(args, *extra_args, **extra_kwargs)\r\n  File \"/home/ubuntu/fairseq/fairseq/data/encoders/sentencepiece_bpe.py\", line 21, in __init__\r\n    vocab = file_utils.cached_path(args.sentencepiece_vocab)\r\nAttributeError: 'Namespace' object has no attribute 'sentencepiece_vocab'\r\n```\r\n\r\n#### What have you tried?\r\nI have tried giving it various paths for the sentencepiece, but nothing works. I can't seem to figure exactly how `hub_utils` functions.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq 0.9\r\n - PyTorch 1.5\r\n - OS ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source, about a week ago\r\n - Build command you used (if compiling from source): same as official readme\r\n - Python version: 3.7.4\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: p3.2 instance on amazon\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1879/comments",
    "author": "alrojo",
    "comments": [
      {
        "user": "kkaiser",
        "created_at": "2020-03-22T15:49:27Z",
        "body": "`bpe_codes` takes a file that must be in the same directory as specified in the first argument\r\n```\r\nfrom fairseq.models.transformer import TransformerModel\r\nde2en = TransformerModel.from_pretrained(\r\n  'checkpoints/transformer/',\r\n  checkpoint_file='checkpoint_best.pt',\r\n  data_name_or_path='data-bin/de-en/',\r\n  bpe='sentencepiece',\r\n  bpe_codes='sentencepiece.bpe.model'\r\n)\r\nprint(de2en.translate('du bist ein ferd'))\r\n```\r\n\r\npath to file: `checkpoints/transformer/sentencepiece.bpe.model`"
      },
      {
        "user": "alrojo",
        "created_at": "2020-03-23T12:07:17Z",
        "body": "Thank you, this solved the issue."
      }
    ]
  },
  {
    "number": 1870,
    "title": "How do I keep pre-training mBART?",
    "created_at": "2020-03-19T22:56:31Z",
    "closed_at": "2022-05-01T02:22:21Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1870",
    "body": "Can you provide the script I can use to keep pretraining mBART? For example if I want to do more pretraining on one language, or to add a new language?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1870/comments",
    "author": "moyid",
    "comments": [
      {
        "user": "SunbowLiu",
        "created_at": "2020-06-19T07:54:50Z",
        "body": "> Can you provide the script I can use to keep pretraining mBART? For example if I want to do more pretraining on one language, or to add a new language?\r\n> \r\n> Thank you!\r\n\r\nHello,\r\n\r\nHave you got some updates?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:33Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "firdota",
        "created_at": "2021-07-26T03:27:07Z",
        "body": "> > Can you provide the script I can use to keep pretraining mBART? For example if I want to do more pretraining on one language, or to add a new language?\r\n> > Thank you!\r\n> \r\n> Hello,\r\n> \r\n> Have you got some updates?\r\n\r\nhello, have you got any progress?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1859,
    "title": "❓ Pytorch 1.2 => 1.4 gives lower results",
    "created_at": "2020-03-18T01:34:47Z",
    "closed_at": "2022-04-28T22:22:01Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1859",
    "body": "## ❓ Questions and Help\r\n\r\nI've been using BART in Fairseq release 0.9.0 with PyTorch 1.2.0 so far.  \r\nI wanted to upgrade dependencies of my project and use latest Fairseq master with PyTorch 1.4.0.\r\n\r\nBut I noticed lower ROUGE scores. After investigating, it seems to come from PyTorch update, as the results are similar (lower) when using Fairseq 0.9.0 with PyTorch 1.4.0.\r\n\r\n---\r\n\r\n**Did anyone noticed similar decrease in score when upgrading ?**\r\n\r\n---\r\n\r\n_I searched on Google, but couldn't find any results. Maybe it's linked to Fairseq library ?_\r\n\r\n---\r\n\r\nFor reference, this is my score on CNN/DM dataset using BART, for the first 100 samples :\r\n\r\n|                               | R1     | R2     | RL     |\r\n|-------------------------------|--------|--------|--------|\r\n| FairSeq 0.9.0 / PyTorch 1.2.0 | 35.413 | 15.473 | 32.358 |\r\n| FairSeq 0.9.0 / PyTorch 1.4.0 | 35.218 | 15.347 | 32.208 |\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1859/comments",
    "author": "astariul",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:32Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1855,
    "title": "How to run multi node distribution training using srun",
    "created_at": "2020-03-17T16:19:45Z",
    "closed_at": "2020-03-20T17:42:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1855",
    "body": "## 📚 Documentation\r\n\r\nsrun --gres=gpu:8 --ntasks=16 --ntasks-per-node=8 fairseq-train\r\n\r\nCan you share the command to run multi-node training with srun. The code above want to run 16 GPU on 2 node with each node 8 GPU.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1855/comments",
    "author": "gaopengcuhk",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-03-20T17:42:51Z",
        "body": "Generally you just need to set `--distributed-port` and `--ntasks-per-node 1`. Something like this should work: `srun --nodes 2 --gres=gpu:8 --ntasks-per-node 1 fairseq-train --distributed-port 12345 --no-save --disable-validation --task dummy_masked_lm --arch dummy_model --optimizer sgd --lr 1e-4 --max-sentences 8 --tokens-per-sample 512 --dataset-size 10000 --max-epoch 1 --dict-size 49995 --log-interval 10`"
      }
    ]
  },
  {
    "number": 1854,
    "title": "CNN BART produces summaries that consist mostly of numbers not words",
    "created_at": "2020-03-17T09:05:53Z",
    "closed_at": "2022-04-28T22:21:59Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1854",
    "body": "I have been trying to use the pre-trained Bart.CNN checkpoint to generate summaries. I use the script provided in the readme: \r\n```python\r\nwith open('cnn_dm/test.source') as source, open('cnn_dm/test.hypo', 'w') as fout:\r\n    sline = source.readline().strip()\r\n    slines = [sline]\r\n    for sline in source:\r\n        if count % bsz == 0:\r\n            with torch.no_grad():\r\n                hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n\r\n            for hypothesis in hypotheses_batch:\r\n                fout.write(hypothesis + '\\n')\r\n                fout.flush()\r\n            slines = []\r\n\r\n        slines.append(sline.strip())\r\n        count += 1\r\n    if slines != []:\r\n        hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n        for hypothesis in hypotheses_batch:\r\n            fout.write(hypothesis + '\\n')\r\n            fout.flush()\r\n```\r\nI get the following generations: \r\n```\r\nInput: 'The Brooklyn Nets will be holding their training camp at Duke University.'\r\nOutput: '33 3099 363 24796 3099 347 3099 362 363 373 32862 287 7420 9671 13. 33 3099 364.33 363.33 362.33 3400.33 3330.33 347.33 3220.33. 3330 363.3 363.7 363.6 363.8. 3300.3. 3399.3 364.7. 3360.3 3400+.3330.3 3500%.3360.7 3400!3360!3400!3500!3600!3700!3800!3900!4050!4060!4070!4080!4090!40'\r\n\r\nInput: The National Bank of Ukraine is not mulling the introduction of restrictions against banks operating in Crimea.\r\nOutput: '4663.4663 5631 14798 257 3210 5794 329 1204 290 5096 2450 13. 5794.4661 5631.4662 14798 256 3210 4.4664 5631 1204 1204 14798 4.4673 14798 3.4665 14798 2.4666 14798 1.4667 14798 5794 1204 1084 1204 5794 5794 4.4683 1204 1184 1084 5794 3.4674 14798 1204 888 1204 4.4693 1498 1498 1.4675 1498 2.4676 1498 3.4684 1498 4'\r\n\r\n```\r\n\r\nAnyone else experiencing this?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1854/comments",
    "author": "mukhal",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:46Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1822,
    "title": "Can I feed embeddings from XLM- RoBERTa to transformer seq2seq model?",
    "created_at": "2020-03-11T20:05:16Z",
    "closed_at": "2022-04-28T00:22:28Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1822",
    "body": "## ❓ Questions and Help\r\n\r\nI am a new researcher on the field of deep learning. Please help/example/guide me on feeding embeddings from XLM- RoBERTa to transformer seq2seq model? Thanks!!!\r\n\r\n#### What's your environment?\r\n\r\nWhat's your environment?\r\nXLM-RoBERTa\r\nPyTorch 1.4\r\nWindows\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1822/comments",
    "author": "JohnasSolomon",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:48Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:57Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1819,
    "title": "vq-wav2vec : Do you use EOS and BOS  when training VQ-wav2vec with Roberta base ?",
    "created_at": "2020-03-11T11:46:25Z",
    "closed_at": "2022-04-28T22:21:55Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1819",
    "body": "In the VQ-wav2vec paper, it says, quantized audio tokens are used to train the Roberta base on the masking task. Just want to clarify whether we use EOS and BOS tokens similar to Roberta training for the text.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1819/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:26Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1807,
    "title": "Why does padding_idx differ from padding_value in LSTMEncoder?",
    "created_at": "2020-03-09T12:01:53Z",
    "closed_at": "2020-03-10T19:11:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1807",
    "body": "Class `LSTMEncoder` has two fields: `padding_idx` and `padding_value`. First one comes from dictionary, which is passed to `__init()__`, second one is explicitly passed to the same function. Isn't it redundance? Shouldn't padding value come come only from dictionary and be consistent with it?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1807/comments",
    "author": "MichalTurski",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-03-10T15:59:27Z",
        "body": "Good call, I'll fix this."
      }
    ]
  },
  {
    "number": 1799,
    "title": "What are the different checkpoints given in VQ-wav2vec ?",
    "created_at": "2020-03-08T10:47:32Z",
    "closed_at": "2022-04-28T00:22:24Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1799",
    "body": "\r\nCan you please add a short description of different checkpoints given for VQ-wav2vec model. Even a reference from the paper would be very useful.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1799/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:53Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:53Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1795,
    "title": "Machine Translation Using Bert Embeddings",
    "created_at": "2020-03-07T14:52:41Z",
    "closed_at": "2022-04-28T00:22:25Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1795",
    "body": "#### What is your question?\r\n\r\nHello.\r\n\r\nI am a CS student. I need to do paraphrase generation using Bert Embeddings for my semester project. \r\n\r\nI have 50.000 Turkish paraphrase pair sentences. I had trained a transformer using Open-NMT library. But, I think that if I use Bert language model as encoder, I will more succesfull on my task. Can any one help me to do paraphrase generation or machine translation using Bert Embeddings on Fairseq Library?\r\n\r\nThanks.\r\n\r\n#### Code\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0) 1.4\r\n - OS (e.g., Linux): Linux(Colab)\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): source\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:Tesla P4, T4, P100 (Colab)\r\n - Any other relevant information:\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1795/comments",
    "author": "ahmetbagci8",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:52Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1793,
    "title": "vq-wav2vec ➡ RoBERTa pipeline",
    "created_at": "2020-03-06T18:21:25Z",
    "closed_at": "2020-03-11T03:14:12Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1793",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI am trying to take the outputs from the vqwav2vec model and pass them to the pre-trained RoBERTa model. After looking at the docs and source code I believe what I have supplied is the correct vqwav2vec / RoBERTa pipeline to extract RoBERTa features, could this please be confirmed?\r\n\r\nAlso, is wrapping the sequence with `<s>` tokens necessary in the audio use-case?\r\n\r\nMany thanks\r\n\r\n#### Code\r\n\r\n```py\r\ndef indices_to_string(idxs):\r\n    # based on fairseq/examples/wav2vec/vq-wav2vec_featurize.py\r\n    return \" \".join(\"-\".join(map(str, a.tolist())) for a in idxs.squeeze(0))\r\n\r\nz = vqwav2vec.feature_extractor(x.unsqueeze(0))\r\n_, idxs = vqwav2vec.vector_quantizer.forward_idx(z)\r\n\r\nidx_str = indices_to_string(idxs)\r\ntokens = roberta.task.source_dictionary.encode_line(idx_str, append_eos=False, add_if_not_exist=False)\r\nlast_layer_features = roberta.extract_features(tokens)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1793/comments",
    "author": "david-macleod",
    "comments": [
      {
        "user": "shamanez",
        "created_at": "2020-03-09T19:13:05Z",
        "body": "I am also following your pipeline. I have used first wav2vec k means pipeline to extract tokens and send the tokens though the given roberta checkpoint. Just to clarify,  did you also use the dictionary given inside Roberta-wav2vec checkpoint folder?  Also what are the architectural information in the given roberta checkpoint."
      },
      {
        "user": "alexeib",
        "created_at": "2020-03-11T03:14:12Z",
        "body": "you extract the codes to a text file, then you run preprocess.py as if it was a regular text file. specify --source-dict and point to the dict file in the tar (otherwise it will construct a new dict which wont match to what it was trained with). e.g. if you extracted codes to train.src and your corresponding labels file is train.ltr. if you dont have labels then use --only-source\r\n```\r\npython ~/fairseq-py/preprocess.py --dataset-impl mmap --trainpref train --destdir . --workers 40  --srcdict dict.txt --validpref valid -s src -t ltr\r\n```"
      },
      {
        "user": "shamanez",
        "created_at": "2020-03-11T03:18:16Z",
        "body": "Thanks a lot."
      },
      {
        "user": "david-macleod",
        "created_at": "2020-03-11T09:35:54Z",
        "body": "I have run the following command \r\n```\r\npython ~/fairseq/preprocess.py --dataset-impl mmap --trainpref train --destdir . --workers 40  --srcdict dict.txt -s src --only-source\r\n```\r\nand it creates two files `train.src-None.src.bin` and `train.src-None.src.idx`.  What do I do with these files, in order to extract roberta features?\r\n"
      },
      {
        "user": "shamanez",
        "created_at": "2020-03-11T11:34:30Z",
        "body": "@david-macleod  I tried to train the Roberta base by feeding tokens that are generated by following the @alexeib's pipeline. In those tokens, I saw  Beginning Of Sentence (BOS) and End Of Sentence (EOS) tokens similar to Roberta.  Other tokens are perfectly matched with your pipeline. So this is the same as the Roberta. In your pine line, just add 0 to begging and 2 to the end."
      },
      {
        "user": "david-macleod",
        "created_at": "2020-03-11T14:10:36Z",
        "body": "thanks @shamanez, so I could just update my function like so:\r\n```py\r\ndef indices_to_string(idxs):\r\n    # based on fairseq/examples/wav2vec/vq-wav2vec_featurize.py\r\n    return \"<s>\" + \" \".join(\"-\".join(map(str, a.tolist())) for a in idxs.squeeze(0)) + \"</s>\"\r\n```\r\n\r\nJust out of interest, how did you go from the outputs of preprocess.py (the .bin and .idx files) to training Roberta?"
      },
      {
        "user": "alexeib",
        "created_at": "2020-03-11T16:38:52Z",
        "body": "Sorry, i thought the question was how to train using the vq-wav2vec codes. \r\n\r\nif you use --only-source, then omit the --s and --t and specify the full file name in trainpref. e.g. \r\n```\r\npython ~/fairseq-py/preprocess.py --dataset-impl mmap --trainpref train.src --destdir . --workers 40 --only-source --validpref valid.src\r\n```\r\nyou then will get a train.bin and train.idx (and valid/test if you specify validpref and testpref). you can then train a roberta model by following the roberta examples and providing path with your train.bin/idx as data path.\r\n\r\nfor extracting representations from roberta, you can load the roberta model, then iterate over the lines in your text file and do something like\r\n```\r\nfor words in lines:\r\n            vec = [0] + [self.dict.index(w) for w in words] + [2]\r\n            x = torch.LongTensor(vec).unsqueeze(0).cuda()\r\n            z = self.roberta_model.extract_features(x)\r\n```"
      },
      {
        "user": "shamanez",
        "created_at": "2020-03-11T22:48:14Z",
        "body": "\r\n@david-macleod \r\n\r\n> Just out of interest, how did you go from the outputs of preprocess.py (the .bin and .idx files) to training Roberta?\r\n\r\nFirst, you need to run the following script to convert wav files in to K-means tokenized representations.\r\n```\r\n $ PYTHONPATH /path/to/fairseq python examples/wav2vec/vq-wav2vec_featurize.py --data-dir /manifest/path --output-dir /path/to/output \\\r\n--checkpoint /model/path/checkpoint_best.pt --split train valid test --extension tsv\r\n```\r\n\r\nThen follow the first part of the above @alexeib 's answer."
      },
      {
        "user": "shamanez",
        "created_at": "2020-03-12T00:59:35Z",
        "body": "@alexeib  what is the **self.dict.** in your answer?  Can I load the dict.txt given in checkpoint directly?\r\n\r\nWhat if I use the following method to extract tokens?\r\n`tokens = roberta.task.source_dictionary.encode_line(idx_str, append_eos=False, add_if_not_exist=False)`\r\n\r\nThis way the dictionary is modified with special tokens and indexes are shifted by three values. Because in a usual Roberta task you have to add` <s>, </s>,<pad>  and , <unk>`."
      },
      {
        "user": "alexeib",
        "created_at": "2020-03-12T07:41:50Z",
        "body": "yes it is the loaded dict.txt using Dictionary class"
      },
      {
        "user": "shamanez",
        "created_at": "2020-03-12T08:14:38Z",
        "body": "What if I use the following method to extract tokens?\r\n`tokens = roberta.task.source_dictionary.encode_line(idx_str, append_eos=False, add_if_not_exist=False)`\r\n\r\nThis way the dictionary is modified with special tokens and indexes are shifted by three values. Because in a usual Roberta task you have to add` <s>, </s>,<pad>  and , <unk>`.\r\n"
      },
      {
        "user": "david-macleod",
        "created_at": "2020-03-12T11:53:15Z",
        "body": "@alexeib @shamanez I am also interested to know if using `roberta.task.source_dictionary` for the token mapping is as valid approach, compared to loading dict.txt. It also has a `<mask>` token in position 23672."
      },
      {
        "user": "alexeib",
        "created_at": "2020-03-12T18:54:42Z",
        "body": "both approaches should work (that mask will be at last position so it shouldn't matter)"
      }
    ]
  },
  {
    "number": 1769,
    "title": "No dict files in released model (bart.large finetuned on Xsum)",
    "created_at": "2020-03-04T05:19:35Z",
    "closed_at": "2020-03-11T19:27:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1769",
    "body": "There are no dict.document.txt and dict.summary.txt files in released pre-trained bart model (bart.large finetuned on Xsum). \r\nAre they same as the dict files in the model (bart.large finetuned on CNN)?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1769/comments",
    "author": "jackyuanjie1990",
    "comments": [
      {
        "user": "ngoyal2707",
        "created_at": "2020-03-11T19:27:43Z",
        "body": "This should be fixed now"
      }
    ]
  },
  {
    "number": 1764,
    "title": "Batch size of wiki103 model",
    "created_at": "2020-03-02T17:03:50Z",
    "closed_at": "2020-03-03T18:53:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1764",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI have a few questions related to the Wiki103 pretrained model and the provided training script.\r\n\r\n1)  In the training script code you have \r\n\r\n> --max-tokens 3072  --tokens-per-sample 3072\r\n\r\nHowever in the paper, you state that \r\n> For WIKITEXT-103 we partition the training data into blocks of 512 contiguous tokens\r\n\r\nI'm wondering where/how this is happening given the provided training example or if the training example does not match the paper?  In general, I am confused about how batch size is determined in the fairseq framework.  Running the below code with the wiki103 comandline args provided gives src_tokens with size [1, 3072].  \r\n\r\n2)  For multiple gpus, are  --max-tokens   --tokens-per-sample per gpu or do they get split across gpus?\r\n\r\n3)  Loading the the model, the saved args have the arch as 'transformer_lm_gbw' and not 'transformer_lm_wiki103'.  Why is this?\r\n\r\n\r\n#### Code\r\n```    \r\n        reg_task = LanguageModelingTask.setup_task(args)\r\n        reg_task.load_dataset(split)\r\n        reg_iter = reg_task.get_batch_iterator(reg_task.datasets[split], max_tokens=args.max_tokens,\r\n                                               max_sentences=args.max_sentences,\r\n                                               max_positions=args.max_target_positions)\r\n        reg_e_iter = reg_iter.next_epoch_itr(shuffle=True)\r\n\r\n        for sample in reg_e_iter:\r\n            print(sample, sample['id'].shape, 'id shape')\r\n            print(sample['net_input']['src_tokens'].shape)\r\n```\r\n\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9\r\n - PyTorch Version (e.g., 1.0)  1.4\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:  TitanX and others\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1764/comments",
    "author": "arvieFrydenlund",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-03-03T14:50:47Z",
        "body": "> (...) However in the paper, you state that (...)\r\n\r\nSee Section 5.1 of the paper: \"Table 2 shows our result on WIKITEXT-103 where adaptive inputs achieve 18.7 perplexity. For this result only, we partition the training data into blocks of 3072 contiguous tokens instead of 512 tokens as for other experiments.\" I believe this is the model that was released.\r\n\r\n> For multiple gpus, are --max-tokens --tokens-per-sample per gpu or do they get split across gpus?\r\n\r\n`--max-tokens` and `--tokens-per-sample` are per GPU. So if you have two GPUs then you'll effectively have double the max tokens.\r\n\r\n> Loading the the model, the saved args have the arch as 'transformer_lm_gbw' and not 'transformer_lm_wiki103'. Why is this?\r\n\r\nYou can mostly ignore the \"arch\" value in the checkpoint, since the other configuration can be overridden elsewhere in the args. You should look at `decoder_layers`, `decoder_embed_dim`, ..., directly."
      },
      {
        "user": "arvieFrydenlund",
        "created_at": "2020-03-03T14:59:02Z",
        "body": "Thanks, that helped a lot!"
      }
    ]
  },
  {
    "number": 1762,
    "title": "BART: how are the spans computed during training",
    "created_at": "2020-02-29T22:09:59Z",
    "closed_at": "2020-02-29T23:37:33Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1762",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI can't find any code for the unsupervised pretraining of BART or mBART, so I'll ask here:\r\n\r\nthe BART paper says:\r\n\"Text Infilling A number of text spans are sampled,with  span  lengths  drawn  from  a  Poisson  distribution(λ= 3).  Each span is replaced with a single [MASK] token.   0-length  spans  correspond  to  the  insertion  of [MASK] tokens. \"\r\n\r\nHow many spans are sampled? Is it just one per sentence or more than one. What is the scale of the Poisson distribution, is it number of tokens or number of tokens / sentence length?\r\n\r\nThe mBART paper says:\r\n\"We mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (λ= 3.5)\"\r\n\r\nDoes this mean that there is only one span per sentence?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1762/comments",
    "author": "Avmb",
    "comments": [
      {
        "user": "yinhanliu",
        "created_at": "2020-02-29T23:37:04Z",
        "body": "@Avmb Thanks for the interest.  I will add a README for both of them. To your question - there are multiple spans (average length is 3) were added as noise, which resulted a 35% input tokens are replaced as [Mask]. Poisson distribution sampled # tokens in a span. It doesn't depend sentence length. "
      }
    ]
  },
  {
    "number": 1761,
    "title": "Finetune XLM-R for neural machine translation b/n the same low resource language?",
    "created_at": "2020-02-29T18:06:35Z",
    "closed_at": "2022-04-28T00:22:19Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1761",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI wonder if it is possible to finetune XLM-R for neural machine translation b/n the same low resource language? I am a new researcher on the field of deep learning. Thank you.\r\n\r\nFor example: Input: He sells food(in Catalan)    \r\nOutput: Food he sells(in Catalan)\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What's your environment?\r\n - XLM-R\r\n - PyTorch 1.4\r\n - Windows\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1761/comments",
    "author": "JohnasSolomon",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:29Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1760,
    "title": "BART generated summaries on CNN/DM and XSum",
    "created_at": "2020-02-28T21:33:05Z",
    "closed_at": "2020-03-01T18:07:18Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1760",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n#### What is your question?\r\n\r\nCould you release the output of BART on CNN/DM and XSum for better reproducibility? Thank you!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1760/comments",
    "author": "morningmoni",
    "comments": [
      {
        "user": "yinhanliu",
        "created_at": "2020-02-29T06:24:05Z",
        "body": "@morningmoni  Sorry we can't here due to legal reason. I will release Xsum readme shortly. What stopped you from reproducing CNN/DM"
      }
    ]
  },
  {
    "number": 1756,
    "title": "Preprocessing when fine tune mBART",
    "created_at": "2020-02-28T19:35:57Z",
    "closed_at": "2020-02-29T06:27:37Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1756",
    "body": "## ❓ Questions and Help\r\n\r\nFrom the paper, it seems that for both training and inference we need to append `language id` after each input sentence. Is this true? Do we need to add this before doing BPE?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1756/comments",
    "author": "xinyuzhao",
    "comments": [
      {
        "user": "yinhanliu",
        "created_at": "2020-02-29T06:27:33Z",
        "body": "@xinyuzhao Yes, it is True.  The language id is added in the code (in fairseq/task/denoising). BPE was done before running the code. The answer to your question is no. We don't need to add this before BPE. "
      }
    ]
  },
  {
    "number": 1749,
    "title": "XLM-R finetuning but OOM even with Batchsize as 1?",
    "created_at": "2020-02-26T02:57:53Z",
    "closed_at": "2022-04-18T03:21:10Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1749",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\nI'm trying to finetune XLM-R.large model for sentence prediction task. And it ran in OOM at the first moment even though I decreased the Batchsize to even 1. I tried two different GPU instance type: g4dn.16xlarge (single GPU) and g4dn.12xlarge (multi GPU), but the error seems no differences. Meanwhile it shows that I could load the checkpoint.\r\n`RuntimeError: CUDA out of memory. Tried to allocate 2.09 GiB (GPU 0; 14.73 GiB total capacity; 11.98 GiB already allocated; 1.33 GiB free; 577.00 MiB cached)`\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n```\r\nMAX_TOKENS=1000\r\nMAX_SENTENCES=1\r\nUPDATE_FREQ=1\r\nXLMR_PATH=\"xlmr.large/model.pt\"\r\n```\r\n#### What's your environment?\r\n\r\n - fairseq Version: master\r\n - PyTorch Version: 1.2.0\r\n - OS (e.g., Linux):Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: \r\n    - g4dn.16xlarge: 1 GPU, 64 vCPU, 256G Mem, 16G GPU Memory\r\n    - g4dn.12xlarge: 4 GPU, 48 vCPU, 192G Mem, 64G GPU Memory\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1749/comments",
    "author": "TingtingLi0101",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-02-26T16:16:07Z",
        "body": "Hmm, 16GB is a bit tight for the large model, but you should be able to train something.\r\n\r\nCan you share the rest of your training command? Also be careful about sequence lengths -- the memory requirements grow a lot with longer sequences, so if your data has very long sequences and you're not filtering them then that could be a problem."
      },
      {
        "user": "TingtingLi0101",
        "created_at": "2020-02-26T18:58:48Z",
        "body": "Yeah, I only changed the MAX_TOKENS, MAX_SENTENCES, and UPDATE_FREQ, and would like to set them as below, but even though I decreased MAX_TOKENS=1000, MAX_SENTENCES=1. Seems still OOM.\r\n\r\nAlso, the maximum memory/GPU that AWS could provide is 16GB, I'm wondering what kind of GPU that your team use to train the model?\r\n \r\n```\r\nTOTAL_NUM_UPDATES=140_000\r\nWARMUP_UPDATES=int(TOTAL_NUM_UPDATES * 0.06)\r\nLR=1e-05\r\nNUM_CLASSES=3 \r\nMAX_TOKENS=2000\r\nMAX_SENTENCES=16\r\nUPDATE_FREQ=4\r\nXLMR_PATH=\"xlmr.large/model.pt\"\r\n\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 python train.py data-base-bin \\\r\n    --restore-file $XLMR_PATH \\\r\n    --max-sentences $MAX_SENTENCES \\\r\n    --max-tokens $MAX_TOKENS \\\r\n    --task sentence_prediction \\\r\n    --max-positions 512 \\\r\n    --reset-optimizer --reset-dataloader --reset-meters \\\r\n    --required-batch-size-multiple 1 \\\r\n    --init-token 0 --separator-token 2 \\\r\n    --num-classes $NUM_CLASSES \\\r\n    --arch roberta_base \\\r\n    --criterion sentence_prediction \\\r\n    --dropout 0.1 --attention-dropout 0.1  \\\r\n    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\r\n    --clip-norm 0.0 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --update-freq $UPDATE_FREQ \\\r\n    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\r\n    --max-epoch 10 \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --truncate-sequence \\\r\n    --save-dir i18n_checkpoints_1e5 \\\r\n    --tensorboard-logdir \"i18n_tf_board_1e5/\" \\\r\n    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\r\n    --find-unused-parameters;\r\n```"
      },
      {
        "user": "myleott",
        "created_at": "2020-02-27T01:23:50Z",
        "body": "> the maximum memory/GPU that AWS could provide is 16GB\r\n\r\nAWS does have `p3dn.24xlarge` instances with 32GB V100 GPUs, which is the same type of GPU that we used.\r\n\r\ncc @ngoyal2707 about expected memory usage for the large model."
      },
      {
        "user": "TingtingLi0101",
        "created_at": "2020-02-27T19:13:57Z",
        "body": "Ah, that make sense to me. But AWS doesn't provide `p3dn.24xlarge` instance for per hour usage, I could start with `xlmr.base` model instead. Thanks for your reply!"
      },
      {
        "user": "mohammedayub44",
        "created_at": "2020-05-05T20:08:05Z",
        "body": "@myleott  any runtime stats that you can share for large or small model. Per epoch runtime, total time taken etc. \r\n\r\n@ngoyal2707  any memory usage stats you can share for training the large or small xlm-r model.\r\n\r\nThanks !"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:37Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1742,
    "title": "Encoder.forward(..) seems to return the output of Encoder n-times",
    "created_at": "2020-02-24T20:37:03Z",
    "closed_at": "2020-03-01T22:07:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1742",
    "body": "In a multi-GPU setting, `trainer.get_model().encoder.forward(..)` seems to return the output of the encoder n-number of times. Where n is the number of GPUs that is in use. Is my observation right? What is its implication?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1742/comments",
    "author": "ajesujoba",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-02-26T15:43:42Z",
        "body": "I don't understand, can you please clarify what you mean, preferably with an example?"
      }
    ]
  },
  {
    "number": 1741,
    "title": "Should sentences be split for the (masked) language modeling task?",
    "created_at": "2020-02-24T10:42:32Z",
    "closed_at": "2020-02-24T16:21:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1741",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nIn the `wikitext` dataset suggested in the language modeling task (and used as well by the RoBERTa example), sentences are not split into different lines. Instead, in this dataset, newlines denote a new paragraph (and double new line denotes change of document, as mandated by the \"language modeling format\" mentioned in the docs).\r\n\r\nMy question is: is sentence splitting something that we should consider when training our own language models? In the case of BERT, it is obvious that it is a hard requirement (for the NSP objective), while in the case of BART, I'm not sure because there are no examples of training BART from scratch, but I think that it's necessary because of the sentence permutation. In the case of RoBERTa, it is not a requirement, and it doesn't appear in the example, but is it something that would be beneficial? Did you use it when building your models? So far, I haven't found any mention of this in the original articles or fairseq's documentation.\r\n\r\nIn summary: even if sentence splitting (into newlines) is not required for RoBERTa, is it something that would be beneficial? Did you do it? In the case of BART, it is a hard requirement, right?\r\n\r\nMany thanks in advance.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1741/comments",
    "author": "jordiae",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-02-24T16:16:50Z",
        "body": "Good question. For RoBERTa we always put a blank newline between \"documents\", so for books there's a blank newline between each book, for wikipedia a blank newline between articles, etc.\r\n\r\nWithin each \"document,\" we split sentences for books and wikipedia. STORIES also seems to split on sentences. Both CC-NEWS and OpenWebText usually have one paragraph per line.\r\n\r\nSo for example, in Wikipedia we have one sentence per line, with blank lines between articles:\r\n```\r\nJean Bernard Bossu (1720–1792) was a captain in the French navy, adventurer and explorer.\r\nHe travelled several times to New France, where he explored the regions along the Mississippi.\r\n(...)\r\n\r\nThe long-tailed Talaud mosaic-tailed rat or the long-tailed Talaud melomys (\"Melomys talaudium\") is a species of rodent in the family Muridae.\r\nIt is endemic to Karakelong and Salebabu in the Talaud Islands in Indonesia where it occurs in forest habitats.\r\n(...)\r\n```\r\n\r\nFor OpenWebText we usually have one paragraph per line, with blank lines between articles:\r\n```\r\nSt Columba Day: the Christianization of Scotland\r\nToday is the feast day of St Columba, a Christian missionary known for the spread of Christianity in what is now known as Scotland. Columba was born in Ireland in 591 CE, and was a monk of some renown, and the story about him is interesting. He made a copy of the Psalms under the direction of another monk, intending to keep the copy. The dispute between ownership grew beyond Columba and the monk to their respective groups, and eventually led to an actual battle in 561. Later, Columba also induced another battle in violation of the King Ireland’s order.\r\n(...)\r\n\r\nGeorgia Tech players expressed disappointment over not being able to play against Central Florida on Saturday after the game was canceled Monday because of effects of Hurricane Irma.\r\n“We’re always ready to play,” quarterback TaQuon Marshall said Wednesday following the team’s practice. “We were looking forward to playing. I know a lot of the guys from Florida were looking forward to going down and playing in their hometown. It’s disappointing, but we’re happy we can get a break also and rest our bodies and move on to next week.”\r\n(...)\r\n```"
      },
      {
        "user": "myleott",
        "created_at": "2020-02-24T16:17:56Z",
        "body": "I think the key is putting blank lines between articles, which gives the model an explicit separator. This also enables you to train with `--sample-break-mode=complete_doc`, which we found gives slightly better performance than `complete`."
      },
      {
        "user": "jordiae",
        "created_at": "2020-02-24T16:21:17Z",
        "body": "@myleott Understood, many thanks for your answer."
      },
      {
        "user": "leo-liuzy",
        "created_at": "2021-07-29T21:47:35Z",
        "body": "@myleott Could you also comment on cc-100 as well?"
      }
    ]
  },
  {
    "number": 1739,
    "title": "Convert FP32 model to FP16 model and resume training.",
    "created_at": "2020-02-23T14:46:54Z",
    "closed_at": "2020-05-02T19:36:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1739",
    "body": "What would happen if I train with FP32 for some epochs and then want to convert it to FP16 and continue training?\r\nCan I just add --FP16 option and hope for some force conversion between float32 and float16, or do I have to do something extra manually?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1739/comments",
    "author": "ever4244",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-02-24T15:54:05Z",
        "body": "I've never tried this before, so not sure. Something might break in the optimizer, but it might also work. Please try it and report back!"
      },
      {
        "user": "ever4244",
        "created_at": "2020-02-25T07:06:02Z",
        "body": "> I've never tried this before, so not sure. Something might break in the optimizer, but it might also work. Please try it and report back!\r\n\r\nI added:\r\n--fp16 --reset-optimizer\r\n\r\n--reset-optimizer is forced to add due to a different optimizer error.\r\n\r\nThe result is an overflow gradient. \r\nI am doing this because I want to move from a 1080Ti trained FP32 model to a 2080ti machine with FP16 support.\r\n`\r\n2020-02-25 14:47:06 | INFO | fairseq_cli.train | model laser, criterion CrossEntropyCriterion\r\n2020-02-25 14:47:06 | INFO | fairseq_cli.train | num. model params: 178511292 (num. trained: 178511292)\r\n2020-02-25 14:47:07 | INFO | fairseq_cli.train | training on 3 GPUs\r\n2020-02-25 14:47:07 | INFO | fairseq_cli.train | max tokens per GPU = 2000 and max sentences per GPU = None\r\n2020-02-25 14:47:09 | INFO | fairseq.trainer | loaded checkpoint checkpoints/laser_lstm_fp32_ep6/checkpoint_last.pt (epoch 6 @ 0 updates)\r\n2020-02-25 14:47:09 | INFO | fairseq.trainer | loading train data for epoch 6\r\n2020-02-25 14:47:09 | INFO | fairseq.data.data_utils | loaded 1869785 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.de-en.de\r\n2020-02-25 14:47:09 | INFO | fairseq.data.data_utils | loaded 1869785 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.de-en.en\r\n2020-02-25 14:47:09 | INFO | fairseq.tasks.translation | data-bin/europarl.de_en_es_fr.bpe40k train de-en 1869785 examples\r\n2020-02-25 14:47:10 | INFO | fairseq.data.data_utils | loaded 1747420 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.de-es.de\r\n2020-02-25 14:47:10 | INFO | fairseq.data.data_utils | loaded 1747420 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.de-es.es\r\n2020-02-25 14:47:10 | INFO | fairseq.tasks.translation | data-bin/europarl.de_en_es_fr.bpe40k train de-es 1747420 examples\r\n2020-02-25 14:47:10 | INFO | fairseq.data.data_utils | loaded 1938685 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.en-es.en\r\n2020-02-25 14:47:10 | INFO | fairseq.data.data_utils | loaded 1938685 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.en-es.es\r\n2020-02-25 14:47:10 | INFO | fairseq.tasks.translation | data-bin/europarl.de_en_es_fr.bpe40k train en-es 1938685 examples\r\n2020-02-25 14:47:10 | INFO | fairseq.data.data_utils | loaded 1938685 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.en-es.es\r\n2020-02-25 14:47:10 | INFO | fairseq.data.data_utils | loaded 1938685 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.en-es.en\r\n2020-02-25 14:47:10 | INFO | fairseq.tasks.translation | data-bin/europarl.de_en_es_fr.bpe40k train es-en 1938685 examples\r\n2020-02-25 14:47:10 | INFO | fairseq.data.data_utils | loaded 1954044 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.en-fr.fr\r\n2020-02-25 14:47:11 | INFO | fairseq.data.data_utils | loaded 1954044 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.en-fr.en\r\n2020-02-25 14:47:11 | INFO | fairseq.tasks.translation | data-bin/europarl.de_en_es_fr.bpe40k train fr-en 1954044 examples\r\n2020-02-25 14:47:11 | INFO | fairseq.data.data_utils | loaded 1895440 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.es-fr.fr\r\n2020-02-25 14:47:11 | INFO | fairseq.data.data_utils | loaded 1895440 examples from: data-bin/europarl.de_en_es_fr.bpe40k/train.es-fr.es\r\n2020-02-25 14:47:11 | INFO | fairseq.tasks.translation | data-bin/europarl.de_en_es_fr.bpe40k train fr-es 1895440 examples\r\n2020-02-25 15:00:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\r\n2020-02-25 15:00:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\r\n2020-02-25 15:00:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\r\n2020-02-25 15:00:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\r\n2020-02-25 15:00:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0\r\n2020-02-25 15:00:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0\r\n2020-02-25 15:00:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0\r\n2020-02-25 15:00:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5\r\n2020-02-25 15:00:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25\r\n2020-02-25 15:00:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.125\r\n2020-02-25 15:00:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625\r\n2020-02-25 15:00:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.03125\r\n2020-02-25 15:00:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.015625\r\n2020-02-25 15:00:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0078125\r\n2020-02-25 15:00:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00390625\r\n2020-02-25 15:00:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.001953125\r\n2020-02-25 15:00:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0009765625\r\n2020-02-25 15:00:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.00048828125\r\n2020-02-25 15:00:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.000244140625\r\n2020-02-25 15:00:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0001220703125\r\nTraceback (most recent call last):\r\n  File \"/4tssd/wliax/anaconda3/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/4tssd/wliax/research_2020/fairseq/fairseq_cli/train.py\", line 303, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/4tssd/wliax/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/4tssd/wliax/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/4tssd/wliax/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/4tssd/wliax/research_2020/fairseq/fairseq_cli/train.py\", line 270, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/4tssd/wliax/research_2020/fairseq/fairseq_cli/train.py\", line 102, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/4tssd/wliax/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/4tssd/wliax/research_2020/fairseq/fairseq_cli/train.py\", line 171, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/4tssd/wliax/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/4tssd/wliax/research_2020/fairseq/fairseq/trainer.py\", line 368, in train_step\r\n    grad_norm = self.optimizer.clip_grad_norm(self.args.clip_norm)\r\n  File \"/4tssd/wliax/research_2020/fairseq/fairseq/optim/fp16_optimizer.py\", line 175, in clip_grad_norm\r\n    ).format(self.min_loss_scale))\r\nFloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.\r\n`"
      },
      {
        "user": "myleott",
        "created_at": "2020-02-25T14:14:12Z",
        "body": "Hmm, a couple possibilities come to mind:\r\n1) the FP32 weights don't work well in FP16, so you get immediate overflows\r\n2) the FP32 weights work for the first step in FP16, but then the step breaks the model, causing a bunch overflows from steps 2 onward.\r\n\r\nCan you try relaunching with `--lr 0.0 --log-interval 1` and see if you get any additional info?"
      },
      {
        "user": "ever4244",
        "created_at": "2020-05-02T19:36:33Z",
        "body": "After a few trials, I think it can automatically convert between fp16 and fp32 without obvious problems. The only issue is I have to set --reset-optimizer"
      }
    ]
  },
  {
    "number": 1736,
    "title": "some tutorial about distillation dataset",
    "created_at": "2020-02-22T09:16:18Z",
    "closed_at": "2022-04-18T02:21:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1736",
    "body": "Hi!\r\nNow, I am focusing on the non-autoregressive model for translation. I want to distill some other datasets. I have trained a teacher model using Transformer. But I don't know how to save the generated data. Could you give me some suggestions?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1736/comments",
    "author": "hljjjmssyh",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-02-26T15:37:39Z",
        "body": "@MultiPath "
      },
      {
        "user": "Ir1d",
        "created_at": "2020-04-13T00:22:21Z",
        "body": "I tried `fairseq-generate data-bin/wmt17_en_de_joined --path checkpoints/transformer_vaswani_wmt_en_de_big/checkpoint_best.pt --batch-size 64 --beam 5 --remove-bpe --gen-subset train --results-path data-bin/wmt17_en_de_distill` but no result file was generated in anywhere..\r\nHave you found out how to generate the distillation dataset?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:55Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1718,
    "title": "Can we use BART for next word prediction or autocompletion task?",
    "created_at": "2020-02-17T16:21:36Z",
    "closed_at": "2020-02-20T19:07:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1718",
    "body": "I am trying to use BART for next word prediction like what BERT can do. Is there a way we could do this?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1718/comments",
    "author": "vanh17",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-02-18T15:31:11Z",
        "body": "CC @ngoyal2707 "
      },
      {
        "user": "ngoyal2707",
        "created_at": "2020-02-20T19:07:40Z",
        "body": "@vanh17 Depends on what exact task you have.\r\n\r\n1) In autocompletion typically, you have first `n` tokens and you need to predict `n+1`th token. For this, I don't expect `BERT` to be great because it never sees this type of data during pretraining. Either `UniLM (example: GPT2)`  or `BART` would work better. I have a feeling that similar sized `BART` model should outperform `UniLM` but haven't tested it.\r\n\r\n2) Masked filling task: you have one or more tokens missing in the whole sentence. In this case, `BERT` should be good. `BART` should give you competing result also.\r\n\r\nI haven't tried either myself but as I mentioned, both are fair things to try with `BART` model."
      },
      {
        "user": "vanh17",
        "created_at": "2020-02-20T20:07:55Z",
        "body": "I looked at the interface hub and see there is a function called generate. Is that a function we use to do next work prediction? If so how could we use such function for the task. "
      }
    ]
  },
  {
    "number": 1712,
    "title": "Access to each batch's gradients while accumulating gradients?",
    "created_at": "2020-02-16T11:34:04Z",
    "closed_at": "2020-02-18T16:28:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1712",
    "body": "Hello, \r\n\r\nI have one question regarding the gradient accumulation.\r\nFairseq provides \"gradient accumulation\", which accumulates gradients for several batches.\r\n\r\nFrom my understanding, the loss.backward() stores the gradients in variables' ```.grad``` which gets summed up for every 8 batches(If accumulating gradients for 8 batches). And optimizer.step() (which is called every update-frequency, e.g. 8) will use these \"summed\" gradients for the update of the parameters.\r\n\r\nHowever, I'd like to get access to each batch's gradients(after every loss.backward()) while training with gradient accumulation. But as far as I know, the gradients stored in ```.grad``` are summed up, so gradients from other bathes are mixed up.\r\n\r\nIs there an easy way of getting each batch's gradient information?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1712/comments",
    "author": "JJumSSu",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-02-16T14:35:17Z",
        "body": "Hmm, one option is to disable fairseq's gradient accumulation, then define an optimizer that does the accumulation internally (and in the process gives you access to the intermediate gradients)."
      },
      {
        "user": "JJumSSu",
        "created_at": "2020-02-17T07:22:40Z",
        "body": "@myleott \r\nThank you for your reply!\r\n\r\nFrom my understanding, after calling loss.backward() for several batches, the gradients are accumulated(summed) in ```.grad``` variable. And optimizer steps with gradients information stored in ```.grad```. If I turn off the gradient accumulation, then zero.grad() gets called every batch, so the gradients information is gone, so the gradients are not accumulated.\r\n\r\nSo can you explain the optimizer that does the accumulation internally more specifically? \r\n\r\n"
      },
      {
        "user": "myleott",
        "created_at": "2020-02-17T16:12:27Z",
        "body": "In the optimizer you could loop over the params, copy/accumulate the gradients into another buffer, and every k steps update the model params. Something like this (I haven't tested it though):\r\n\r\n```python\r\nfor group in self.param_groups:\r\n  for p in group['params']:\r\n    if p.grad is None:\r\n      continue\r\n\r\n    # accumulate grads\r\n    if p in self.acc_grads:\r\n      self.acc_grads[p] += p.grad\r\n    else:\r\n      self.acc_grads[p] = p.grad.clone()\r\n    self.num_acc += 1\r\n\r\n    if self.num_acc % num_to_accumulate == 0\r\n      # do regular step, using acc_grads[p]\r\n\r\n      # reset accumulation\r\n      self.num_acc = 0\r\n      self.acc_grads = {}\r\n```"
      },
      {
        "user": "JJumSSu",
        "created_at": "2020-02-18T13:08:03Z",
        "body": "@myleott \r\nThank you for such a nice and meticulous reply. \r\nI'll try it out and let you know the results.\r\nThank you!!"
      },
      {
        "user": "lematt1991",
        "created_at": "2020-02-18T16:28:25Z",
        "body": "Seems this has been resolved.  Please open a new issue if you are still having problems."
      }
    ]
  },
  {
    "number": 1705,
    "title": "Help with --sampling-topp hyperparameter ?",
    "created_at": "2020-02-14T07:46:09Z",
    "closed_at": "2020-02-18T06:32:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1705",
    "body": "I tried experimenting with --sampling-topp hyperparamter\r\npython interactive.py test/ --path checkpoints/models_anv/checkpoint_best.pt  --source-lang en --target-lang hi --nbest 5 --sampling --sampling-topp 0.1\r\npython interactive.py test/ --path checkpoints/models_anv/checkpoint_best.pt  --source-lang en --target-lang hi --nbest 5 --sampling --sampling-topp 0.9\r\n\r\nI am not able to understand the outputs. When I use p = 0.1, all of my 5 best outputs are same with\r\nH-0     -1.0333465788368796\r\n\r\nWhen I use p = 0.9 , I get different outputs but the max score is \r\nH-0     -1.2899561307704726\r\nwhich is poorer than p = 0.1 and also beam search output\r\n\r\nCan anyone tell me where I am missing with the fundamentals of topp sampling(nucleus sampling) ?\r\nAnd what excatly this means in the documentation:\r\n\"\"sample from the smallest set whose cumulative probability mass exceeds p for next words\"\"",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1705/comments",
    "author": "aj7tesh",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-02-14T16:43:13Z",
        "body": "Suppose the model predicts the following probability distribution for the next word:\r\n```\r\ntoken  prob\r\na      0.4\r\nb      0.2\r\nc      0.15\r\nd      0.10\r\ne      0.06\r\nf      0.01\r\n...\r\n```\r\n\r\nWhen you do `--sampling-topp=0.1` then you're going to sample from the top 10% of the probability mass. In this case the first candidate (`a`) covers 40% of the probability mass so you'll always sample `a`.\r\n\r\nWhen you do `--sampling-topp=0.9` then you're going to sample from the top 90% of the probability mass. In this case you'll sample from `a`-`e`, which covers 91% of the mass.\r\n\r\nDoes that make sense?"
      },
      {
        "user": "aj7tesh",
        "created_at": "2020-02-18T06:32:01Z",
        "body": "yes, thanks @myleott "
      }
    ]
  },
  {
    "number": 1702,
    "title": "AssertionError: --sampling requires --nbest to be equal to --beam",
    "created_at": "2020-02-13T07:15:29Z",
    "closed_at": "2020-02-13T17:06:25Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1702",
    "body": "Sampling is altogether different decoding algorithm than beam search. Then why do i get this error when trying to use sampling and not beam search?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1702/comments",
    "author": "aj7tesh",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-02-13T17:06:25Z",
        "body": "Please follow the issue templates.  Include the full command you used to run and the backtrace of the error your received."
      }
    ]
  },
  {
    "number": 1700,
    "title": "Programme suddenly stops without error print in multi-nodes",
    "created_at": "2020-02-12T22:07:54Z",
    "closed_at": "2020-02-13T17:09:01Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1700",
    "body": "Only the exception below\r\n\r\n\r\n```\r\n/workspace/fairseq-0.8.0/fairseq/tasks/fairseq_task.py:354: UserWarning: Criterions should implement the reduce_metrics API. Falling back to deprecated aggregate_logging_outputs API.\r\ncriterion.__class__.reduce_metrics(logging_outputs)\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:105:185 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 11. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:107:189 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 13. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:104:199 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 10. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:108:203 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 14. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:103:175 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 9. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:94:205 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 0. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:102:170 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 8. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\n\r\ncontainer-e28-1580189644036-2850-02-000004:100:235 [0] include/socket.h:403 NCCL WARN Net : Connection closed by remote peer\r\ncontainer-e28-1580189644036-2850-02-000004:100:235 [0] NCCL INFO transport/net_socket.cc:369 -> 2\r\ncontainer-e28-1580189644036-2850-02-000004:100:235 [0] NCCL INFO include/net.h:29 -> 2\r\ncontainer-e28-1580189644036-2850-02-000004:100:235 [0] NCCL INFO transport/net.cc:533 -> 2\r\ncontainer-e28-1580189644036-2850-02-000004:100:235 [0] NCCL INFO transport.cc:163 -> 2 [Proxy Thread]\r\n\r\ncontainer-e28-1580189644036-2850-02-000004:100:236 [0] include/socket.h:408 NCCL WARN Call to recv failed : Connection reset by peer\r\n\r\ncontainer-e28-1580189644036-2850-02-000004:100:236 [0] transport/net_socket.cc:167 NCCL WARN NET/Socket : socket progress error\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:95:187 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 1. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\n\r\ncontainer-e28-1580189644036-2850-02-000003:96:191 [0] init.cc:1247 NCCL WARN Mismatched collective detected, please check your collective calls at and around rank 2. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs\r\ncontainer-e28-1580189644036-2850-02-000003:94:3333 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\r\ncontainer-e28-1580189644036-2850-02-000003:108:3338 [14] NCCL INFO Setting affinity for GPU 14 to febfaf00,0000febf,af000000\r\ncontainer-e28-1580189644036-2850-02-000003:107:3334 [13] NCCL INFO Setting affinity for GPU 13 to febfaf00,0000febf,af000000\r\ncontainer-e28-1580189644036-2850-02-000003:104:3336 [10] NCCL INFO Setting affinity for GPU 10 to febfaf00,0000febf,af000000\r\ncontainer-e28-1580189644036-2850-02-000003:105:3335 [11] NCCL INFO Setting affinity for GPU 11 to febfaf00,0000febf,af000000\r\ncontainer-e28-1580189644036-2850-02-000003:103:3337 [9] NCCL INFO Setting affinity for GPU 9 to febfaf00,0000febf,af000000\r\ncontainer-e28-1580189644036-2850-02-000003:102:3339 [8] NCCL INFO Setting affinity for GPU 8 to febfaf00,0000febf,af000000\r\ncontainer-e28-1580189644036-2850-02-000003:95:3340 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1700/comments",
    "author": "LibertFan",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-02-13T17:09:01Z",
        "body": "Please follow the issue templates.  Include the exact command you ran and the full log/error trace"
      }
    ]
  },
  {
    "number": 1665,
    "title": "Story Generation: Why can't the provided model produce stories as good as the results given?",
    "created_at": "2020-02-02T04:21:22Z",
    "closed_at": "2020-02-04T00:38:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1665",
    "body": "#1469 ## What is your question?\r\n\r\nI tried to generate stories with the provided models. I followed the command at README.md. But the stories I generated are of poor quality. Let me give you an example:\r\n\r\nS-8782\t[ WP ] Make a creation myth\r\nH-8782\t-1.0844866037368774\tI had been in a great place , and I was a proud man . He was a great man and a good man , but he was the best man I had ever met . <newline> <newline> I was a good man , and he was great . I was always the best man , but he was a great man . He was good at what I did , but he was good at how I did . He was great at that . <newline> <newline> I was proud to be him , and he was proud to be a good man . I would never know what he was like , I would never know . <newline> <newline> I was proud to be his good man . He was the best man in the world . He was good at that . <newline> <newline> I was proud to be a good man . <newline> <newline> The first time I saw him , I was proud to be his bad man . I was proud to be a good man , but I knew that I was proud to be a good man .\r\n\r\n#### Code\r\n\r\nfairseq-generate data-bin/writingPrompts \\\r\n    --path models/fusion_checkpoint.pt \\\r\n    --batch-size 32 \\\r\n    --beam 1 \\\r\n    --sampling \\\r\n    --sampling-topk 10 \\\r\n    --temperature 0.8 \\\r\n    --nbest 1 \\\r\n    --max-len-a 10 \\\r\n    --max-len-b 200 \\\r\n    --model-overrides \"{'pretrained_checkpoint':'models/pretrained_checkpoint.pt'}\"\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version: 0.4.0\r\n - PyTorch Version: 1.0.0\r\n - OS: Linux\r\n - How you installed fairseq: pip\r\n - Python version: 3.6.2\r\n - CUDA/cuDNN version: 8.0/7.6.5\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1665/comments",
    "author": "Mhzzzzz",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2020-02-04T00:38:33Z",
        "body": "thanks for your question, indeed not every story is good. The pretrained model we provide was trained with convolutional seq2seq, which is no longer the state of the art architecture (Transformer) and we have seen a lot of advancements in generation now with pretraining. It would be interesting to visit these on the story generation dataset. For this specific example, I recommend trying with a more specific or longer prompt, as this gives the model more to condition upon. "
      }
    ]
  },
  {
    "number": 1661,
    "title": "Wav2Vec: loss curve",
    "created_at": "2020-02-01T19:58:45Z",
    "closed_at": "2022-04-18T05:21:16Z",
    "labels": [
      "question",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1661",
    "body": "Could you provide loss curve that you get during training wav2vec? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1661/comments",
    "author": "Oktai15",
    "comments": [
      {
        "user": "sgxthsat2019",
        "created_at": "2020-05-18T01:30:48Z",
        "body": "Hi,\r\nDid you handle this? I want to plot the loss curve, too. Thank you!"
      },
      {
        "user": "sgxthsat2019",
        "created_at": "2020-05-18T01:31:03Z",
        "body": "@Oktai15 "
      },
      {
        "user": "Oktai15",
        "created_at": "2020-05-18T06:22:08Z",
        "body": "@sgxthsat2019 unfortunately, no, I didn’t resolve problem "
      },
      {
        "user": "MarvinLvn",
        "created_at": "2020-07-01T15:32:07Z",
        "body": "Hi both !\r\n\r\nI'm working on VQ-wav2vec, but adding the option `--tensorboard-logdir /path/to/tensorboard_logdir` should solve your problem.\r\nThen, you'll just have to call `tensorboard --logdir=/path/to/tensorboard_logdir` to display the tensorboard interface, showing you the validation loss (amongst other things)\r\n\r\nI haven't found the way to store this information under a `.json` file format or something similar though ..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:51Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1657,
    "title": "making sure some words aren't broken by subword tokenizer ",
    "created_at": "2020-01-30T01:21:57Z",
    "closed_at": "2020-02-10T16:29:59Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1657",
    "body": "Hi\r\nI have some words like [\"\\<A0>\" , \"\\<A1>\" , \"\\</s>\"] which I do not want to be broken into subwords [ < , A, 0 , >] rather treated as whole words <A0> \r\n\r\nThe current code and bpe encoder I believe breaks it \r\nIs there a way to prevent ?\r\nI added the new words in  encoder.json , what should I add in the dict.txt\r\nand do I need to change anything in the dict.txt ?\r\n\r\nAny help is appreciated",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1657/comments",
    "author": "tuhinjubcse",
    "comments": [
      {
        "user": "tuhinjubcse",
        "created_at": "2020-01-30T01:22:16Z",
        "body": "@myleott  @ngoyal2707 "
      },
      {
        "user": "lematt1991",
        "created_at": "2020-02-10T16:29:59Z",
        "body": "Please follow the issue template.  "
      }
    ]
  },
  {
    "number": 1656,
    "title": "ASR models and modules in examples",
    "created_at": "2020-01-29T14:07:50Z",
    "closed_at": "2020-01-29T21:59:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1656",
    "body": "I am wondering why the code for ASR has been released in the example section, rather than as normal tasks/models. Any reason for that? Does this imply something about its maintenance? Thanks.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1656/comments",
    "author": "mgaido91",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-01-29T21:59:44Z",
        "body": "Good question. The team working on it just wanted a little more isolation from the text-focused tasks/models. We may move more things to examples in the future as well."
      },
      {
        "user": "mgaido91",
        "created_at": "2020-01-30T09:53:22Z",
        "body": "Thanks for your answer @myleott. What if we have an `extensions` section then, rather than having these things in `examples`?"
      }
    ]
  },
  {
    "number": 1651,
    "title": "Base-size pre-trained models",
    "created_at": "2020-01-27T18:39:34Z",
    "closed_at": "2022-04-28T00:22:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1651",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n1) Does Bart offer base-size(6-layer encoder, 6-layer decoder, hidden size 768) pre-trained models? Since in the summarization task, the baseline BERTSUMABS is trained on bert-base(12-layer encoder, 6-layer decoder, both hidden size 768), have you ever compared base-size Bart with it?\r\n\r\n2) Could you please offer a README file for XSum (similar with the CNN one)?\r\n\r\n3) How much time does the XSum fine-tuning take with smaller GPUs (like 4 11GB GPUs)?\r\n\r\n@myleott @yinhanliu @ngoyal2707",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1651/comments",
    "author": "XinnuoXu",
    "comments": [
      {
        "user": "yinhanliu",
        "created_at": "2020-01-30T16:24:39Z",
        "body": "1. our base model is trained on wiki-bookcorpus only.\r\n2. will do\r\n3. we use 16 32gpus for 1 hour (30K steps). so in your case it is 8 hours. "
      },
      {
        "user": "YizhuLiu",
        "created_at": "2020-02-09T08:21:00Z",
        "body": "@XinnuoXu Hi, Have you evaluated the bart.large.cnn model? Did you get the same R-2 score on CNN/DM datase as published?  I used pre-trained model to fine-tune CNN/DM training. But the ROUGE-2 is 19.19 (R-2 in published paper is 21.28).\r\nThank you very much!"
      },
      {
        "user": "yinhanliu",
        "created_at": "2020-02-09T20:25:33Z",
        "body": "@YizhuLiu you need to use the right max-len, min-len, Len-penalty and beam size values."
      },
      {
        "user": "YizhuLiu",
        "created_at": "2020-02-10T09:38:21Z",
        "body": "@yinhanliu Thank you for your reply. We set these values as shown in \"Evaluating the bart.large.cnn model\": beam=4, lenpen=2.0, max_len_b=140, min_len=55. With this setting, the R-2 score is 20.03. Are they right? If not, how can I get the same R-2 score on CNN/DM as published? "
      },
      {
        "user": "ricardorei",
        "created_at": "2020-02-17T18:55:34Z",
        "body": "Will the Bart base-size(6-layer encoder, 6-layer decoder, hidden size 768) pre-trained models be released? I would like to play with them and it is hard for me to fine-tune the large model."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:33Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:45Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1640,
    "title": "❓ BART : how to use sampling",
    "created_at": "2020-01-22T00:49:06Z",
    "closed_at": "2022-04-18T02:21:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1640",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI'm trying to use sampling instead of Beam search with Bart, but I can't make it work...\r\n\r\n#### Code\r\n\r\nThis is the line I use for sampling with Beam search :\r\n```\r\nhypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, no_repeat_ngram_size=3)\r\n```\r\n\r\nI tried to use sampling with following line :\r\n```\r\nhypotheses_batch = bart.sample(slines, sampling=True, sampling_topp=0.9, temperature=1.)\r\n``` \r\n\r\nBut I receive following error :\r\n\r\n>File \"eval.py\", line 35, in _sample\r\n    return bart.sample(slines, sampling=True, sampling_topp=0.9, temperature=1.)\r\n  File \"/home/user/.venv/bart/lib/python3.6/site-packages/fairseq/models/bart/hub_interface.py\", line 118, in generate\r\n    prefix_tokens=sample['net_input']['src_tokens'].new_zeros((len(tokens), 1)).fill_(self.task.source_dictionary.bos()),\r\n  File \"/home/user/.venv/bart/lib/python3.6/site-packages/fairseq/tasks/fairseq_task.py\", line 265, in inference_step\r\n    return generator.generate(models, sample, prefix_tokens=prefix_tokens)\r\n  File \"/home/user/.venv/bart/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.venv/bart/lib/python3.6/site-packages/fairseq/sequence_generator.py\", line 113, in generate\r\n    return self._generate(model, sample, **kwargs)\r\n  File \"/home/user/.venv/bart/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.venv/bart/lib/python3.6/site-packages/fairseq/sequence_generator.py\", line 390, in _generate\r\n    eos_mask[:, :beam_size][blacklist] = 0\r\nRuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version : 0.9.0\r\n - PyTorch Version : 1.4.0\r\n - OS : Linux\r\n - How you installed fairseq : `pip`\r\n - Python version : 3.6.9\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1640/comments",
    "author": "astariul",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-04-20T13:34:36Z",
        "body": "Can you try running with `CUDA_LAUNCH_BLOCKING=1`.  "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:56Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1637,
    "title": "Create Sentence from Tensor",
    "created_at": "2020-01-21T00:48:26Z",
    "closed_at": "2020-01-23T15:34:57Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1637",
    "body": "I have an a sentence as tensor in  fairseq, \r\n`tensor([1829,   3,    3,    3, 6046,    3,    4,    3,    3, 6421, 6001,    3,\r\n           3,    3,    5, 6157,   23, 5392,    3, 6046,    3,    3,  460,    4,\r\n        6203,  105,    3,    3,    3,    3,    3,    3,    3,    4, 5572,    3,\r\n        5392,    3,    4,    3,    3,    3,    3,    4,    3,    3, 4692,    3,\r\n           3,    3,    5,    2])`\r\nCan I generate this actual sentence in fairseq, given the dictionary?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1637/comments",
    "author": "ajesujoba",
    "comments": [
      {
        "user": "sc16s2w",
        "created_at": "2022-02-12T20:35:09Z",
        "body": "hi, do you know how to solve it now?"
      }
    ]
  },
  {
    "number": 1635,
    "title": "Training failed to start when using multiple GPUs",
    "created_at": "2020-01-20T13:08:09Z",
    "closed_at": "2020-01-21T20:57:31Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1635",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nWhen I was trying to train a model using a corpus contains about 100M lines, it crashed and reported a multiprocess related error which probably involves torch:\r\n\r\n```\r\n2020-01-20 19:32:08.000 [INFO] [Driver] | model transformer_vaswani_wmt_en_de_big, criterion LabelSmoothedCrossEntropyCriterion\r\n2020-01-20 19:32:08.000 [INFO] [Driver] | num. model params: 260337664 (num. trained: 260337664)\r\n2020-01-20 19:32:41.000 [INFO] [Driver] | training on 4 GPUs\r\n2020-01-20 19:32:41.000 [INFO] [Driver] | max tokens per GPU = 2048 and max sentences per GPU = None\r\n2020-01-20 19:32:41.000 [INFO] [Driver] | no existing checkpoint found checkpoints/checkpoint_last.pt\r\n2020-01-20 19:32:41.000 [INFO] [Driver] | loading train data for epoch 0\r\n2020-01-20 19:32:51.000 [INFO] [Driver] | loaded *** examples from: data_bin/train.zh-en.zh\r\n2020-01-20 19:33:03.000 [INFO] [Driver] | loaded *** examples from: data_bin/train.zh-en.en\r\n2020-01-20 19:33:03.000 [INFO] [Driver] | data_bin train zh-en *** examples\r\n2020-01-20 19:45:24.000 [INFO] [Driver] Traceback (most recent call last):\r\n2020-01-20 19:45:24.000 [INFO] [Driver] File \"/opt/conda/bin/fairseq-train\", line 11, in <module>\r\n2020-01-20 19:45:24.000 [INFO] [Driver] load_entry_point('fairseq==0.9.0', 'console_scripts', 'fairseq-train')()\r\n2020-01-20 19:45:24.000 [INFO] [Driver] File \"/opt/conda/lib/python3.7/site-packages/fairseq_cli/train.py\", line 329, in cli_main\r\n2020-01-20 19:45:24.000 [INFO] [Driver] nprocs=args.distributed_world_size,\r\n2020-01-20 19:45:24.000 [INFO] [Driver] File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n2020-01-20 19:45:24.000 [INFO] [Driver] while not spawn_context.join():\r\n2020-01-20 19:45:24.000 [INFO] [Driver] File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 107, in join\r\n2020-01-20 19:45:24.000 [INFO] [Driver] (error_index, name)\r\n2020-01-20 19:45:24.000 [INFO] [Driver] Exception: process 1 terminated with signal SIGKILL\r\n```\r\n\r\nI tried to use a single GPU instead and it seems everything was OK (at least it trained several batches but took too long time). What's more, using the same hyperparameters I was able to train a model before.\r\n\r\nPlease kindly help me to figure out why, thanks a lot.\r\n\r\n#### Code\r\n\r\n```bash\r\n# Here is my script for training \r\n\r\ndata_dir=data_bin\r\nsave_dir=checkpoints\r\n\r\nfairseq-train $data_dir \\\r\n  --task translation \\\r\n  --dataset-impl lazy \\    # I used the same strategy to preprocess the corpus\r\n  --arch transformer_vaswani_wmt_en_de_big \\\r\n  --save-dir $save_dir \\\r\n  --optimizer adam \\\r\n    --adam-betas \"(0.9,0.98)\" \\\r\n  --clip-norm 0.0 \\\r\n  --lr-scheduler inverse_sqrt \\\r\n  --lr 0.001 \\\r\n  --min-lr 1e-09 \\\r\n  --criterion label_smoothed_cross_entropy \\\r\n  --log-format json \\\r\n  --max-tokens 2048 \\\r\n  --update-freq 8 \\\r\n  --encoder-normalize-before \\\r\n  --decoder-normalize-before \\\r\n  --dropout 0.3 \\\r\n  --relu-dropout 0.1 \\\r\n  --attention-dropout 0.1 \\\r\n  --max-update 10000000 \\\r\n  --max-epoch 100 \\\r\n  --keep-interval-updates 100 \\\r\n  --save-interval-updates 5000 \\\r\n  --log-interval 100 \\\r\n  --share-decoder-input-output-embed \\\r\n  --fp16 \\\r\n  --warmup-updates 15000 \\\r\n  --warmup-init-lr 1e-07\r\n```\r\n\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.9\r\n - PyTorch Version (e.g., 1.0): 1.3 perhaps\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: Tesla V100\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1635/comments",
    "author": "TingxunShi",
    "comments": [
      {
        "user": "TingxunShi",
        "created_at": "2020-01-21T03:18:56Z",
        "body": "After adding the option --ddp-backend=no_c10d training seems good now, but still want to confirm this is a right solution = ="
      },
      {
        "user": "myleott",
        "created_at": "2020-01-21T20:57:31Z",
        "body": "`--ddp-backend=no_c10d` is fine. It's a bit more robust, but can be slightly slower when training on multiple nodes. Since you're training on a single node it won't make any difference, so might as well use it."
      },
      {
        "user": "TingxunShi",
        "created_at": "2020-01-28T02:23:39Z",
        "body": "@myleott Sorry to update this issue again. The task crashed due to some issue so I restarted it. Although I've added `--ddp-backend=no_c10d`, the same error occured again and I have no idea now. Really appreciate if you could kindly give any other advice"
      },
      {
        "user": "myleott",
        "created_at": "2020-01-29T16:30:33Z",
        "body": "Can you try updating to fairseq master and pytorch 1.4? It's possible it's a pytorch bug, and the latest version of fairseq should have better logging in case of some failures."
      },
      {
        "user": "TingxunShi",
        "created_at": "2020-01-31T03:55:57Z",
        "body": "Thanks for the advice"
      }
    ]
  },
  {
    "number": 1609,
    "title": "out of memory for sharded data",
    "created_at": "2020-01-10T07:45:41Z",
    "closed_at": "2020-01-10T09:11:51Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1609",
    "body": "fairseq: 0.9.0\r\npytorch: 1.2\r\nhardware: 128G memory, 4 * 32G V100 gpus\r\ndataset :  two sharded data with each 50M pairs,  so that I need reload dataset each epoch.\r\nThe question is: after each reload, I found the memory increases and after several epochs I catch the exception with out of memory. Is that a bug?  some hyperparamters I missed. \r\nThanks ,guys.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1609/comments",
    "author": "zherowolf",
    "comments": [
      {
        "user": "zherowolf",
        "created_at": "2020-01-10T09:11:50Z",
        "body": "got it. closing now."
      },
      {
        "user": "nomadlx",
        "created_at": "2020-04-19T19:03:16Z",
        "body": "How did you solve the problem? I had the same problem."
      }
    ]
  },
  {
    "number": 1603,
    "title": "In BART pre training do we replace multiple [MASK] with single one?",
    "created_at": "2020-01-08T18:52:25Z",
    "closed_at": "2022-04-28T00:22:11Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1603",
    "body": "Hello,\r\n\r\nIn BART pretraining, while creating input sequence, we end up with something like the following input sequence:\r\n\r\n [BOS] a b [MASK] [MASK] e \r\n\r\ndo we replace it with\r\n\r\n[BOS] a b [MASK] e\r\n\r\nBasically do we replace multiple consecutive \"[MASK]\" tokens with a single one?\r\n\r\n@ngoyal2707 ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1603/comments",
    "author": "saksham-singhal",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-02-10T16:31:11Z",
        "body": "No, I don't believe so.  CC @ngoyal2707 for confirmation "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:37Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:41Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1587,
    "title": "--lr-scheduler flag does not work other than \"polynomial_decay\"",
    "created_at": "2020-01-06T01:34:45Z",
    "closed_at": "2020-01-06T13:21:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1587",
    "body": "## 🐛 Bug\r\nHello,\r\n\r\nI am finetuning Roberta-large model for text classification task with the  following command:\r\n```\r\n!CUDA_VISIBLE_DEVICES=0,1,2,3 python ./train.py /workspace/fairseq/data/ --restore-file $ROBERTA_PATH --max-positions 512 --max-sentences $MAX_SENTENCES --max-tokens 5200 --task sentence_prediction --reset-optimizer --reset-dataloader --reset-meters --required-batch-size-multiple 1  --init-token 0 --separator-token 2 --arch roberta_large --criterion sentence_prediction --classification-head-name $HEAD_NAME --num-classes $NUM_CLASSES --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 --clip-norm 0.0 --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 --max-epoch 3 --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric --truncate-sequence --find-unused-parameters --update-freq 4 --save-dir \"/workspace/result/checkpoints/\" \r\n\r\n```\r\nThat works without issue, but when I want to change the learning rate scheduler param as  `--lr-scheduler fixed`  or  `--lr-scheduler reduce_lr_on_plateau` it yields the following error:\r\n\r\n```\r\ntrain.py: error: unrecognized arguments: --total-num-update 330\r\n\r\n```\r\nHow this can be fixed?\r\n\r\nThanks.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1587/comments",
    "author": "rnyak",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-01-06T13:21:56Z",
        "body": "You need to remove the `--total-num-update` flag, which only works with the polynomial LR scheduler."
      }
    ]
  },
  {
    "number": 1580,
    "title": "How do I set parameters to automatically end training after the loss is stable?",
    "created_at": "2020-01-05T04:08:06Z",
    "closed_at": "2020-01-05T21:13:47Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1580",
    "body": "environment：python3.6,torch1.2.0,gcc 4.9 cuda 10  Graphics card：titan \r\ntrain code:\r\nCUDA_VISIBLE_DEVICES=2,3,6 python train.py data-bin/data --lr 0.5 --ddp-backend no_c10d --clip-norm 0.1 --min-lr 1e-09 --dropout 0.2 --optimizer sgd --batch-size 128 --max-tokens 4096 --max-epoch 500 --criterion cross_entropy --no-epoch-checkpoints  --arch fconv --save-dir checkpoints/fconv15\r\n\r\n1、How do I set parameters to automatically end training after the loss is stable?\r\n2、May I use --fp 16?，When I use the parameter --fp16, it appears：\r\ndata-bin/DBNQA_data train en-sparql 715600 examples\r\n| epoch 001:   0%| | 9/1864 [00:06<21:09,  1.46it/s, loss=16.447, ppl=| WARNING: overflow detected, setting loss scale to: 64.0\r\n| epoch 001:   1%| | 17/1864 [00:12<18:18,  1.68it/s, loss=16.055, ppl| WARNING: overflow detected, setting loss scale to: 32.0\r\n| epoch 001:   1%| | 22/1864 [00:15<17:56,  1.71it/s, loss=15.796, ppl\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1580/comments",
    "author": "DonnieZhang586",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-01-05T21:13:47Z",
        "body": "You can use a combination of `--reduce-lr-on-plateau` and `--min-lr` to accomplish this."
      },
      {
        "user": "DonnieZhang586",
        "created_at": "2020-01-06T05:04:39Z",
        "body": "When I use this parameter, it appears error：unrecognized arguments:--reduce-lr-on-plateau"
      },
      {
        "user": "lematt1991",
        "created_at": "2020-01-06T14:35:49Z",
        "body": "Sorry, actually it looks like it should be `--lr-scheduler redue_lr_on_plateau`.  That said, I forgot that #904, added `--patience <int>`, which will stop training if the validation loss does not improve after a certain number of epochs.  This may be more fitting for what you are trying to accomplish.  Hope this helps!"
      }
    ]
  },
  {
    "number": 1578,
    "title": "The language model inference for wikitext-103 ",
    "created_at": "2020-01-04T03:20:31Z",
    "closed_at": "2020-01-04T15:08:47Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1578",
    "body": "I have trained a language model for wikitext-103 with your offical configuration.\r\ntraining parameters:\r\n fairseq-train --task language_modeling data-bin/wikitext-103 \\\r\n  --save-dir checkpoints/transformer_wikitext-103 --arch transformer_lm_wiki103 \\\r\n  --max-update 286000 --max-lr 1.0 --t-mult 2 --lr-period-updates 270000 --lr-scheduler cosine --lr-shrink 0.75 \\\r\n  --warmup-updates 16000 --warmup-init-lr 1e-07 --min-lr 1e-09 --optimizer nag --lr 0.0001 --clip-norm 0.1 \\\r\n  --criterion adaptive_loss --max-tokens 1640--update-freq 5--tokens-per-sample 3072 --seed 1 \\\r\n  --sample-break-mode none --skip-invalid-size-inputs-valid-test --ddp-backend=no_c10d\r\n\r\nmy train device is TITAN V 8 GPU,and the best model size is 1.4G, and your official model size is 1.4G. But when I  inference with your model,  I can run on the GPU without out of memory with your inference parameters(max-tokens 3072,context-windows 2560 softmax-batch 1024), but my model can not inference, although I lower the max-token to 2048,1024 .\r\nwhat is the problem?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1578/comments",
    "author": "liutengbo",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-01-04T15:08:47Z",
        "body": "Please follow the issue templates."
      }
    ]
  },
  {
    "number": 1574,
    "title": "Matrix mismatch error when using pretrained model",
    "created_at": "2020-01-03T15:28:04Z",
    "closed_at": "2020-01-04T08:25:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1574",
    "body": "When I use the given pretrained model ``transformer.wmt16.en-de`` from paper Scaling Neural Machine Translation, here reported a matrix mismatch error:\r\n\r\n```\r\npython interactive.py ../wmt16.en-de.joined-dict.transformer/ --path ../wmt16.en-de.joined-dict.transformer/model.pt --task translation --remove-bpe -s en -t de\r\nNamespace(beam=5, bpe=None, buffer_size=1, cpu=False, criterion='cross_entropy', data='../wmt16.en-de.joined-dict.transformer/', dataset_impl=None, decoding_iterations=None, decoding_strategy='left_to_right', dehyphenate=False, diverse_beam_groups=-1, diverse_beam_strength=0.5, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', gold_target_len=False, input='-', lazy_load=False, left_pad_source='True', left_pad_target='False', length_beam=5, lenpen=1, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=1, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=0, optimizer='nag', path='../wmt16.en-de.joined-dict.transformer/model.pt', prefix_size=0, print_alignment=False, quiet=False, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='de', task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)\r\n| [en] dictionary: 32769 types\r\n| [de] dictionary: 32769 types\r\n| loading model(s) from ../wmt16.en-de.joined-dict.transformer/model.pt\r\nTraceback (most recent call last):\r\n  File \"interactive.py\", line 195, in <module>\r\n    cli_main()\r\n  File \"interactive.py\", line 191, in cli_main\r\n    main(args)\r\n  File \"interactive.py\", line 84, in main\r\n    task=task,\r\n  File \"/root/code/ft_local/Mask-Predict-master/fairseq/checkpoint_utils.py\", line 156, in load_model_ensemble\r\n    ensemble, args, _task = load_model_ensemble_and_task(filenames, arg_overrides, task)\r\n  File \"/root/code/ft_local/Mask-Predict-master/fairseq/checkpoint_utils.py\", line 175, in load_model_ensemble_and_task\r\n    model.load_state_dict(state['model'], strict=True)\r\n  File \"/root/code/ft_local/Mask-Predict-master/fairseq/models/fairseq_model.py\", line 72, in load_state_dict\r\n    return super().load_state_dict(state_dict, strict)\r\n  File \"/root/miniconda2/envs/py3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 839, in load_state_dict\r\n    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for TransformerModel:\r\n        size mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([32768, 1024]) from checkpoint, the shape in current model is torch.Size([32769, 1024]).\r\n        size mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([32768, 1024]) from checkpoint, the shape in current model is torch.Size([32769, 1024]).\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1574/comments",
    "author": "alphadl",
    "comments": [
      {
        "user": "alphadl",
        "created_at": "2020-01-03T15:28:51Z",
        "body": "@myleott "
      },
      {
        "user": "lematt1991",
        "created_at": "2020-01-03T17:18:06Z",
        "body": "How did you pre-process the data?  If you want to use the pre-trained model provided in the README, you'll need to provide the dictionaries from the tar file.  Specifically:\r\n\r\n```\r\nfairseq-preprocess \\\r\n    --source-lang en --target-lang de \\\r\n    --trainpref $TEXT/train.tok.clean.bpe.32000 \\\r\n    --validpref $TEXT/newstest2013.tok.bpe.32000 \\\r\n    --testpref $TEXT/newstest2014.tok.bpe.32000 \\\r\n    --destdir data-bin/wmt16_en_de_bpe32k --workers 20 \\\r\n    --joined-dictionary --srcdict wmt16.en-de.joined-dict.transformer/dict.en.txt\r\n```\r\n\r\n"
      },
      {
        "user": "alphadl",
        "created_at": "2020-01-04T08:25:04Z",
        "body": "Thanks! @lematt1991 "
      },
      {
        "user": "Tikquuss",
        "created_at": "2020-05-31T14:23:11Z",
        "body": "Thanks! @lematt1991"
      }
    ]
  },
  {
    "number": 1572,
    "title": "Update README to include reference perplexity for language modeling example",
    "created_at": "2020-01-03T05:16:23Z",
    "closed_at": "2020-01-11T21:45:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1572",
    "body": "data-bin/wikitext-103 \\\r\n  --save-dir checkpoints/transformer_wikitext-103 \\\r\n  --arch transformer_lm --share-decoder-input-output-embed \\\r\n  --dropout 0.1 \\\r\n  --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \\\r\n  --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\r\n  --tokens-per-sample 512 --sample-break-mode none \\\r\n  --max-tokens 2048 --update-freq 16 \\\r\n  --fp16 \\\r\n  --max-update 50000",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1572/comments",
    "author": "liutengbo",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-01-03T16:55:04Z",
        "body": "Please follow the issue templates"
      },
      {
        "user": "myleott",
        "created_at": "2020-01-09T16:08:53Z",
        "body": "I'll add some evaluation instructions to the README."
      }
    ]
  },
  {
    "number": 1568,
    "title": "Lots of unk generated when translating with right-2-left decoding.",
    "created_at": "2020-01-02T03:31:27Z",
    "closed_at": "2022-04-28T00:22:12Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1568",
    "body": "I trained two en-de transformer models with some back-translation data. One is left-2-right decoding based, the other is right-2-left decoding based.\r\n\r\nThe L2R one works as expected, on WMT14, it obtained 29.7 bleu. However the R2L one generates lots of `<unk>` tokens. The R2L one can only got 28.0 bleu if those `<unk>` tokens weren't suppressed (`<unk>` can be suppressed with --unkpen 10.0 option during decoding.)\r\n\r\nWhy the R2L model generates so many `<unk>` tokens, while the L2R one generates none ?\r\n\r\n#### Code\r\nFor the R2L one, I trained with the following code:\r\n\r\n    import torch\r\n    import fairseq.tasks.translation\r\n    import fairseq.data\r\n    \r\n    \r\n    class R2LDataset(fairseq.data.BaseWrapperDataset):\r\n        def __getitem__(self, item):\r\n            tensor = self.dataset[item]\r\n            assert len(tensor.shape) == 1\r\n            assert tensor[-1].item() == 2  # </s> symbol\r\n    \r\n            result = torch.empty_like(tensor)\r\n            result[:-1] = torch.flip(tensor[:-1], (0,))\r\n            result[-1] = tensor[-1]\r\n            return result\r\n    \r\n    \r\n    @fairseq.tasks.register_task('r2l_translation')\r\n    class R2LTranslation(fairseq.tasks.translation.TranslationTask):\r\n        def load_dataset(self, split, epoch=0, combine=False, **kwargs):\r\n            super().load_dataset(split, epoch=epoch, combine=combine, **kwargs)\r\n            original = self.datasets[split]\r\n            assert type(original) == fairseq.data.LanguagePairDataset\r\n            original.tgt = R2LDataset(original.tgt)\r\n\r\nTraining command line options are like:\r\n\r\n    fairseq-train  data \\\r\n    --arch transformer_wmt_en_de_big \\\r\n    --share-all-embeddings \\\r\n    --optimizer adam \\\r\n    --adam-betas '(0.9,0.98)' \\\r\n    --clip-norm 0.0 \\\r\n    --lr 0.0005 \\\r\n    --lr-scheduler inverse_sqrt \\\r\n    --warmup-updates 4000 \\\r\n    --warmup-init-lr 1e-07 \\\r\n    --dropout 0.3 --weight-decay 0.0 \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --label-smoothing 0.1 \\\r\n    --max-tokens 5000 \\\r\n    --attention-dropout 0.1 \\\r\n    --update-freq 10 \\\r\n    --max-update 41000 \\\r\n    --tensorboard-logdir /cache/nmt/tensorboard \\\r\n    --save-dir /cache/nmt/model \\\r\n    --fp16 \\\r\n    --log-format simple \\\r\n    --log-interval 1 \\\r\n    --user-dir ./code \\\r\n    --task r2l_translation\r\n\r\n\r\n#### What's your environment?\r\nThe training node has 8 x Tesla v100 GPUs\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1568/comments",
    "author": "torshie",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-01-03T17:22:33Z",
        "body": "Sorry, nothing immediately comes to mind as to why you'd be getting so many `unk`s for the R2L direction.  Maybe @myleott has thoughts?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1567,
    "title": "Is there  a  configuration for PTB",
    "created_at": "2020-01-02T02:53:00Z",
    "closed_at": "2020-01-02T15:12:20Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1567",
    "body": "I use the Fairseq to run PTB dataset,but I can not get a good score,is there a good configuration for PTB dataset\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1567/comments",
    "author": "liutengbo",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-01-02T15:12:20Z",
        "body": "Please fill out the template"
      }
    ]
  },
  {
    "number": 1558,
    "title": "How to implement Dual encoder in fairseq as described in Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model",
    "created_at": "2019-12-29T23:47:58Z",
    "closed_at": "2020-01-03T21:49:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1558",
    "body": "\r\n\r\nI am trying to use fairseq for implementing the paper titled **Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model**. \r\nThe encoder and decoder for the translation task share the same weights. \r\nBut I am having difficulties in modelling the dual encoder setup .\r\nAny pointers in this direction would be really helpful.\r\n\r\n - fairseq Version (e.g., 1.0 or master):master\r\n - PyTorch Version (e.g., 1.0) 1.3.1\r\n - OS (e.g., Linux): Ubuntu 18\r\n - How you installed fairseq (`pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Nvidia v100 x4\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1558/comments",
    "author": "thak123",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2019-12-30T14:40:20Z",
        "body": "Can you elaborate more on what specifically you are trying to accomplish?  What have you tried so far?"
      },
      {
        "user": "lematt1991",
        "created_at": "2020-01-03T21:49:44Z",
        "body": "Closing due to inactivity."
      }
    ]
  },
  {
    "number": 1554,
    "title": "How to generate my own distillation dataset on Levenshtien Transformer",
    "created_at": "2019-12-28T10:24:29Z",
    "closed_at": "2020-01-08T14:02:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1554",
    "body": "## What is your question?\r\nAbout the Levenshtien Transformer, I want to experiment with my own dataset，but it needs distillation dataset for this experiment.\r\n\r\nSo how can I generate my own distillation dataset?\r\n\r\nAnd what is the specific process？\r\n\r\nand when I run \"fairseq-train...\", there is a error:\r\nSegmentation fault.How to tackle it?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1554/comments",
    "author": "zhaoxv",
    "comments": [
      {
        "user": "MultiPath",
        "created_at": "2019-12-29T19:21:29Z",
        "body": "Hi @zhaoxv \r\n\r\nIt should be the same as running ``generate.py``with a pretrained autoregressive model by passing ``--gen-subset train``.\r\n"
      },
      {
        "user": "zhaoxv",
        "created_at": "2019-12-30T09:31:02Z",
        "body": "Ok.thanks for your reply."
      },
      {
        "user": "zhaoxv",
        "created_at": "2019-12-30T09:58:40Z",
        "body": "when I run \"fairseq-train...\", there is an error:\r\n**Segmentation fault.**\r\nHow to tackle it?"
      },
      {
        "user": "lematt1991",
        "created_at": "2019-12-30T15:36:31Z",
        "body": "Please post the full command that you used to launch the job as well as a full stack trace of the error you are getting."
      },
      {
        "user": "MultiPath",
        "created_at": "2019-12-30T16:21:29Z",
        "body": "> when I run \"fairseq-train...\", there is an error:\r\n> **Segmentation fault.**\r\n> How to tackle it?\r\n\r\nDid you compile all the C++ files? Try running ```python setup.py build_ext --inplace```"
      },
      {
        "user": "zhaoxv",
        "created_at": "2019-12-31T09:11:11Z",
        "body": "Hi @MultiPath \r\nAfter running “`generate.py`with a pretrained autoregressive model by passing `--gen-subset train`”, where is the distillation dataset stored？\r\nI cannot find it.\r\n\r\n"
      },
      {
        "user": "zhaoxv",
        "created_at": "2020-01-01T07:38:15Z",
        "body": "Hi @lematt1991 **The command is:**\r\n\"fairseq-train data-bin/wmt17_en_de --save-dir checkpoints1 --ddp-backend=no_c10d --task translation_lev --criterion nat_loss --arch levenshtein_transformer --noise random_delete --share-all-embeddings --optimizer adam --adam-betas '(0.9,0.98)' --lr 0.0005 --lr-scheduler inverse_sqrt --min-lr '1e-09' --warmup-updates 1000 --warmup-init-lr '1e-07' --label-smoothing 0.1 --dropout 0.3 --weight-decay 0.01 --decoder-learned-pos --encoder-learned-pos --apply-bert-init --log-format 'simple' --log-interval 10 --fixed-validation-seed 7 --max-tokens 800 --save-interval-updates 100 --max-update 300\"\r\n**The error is:**\r\n| model levenshtein_transformer, criterion LabelSmoothedDualImitationCriterion\r\n| num. model params: 65850368 (num. trained: 65850368)\r\n| training on 1 GPUs\r\n| max tokens per GPU = 800 and max sentences per GPU = None\r\n| no existing checkpoint found checkpoints1/checkpoint_last.pt\r\n| loading train data for epoch 0\r\n| loaded 3961179 examples from: data-bin/wmt17_en_de/train.en-de.en\r\n| loaded 3961179 examples from: data-bin/wmt17_en_de/train.en-de.de\r\n| data-bin/wmt17_en_de train en-de 3961179 examples\r\nSegmentation fault"
      },
      {
        "user": "zhaoxv",
        "created_at": "2020-01-04T09:49:42Z",
        "body": "@MultiPath @lematt1991 "
      },
      {
        "user": "lematt1991",
        "created_at": "2020-01-06T21:54:42Z",
        "body": "> Did you compile all the C++ files? Try running `python setup.py build_ext --inplace`\r\n\r\n"
      },
      {
        "user": "Ir1d",
        "created_at": "2020-04-12T02:14:20Z",
        "body": "@zhaoxv @MultiPath could you please share how to generate a new distillation dataset after training the teacher models?\r\n\r\nI tried `fairseq-generate data-bin/wmt17_en_de_joined --path checkpoints/transformer_vaswani_wmt_en_de_big/checkpoint_best.pt --batch-size 128 --beam 5 --remove-bpe --gen-subset train --results-path data-bin/wmt17_en_de_distill` but after 20 hours and in the end it told me `CUDA out of memory`. No result was generated in anywhere..\r\n"
      },
      {
        "user": "Ir1d",
        "created_at": "2020-04-12T23:59:43Z",
        "body": "Hi I reduced the batch-size and there's no CUDA error, but after the script there's still no result. Could you help me out?\r\n\r\nupd: solved in #2003\r\nupd: not sure if this is actually solved, updated in #2003"
      }
    ]
  },
  {
    "number": 1540,
    "title": "XLM-R: pre-processing steps",
    "created_at": "2019-12-23T14:18:54Z",
    "closed_at": "2019-12-26T16:05:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1540",
    "body": "Hi,\r\n\r\nI'm very interested in the pre-processing steps for training a RoBERTa/XLM-R model from scratch 😅\r\n\r\n# BPE with `sentencepiece`\r\n\r\nCould you provide the `spm_train` command that was used to create the `sentencepiece` model. Interesting part would be the `--control_symbols` (and their order), `--unk_piece` and `--pad_piece`. \r\n\r\n# BPE encoder\r\n\r\nIn my understanding, the next part after `spm_train` is the following:\r\n\r\n```bash\r\n cat {train,dev,test}.txt | spm_encode --model=<model_file> --output_format=piece > {train,dev,test}.bpe\r\n```\r\n\r\nI would do that step with train, validation and test dataset. Is that correct?\r\n\r\n# `fairseq` preprocessing\r\n\r\n```\r\nfairseq-preprocess \\\r\n    --only-source \\\r\n    --trainpref train.bpe \\\r\n    --validpref valid.bpe \\\r\n    --testpref test.bpe \\\r\n    --destdir output \\\r\n    --workers 60\r\n```\r\n\r\nAre these the correct pre-processing steps? \r\n\r\nMy aim is to convert a trained model with the 🤗/Transformers and use it for various NLP tasks, so the correct tokenization/sentence piece model is one crucial part 😅\r\n\r\nThanks many in advance!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1540/comments",
    "author": "stefan-it",
    "comments": [
      {
        "user": "ngoyal2707",
        "created_at": "2019-12-26T16:05:00Z",
        "body": "Hey,\r\n\r\n1) Yes, so we follow the same process as `XLM` paper for training SPM model. The SPM model was trained only on subsample of data.  As the whole data seemed too big to train SPM model and we saw that subsample of data was good representative of vocab. While subsampling, we up/down sample language based on the probability formula mentioned in the paper with smoothing value of `0.7`.\r\nRest all arg are default, so `<unk>` and `<pad>`.\r\n\r\n2) Yes exactly.\r\n\r\n3) That's exactly correct.\r\n\r\n\r\nThere were no other language specific tokenization or segmentation.\r\n  "
      }
    ]
  },
  {
    "number": 1534,
    "title": "How to evaluate RACE dataset",
    "created_at": "2019-12-21T05:56:21Z",
    "closed_at": "2020-01-09T00:27:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1534",
    "body": "Please tell me the way to evaluate RACE dataset",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1534/comments",
    "author": "west-urad24",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-01-09T00:26:56Z",
        "body": "There are a few options:\r\n* During training you can set `--valid-subset valid,test,test1` and it will print results for valid, test and test1 during training.\r\n* Alternatively you can use `fairseq-validate` to evaluate an existing finetuned model on a new dataset:\r\n```\r\nfairseq-validate RACE-mmap2-bin \\\r\n  --path checkpoint_best.pt \\\r\n  --max-sentences 1 \\\r\n  --valid-subset test  \\\r\n  --task sentence_ranking --criterion sentence_ranking \\\r\n  --save-predictions preds.tsv\r\n```"
      }
    ]
  },
  {
    "number": 1529,
    "title": "FloatingPointError: Minimum loss scale reached (0.0001).",
    "created_at": "2019-12-20T04:20:56Z",
    "closed_at": "2019-12-20T15:04:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1529",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nHi, guys.\r\nI met the same issue as  #515 .\r\nI tried some methods, such as reducing the learning rate and increasing the batch-size, but none of them can solve the problem .\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior (**always include the command you ran**):\r\n\r\nMy training command as follow:\r\n```\r\nexport CUDA_VISIBLE_DEVICES=4,5,6,7\r\npython train.py $data_bin \\\r\n      -s zh -t en \\\r\n      --lr 0.0005 --min-lr 1e-09 \\\r\n      --weight-decay 0 --clip-norm 0.0 \\\r\n      --dropout 0.3 \\\r\n      --max-tokens 30000 \\\r\n      --arch transformer \\\r\n      --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n      --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 \\\r\n      --warmup-updates 4000 \\\r\n      --ddp-backend=no_c10d \\\r\n      --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\r\n      --save-dir $checkpoints \\\r\n      --fp16 \r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nTraining log:\r\n```\r\n| epoch 028 | loss 3.976 | nll_loss 2.204 | ppl 4.61 | wps 279460 | ups 3 | wpb 110529.673 | bsz 3780.211 | num_updates 9165 | lr 0.000330319 | gnorm 0.635 | clip 0.000 | oom 0.000 | loss_scale 0.000 | wall 4043 | train_wall 3606\r\n| epoch 028 | valid on 'valid' subset | loss 9.329 | nll_loss 8.088 | ppl 272.04 | num_updates 9165 | best_loss 9.11817\r\n| saved checkpoint ./checkpoints/LDC_zh-en_32k_fp16/checkpoint28.pt (epoch 28 @ 9165 updates) (writing took 7.268606901168823 seconds)\r\n| WARNING: overflow detected, setting loss scale to: 0.0001220703125\r\n| epoch 029 | loss 3.979 | nll_loss 2.176 | ppl 4.52 | wps 280188 | ups 3 | wpb 110521.492 | bsz 3796.456 | num_updates 9492 | lr 0.00032458 | gnorm 0.652 | clip 0.000 | oom 0.000 | loss_scale 0.000 | wall 4189 | train_wall 3734\r\n| epoch 029 | valid on 'valid' subset | loss 9.580 | nll_loss 8.300 | ppl 315.24 | num_updates 9492 | best_loss 9.11817\r\n| saved checkpoint ./checkpoints/LDC_zh-en_32k_fp16/checkpoint29.pt (epoch 29 @ 9492 updates) (writing took 7.078469276428223 seconds)\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 337, in <module>\r\n    cli_main()\r\n  File \"train.py\", line 329, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/duantea/anaconda3/envs/torch1.2cuda10/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/duantea/anaconda3/envs/torch1.2cuda10/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/duantea/anaconda3/envs/torch1.2cuda10/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/duantea/mmyin/fairseq/train.py\", line 296, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/duantea/mmyin/fairseq/train.py\", line 86, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/home/duantea/mmyin/fairseq/train.py\", line 127, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/duantea/mmyin/fairseq/fairseq/trainer.py\", line 437, in train_step\r\n    grad_norm = self.optimizer.clip_grad_norm(self.args.clip_norm)\r\n  File \"/home/duantea/mmyin/fairseq/fairseq/optim/fp16_optimizer.py\", line 146, in clip_grad_norm\r\n    ).format(self.min_loss_scale))\r\nFloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.\r\n\r\n\r\n```\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): master\r\n - PyTorch Version (e.g., 1.0): v1.3\r\n - OS (e.g., Linux): Linnux\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.6\r\n - CUDA/cuDNN version: CUDA-v10.1\r\n - GPU models and configuration: V100-32G\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1529/comments",
    "author": "KelleyYin",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-20T15:04:50Z",
        "body": "The loss is overflowing repeatedly, which causes batches to be thrown away. fairseq eventually terminates training so that you don't waste computation indefinitely.\r\n\r\nThere are a few options:\r\n1. `--fp16-scale-tolerance=0.25`: Allow some tolerance before decreasing the loss scale. This setting will allow one out of every four updates to overflow before lowering the loss scale. I'd recommend trying this first.\r\n2. `--min-loss-scale=0.5`: Prevent the loss scale from going below a certain value (in this case 0.5). Note that this could waste a lot of computation -- we may throw away a lot of batches due to overflow and not make any progress on training.\r\n3. Further decrease the learning rate.\r\n4. Switch to FP32 training."
      },
      {
        "user": "008karan",
        "created_at": "2020-10-01T13:46:55Z",
        "body": "@myleott I am facing a similar issue while finetuning wav2vec2. \r\nWas getting 'your system doesn't support fp16' warning earlier so I switched to FP32 by removing `--fp16` flag. Training resumed but loss became nun.\r\n```\r\n2020-10-01 10:25:50 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\r\n2020-10-01 10:25:50 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:25:50 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:25:50 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:25:50 | INFO | train | {\"epoch\": 73, \"train_loss\": \"nan\", \"train_ntokens\": \"15558.8\", \"train_nsentences\": \"427.098\", \"t\r\nrain_nll_loss\": \"nan\", \"train_wps\": \"2443\", \"train_ups\": \"0.16\", \"train_wpb\": \"15558.8\", \"train_bsz\": \"427.1\", \"train_num_updates\": \"\r\n9291\", \"train_lr\": \"1e-08\", \"train_gnorm\": \"nan\", \"train_loss_scale\": null, \"train_train_wall\": \"867\", \"train_wall\": \"0\"}\r\n2020-10-01 10:25:50 | INFO | fairseq.trainer | begin training epoch 74\r\n2020-10-01 10:40:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\r\n2020-10-01 10:42:40 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:42:40 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-10-01 10:42:40 | INFO | valid | {\"epoch\": 74, \"valid_loss\": \"nan\", \"valid_ntokens\": \"2570.42\", \"valid_nsentences\": \"71.4286\", \"v\r\nalid_nll_loss\": \"nan\", \"valid_uer\": \"100\", \"valid_wer\": \"100\", \"valid_raw_wer\": \"100\", \"valid_wps\": \"2780.3\", \"valid_wpb\": \"2570.4\", \r\n\"valid_bsz\": \"71.4\", \"valid_num_updates\": \"9454\", \"valid_best_wer\": \"100\"}\r\n```\r\nAny suggestions on how to solve this issue?"
      }
    ]
  },
  {
    "number": 1525,
    "title": "BART training time",
    "created_at": "2019-12-19T06:05:40Z",
    "closed_at": "2019-12-19T14:56:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1525",
    "body": "May I know how much time BART pre-training took in which GPU configuration? I can see in the paper its written 500K steps with batch size 8k but I want to know the time it took. Many thanks.\r\n\r\n ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1525/comments",
    "author": "sunilitggu",
    "comments": [
      {
        "user": "ngoyal2707",
        "created_at": "2019-12-19T14:56:23Z",
        "body": "The time can depend on the type and numbers of gpus. We trained for around 11-12 days on 256 gpus. "
      }
    ]
  },
  {
    "number": 1523,
    "title": "Levenshtien Transformer Performance",
    "created_at": "2019-12-18T23:59:38Z",
    "closed_at": "2019-12-30T15:38:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1523",
    "body": "Hi I ran the Levenshtein Transformer Model on Both Original and Distilled Dataset.However I GET bleu of 23.87 on DIstilled after 230000k steps and training using fp--16 on single GPU without using --freq-update for GPU Simulation and 23.68 on original dataset after 260000k on 4 GPUs without fp--16.\r\n\r\nWhat will I need to do to achieve the paper performance of 25.20 and 27.3 using Teacher Rescorer.\r\n\r\nALso when I dont give Batch size while evaluation The BLEU4 is 23.87 and 23.68.It takes 250 as Default batch size. However if I mention Batch size 1 the BLEU DROPS to 4.16 and speed to 7.14 sentences/Second.\r\n\r\nWHY SO?\r\n@kahne @MultiPath @myleott \r\n\r\nHereby Giving the outputs\r\n\r\nResult with Batch size 1\r\n| Translated 3003 sentences (38803 tokens) in 389.2s (7.72 sentences/s, 99.69 tokens/s)                                                                             \r\n| Generate test with beam=1: BLEU4 = 4.30, 51.4/16.2/8.4/4.6 (BP=0.320, ratio=0.467, syslen=30151, reflen=64504)\r\n\r\nResult with Default Batch size of  250 \r\n\r\n| Translated 3003 sentences (84361 tokens) in 9.3s (321.44 sentences/s, 9030.04 tokens/s)                                                                           \r\n| Generate test with beam=4: BLEU4 = 23.87, 56.5/29.9/17.7/10.9 (BP=1.000, ratio=1.000, syslen=64536, reflen=64506)\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1523/comments",
    "author": "spprabhu",
    "comments": [
      {
        "user": "MultiPath",
        "created_at": "2019-12-19T03:02:22Z",
        "body": "@spprabhu  We recently found there is a new bug introduced by a recent refactor for batch size=1 decoding 5d7a81099462e9f19715ce5fa37c03816a750e12. Please update the code."
      },
      {
        "user": "MultiPath",
        "created_at": "2019-12-19T03:04:58Z",
        "body": "@spprabhu please do not put \"needs triage\" label."
      },
      {
        "user": "myleott",
        "created_at": "2019-12-19T14:25:22Z",
        "body": "Ah, the \"needs triage\" label gets added automatically by the new Issue templates"
      },
      {
        "user": "lematt1991",
        "created_at": "2019-12-30T15:38:01Z",
        "body": "Sounds like 5d7a81099462e9f19715ce5fa37c03816a750e12 should resolve the problem.  Closing..."
      },
      {
        "user": "alphadl",
        "created_at": "2020-04-20T07:36:28Z",
        "body": "@spprabhu Have you reproduced the result reported in the paper? "
      },
      {
        "user": "spprabhu",
        "created_at": "2020-04-21T10:50:54Z",
        "body": "Yes, But I didn't get the same results.\r\nI could translate 3003 sentences in 9.3 sec with test beam size 4 and BLEU4 = 23.87\r\nBut I could only train it for 230000 steps and I used 1V100 using --fp16\r\nThe training lasted for about 15 hrs"
      },
      {
        "user": "spprabhu",
        "created_at": "2020-04-21T10:52:10Z",
        "body": "Also if you want to translate just a single sentence at a time the BLEU decreases significantly."
      },
      {
        "user": "spprabhu",
        "created_at": "2020-04-21T10:54:23Z",
        "body": "Hi @alphadl did you get the exact results\r\n"
      }
    ]
  },
  {
    "number": 1522,
    "title": "Levenshtien Multiple Translations Issue",
    "created_at": "2019-12-18T17:30:27Z",
    "closed_at": "2019-12-30T15:37:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1522",
    "body": "Hi\r\nI am trying to generate translations for Levenshtien Transformer using Teacher Rescorer .\r\nHowever I am getting following errors\r\n\r\n| [en] dictionary: 40624 types\r\n| [de] dictionary: 40624 types\r\n| loaded 3003 examples from: data-bin/test.en-de.en\r\n| loaded 3003 examples from: data-bin/test.en-de.de\r\n| data-bin test en-de 3003 examples\r\n| loading model(s) from checkpoints/lv/checkpoint_best.pt:checkpoints/lv/checkpoint_baset.pt\r\n  0%|                                                                                                                                        | 0/12 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/home/ubuntu/miniconda3/bin/fairseq-generate\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')()\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/generate.py\", line 199, in cli_main\r\n    main(args)\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/generate.py\", line 104, in main\r\n    hypos = task.inference_step(generator, models, sample, prefix_tokens)\r\n  File \"/home/ubuntu/fairseq/fairseq/tasks/fairseq_task.py\", line 265, in inference_step\r\n    return generator.generate(models, sample, prefix_tokens=prefix_tokens)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/fairseq/fairseq/iterative_refinement_generator.py\", line 118, in generate\r\n    assert self.beam_size > 1, \"Reranking requires multiple translation for each example\"\r\n\r\n@kahne @myleott @MultiPath \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1522/comments",
    "author": "spprabhu",
    "comments": [
      {
        "user": "spprabhu",
        "created_at": "2019-12-18T17:31:20Z",
        "body": "This is my Generate Command\r\n\r\nfairseq-generate     data-bin     --gen-subset test     --task translation_lev     --path checkpoints/lv/checkpoint_best.pt:checkpoints/lv/checkpoint_baset.pt     --iter-decode-max-iter 1           --iter-decode-with-external-reranker   --remove-bpe     --print-step --beam 1\r\n"
      },
      {
        "user": "MultiPath",
        "created_at": "2019-12-18T18:24:03Z",
        "body": "@spprabhu  Levenshtein Transformer does not support \"--iter-decode-with-external-reranker\""
      },
      {
        "user": "spprabhu",
        "created_at": "2019-12-18T23:51:24Z",
        "body": "So how do I implement the AR Teacher Rescorer expert policy"
      },
      {
        "user": "spprabhu",
        "created_at": "2019-12-18T23:51:52Z",
        "body": "So how do I implement the AR Teacher Rescorer expert policy\r\n@MultiPath @myleott @kahne "
      },
      {
        "user": "MultiPath",
        "created_at": "2019-12-19T03:03:08Z",
        "body": "@spprabhu  sorry, I don't understand your question."
      },
      {
        "user": "lematt1991",
        "created_at": "2019-12-30T15:37:28Z",
        "body": "Closing due to inactivity. "
      }
    ]
  },
  {
    "number": 1520,
    "title": "UnicodeEncodeError: 'ascii' codec can't encode character '\\xe4' in position 8: ordinal not in range(128)",
    "created_at": "2019-12-18T07:31:23Z",
    "closed_at": "2019-12-19T14:24:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1520",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nwhen I run the following code，I have faced the error.\r\n\r\n“fairseq-generate data-bin3/iwslt14.tokenized.de-en --path checkpoints2/transformer_iwslt_de_en/checkpoint_best.pt --batch-size 128 --beam 5 --remove-bpe\r\n”\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python3/bin/fairseq-generate\", line 8, in <module>\r\n    sys.exit(cli_main())\r\n  File \"/usr/local/python3/lib/python3.6/site-packages/fairseq_cli/generate.py\", line 203, in cli_main\r\n    main(args)\r\n  File \"/usr/local/python3/lib/python3.6/site-packages/fairseq_cli/generate.py\", line 135, in main\r\n    print('S-{}\\t{}'.format(sample_id, src_str))\r\nUnicodeEncodeError: 'ascii' codec can't encode character '\\xe4' in position 8: ordinal not in range(128)\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1520/comments",
    "author": "zhaoxv",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-18T13:08:49Z",
        "body": "Usually that means your locale environment variables are not set properly. Can you try running:\r\n```bash\r\nlocale -a\r\n```\r\n\r\nand then (you may need to adjust based on the output above, the important part is UTF-8):\r\n```bash\r\nLC_ALL=en_US.UTF-8 fairseq-generate (...)\r\n```"
      },
      {
        "user": "zhaoxv",
        "created_at": "2019-12-18T14:00:32Z",
        "body": "Thanks for your reply. I will try."
      },
      {
        "user": "zhaoxv",
        "created_at": "2019-12-19T12:26:02Z",
        "body": "Thanks for your help! I add \"PYTHONIOENCODING=utf-8\" ,now it can run properly.\r\nAs follows:\r\n\"PYTHONIOENCODING=utf-8 fairseq-generate data-bin3/iwslt14.tokenized.de-en --path checkpoints2/transformer_iwslt_de_en/checkpoint_best.pt --batch-size 128 --beam 5 --remove-bpe\"\r\n\r\n"
      }
    ]
  },
  {
    "number": 1518,
    "title": "build failed in ppc64le",
    "created_at": "2019-12-18T01:07:53Z",
    "closed_at": "2019-12-23T12:23:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1518",
    "body": "I'm trying to install fairseq on our IBM server, that has ppc64le cpu.\r\nI made powerai docker container 1.6.2, which has python3.6, pytorch 1.2.0\r\nAnd install with pip command.\r\nIt worked on other x86 servers. Does it matter with cpu architecture?\r\nHere is my error messages\r\n\r\n```\r\nRunning setup.py install for fairseq ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /opt/anaconda/envs/wmlce/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ttzxccf7/fairseq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ttzxccf7/fairseq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-7g5agz8o/install-record.txt --single-version-externally-managed --compile\r\n         cwd: /tmp/pip-install-ttzxccf7/fairseq/\r\n    Complete output (308 lines):\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib.linux-ppc64le-3.6\r\n    creating build/lib.linux-ppc64le-3.6/examples\r\n    copying examples/__init__.py -> build/lib.linux-ppc64le-3.6/examples\r\n    creating build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/binarizer.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/bleu.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/checkpoint_utils.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/distributed_utils.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/file_utils.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/hub_utils.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/iterative_refinement_generator.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/legacy_distributed_data_parallel.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/meters.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/options.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/pdb.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/progress_bar.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/registry.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/search.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/sequence_generator.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/sequence_scorer.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/tokenizer.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/trainer.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    copying fairseq/utils.py -> build/lib.linux-ppc64le-3.6/fairseq\r\n    creating build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    copying fairseq_cli/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    copying fairseq_cli/eval_lm.py -> build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    copying fairseq_cli/generate.py -> build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    copying fairseq_cli/interactive.py -> build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    copying fairseq_cli/preprocess.py -> build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    copying fairseq_cli/score.py -> build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    copying fairseq_cli/setup.py -> build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    copying fairseq_cli/train.py -> build/lib.linux-ppc64le-3.6/fairseq_cli\r\n    creating build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    copying examples/noisychannel/__init__.py -> build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank.py -> build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_generate.py -> build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_options.py -> build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_score_bw.py -> build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_score_lm.py -> build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_tune.py -> build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    copying examples/noisychannel/rerank_utils.py -> build/lib.linux-ppc64le-3.6/examples/noisychannel\r\n    creating build/lib.linux-ppc64le-3.6/examples/speech_recognition\r\n    copying examples/speech_recognition/__init__.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition\r\n    copying examples/speech_recognition/infer.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition\r\n    copying examples/speech_recognition/w2l_decoder.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition\r\n    creating build/lib.linux-ppc64le-3.6/examples/speech_recognition/criterions\r\n    copying examples/speech_recognition/criterions/ASG_loss.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/criterions\r\n    copying examples/speech_recognition/criterions/CTC_loss.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/criterions\r\n    copying examples/speech_recognition/criterions/__init__.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/criterions\r\n    copying examples/speech_recognition/criterions/cross_entropy_acc.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/criterions\r\n    creating build/lib.linux-ppc64le-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/__init__.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/asr_dataset.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/collaters.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/data_utils.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/data\r\n    copying examples/speech_recognition/data/replabels.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/data\r\n    creating build/lib.linux-ppc64le-3.6/examples/speech_recognition/models\r\n    copying examples/speech_recognition/models/__init__.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/models\r\n    copying examples/speech_recognition/models/vggtransformer.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/models\r\n    copying examples/speech_recognition/models/w2l_conv_glu_enc.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/models\r\n    creating build/lib.linux-ppc64le-3.6/examples/speech_recognition/tasks\r\n    copying examples/speech_recognition/tasks/__init__.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/tasks\r\n    copying examples/speech_recognition/tasks/speech_recognition.py -> build/lib.linux-ppc64le-3.6/examples/speech_recognition/tasks\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/binary_cross_entropy.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/composite_loss.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/cross_entropy.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/legacy_masked_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/masked_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/nat_loss.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/sentence_prediction.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    copying fairseq/criterions/sentence_ranking.py -> build/lib.linux-ppc64le-3.6/fairseq/criterions\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/append_token_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/base_wrapper_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/colorize_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/concat_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/concat_sentences_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/data_utils.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/denoising_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/dictionary.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/fairseq_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/id_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/indexed_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/iterators.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/language_pair_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/list_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/lm_context_window_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/lru_cache_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/mask_tokens_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/monolingual_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/multi_corpus_sampled_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/nested_dictionary_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/noising.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/num_samples_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/numel_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/offset_tokens_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/pad_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/plasma_utils.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/prepend_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/prepend_token_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/raw_label_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/replace_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/resampling_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/roll_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/sharded_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/sort_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/strip_token_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/subsample_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/token_block_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/transform_eos_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/transform_eos_lang_pair_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    copying fairseq/data/truncate_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/cmlm_transformer.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/composite_encoder.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/fairseq_decoder.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/fairseq_encoder.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/fairseq_model.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/fconv.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/fconv_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/fconv_self_att.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/insertion_transformer.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/iterative_nonautoregressive_transformer.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/levenshtein_transformer.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/lightconv.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/lightconv_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/lstm.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/masked_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/model_utils.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/multilingual_transformer.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/nonautoregressive_ensembles.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/nonautoregressive_transformer.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/transformer.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/transformer_from_pretrained_xlm.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/transformer_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    copying fairseq/models/wav2vec.py -> build/lib.linux-ppc64le-3.6/fairseq/models\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/adaptive_input.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/beamable_mm.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/character_token_embedder.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/conv_tbc.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/dynamic_convolution.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/gelu.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/grad_multiply.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/highway.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/layer_norm.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/lightweight_convolution.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/linearized_convolution.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/logsumexp_moe.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/mean_pool_gating_network.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/multihead_attention.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/positional_embedding.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/scalar_bias.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/sparse_multihead_attention.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/sparse_transformer_sentence_encoder.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/sparse_transformer_sentence_encoder_layer.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/transformer_layer.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/transformer_sentence_encoder.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/unfold.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    copying fairseq/modules/vggblock.py -> build/lib.linux-ppc64le-3.6/fairseq/modules\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/adadelta.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/adafactor.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/adagrad.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/adam.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/adamax.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/bmuf.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/nag.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    copying fairseq/optim/sgd.py -> build/lib.linux-ppc64le-3.6/fairseq/optim\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/audio_pretraining.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/cross_lingual_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/denoising.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/fairseq_task.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/language_modeling.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/legacy_masked_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/masked_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/multilingual_masked_lm.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/semisupervised_translation.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/sentence_prediction.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/sentence_ranking.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/translation.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/translation_from_pretrained_xlm.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/translation_lev.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    copying fairseq/tasks/translation_moe.py -> build/lib.linux-ppc64le-3.6/fairseq/tasks\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/data/audio\r\n    copying fairseq/data/audio/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/data/audio\r\n    copying fairseq/data/audio/raw_audio_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data/audio\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/fastbpe.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/gpt2_bpe.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/gpt2_bpe_utils.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/hf_bert_bpe.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/moses_tokenizer.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/nltk_tokenizer.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/sentencepiece_bpe.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/space_tokenizer.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/subword_nmt_bpe.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    copying fairseq/data/encoders/utils.py -> build/lib.linux-ppc64le-3.6/fairseq/data/encoders\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/data/legacy\r\n    copying fairseq/data/legacy/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/data/legacy\r\n    copying fairseq/data/legacy/block_pair_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data/legacy\r\n    copying fairseq/data/legacy/masked_lm_dataset.py -> build/lib.linux-ppc64le-3.6/fairseq/data/legacy\r\n    copying fairseq/data/legacy/masked_lm_dictionary.py -> build/lib.linux-ppc64le-3.6/fairseq/data/legacy\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/models/bart\r\n    copying fairseq/models/bart/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/models/bart\r\n    copying fairseq/models/bart/hub_interface.py -> build/lib.linux-ppc64le-3.6/fairseq/models/bart\r\n    copying fairseq/models/bart/model.py -> build/lib.linux-ppc64le-3.6/fairseq/models/bart\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/models/roberta\r\n    copying fairseq/models/roberta/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/models/roberta\r\n    copying fairseq/models/roberta/alignment_utils.py -> build/lib.linux-ppc64le-3.6/fairseq/models/roberta\r\n    copying fairseq/models/roberta/hub_interface.py -> build/lib.linux-ppc64le-3.6/fairseq/models/roberta\r\n    copying fairseq/models/roberta/model.py -> build/lib.linux-ppc64le-3.6/fairseq/models/roberta\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/modules/dynamicconv_layer\r\n    copying fairseq/modules/dynamicconv_layer/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/modules/dynamicconv_layer\r\n    copying fairseq/modules/dynamicconv_layer/cuda_function_gen.py -> build/lib.linux-ppc64le-3.6/fairseq/modules/dynamicconv_layer\r\n    copying fairseq/modules/dynamicconv_layer/dynamicconv_layer.py -> build/lib.linux-ppc64le-3.6/fairseq/modules/dynamicconv_layer\r\n    copying fairseq/modules/dynamicconv_layer/setup.py -> build/lib.linux-ppc64le-3.6/fairseq/modules/dynamicconv_layer\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/modules/lightconv_layer\r\n    copying fairseq/modules/lightconv_layer/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/modules/lightconv_layer\r\n    copying fairseq/modules/lightconv_layer/cuda_function_gen.py -> build/lib.linux-ppc64le-3.6/fairseq/modules/lightconv_layer\r\n    copying fairseq/modules/lightconv_layer/lightconv_layer.py -> build/lib.linux-ppc64le-3.6/fairseq/modules/lightconv_layer\r\n    copying fairseq/modules/lightconv_layer/setup.py -> build/lib.linux-ppc64le-3.6/fairseq/modules/lightconv_layer\r\n    creating build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/polynomial_decay_schedule.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-ppc64le-3.6/fairseq/optim/lr_scheduler\r\n    running build_ext\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-install-ttzxccf7/fairseq/setup.py\", line 161, in <module>\r\n        zip_safe=False,\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/site-packages/setuptools/__init__.py\", line 145, in setup\r\n        return distutils.core.setup(**attrs)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/core.py\", line 148, in setup\r\n        dist.run_commands()\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/dist.py\", line 955, in run_commands\r\n        self.run_command(cmd)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/site-packages/setuptools/command/install.py\", line 61, in run\r\n        return orig.install.run(self)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/command/install.py\", line 545, in run\r\n        self.run_command('build')\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n        self.distribution.run_command(command)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/command/build.py\", line 135, in run\r\n        self.run_command(cmd_name)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n        self.distribution.run_command(command)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 84, in run\r\n        _build_ext.run(self)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py\", line 186, in run\r\n        _build_ext.build_ext.run(self)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/distutils/command/build_ext.py\", line 339, in run\r\n        self.build_extensions()\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 243, in build_extensions\r\n        self._check_abi()\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 383, in _check_abi\r\n        check_compiler_abi_compatibility(compiler)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 173, in check_compiler_abi_compatibility\r\n        if not check_compiler_ok_for_platform(compiler):\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 149, in check_compiler_ok_for_platform\r\n        which = subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/subprocess.py\", line 356, in check_output\r\n        **kwargs).stdout\r\n      File \"/opt/anaconda/envs/wmlce/lib/python3.6/subprocess.py\", line 438, in run\r\n        output=stdout, stderr=stderr)\r\n    subprocess.CalledProcessError: Command '['which', 'g++']' returned non-zero exit status 1.\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /opt/anaconda/envs/wmlce/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ttzxccf7/fairseq/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ttzxccf7/fairseq/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-7g5agz8o/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\r\n```\r\n#### My environment\r\n\r\n - fairseq Version (e.g., 1.0 or master): 0.8\r\n - PyTorch Version (e.g., 1.0) 1.2.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed fairseq (`pip`, source): pip install fairseq\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version:10.1\r\n - GPU models and configuration:V100\r\n - Any other relevant information: AC922 IBM ppc64le\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1518/comments",
    "author": "petanerd",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-18T13:01:40Z",
        "body": "It looks like g++ isn’t installed, based on the error: `Command '['which', 'g++']' returned non-zero exit status 1`. There are some C extensions that require building. Can you please install gxx and try again?"
      },
      {
        "user": "petanerd",
        "created_at": "2019-12-18T22:54:15Z",
        "body": "Thank you. But first I installed, the error message shows no gcc. so I installed gcc. and then there is no g++ error again. Also I installed g++, the error message was same again."
      },
      {
        "user": "myleott",
        "created_at": "2019-12-19T00:02:24Z",
        "body": "What happens if you run:\r\n```bash\r\n$ which -a g++\r\n```\r\n\r\nAlso, you're using anaconda, right? Can you install with: `conda install gxx_linux-ppc64le gcc_linux-ppc64le`, then try again?"
      }
    ]
  },
  {
    "number": 1513,
    "title": "BART Summary length",
    "created_at": "2019-12-17T09:59:10Z",
    "closed_at": "2019-12-17T13:47:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1513",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nHi\r\nI am trying to play with the length of the generated summary. It seems that there are several parameters relevant to the length in bart.sample(): min_len, max_len_a, max_len_b.\r\nAm I correct?\r\nI have played with different values max_len_a, but I get the same summary for different values of max_len_a.\r\nThanks\r\n\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):\r\n - PyTorch Version (e.g., 1.0)\r\n - OS (e.g., Linux):\r\n - How you installed fairseq (`pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1513/comments",
    "author": "jmamou",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-17T13:47:11Z",
        "body": "As you note there are three flags: `min_len`, `max_len_a` and `max_len_b`. The final output size will be constrained by these:\r\n```\r\nmin_len <= output_len <= (max_len_a * input_len) + max_len_b\r\n```\r\n\r\nThe default value for `max_len_b` is 200, so you probably want to increase that. You can also increase `min_len` if you want to force the model to generate longer outputs.\r\n\r\nFinally you can try adjust `lenpen`. Values > 1 encourage the model to produce longer outputs, so you can try `lenpen=1.5`."
      }
    ]
  },
  {
    "number": 1511,
    "title": "Hyperparameters for Levenshtien Transformer",
    "created_at": "2019-12-16T17:57:39Z",
    "closed_at": "2019-12-16T18:00:18Z",
    "labels": [
      "question",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1511",
    "body": "Hi I am getting CUDA ERROR :OUT OF MEMORY.\r\nI am using 4 V100 GPUS.KIndly please suggest appropriate parameters\r\n\r\n(base) ubuntu@ip-172-31-8-60:~/fairseq$ fairseq-train     data-bin/org     --save-dir checkpoints/lv     --ddp-backend=no_c10d     --task translation_lev     --criterion nat_loss     --arch levenshtein_transformer     --noise random_delete     --share-all-embeddings     --optimizer adam --adam-betas '(0.9,0.98)'     --lr 0.0005 --lr-scheduler inverse_sqrt     --min-lr '1e-09' --warmup-updates 10000     --warmup-init-lr '1e-07' --label-smoothing 0.1     --dropout 0.3 --weight-decay 0.01     --decoder-learned-pos     --encoder-learned-pos     --apply-bert-init     --log-format 'simple' --log-interval 100     --fixed-validation-seed 7     --max-tokens 8000     --save-interval-updates 10000\r\n| distributed init (rank 3): tcp://localhost:12081\r\n| distributed init (rank 2): tcp://localhost:12081\r\n| distributed init (rank 1): tcp://localhost:12081\r\n| distributed init (rank 0): tcp://localhost:12081\r\n| initialized host ip-172-31-8-60 as rank 3\r\n| initialized host ip-172-31-8-60 as rank 2\r\n| initialized host ip-172-31-8-60 as rank 1\r\n| initialized host ip-172-31-8-60 as rank 0\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/miniconda3/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 329, in cli_main\r\n    nprocs=args.distributed_world_size,\r\n  File \"/home/ubuntu/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/ubuntu/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 3 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 296, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 35, in main\r\n    args.distributed_rank = distributed_utils.distributed_init(args)\r\n  File \"/home/ubuntu/fairseq/fairseq/distributed_utils.py\", line 91, in distributed_init\r\n    dist.all_reduce(torch.zeros(1).cuda())\r\nRuntimeError: CUDA error: out of memory\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1511/comments",
    "author": "spprabhu",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-16T18:00:18Z",
        "body": "Duplicate of #1510"
      }
    ]
  },
  {
    "number": 1510,
    "title": "Levenshtien Transformer ideal Hyperparameters",
    "created_at": "2019-12-16T17:47:58Z",
    "closed_at": "2020-01-03T21:50:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1510",
    "body": "\r\nHey can you please suggest appropriate hyperparameters for training on 4 GPUs,with both normal and FP-16 Version.\r\n\r\nAlso what would be the parameters if I am using SIngle GPU and FP-16 to achieve same accuracy,should they be same as Base or should they be changed.\r\n@myleott @MultiPath @kahne",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1510/comments",
    "author": "spprabhu",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-16T17:53:04Z",
        "body": "Hyperparameters typically need to be tuned yourself, depending on the dataset.\r\n\r\nFP16 shouldn't require changing any hyperparameters compared to FP32.\r\n\r\nWhen training on fewer GPUs you need to use `--update-freq` to match the batch size."
      },
      {
        "user": "spprabhu",
        "created_at": "2019-12-16T17:59:57Z",
        "body": "Yeah but even that is giving CUDA : Memory error\r\nThis is what I am getting\r\n\r\nHi I am getting CUDA ERROR :OUT OF MEMORY.\r\nI am using 4 V100 GPUS.KIndly please suggest appropriate parameters\r\n\r\n(base) ubuntu@ip-172-31-8-60:~/fairseq$ fairseq-train data-bin/org --save-dir checkpoints/lv --ddp-backend=no_c10d --task translation_lev --criterion nat_loss --arch levenshtein_transformer --noise random_delete --share-all-embeddings --optimizer adam --adam-betas '(0.9,0.98)' --lr 0.0005 --lr-scheduler inverse_sqrt --min-lr '1e-09' --warmup-updates 10000 --warmup-init-lr '1e-07' --label-smoothing 0.1 --dropout 0.3 --weight-decay 0.01 --decoder-learned-pos --encoder-learned-pos --apply-bert-init --log-format 'simple' --log-interval 100 --fixed-validation-seed 7 --max-tokens 8000 --save-interval-updates 10000\r\n| distributed init (rank 3): tcp://localhost:12081\r\n| distributed init (rank 2): tcp://localhost:12081\r\n| distributed init (rank 1): tcp://localhost:12081\r\n| distributed init (rank 0): tcp://localhost:12081\r\n| initialized host ip-172-31-8-60 as rank 3\r\n| initialized host ip-172-31-8-60 as rank 2\r\n| initialized host ip-172-31-8-60 as rank 1\r\n| initialized host ip-172-31-8-60 as rank 0\r\nTraceback (most recent call last):\r\nFile \"/home/ubuntu/miniconda3/bin/fairseq-train\", line 11, in \r\nload_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\nFile \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 329, in cli_main\r\nnprocs=args.distributed_world_size,\r\nFile \"/home/ubuntu/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\nwhile not spawn_context.join():\r\nFile \"/home/ubuntu/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\nraise Exception(msg)\r\nException:\r\n\r\n-- Process 3 terminated with the following error:\r\nTraceback (most recent call last):\r\nFile \"/home/ubuntu/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\nfn(i, *args)\r\nFile \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 296, in distributed_main\r\nmain(args, init_distributed=True)\r\nFile \"/home/ubuntu/fairseq/fairseq_cli/train.py\", line 35, in main\r\nargs.distributed_rank = distributed_utils.distributed_init(args)\r\nFile \"/home/ubuntu/fairseq/fairseq/distributed_utils.py\", line 91, in distributed_init\r\ndist.all_reduce(torch.zeros(1).cuda())\r\nRuntimeError: CUDA error: out of memory"
      },
      {
        "user": "myleott",
        "created_at": "2019-12-16T18:02:29Z",
        "body": "Please run `nvidia-smi`. You probably have other zombie processes using your GPU memory."
      },
      {
        "user": "MultiPath",
        "created_at": "2019-12-16T20:29:28Z",
        "body": "@spprabhu You should use --fp16 in order to run on --max-tokens 8000\r\nIn order to increase the batch-size, you can set --update-freq larger than 1 to enable gradient accumulation. It will simulate a bigger batch size. \r\nFor example, if you set --max-tokens 8000 --update-freq 8 on single GPU, then you get batch-size as 64K which is the number we used in the paper,"
      },
      {
        "user": "lematt1991",
        "created_at": "2020-01-03T21:50:52Z",
        "body": "Closing due to inactivity. "
      }
    ]
  },
  {
    "number": 1507,
    "title": "XLM-R License ",
    "created_at": "2019-12-16T10:15:04Z",
    "closed_at": "2019-12-16T15:44:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1507",
    "body": "XLM-R model has been released in different repositories (fairseq, Pytext and XLM) each one with a different license (MIT, BSD and Attribution-NonCommercial 4.0). The license in this repository says: \"fairseq(-py) is MIT-licensed. **The license applies to the pre-trained models as well**\". Does this also applies to the XLM-R model and weights?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1507/comments",
    "author": "ikergarcia1996",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-16T15:44:17Z",
        "body": "I can't speak to the other repositories, but yes the MIT license applies for all fairseq code + models."
      }
    ]
  },
  {
    "number": 1506,
    "title": "BART : Release of the base version ?",
    "created_at": "2019-12-16T06:30:32Z",
    "closed_at": "2020-06-25T00:16:40Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1506",
    "body": "Hi @ngoyal2707 @yinhanliu,\r\n\r\n**Are you planning to release the pre-trained base version of BART ?**",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1506/comments",
    "author": "astariul",
    "comments": [
      {
        "user": "ricardorei",
        "created_at": "2020-02-17T18:53:02Z",
        "body": "Is there any news about this? I would love to have the base version of the model to play with..."
      }
    ]
  },
  {
    "number": 1504,
    "title": "BART : Release of the pre-training procedure ?",
    "created_at": "2019-12-16T05:27:21Z",
    "closed_at": "2022-04-27T23:22:00Z",
    "labels": [
      "enhancement",
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1504",
    "body": "Hi @ngoyal2707 @yinhanliu,\r\n\r\n**Are you planning to release the pre-training procedure of BART ?**",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1504/comments",
    "author": "astariul",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T21:20:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1495,
    "title": "mask current time step",
    "created_at": "2019-12-13T07:37:54Z",
    "closed_at": "2019-12-18T00:52:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1495",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1495/comments",
    "author": "ehsan-soe",
    "comments": [
      {
        "user": "ehsan-soe",
        "created_at": "2019-12-13T07:40:19Z",
        "body": "I just realized that in the ```SingleHeadAttention``` the current time step is also masked in the decoder when ```mask_future_timesteps=True``` (unlike attention paper where only future times are masked)\r\nThis will lead to mask being 0 everywhere and attention weights be filled by ```-inf```. And the ```softmax``` would be ```nan```.\r\nDoesn't this problematic? "
      },
      {
        "user": "huihuifan",
        "created_at": "2019-12-16T21:28:07Z",
        "body": "For the stories paper, we always used self-attention with the zero vector as well, so if the model did not want to attend to any timesteps it would attend to the zero vector. "
      }
    ]
  },
  {
    "number": 1486,
    "title": "Dimensions for WAV2VEC for downstream tasks",
    "created_at": "2019-12-11T07:02:25Z",
    "closed_at": "2022-04-18T05:21:26Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1486",
    "body": "Please help me with the output dimensions for wav2vec. \r\n\r\nFor **0.5 second** of audio, the output dimensions are **[1x512x48]**\r\nFor **1 second** of audio, the output dimensions are **[1x512x98]**\r\nFor **2 second** of audio, the output dimensions are **[1x512x198]**\r\nFor **3 second** of audio, the output dimensions are **[1x512x298]**\r\n\r\n**Am I correct to say that the stride length of features is approx. 10ms?**\r\n\r\nCan we increase the stride length? say, we get a [1x512] vector every second? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1486/comments",
    "author": "happypanda5",
    "comments": [
      {
        "user": "JinmingZhao",
        "created_at": "2020-07-17T13:54:18Z",
        "body": "@myleott   how to use the wav2vec for downstream tasks, such as emotion recognition, speaker recognition?\r\n\r\nThank you very much!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:56Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1468,
    "title": "Can wav2vec vectors be converted back to audio?",
    "created_at": "2019-12-06T21:55:39Z",
    "closed_at": "2020-04-04T20:50:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1468",
    "body": "Assuming I train some dataset and get a bunch of vectors, can those vectors then be converted back to audio?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1468/comments",
    "author": "shamoons",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2019-12-06T22:01:04Z",
        "body": "CC @alexeib "
      },
      {
        "user": "caesar-one",
        "created_at": "2020-04-02T13:17:36Z",
        "body": "I'm interested too"
      },
      {
        "user": "alexeib",
        "created_at": "2020-04-04T20:50:15Z",
        "body": "i dont think you can do this algorithmically. you can try to train another network that maps wav2vec features -> audio, i would be interested to see whether and how that works. we have not tried it."
      },
      {
        "user": "shamoons",
        "created_at": "2020-04-04T21:16:13Z",
        "body": "I’m happy to give it a shot. Will any data work? Librispeech perhaps?"
      },
      {
        "user": "shamoons",
        "created_at": "2020-04-06T14:13:10Z",
        "body": "I wonder if there's a paper in this if I get it to work :-). Or even if I get it to NOT work."
      },
      {
        "user": "alexeib",
        "created_at": "2020-04-07T19:50:15Z",
        "body": "you can certainly try librispeech. many tts models learn logmel -> audio mapping (e.g. tacotron), so that would be a good starting point to look at"
      },
      {
        "user": "shamoons",
        "created_at": "2020-10-25T12:53:43Z",
        "body": "I'm trying to work on this and what I've done is create the data set from audio to features (400 samples to 512-dimensional vector). I have the LibriSpeech `train-clean-100` dataset chunked out this way. I'm then applying a few `Conv1d` layers (kernel size of 1) all the way through to 2 linear layers at the end. It's producing some results, but still not clean. I'm wondering what I'm missing to make it really fully restored?"
      },
      {
        "user": "yt605155624",
        "created_at": "2023-01-06T10:40:21Z",
        "body": "mark "
      }
    ]
  },
  {
    "number": 1455,
    "title": "BART: release XSum-tuned model",
    "created_at": "2019-12-04T10:03:28Z",
    "closed_at": "2020-03-11T14:16:43Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1455",
    "body": "Hi\r\nDo you plan to release this model?\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1455/comments",
    "author": "jmamou",
    "comments": [
      {
        "user": "JohannesTK",
        "created_at": "2020-01-30T11:08:08Z",
        "body": "+1"
      },
      {
        "user": "yuhui-zh15",
        "created_at": "2020-02-27T00:55:36Z",
        "body": "+1"
      },
      {
        "user": "ngoyal2707",
        "created_at": "2020-03-11T14:16:43Z",
        "body": "The model has been released"
      }
    ]
  },
  {
    "number": 1410,
    "title": "Right way to initialize only encoder/decoder weights from a pre-trained model",
    "created_at": "2019-11-21T04:29:50Z",
    "closed_at": "2022-04-29T16:22:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1410",
    "body": "I have trained a Transformer model, and I want to initialize a new Transformer model with ONLY the encoder or decoder weights of the old model, and keep its parameters fixed while training. I have tried the following method but it didn't work.\r\n\r\nRefers to #1196 , I have tried to add the following codes in the `upgrade_state_dict_named` in the `TransformerEncoder` class:\r\n```\r\ndef upgrade_state_dict_named(self, state_dict, name):\r\n        # my codes\r\n        if self.random_init_only_encoder:\r\n            for k in self.state_dict().keys():\r\n                here_key = 'encoder.' + k\r\n                if here_key in state_dict.keys():\r\n                    state_dict[here_key] = self.state_dict()[k]\r\n\r\n        \"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"\r\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\r\n            weights_key = '{}.embed_positions.weights'.format(name)\r\n            if weights_key in state_dict:\r\n                del state_dict[weights_key]\r\n            state_dict['{}.embed_positions._float_tensor'.format(name)] = torch.FloatTensor(1)\r\n        for i in range(len(self.layers)):\r\n            # update layer norms\r\n            self.layers[i].upgrade_state_dict_named(state_dict, \"{}.layers.{}\".format(name, i))\r\n\r\n        version_key = '{}.version'.format(name)\r\n        if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) < 2:\r\n            # earlier checkpoints did not normalize after the stack of layers\r\n            self.layer_norm = None\r\n            self.normalize = False\r\n            state_dict[version_key] = torch.Tensor([1])\r\n\r\n        return state_dict\r\n```\r\nAnd similar codes are added to the `upgrade_state_dict_named` function in the `TransformerDecoder` class.\r\nAnd to keep its parameter fixed, I added few lines at the end of the `build_model` function of `TransformerModel` class:\r\n```\r\n @classmethod\r\n    def build_model(cls, args, task):\r\n        \"\"\"Origin codes.\"\"\"\r\n        encoder = cls.build_encoder(args, src_dict, encoder_embed_tokens)\r\n        decoder = cls.build_decoder(args, tgt_dict, decoder_embed_tokens)\r\n\r\n        if args.random_init_only_encoder:\r\n            for param in decoder.parameters():\r\n                param.requires_grad = False\r\n        if args.random_init_only_decoder:\r\n            for param in encoder.parameters():\r\n                param.requires_grad = False\r\n\r\n        return TransformerModel(encoder, decoder)\r\n```\r\nwhere random_init_only_encoder/decoder indicates that initialize the encoder/decoder randomly while initialize the other component by the old model.\r\n\r\nBut the above codes didn't work. The trained model produced random translation results with 0 BLEU score. Therefore I open this issue hoping to find the right way to achieve that.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1410/comments",
    "author": "lemmonation",
    "comments": [
      {
        "user": "yukiyakiZ",
        "created_at": "2020-04-18T00:27:13Z",
        "body": "Have the same problem. Any help would be appreciated."
      },
      {
        "user": "jppgks",
        "created_at": "2020-06-02T20:48:16Z",
        "body": "I used the method from #705 and modified `trainer.py`:\r\n\r\n```python\r\n# self.get_model().load_state_dict(state[\"model\"], strict=True, args=self.args)\r\nself.get_model().encoder = load_pretrained_component_from_model(self.get_model().encoder, filename)\r\n```"
      },
      {
        "user": "fenimi",
        "created_at": "2021-06-14T15:18:53Z",
        "body": "\r\nPlease how can I use both decoder and encoder weights with a new language pair? I'm still a bit confused on how to do this?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T17:20:37Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:21:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "mahendraphd",
        "created_at": "2024-02-21T18:44:59Z",
        "body": "There is a mismatch in the size of the tensors. How to initialize encoder decoder/decoder weights with pre-trained model, just want to initialize the common weights.  How to do that?"
      }
    ]
  },
  {
    "number": 1343,
    "title": "fp32 inference is faster than fp16, pytorch >= 1.2",
    "created_at": "2019-11-04T04:52:23Z",
    "closed_at": "2019-12-24T07:00:41Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1343",
    "body": "I'd like to leave an interesting nugget.\r\n\r\nOn pytorch==1.0, fp16 inference is faster than fp32 but on pytorch>=1.2,\r\n\r\nOn german news-test 2014\r\n\r\n| Precision | Inference speed (sents/sec) | \r\n| --- | --- |\r\n| fp32 | 324.16 |\r\n| fp16 | 307.11 |\r\n\r\n*german news-test 2014 was pre-sorted according to the number of tokens in each sentence before batching. This is to minimize the number of padding tokens required.\r\n\r\nEnvironmental setup\r\n\r\nCPU: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\r\nGPU: V100\r\n\r\nCUDA==10.0 (conda)\r\ncuDNN==7.6.3 (conda)\r\n\r\npython==3.6\r\nfairseq==0.8.0 (pip)\r\npytorch==1.3 (conda)\r\n\r\n@myleott Could I check with you if these results are expected?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1343/comments",
    "author": "mingruimingrui",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-16T23:20:57Z",
        "body": "This is not expected."
      }
    ]
  },
  {
    "number": 1291,
    "title": "Why doesn't it work when I use --replace-unk?",
    "created_at": "2019-10-23T13:59:59Z",
    "closed_at": "2022-04-27T23:22:16Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1291",
    "body": "I used the seq2seq structure for the text summarization, and a lot of  `<unk>` appeared in the result, so I tried to replace then with` --replace-unk`\r\n```\r\npython ../interactive.py ../data_bin_giga/ \\\r\n--path ../checkpoints/checkpoint$num.pt \\\r\n--replace-unk  \\\r\n--print-alignment \\\r\n--beam 10\r\n```\r\n\r\nwhen I do not use` --replace-unk`, the result of one of these is:\r\n`french police arrest five anti-<unk> protesters`\r\nBut now the result is:\r\n```\r\nS-0\t▁police ▁arrested ▁five ▁anti - ▁protesters ▁thursday ▁after ▁they ▁sought ▁to ▁disrupt ▁loading ▁of ▁a ▁french tic ▁research ▁and ▁supply ▁vessel ▁, ▁a ▁spokesman ▁for ▁the ▁protesters ▁said ▁.\r\nH-0\t-0.5430362224578857\t▁french ▁police ▁arrest ▁five ▁anti - ▁. ▁protesters\r\nP-0\t-1.8554 -0.4735 -0.1768 -0.8874 -0.6611 -0.0985 -0.0840 -0.1512 -0.4994\r\nA-0\t30 30 30 30 30 30 30 30 30\r\n```\r\nYou can see that` <unk>` is simply replaced by \".\", and it's been replaced by \".\" in many cases(I think the model perfer the last token, and the last token is usually \".\")\r\nSo I remove the \".\" in source sentence, the result is:\r\n```\r\nS-3\t▁police ▁arrested ▁five ▁anti - <unk> ▁protesters ▁thursday ▁after ▁they ▁sought ▁to ▁disrupt ▁loading ▁of ▁a ▁french <unk> tic ▁research ▁and ▁supply ▁vessel ▁, ▁a ▁spokesman ▁for ▁the ▁protesters ▁said\r\nH-3\t-0.5290824770927429\t▁french ▁police ▁arrest ▁five ▁anti - <eos> ▁protesters\r\nP-3\t-1.9868 -0.2966 -0.1489 -0.8684 -0.5291 -0.1027 -0.0742 -0.1147 -0.6403\r\nA-3\t30 30 30 30 30 30 30 30 30\r\n```\r\n\r\nit use `<eos> `to replace the `<unk>`, \r\n\r\nSo what did I do wrong?\r\nI trained the model using the following parameters:\r\n```\r\npython ../train.py \\\r\n../data_bin_giga \\\r\n--arch transformer_summarization \\\r\n--share-all-embeddings \\\r\n--optimizer adam \\\r\n--adam-betas '(0.9,0.98)' \\\r\n--adam-eps 1e-09 \\\r\n--criterion label_smoothed_cross_entropy \\\r\n--label-smoothing 0.1 \\\r\n--warmup-updates 4000 \\\r\n--warmup-init-lr 1e-07 \\\r\n--lr 0.0005 \\\r\n--min-lr 1e-11 \\\r\n--lr-scheduler inverse_sqrt \\\r\n--max-tokens 4096 \\\r\n--fp16 \\\r\n--keep-last-epochs 300 \\\r\n--save-dir ../checkpoints \\\r\n> train.log 2>&1\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1291/comments",
    "author": "Dod-o",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T21:20:28Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1280,
    "title": "Does wav2vec work with different sampling rates?",
    "created_at": "2019-10-21T19:46:27Z",
    "closed_at": "2020-03-11T03:09:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1280",
    "body": "Does wav2vec work with another sampling rate other than 16Khz? \r\nIf so, does that mean the receptive field will change? In the paper, it says `The total receptive field of the context network is about 210 ms.` using 16Khz. If I use 32Khz, will the receptive field of the context network change to 105 ms? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1280/comments",
    "author": "rewreu",
    "comments": [
      {
        "user": "bradgrimm",
        "created_at": "2020-01-26T18:03:18Z",
        "body": "@rewreu Yes. But the fairseq code will resample it to 16k for you."
      },
      {
        "user": "alexeib",
        "created_at": "2020-03-11T03:09:53Z",
        "body": "you can specify a different sample rate with --sample-rate argument. audio will be resampled (via simple interpolation, dont expect great results) for you to this sample rate. you can certainly use 32k if you want but then you might want to adjust the encoder architecture to keep 100hz rate, or else see if a different rate works well for you"
      }
    ]
  },
  {
    "number": 1276,
    "title": "Support for character-based models",
    "created_at": "2019-10-20T16:18:50Z",
    "closed_at": "2019-12-16T22:22:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1276",
    "body": "Hi team,\r\n\r\nMore of a question rather than an issue. I saw the discussion from #438 that there was going to be support for custom tokenizer/dictionary, so wanted to check if this is a thing already, and how to go about achieving this? Is there a flag that we can specify from `fairseq-preprocess` perhaps? Thanks.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1276/comments",
    "author": "ethen8181",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2019-12-16T22:22:32Z",
        "body": "We still mostly expect tokenization to be handled outside of fairseq, but character tokenization should be fine. Just split your input into characters (with spaces between them) and use the standard fairseq-preprocess process."
      }
    ]
  },
  {
    "number": 1253,
    "title": "runtime error, speech recognition can't use multi GPU to train ",
    "created_at": "2019-10-16T08:59:44Z",
    "closed_at": "2019-12-16T22:36:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1253",
    "body": "Hello,\r\n\r\nWhen I train speech recognition task on multi GPU, basically using Librispeech DB\r\n\r\nI can't train over the 1 epoch, 834 updates.\r\n\r\nThe training looks like feezing.\r\n\r\nThree GPU run 100% but the power is low watt about 45W/250W\r\n\r\nI set the `--num-works` to \"0\".\r\n\r\nHow can i fix it?\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1253/comments",
    "author": "LeeYongHyeok",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2019-10-16T19:14:39Z",
        "body": "@okhonko "
      },
      {
        "user": "myleott",
        "created_at": "2019-12-16T22:36:56Z",
        "body": "Please reopen with more details about what commands you ran, environment, etc."
      }
    ]
  },
  {
    "number": 1086,
    "title": "How to preprocessing BOOK + WIKI",
    "created_at": "2019-08-28T20:15:21Z",
    "closed_at": "2022-04-27T23:22:02Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1086",
    "body": "As the name implied, can you provide the exact preprocessing you clean the BOOK and WIKI dataset?\r\n\r\nBy the way, how do you concat BOOK and WIKI?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1086/comments",
    "author": "gaopengcuhk",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T21:20:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:32Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 898,
    "title": "Reproduce Billion Word benchmark for paper by Baevski and Auli, 2018.",
    "created_at": "2019-07-22T18:04:13Z",
    "closed_at": "2022-04-27T23:22:07Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/898",
    "body": "There is only example on how to run for WikiText-103, but not Billion Word, for the adaptive input paper by Baevski and Auli, 2018.\r\n\r\nThere is a difference between the two, as Billion Word is of short sentences. Can anyone help out?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/898/comments",
    "author": "yilegu",
    "comments": [
      {
        "user": "huihuifan",
        "created_at": "2019-07-26T09:06:00Z",
        "body": "@alexeib "
      },
      {
        "user": "jeremyasapp",
        "created_at": "2019-08-05T20:29:34Z",
        "body": "Would be interested in this too!\r\n"
      },
      {
        "user": "apeterswu",
        "created_at": "2019-08-26T02:03:17Z",
        "body": "Also interested in the correct setting to reproduce the results."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T21:20:37Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:37Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "DavidHerel",
        "created_at": "2023-09-16T12:45:55Z",
        "body": "This issue is still not resolved. There is no documentation on how to reproduce results on Billion Word benchamark for paper Baevski and Ali, 2018."
      }
    ]
  },
  {
    "number": 842,
    "title": "Question: what is the batch size for the wav2seq pretrained model.",
    "created_at": "2019-06-29T14:08:11Z",
    "closed_at": "2022-04-27T23:22:09Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/842",
    "body": "In section 3.4 in the wav2vec paper, it says the model is pretrained for 400k steps with 8 gpus. I am curious what is the batch size for each gpu? I couldn't find this information in the wav2vec example here, either.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/842/comments",
    "author": "wanglouis49",
    "comments": [
      {
        "user": "wanglouis49",
        "created_at": "2019-07-01T09:08:35Z",
        "body": "Also, is the pretrained module freezed during finetuning or its parameters are allowed to change?"
      },
      {
        "user": "huihuifan",
        "created_at": "2019-07-16T09:30:31Z",
        "body": "@alexeib ?"
      },
      {
        "user": "wanglouis49",
        "created_at": "2019-07-22T11:48:06Z",
        "body": "Any follow-up?"
      },
      {
        "user": "xingchensong",
        "created_at": "2019-07-29T02:55:32Z",
        "body": "also confused about it. "
      },
      {
        "user": "wanglouis49",
        "created_at": "2019-07-30T13:37:48Z",
        "body": "The default batch size per gpu is 8. I ran it and used pdb to check."
      },
      {
        "user": "shamanez",
        "created_at": "2019-07-31T05:26:18Z",
        "body": "> In section 3.4 in the wav2vec paper, it says the model is pretrained for 400k steps with 8 gpus. I am curious what is the batch size for each gpu? I couldn't find this information in the wav2vec example here, either.\r\n\r\nHi, what do you mean by 400k steps? Are that epochs?"
      },
      {
        "user": "wanglouis49",
        "created_at": "2019-07-31T17:08:04Z",
        "body": "> > In section 3.4 in the wav2vec paper, it says the model is pretrained for 400k steps with 8 gpus. I am curious what is the batch size for each gpu? I couldn't find this information in the wav2vec example here, either.\r\n> \r\n> Hi, what do you mean by 400k steps? Are that epochs?\r\n\r\nNo, I think it means 400k steps of mini-batches fed into the model during training."
      },
      {
        "user": "rajeevbaalwan",
        "created_at": "2019-10-18T09:08:48Z",
        "body": "> In section 3.4 in the wav2vec paper, it says the model is pretrained for 400k steps with 8 gpus\r\n\r\nhere how 400k steps is considered. if we train on 8 GPU with batch size 8 then progress bar during training show num_update parameter updated by 1 value in 1 iteration but effectively it should be increased by 8  because in 1 iteration the update is done parallely on 8 batches. if this is the scenario then should we train for 400K/8 i.e 50K update steps or not ???? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T21:20:35Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 632,
    "title": "Unable to replicate wmt14.en-de.newstest2014 data set ",
    "created_at": "2019-04-10T22:12:29Z",
    "closed_at": "2022-04-28T00:21:58Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/632",
    "body": "I'm able to close the gap in bleu scores (referenced in #592) between the Gehring '17 model tested on the provided test set and one produced with preprocess.py down to 25.71, but still have six sentences tokenized differently. \r\n\r\nThe test set generated with preprocess.py requires two additional steps beyond those listed in the examples/translation README to get a bleu score of 25.71: (1) rolling back the moses-decoder repository to before `9fc964d`, which changed tokenization of periods at ends of sentences, and (2) changing the tokenization of hyphens by running `perl -ple 's{(\\S)-(\\S)}{##AT##-##AT##}g'` after tokenization with moses-decoder. \r\n\r\nHowever, six sentences are still tokenized differently:\r\n```\r\n1095c1095\r\n<  S-It gives them the time to do some tests and figure it out and a period of time before they write &apos; male &apos; or &apos; female . &apos;\r\n---\r\n>  S-It gives them the time to do some tests and figure it out and a period of time before they write &apos; male &apos; or &apos; fem@@ al@@ e.@@ &apos;\r\n2613c2613\r\n<  S-We &apos;ve got to remember who we are . &apos;\r\n---\r\n>  S-We &apos;ve got to remember who we are@@ .@@ &apos;\r\n2703c2703\r\n<  S-Without the possibility of domestic currency de@@ valuation , southern Europe finds itself with a built ##AT##-##AT## in productivity disadvantage vis ##AT##-##AT## à ##AT##-##AT## vis Germany .\r\n---\r\n>  S-Without the possibility of domestic currency de@@ valuation , southern Europe finds itself with a built ##AT##-##AT## in productivity disadvantage vis@@ -@@ à-vis Germany .\r\n4118c4118\r\n<  T-Es wird hier das Prinzip des &quot; Prä ##AT##-##AT## Verbre@@ chens &quot; entwickelt , &quot; Pre ##AT##-##AT## Cri@@ me &quot; , also das , was inzwischen mehr oder weniger offizielle Dok@@ tr@@ in des Wei@@ ßen Hauses ist .\r\n---\r\n>  T-Es wird hier das Prinzip des &quot; Prä@@ -@@ Verbre@@ chens &quot; entwickelt , &quot; Pre ##AT##-##AT## Cri@@ me &quot; , also das , was inzwischen mehr oder weniger offizielle Dok@@ tr@@ in des Wei@@ ßen Hauses ist .\r\n5045c5045\r\n<  T-Snow@@ den bereit , mit Deutschland in Bezug auf US ##AT##-##AT## Überwachung zu „ kooperieren “\r\n---\r\n>  T-Snow@@ den bereit , mit Deutschland in Bezug auf US@@ -@@ Überwachung zu „ kooperieren “\r\n5105c5105\r\n<  T-Tausende von Auto@@ fahr@@ ern haben die Fahr@@ ten@@ schrei@@ ber , von denen einige mit GPS ##AT##-##AT## Überwachung ausgestattet sind , bereits getestet .\r\n---\r\n>  T-Tausende von Auto@@ fahr@@ ern haben die Fahr@@ ten@@ schrei@@ ber , von denen einige mit G@@ PS@@ -@@ Überwachung ausgestattet sind , bereits getestet .\r\n```\r\n\r\nCan you provide clarification on how hyphens are tokenized in your process and which commit of the moses-decoder you used?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/632/comments",
    "author": "tholiao",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:28Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 616,
    "title": "Is there a way to modify data input pipeline during training?",
    "created_at": "2019-04-02T03:20:15Z",
    "closed_at": "2022-04-18T05:21:18Z",
    "labels": [
      "question",
      "documentation",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/616",
    "body": "I want to apply curriculum learning into fairseq. One necessary step is to change the data input pipeline to select samples following a specific schedule. I only found \"--curriculum\" but it is not the method I need. Could you give me some advice on how to modify the data input pipeline in fairseq?",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/616/comments",
    "author": "gxzks",
    "comments": [
      {
        "user": "mohamedafify2001",
        "created_at": "2020-07-05T09:21:29Z",
        "body": "Hello, same question here. Is there an existing implementation or we have to rewrite it. Thanks "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:48Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 5121,
    "title": "Is multi-speaker diarization part of MMS toolkit? If not, is it planned?",
    "created_at": "2023-05-22T23:34:09Z",
    "closed_at": "2023-05-23T16:00:40Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/5121",
    "body": "## 🚀 Feature Request\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/5121/comments",
    "author": "bazooka720",
    "comments": [
      {
        "user": "vineelpratap",
        "created_at": "2023-05-23T16:00:40Z",
        "body": "Hi, we do not have speaker diarization in our plans at the moment. "
      }
    ]
  },
  {
    "number": 4684,
    "title": "Unshuffles test set during generation ",
    "created_at": "2022-08-31T21:41:59Z",
    "closed_at": "2022-09-06T18:00:19Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4684",
    "body": "Hi, \r\n\r\nHow do we keep the test set sentence order during generation? Is there a flag we can pass to the generation to keep the test set sequence untouched? This is very important for my work. I would like to request the new feature. \r\n\r\nThanks! ",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4684/comments",
    "author": "i55code",
    "comments": [
      {
        "user": "cordercorder",
        "created_at": "2022-09-03T02:35:42Z",
        "body": "For test set, fairseq will automatically sort the sentences by length and there is no flag to keep the sentence order. Despite that, you can extract the input and output sentences by regular expression from the results produced by `fairseq-generate`  and reorder the sentences by their sample id (already in the results) to keep the order."
      },
      {
        "user": "i55code",
        "created_at": "2022-09-06T18:00:19Z",
        "body": "Hi @cordercorder , thank you so much! Keeping the order of sentences is important for my work, yes, sample id would work. Thanks!"
      },
      {
        "user": "BrightXiaoHan",
        "created_at": "2022-09-12T02:16:39Z",
        "body": "These commands may help you.\r\n```\r\ngrep ^S generate-test.txt | LC_ALL=C sort -V | cut -f2- > src.txt\r\ngrep ^T generate-test.txt | LC_ALL=C sort -V | cut -f2- > ref.txt\r\ngrep ^H generate-test.txt | LC_ALL=C sort -V | cut -f3- > hyp.txt\r\n``` "
      }
    ]
  },
  {
    "number": 4136,
    "title": "Supporter bsz>1 for VideoClip in mmfusion.py",
    "created_at": "2022-01-13T18:55:25Z",
    "closed_at": "2022-05-02T10:22:12Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4136",
    "body": "## 🚀 Feature Request\r\nIt would be great to add the capability to use a batch size greater than one for VideoClip for faster processing when evaluating. Currently running using MMPT.\r\n\r\nThe current error when a batch size is greater than 1 is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mschiappa/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-52-1d601eb6bb6a>\", line 2, in <module>\r\n    output = model(video, caps, cmasks, return_score=False)\r\n  File \"/home/mschiappa/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"fairseq/examples/MMPT/mmpt/models/mmfusion.py\", line 69, in forward\r\n    assert bsz == 1, \"only bsz=1 is supported now.\"\r\nAssertionError: only bsz=1 is supported now.\r\n```\r\nThe setup was:\r\n```\r\n# Video\r\nvideo = torch.rand(2, 4, 32, 224, 224, 3)\r\n\r\n# Text\r\nall_caps, all_cmasks = list(), list()\r\nfor text in ['some text', 'some other text']:\r\n    test_caps, test_cmasks = aligner._build_text_seq(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\r\n    all_caps.append(test_caps)\r\n    all_cmasks.append(test_cmasks)\r\n\r\ncaps = torch.stack(all_caps)\r\ncmasks = torch.stack(all_cmasks)\r\n\r\n# Pass to model\r\nwith torch.no_grad():\r\n    output = model(video, caps, cmasks, return_score=False)\r\n```\r\n\r\nNOT IMPORTANT:\r\nMaybe another thing to just mention, is transformers is now at version 4.15 and it is possible to fix by updating the import `from transformers.modeling_bert import .` to `from transformers.models.bert.modeling_bert import .` or adding this as a check. \r\n \r\nand in\r\n\r\n`models/fairseq/examples/MMPT/mmpt/models/transformermodel.py` on line 732:\r\n```python\r\n#BEFORE for transformers==3.4\r\nlayer_outputs = layer_module(\r\n                    hidden_states,\r\n                    layer_attention_mask,\r\n                    layer_head_mask,\r\n                    encoder_hidden_states,\r\n                    encoder_attention_mask,\r\n                    output_attentions,\r\n                )\r\n\r\n# AFTER for transformers==4.15\r\nlayer_outputs = layer_module(\r\n    hidden_states,\r\n    layer_attention_mask,\r\n    layer_head_mask,\r\n    encoder_hidden_states,\r\n    encoder_attention_mask,\r\n    output_attentions=output_attentions,\r\n    past_key_value=None\r\n)\r\n```\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4136/comments",
    "author": "Maddy12",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T13:21:26Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T10:21:42Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4084,
    "title": "how to enable every sample in each GPU is from the same language during multilingual NMT training with multi-gpus?",
    "created_at": "2021-12-19T07:22:02Z",
    "closed_at": "2022-05-01T18:22:08Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4084",
    "body": "## 🚀 Feature Request\r\n<!-- A clear and concise description of the feature proposal -->\r\nDuring multilingual multi-gpu training, every batch is randomly sampled and may be not from the same language. How to enable all sentences in each batch of each GPU has the same language ID? \r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nThe feature is related to contrastive learning which requires a large batch size. The large batch is enabled by all-gather operation after calculate the text feature, which is hopefully from the sample language. \r\n\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nDuring a certain train step, all sentences in the batch of each GPU have the same source/target language ID during multilingual NMT training. \r\n\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nNA\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nNA\r\n\r\nThanks for your time and help.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4084/comments",
    "author": "ghchen18",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T15:20:29Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T18:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 4022,
    "title": "WMT21 training code",
    "created_at": "2021-11-18T22:54:38Z",
    "closed_at": "2022-03-02T12:39:34Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/4022",
    "body": "## 🚀 Feature Request\n<!-- A clear and concise description of the feature proposal -->\n\n### Motivation\n\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\n\nRecent release of WMT 21 En-X is a 4.7B multilingual encoder-decoder (seq-to-seq) model.\n\n### Pitch\n\n<!-- A clear and concise description of what you want to happen. -->\nTo provide (simplified) training code for the WMT21 multilingual model.\n\n### Alternatives\n\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\n\n### Additional context\n\n<!-- Add any other context or screenshots about the feature request here. -->\nCurrently only and evaluation script is provided.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/4022/comments",
    "author": "loretoparisi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T11:33:12Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "loretoparisi",
        "created_at": "2022-03-02T12:39:34Z",
        "body": "closing because it can be done using HuggingFace Trainer."
      }
    ]
  },
  {
    "number": 3846,
    "title": "Could you please provide a TINY or SMALL version of xlmr ?",
    "created_at": "2021-09-05T09:22:48Z",
    "closed_at": "2022-04-17T22:21:05Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3846",
    "body": "## 🚀 Feature Request\r\na TINY or SMALL version of xlmr \r\n\r\n### Motivation\r\n\r\nCurrent `xlmr` models, `base` and `large` are too big to train. I want to use `xlmr` for image caption task with multi languages, and the captions are not very complicated. So I think a `tiny` or `small` version `xlmr` is good enough.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3846/comments",
    "author": "thesby",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-03-03T02:32:19Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T22:20:35Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3794,
    "title": "Efficient implementation of ngram blocking (compatible with Torchscript)",
    "created_at": "2021-08-18T08:21:57Z",
    "closed_at": "2022-05-01T17:22:00Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3794",
    "body": "## 🚀 Feature Request\r\nCurrent implementation of n-gram blocking is slow. There's a cuda kernel alternative that can run much faster but it's not compatible with Torchscript (or there's a significant effort required to make it compatible).\r\n\r\n### Motivation\r\n\r\nIf we need to export a model that features n-gram blocking, we observe a huge slowdown because of it (2x slower in Pytorch, and 5x slower in Torchscript). Also, the Torchscript version is much slower compared to the PyTorch one, which is counterintuitive. Current implementation relies on data structures like Lists or Dictionaries, and it process one sequence at a time. \r\n\r\n### Pitch\r\n\r\nI'll propose an alternative implementation free from Lists/Dictionaries that is able to process all sequences in parallel. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3794/comments",
    "author": "madelagua",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-16T16:20:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T17:21:31Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3738,
    "title": "Make src_lengths recalculation in TransformerEncoderBase.forward_scriptable optional",
    "created_at": "2021-07-25T10:42:46Z",
    "closed_at": "2022-05-01T02:22:19Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3738",
    "body": "## 🚀 Feature Request\r\nMake src_lengths recalculation in TransformerEncoderBase.forward_scriptable optional, tested and suggested change:\r\nfrom \r\n`src_lengths = src_tokens.ne(self.padding_idx).sum(dim=1, dtype=torch.int32).reshape(-1, 1).contiguous()`\r\nto\r\n` if src_lengths is None:\r\n            src_lengths = src_tokens.ne(self.padding_idx).sum(dim=1, dtype=torch.int32).reshape(-1, 1).contiguous()`\r\n\r\n### Motivation\r\nTo make the current TransformerEncoderBase compatible with onnx, so that the encoder can be exported to onnx for fast cpu inference\r\n\r\n### Pitch\r\n\r\nThe original calculation is to handle the case when src_lengths is not provided to TransformerEncoderBase.forward_scriptable. In other words, it seems to me that if src_lengths is provided to TransformerEncoderBase.forward_scriptable, then this recalculation is redundant. This alone wouldn't be a huge deal, but when exporting the model to onnx for fast cpu inference, the operator used in the src_lengths recalculation--the sum operator--is currently not supported in any onnx opset versions, meaning that it is currently impossible to export the TransformerEncoderBase without removing the given line.\r\n\r\n### Alternatives\r\nAdd sum opset to onnx, but this won't come too quickly as not as easily as making the src_lengths recalculation optional.\r\n\r\n### Additional context\r\nNone\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3738/comments",
    "author": "johnxin66",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T14:20:53Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-01T02:21:49Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3649,
    "title": "Is `build_self_attention` and `build_fc` really necessary?",
    "created_at": "2021-06-25T17:06:43Z",
    "closed_at": "2022-04-30T11:21:57Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3649",
    "body": "## 🚀 Feature Request\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nremove `build_self_attention` and `build_fc` in `transformer_sentence_encoder_layer`\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n`build_self_attention()` and `build_fc()` are both typical one-line function, it is a bit hard for me to understand why they are here, other than they could be overload in inheritance. \r\n\r\nPlease find below the relevant code.\r\n\r\n```python\r\n        self.self_attn = self.build_self_attention(\r\n            self.embedding_dim,\r\n            num_attention_heads,\r\n            dropout=attention_dropout,\r\n            self_attention=True,\r\n            q_noise=q_noise,\r\n            qn_block_size=qn_block_size,\r\n        )\r\n        self.fc1 = self.build_fc1(\r\n            self.embedding_dim,\r\n            ffn_embedding_dim,\r\n            q_noise=q_noise,\r\n            qn_block_size=qn_block_size,\r\n        )\r\n        self.fc2 = self.build_fc2(\r\n            ffn_embedding_dim,\r\n            self.embedding_dim,\r\n            q_noise=q_noise,\r\n            qn_block_size=qn_block_size,\r\n        )\r\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\r\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\r\n\r\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\r\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\r\n\r\n    def build_self_attention(\r\n        self,\r\n        embed_dim,\r\n        num_attention_heads,\r\n        dropout,\r\n        self_attention,\r\n        q_noise,\r\n        qn_block_size,\r\n    ):\r\n        return MultiheadAttention(\r\n            embed_dim,\r\n            num_attention_heads,\r\n            dropout=dropout,\r\n            self_attention=True,\r\n            q_noise=q_noise,\r\n            qn_block_size=qn_block_size,\r\n        )\r\n```\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nremove `build_self_attention` and `build_fc` in `transformer_sentence_encoder_layer`\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nIf they definitely should be here, consider remove the unused arg `self_attention` in `build_self_attention` and let `build_fc2 = build_fc1` by default as I couldn't find any different between these two. \r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3649/comments",
    "author": "ZhiyuanChen",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-30T11:21:27Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3642,
    "title": "TPU support for wav2vec ",
    "created_at": "2021-06-24T23:30:45Z",
    "closed_at": "2022-04-29T16:22:33Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3642",
    "body": "## 🚀 Feature Request\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAfter commit #3328, TPU support for wav2vec2 was added. Training for  wav2vec on TPU is still not supported maybe because args/params check code dedicated to wavec2 training.\r\n\r\n### Motivation\r\n\r\nWav2vec can be used in scenarios where quantization is not needed. When re-training wav2vec on larger datasets such as librivox, TPU support will be very helpful. \r\n\r\n### Pitch\r\n\r\nProvide code and necessary docs of  wav2vec pre-training on TPUs like wav2vec2. \r\n\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3642/comments",
    "author": "tao-sun",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T16:20:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-29T16:22:04Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3531,
    "title": "Preserve \"patience\" tracking values in last checkpoint",
    "created_at": "2021-05-05T10:38:30Z",
    "closed_at": "2022-04-18T02:21:00Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3531",
    "body": "## 🚀 Feature Request\r\nWhen training a Machine Translation model for example, and using a patience value of 10. The counting starts when the training command is invoked, not from the beginning of training. So if we reached epoch 30, and fairseq about to stop training, if we force stop, and restart training, fairseq will wait not for two epochs, but for 10 more epochs.\r\n\r\n### Motivation\r\n\r\nAdding this information will help a lot when training using AWS spot instances. As they get interrupted, which forces the training to stop and resume after the instance is up again. With restarting the training, we have to wait for these 10 epochs to actually pass, which may never happen when training with a very large corpus that takes days to train on the data for 10 epochs.\r\n\r\n### Pitch\r\n\r\nIt would be great if we can resume training from exactly the point where we left it. \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3531/comments",
    "author": "AbdallahNasir",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-08-21T04:48:01Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:32Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 3519,
    "title": "Emformer architecture option",
    "created_at": "2021-04-28T23:19:46Z",
    "closed_at": "2021-04-29T14:53:40Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3519",
    "body": "## 🚀 Feature Request\r\n<!-- An Emformer implementation from the recent publication-->\r\n\r\n\r\n\r\n\r\n### Alternatives\r\n\r\n<!-- Agmented Memory Transformer would be great as well, preferablly the streaming  efficient version! -->\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3519/comments",
    "author": "taylerpauls",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-04-29T14:53:40Z",
        "body": "Please fill out the issue template"
      }
    ]
  },
  {
    "number": 3252,
    "title": "When will you release mBart-base?",
    "created_at": "2021-02-18T07:06:10Z",
    "closed_at": "2021-02-22T16:25:53Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3252",
    "body": "## 🚀 Feature Request\r\nDear author, I only have two P100 with 16GB graphic memory, so it is nearly impossible for me to fine-tune mBart. I tried the batch size of 1 with max sequence length as 64, but still got OOM. So I wonder will there be a mBart-base model, or you do not have the plan to train a smaller model?\r\n\r\n### Motivation\r\n\r\nI am very poor, and cannot afford bigger GPU. But I still want to do research on mBart. Please help me to seek for my dream.\r\n\r\n### Pitch\r\n\r\nI wish there could be a mbart model with smaller size, enabling me to fine-tune it with only two P100(16GB memory).\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3252/comments",
    "author": "HuihuiChyan",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2021-02-22T16:25:53Z",
        "body": "As far as I know, there aren't any plans to release a smaller model.  You could try training a smaller model from scratch following the methodology in the paper."
      }
    ]
  },
  {
    "number": 3019,
    "title": "MBART fairseq-train error",
    "created_at": "2020-12-11T04:54:43Z",
    "closed_at": "2022-04-17T23:21:24Z",
    "labels": [
      "help wanted",
      "documentation",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/3019",
    "body": "## 🐛 Bug\r\n\r\nCannot load `mbart.cc25.v2` model for fine-tuning\r\n\r\n### To Reproduce\r\nWhen I run the command in the README, I get a warning message that says the model cannot be found:\r\n```\r\n$ PRETRAIN=mbart.cc25.v2 # fix if you moved the downloaded checkpoint\r\n$ langs=ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN\r\n\r\n$ fairseq-train results/v0 \\\r\n  --encoder-normalize-before --decoder-normalize-before \\\r\n  --arch mbart_large --layernorm-embedding \\\r\n  --task translation_from_pretrained_bart \\\r\n  --source-lang en_XX --target-lang ro_RO \\\r\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\r\n  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\r\n  --lr-scheduler polynomial_decay --lr 3e-05 --warmup-updates 2500 --total-num-update 40000 \\\r\n  --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\\r\n  --max-tokens 512 --update-freq 2 \\\r\n  --save-interval 1 --save-interval-updates 5000 --keep-interval-updates 10 --no-epoch-checkpoints \\\r\n  --seed 222 --log-format simple --log-interval 2 \\\r\n  --restore-file $PRETRAIN \\\r\n  --reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler \\\r\n  --langs $langs \\\r\n  --ddp-backend no_c10d\r\n\r\n...\r\n2020-12-11 04:45:52 | INFO | fairseq.trainer | no existing checkpoint found models/mbart.cc25\r\n...\r\n```\r\n\r\nIf I change the code to `PRETRAIN=mbart.cc25.v2/model.pt`, then it finds the model but gives an error:\r\n```\r\n...\r\nRuntimeError: [enforce fail at inline_container.cc:143] . PytorchStreamReader failed reading file data/94565435476624: file read failed\r\n```\r\n\r\n### Expected behavior\r\n\r\nExpect the model to load\r\n\r\n### Environment\r\n\r\n - fairseq Version : master\r\n - PyTorch Version : 1.5.0\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed fairseq (`pip`, source): source\r\n - Build command you used (if compiling from source): pip install --editable ./\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10..2\r\n - GPU models and configuration: V100 on AWS\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/3019/comments",
    "author": "bkj",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-12-17T19:16:00Z",
        "body": "Ah, yes that looks like a bug in the docs, it should be `PRETRAIN=mbart.cc25.v2/model.pt`\r\n\r\nAs for the other error, I've never seen that before... can you try redownloading the model archive in case it got corrupted somehow?"
      },
      {
        "user": "zhengxxn",
        "created_at": "2021-02-07T06:02:34Z",
        "body": "@bkj I got the same load error, have you resolved it?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-28T12:36:44Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "PursuitN",
        "created_at": "2021-10-18T08:58:01Z",
        "body": "With PyTorch 1.5.0 I got the same error, and solved the problem using PyTorch1.6."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-03-02T20:32:26Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T23:20:55Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2994,
    "title": "Wav2vec 2.0 multilingual",
    "created_at": "2020-12-05T08:25:10Z",
    "closed_at": "2022-04-19T16:21:42Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2994",
    "body": "## 🚀 Feature Request\r\nPreTraining/Fine-tuning instruction for Wav2vec 2.0 Multilingual\r\n\r\n### Motivation\r\nThanks for amazing work\r\n\r\nI saw that you just release the Multilingual version of Wav2vec 2.0, so I looking for instruction on how I can perform the pretraining/fine-tunning/inference on Multilingual model. \r\n\r\n\r\nI think it would be similar to  monolingual version. But I cound't find the multilingual config file in examples/wav2vec/config/pretraining",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2994/comments",
    "author": "mailong25",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T05:04:47Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:21:09Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2938,
    "title": "many torchscript errors in adaptive_softmax.py ",
    "created_at": "2020-11-23T08:47:34Z",
    "closed_at": "2022-04-19T16:21:21Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2938",
    "body": "#### problem\r\nMany errors occur when i export the gpt lm to torchscript:\r\n\r\n aten::append.Tensor(Tensor[](a!) self, Tensor(c -> *) el) -> (Tensor[](a!)):\r\n  Expected a value of type 'Tensor' for argument 'el' but instead found type 'int'.\r\n\r\n  aten::append.int(int[](a!) self, int el) -> (int[](a!)):\r\n  Expected a value of type 'List[int]' for argument 'self' but instead found type 'List[Tensor]'.\r\n  Empty lists default to List[Tensor]. Add a variable annotation to the assignment to create an empty list of another type (torch.jit.annotate(List[T, []]) where T is the type of elements in the list for Python 2)\r\n\r\n  aten::append.float(float[](a!) self, float el) -> (float[](a!)):\r\n  Expected a value of type 'List[float]' for argument 'self' but instead found type 'List[Tensor]'.\r\n  Empty lists default to List[Tensor]. Add a variable annotation to the assignment to create an empty list of another type (torch.jit.annotate(List[T, []]) where T is the type of elements in the list for Python 2)\r\n\r\n  aten::append.bool(bool[](a!) self, bool el) -> (bool[](a!)):\r\n  Expected a value of type 'List[bool]' for argument 'self' but instead found type 'List[Tensor]'.\r\n  Empty lists default to List[Tensor]. Add a variable annotation to the assignment to create an empty list of another type (torch.jit.annotate(List[T, []]) where T is the type of elements in the list for Python 2)\r\n\r\n  aten::append.t(t[](a!) self, t(c -> *) el) -> (t[](a!)):\r\n  Could not match type int to t in argument 'el': Type variable 't' previously matched to type Tensor is matched to type int.\r\n\r\n  aten::append(str[](a!) self, str? el) -> (str[](a!)):\r\n  Expected a value of type 'List[str]' for argument 'self' but instead found type 'List[Tensor]'.\r\n  Empty lists default to List[Tensor]. Add a variable annotation to the assignment to create an empty list of another type (torch.jit.annotate(List[T, []]) where T is the type of elements in the list for Python 2)\r\n\r\nThe original call is:\r\n  File \"/usr/local/lib/python3.6/dist-packages/fairseq/modules/adaptive_softmax.py\", line 152\r\ntarget_idxs.append(None)\r\n      #~~~~~~~~~~~~~~~~~~ <--- HERE\r\nnew_target.append(None)\r\n\r\nThere are many None that cannot be supported by torchscript in  adaptive_softmax.py .\r\n\r\n#### What have you tried?\r\ni try modify target_idxs.append(None) to target_idxs.append(-1), but errors still occur.\r\n\r\n#### What's your environment?\r\n\r\n - fairseq Version (e.g., 1.0 or master):  0.9\r\n - PyTorch Version (e.g., 1.0)   1.4\r\n - OS (e.g., Linux):  centos7\r\n - How you installed fairseq (`pip`, source): pip install .\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: cu100\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n@myleott",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2938/comments",
    "author": "ismymajia",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-11-23T15:47:42Z",
        "body": "Yep, looks like this module isn't compatible with TorchScript. We don't have any immediate plans to add support, but would welcome a PR if you get a chance to make it work! 😄 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T08:04:38Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T16:20:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2843,
    "title": "More integration tests for hydra are needed",
    "created_at": "2020-11-04T14:11:13Z",
    "closed_at": "2022-04-19T12:21:27Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2843",
    "body": "## 🚀 Feature Request\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\nThe migration to hydra is nice, but has caused a huge number of bugs and regressions. Coupled with the lack of versioning, this makes the surface area for deep bugs quite large.\r\n\r\n### Pitch\r\n\r\nThere need to be integration and regression tests in place to ensure that user-facing behavior doesn't change between commits. This will become more important as fairseq migrates to semantic versioning for releases - if breaking changes happen within a major version, this will make it impossible to use the software in a production environment.\r\n\r\n### Alternatives\r\n\r\nStatus quo\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2843/comments",
    "author": "erip",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-11-04T19:55:42Z",
        "body": "i think thats a great idea. we made sure that all the tests we have (both public and also a large amount of internal integration tests) pass before we push things, but unfortunately some issues remain. if there are certain things that you find break most often plz suggest them, or (if possible) contribute a pr!"
      },
      {
        "user": "erip",
        "created_at": "2020-11-04T20:10:01Z",
        "body": "I try my best to submit PRs when I can. Windows tends to break often which is why I've submitted #2726 which is currently awaiting review and I've submitted other PRs in the past which have gone without review or aged off - I'm not sure what the possibility of opening some amount of ownership to the community would look like, but that might make it possible for more engagement from open-source contributors! 😄 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:57Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2797,
    "title": "What does `--score-reference` entail?",
    "created_at": "2020-10-26T10:42:25Z",
    "closed_at": "2022-04-19T12:21:16Z",
    "labels": [
      "help wanted",
      "documentation",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2797",
    "body": "## 📚 Documentation\r\n\r\nWhat does `--score-reference` do exactly? Report the logprobs of the target (instead of highest/predicted) tokens? Then averages them?\r\n\r\nJust needs a line somewhere in the docs.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2797/comments",
    "author": "munael",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T11:04:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T12:20:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2744,
    "title": "Wav2vec2.0 libri_labels.py not working correctly on windows",
    "created_at": "2020-10-16T20:28:36Z",
    "closed_at": "2020-10-18T17:47:37Z",
    "labels": [
      "bug",
      "help wanted",
      "windows"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2744",
    "body": "## 🐛 Bug\r\n\r\nWhen running the script fairseq/examples/wav2vec/libri_labels.py  to generate librispeech labels on windows , I get the error \r\n```\r\nTraceback (most recent call last):\r\n  File \"libri_labels.py\", line 56, in <module>\r\n    main()\r\n  File \"libri_labels.py\", line 37, in main\r\n    trans_path = f\"{parts[-2]}-{parts[-1]}.trans.txt\"\r\nIndexError: list index out of range\r\n\r\n```\r\n\r\n### To Reproduce\r\n1- python wav2vec_manifest.py  LibriSpeech\\train --dest LibriSpeech\\train --ext flac\r\n2- python libri_labels.py LibriSpeech\\train\\train.tsv --output-dir LibriSpeech\\train --output-name train\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\ngenerate train.ltr and train.wrd in the output-dir LibriSpeech\\train\r\n\r\n### Environment\r\n\r\n - fairseq Version (master):\r\n - OS (windows):\r\n\r\n\r\n### Additional context\r\n\r\nThe problem is because the script relies on the linux path separator \"/\" \r\nI have created a PR to fix this issue\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2744/comments",
    "author": "phantomcoder1996",
    "comments": [
      {
        "user": "wahyubram82",
        "created_at": "2020-10-18T12:32:58Z",
        "body": "it's not bug...\r\n\r\nwindows and linux, recognize path with mark '/'\r\n\r\nin windows there is 3 form that can use to recognize path (in python language):\r\n`D:/Speech/Dataset/Libri` or `D:\\\\Speech\\\\Dataset\\\\Libri` or `r'D:\\Speech\\Dataset\\Libri'`\r\n\r\nin linux, only one form:\r\n`/home/username/Speech/Dataset/Libri`\r\n\r\nthey are similar, and the path can recognize in python, so the command to run python in windows is the false command.\r\nit should be:\r\n`python wav2vec_manifest.py LibriSpeech/train --dest LibriSpeech/train --ext flac`\r\n`python libri_labels.py LibriSpeech/train/train.tsv --output-dir LibriSpeech/train --output-name train`\r\nthe correction may be add the clue to README.md how to use it in Windows. but i'm doubt it will be, may be the developer make it  to anybody who understood python language."
      }
    ]
  },
  {
    "number": 2614,
    "title": "Custom Callbacks while training",
    "created_at": "2020-09-14T15:36:36Z",
    "closed_at": "2022-04-19T06:21:30Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2614",
    "body": "## 🚀 Feature Request\r\n<!-- A clear and concise description of the feature proposal -->\r\nAs the title suggests, the request is very simple - to give the user the ability to customize the callbacks so that the person can have some additional tasks automated and done on time.\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nMy problem was when I wanted to have the created `checkpoint` files from the training to be uploaded to my personal GCS bucket for further use. It creates a problem since I have to take responsibility of ending my training early, running the code to upload all checkpoints, and then resuming it. \r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nA simple flag (if possible) to allow the user to mention their customized callbacks which would most probably be formed as a separate function would do wonders. This makes a lot of things possible - devs can make callbacks to report training as E-mails or any other medium, and it can also run a frequent backup of the trained `.pkl` files to ensure they aren't deleted, and also logs (apart from the one provided by tensorboard flag). \r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered if any. -->\r\nNot any I know, except modifying the `FairSeq` files which I really don't want to do.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2614/comments",
    "author": "neel04",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T13:04:32Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:21:01Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2537,
    "title": "how to share encoder input and output embeddings and some defination question",
    "created_at": "2020-08-29T09:10:05Z",
    "closed_at": "2022-05-02T21:21:59Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2537",
    "body": "## 🚀 Feature Request\r\nhow to share encoder input and output embeddings\r\n\r\n### Motivation\r\n\r\nThere are only two commands about sharing embedding but no sharing encoder:\r\n\"--share-decoder-input-output-embed\" and \"--share-all-embeddings\"\r\n\r\n### Pitch\r\n\r\nI want to make the experiment about only sharing decoder and only sharing encoder. \r\n\r\n\r\n### Additional context\r\n\r\nI’m sorry that the explanation of the document about sharing embeddings is not so clear for me.\r\n'--share-all-embeddings' mean share all the embeddings weighted matrix and \"--share-decoder-input-output-embed\" means share the weighted matrix of input and output embeddings of decoder? right?\r\n\r\nthanks a lot!",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2537/comments",
    "author": "naginoa",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-08-31T19:36:13Z",
        "body": "thats right, when you specify --share-all-embeddings then the embedding matrices for encoder input, decoder input and decoder output are all shared. when you specify --share-decoder-input-output-embed, then the matrices for decoder input and output are shared, but encoder has its own embeddings. does this help?"
      },
      {
        "user": "naginoa",
        "created_at": "2020-09-01T05:20:06Z",
        "body": "It does help me a lot! Thank you very much！\r\nBut I still wonder if I can share only encoder embedding, but the decoder has its own embeddings.\r\nThanks a lot!"
      },
      {
        "user": "alexeib",
        "created_at": "2020-09-01T17:00:07Z",
        "body": "what does sharing encoder embeddings mean? encoder only has input embeddings - what would you share them with?"
      },
      {
        "user": "dxli94",
        "created_at": "2021-01-08T14:38:28Z",
        "body": "Would it make any sense if just sharing the encoder input and decoder input, while leaving decoder output separate? I am wondering whether this would be of any use for seq2seq problem using monolingual language?\r\n\r\nAny response is appreciated."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T03:04:52Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-05-02T21:21:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2470,
    "title": "A checkpoint should always be triggered when stopping criteria triger",
    "created_at": "2020-08-12T14:50:34Z",
    "closed_at": "2020-08-28T00:58:21Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2470",
    "body": "## 🚀 Feature Request\r\nCurrently, when stopping criteria trigger (e.g. `--max-updates`) a checkpoint is not triggered. This is generally fine, but somewhat surprising if, say, at some point in between two save points, early stopping criteria are triggered. The last point in training should be given a chance at validation and saving.\r\n\r\nThis can be a dedicated `model_last.pt` file, for example. Written only when training has decided to end.\r\n\r\n### Motivation\r\n\r\nAllow debugging or continuing training from the actual end point of training.\r\n\r\n### Pitch\r\n\r\nTwo possibilities:\r\n1. Right before terminating training, force an evaluation of the current model as a potential checkpoint candidate.\r\n2. Always, right before terminating training, save the current model in a dedicated checkpoint file. E.g. `model_last.pt`.\r\n\r\n### Alternatives\r\n\r\nNot sure.\r\n\r\n### Additional context\r\n\r\nN/A\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2470/comments",
    "author": "munael",
    "comments": [
      {
        "user": "alexeib",
        "created_at": "2020-08-28T00:58:21Z",
        "body": "what makes you think this is the case?\r\n```\r\n    # Stopping conditions\r\n    max_update = args.max_update or math.inf\r\n    should_stop = (\r\n        should_stop_early(args, valid_losses[0])\r\n        or trainer.get_num_updates() >= max_update\r\n        or (\r\n            args.stop_time_hours > 0\r\n            and trainer.cumulative_training_time() / (60 * 60) > args.stop_time_hours\r\n        )\r\n    )\r\n\r\n    # Save checkpoint\r\n    if do_save or should_stop:\r\n        logger.info(\"begin save checkpoint\")\r\n        checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\r\n```"
      }
    ]
  },
  {
    "number": 2462,
    "title": "Standardize reorder_incremental_states",
    "created_at": "2020-08-11T08:35:49Z",
    "closed_at": "2022-04-19T06:21:07Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2462",
    "body": "## 🚀 Feature Request\r\n\r\nStandardize `reorder_incremental_state` functions such that that `incremental_state` can be `Dict[str, Dict[str, Optional[Tensor]]]` typed.\r\n\r\n### Motivation\r\n\r\nBased on the source code, it seems like torchscript is the next big feature of `fairseq`?\r\n\r\nTorchscript requires typing to be static but at the point of writing, there are inconsistencies in the way each module caches buffers into `incremental_state`.\r\n\r\nFor instance,\r\n| Module Name | Buffer Type |\r\n| --- | --- |\r\n| `MultiheadAttention` | `Dict[str, Optional[Tensor]]` |\r\n| `DynamicConv1dTBC` | `Optional[Tensor]` |\r\n\r\n### Pitch\r\n\r\nEvery other module can be refactored to follow the style laid out by `MultiheadAttention`.\r\n`Dict[str, Optional[Tensor]]` is generic and fits 99.9% of use cases.\r\n\r\nThough some problems can arise if the original buffer type is `List[Tensor]` but I don't think that such edge cases would be likely.\r\n\r\n### Alternatives\r\n\r\nBuffer types can alternatively be `Tuple[Optional[Tensor]]` or `List[Optional[Tensor]]` typed.\r\nBut this would be less consistent with the way the rest of the repo is written.\r\n\r\n### Additional context\r\n\r\nAdditionally, I would like to suggest that tensors in` incremental_state` made to be `[bsz, ...]` shaped.\r\nThis way, reorder_incremental_state can be completely removed.\r\nIt also enables easy implementation of buffered incremental_state, reducing malloc costs during inference which can be a big deal since it's one of the big overheads in beam search.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2462/comments",
    "author": "mingruimingrui",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T15:04:33Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-19T06:20:36Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2461,
    "title": "Customize command-line tool args?",
    "created_at": "2020-08-11T06:07:26Z",
    "closed_at": "2020-08-12T23:39:40Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2461",
    "body": "The command-line tools such as fairseq-train are handy, but it is very difficult to find a clean way to add my own args (args that are not listed).\r\nI could modify the options.py file directly to allow my own args to be passed in. However, it would be much better if the program can accept any new args directly, or allow users to add them in a separate file.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2461/comments",
    "author": "ylmeng",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-08-12T22:51:00Z",
        "body": "We already allow new arguments to be added via user-supplied tasks, criterions, models, optimizers, etc. What kind of other arguments are you envisioning?"
      },
      {
        "user": "ylmeng",
        "created_at": "2020-08-12T23:25:46Z",
        "body": "Sorry, I just realized the --user-dir option can probably meet my needs.\r\n\r\nHowever, there is one thing I am not completely sure by looking at the example. If I want a command-line arg to alter a model attribute (\"model.something\") during inference (fairseq-interactive). It is not clear to me how to do that.\r\nI know I can pass args to customized tasks, but I am not sure if it works on models loaded from checkpoints."
      },
      {
        "user": "ylmeng",
        "created_at": "2020-08-12T23:39:37Z",
        "body": "> Sorry, I just realized the --user-dir option can probably meet my needs.\r\n> \r\n> However, there is one thing I am not completely sure by looking at the example. If I want a command-line arg to alter a model attribute (\"model.something\") during inference (fairseq-interactive). It is not clear to me how to do that.\r\n> I know I can pass args to customized tasks, but I am not sure if it works on models loaded from checkpoints.\r\n\r\nOK I see I can use --model-overrides from fairseq-interactive. My bad. But I hope there can be a comprehensive example in the tutorials."
      }
    ]
  },
  {
    "number": 2307,
    "title": "Add '--truncate-source' flag to 'multilingual_translation' task",
    "created_at": "2020-07-07T11:44:19Z",
    "closed_at": "2022-04-18T05:21:21Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2307",
    "body": "It'd be nice to incorporate the '--truncate-source' flag to the 'multilingual_translation' task.\r\n\r\nThe '--truncate-source' flag already exists in the base 'translation' task.\r\n\r\nNot sure why it was not incorporated into the 'multilingual_translation' task, but the change looks pretty straightforward, since the latter borrows the dataset creation method from the former:\r\n\r\nfrom fairseq.tasks.translation import load_langpair_dataset\r\n\r\n### Environment\r\n\r\n - fairseq Version: master",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2307/comments",
    "author": "jesgim",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2020-07-07T18:08:55Z",
        "body": "Sounds reasonable to me.  Feel free to submit a PR."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:04:46Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:51Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2257,
    "title": "BART- Get cross attention of all layers in the inference ",
    "created_at": "2020-06-21T10:29:53Z",
    "closed_at": "2022-10-19T10:08:53Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2257",
    "body": "## 🚀 Feature Request\r\nWrite now, BART implementation only outputs the cross attention of the last decoder layer. It would be nice to get all the attention weights during the inference. \r\n\r\n### Motivation\r\n\r\nThis will allow us to explore layer-wise attention for the task of abstraction summarization.\r\n\r\n### Pitch\r\n\r\nIt would be nice to integrate this to the **def generate** function in the **hub_interface.py script**.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2257/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "NadjaHergerTR",
        "created_at": "2020-08-05T08:56:37Z",
        "body": "Hi there. I'm running into the same issue. Would you be able to share any updates in terms of extracting attention from other BART layers?"
      },
      {
        "user": "shamanez",
        "created_at": "2020-08-12T00:06:17Z",
        "body": "@NadjaHergerTR  Any update on this? "
      },
      {
        "user": "NadjaHergerTR",
        "created_at": "2020-08-12T04:47:16Z",
        "body": "> @NadjaHergerTR Any update on this?\r\n\r\nHi @shamanez . No updates from my side unfortunately."
      },
      {
        "user": "shamanez",
        "created_at": "2020-08-31T12:22:22Z",
        "body": "I tried to get them out. But it is super hacky. Basically you need to go to transformer.py and take attention layers weights out from the forward function in the Decoder."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-28T12:36:43Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "HeegonJin",
        "created_at": "2022-10-19T06:46:57Z",
        "body": "bump"
      }
    ]
  },
  {
    "number": 2208,
    "title": "Improving levt transformer training speed",
    "created_at": "2020-06-04T10:46:06Z",
    "closed_at": "2022-04-18T05:21:00Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2208",
    "body": "## 🚀 Feature Request\r\nMake training levenshtein transformer faster!\r\n\r\n### Motivation\r\nThe current training scheme under the levenshtein transformer framework for machine translation has a rather slow rate of convergence.\r\n\r\nCompared to the standard transformer training scheme used in for wmt14 en-de. The suggested training parameters takes about 9-12 times as much time.\r\n\r\nImproving training speeds would make the model viability for production use when less powerful hardware is available during training time.\r\n\r\n### Pitch\r\n\r\nLet's keep an issue open to keep track of some issues with translation outputs! From there we can generalize some best practices that can be incorporated into optimizing training framework.\r\n\r\n### Alternatives\r\n\r\nAdditionally, other research papers can be used to theorycraft the information bottlenecks which are preventing the model from easily learning the features required to perform proper translation.\r\n\r\n### Additional context\r\n\r\nUnder local testing, replicating Vaswani et al. 2017 took 12 hours on a single V100 GPU with fp16 training.\r\nTo replicate Gu et al. 2019, 3 V100 GPUs and 57 hours are needed to perform training (similarly with fp16).\r\nHowever, BLEU was still lower than reported.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2208/comments",
    "author": "mingruimingrui",
    "comments": [
      {
        "user": "mingruimingrui",
        "created_at": "2020-06-04T10:55:55Z",
        "body": "An observation is that the 0th step in iterative decoding, (predicting the initial amount of masks and generating the starting sentence) is very different from the task of sentence correction.\r\n\r\nLet's start with a guiding question. Why should a good language model be able to predict sequence length when there isn't a direct correlation between it (the length) and the linguistical contents of a sentence?\r\n\r\nCould we perhaps improve the training strategy by removing the need for the model to predict the initial length?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:41Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T05:20:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2156,
    "title": "When there will be vision results of the paper?",
    "created_at": "2020-05-20T01:50:39Z",
    "closed_at": "2020-05-22T11:59:46Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2156",
    "body": "## 🚀 Feature Request\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2156/comments",
    "author": "thimabru1010",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-05-22T11:59:46Z",
        "body": "Please provide more details about the feature request"
      }
    ]
  },
  {
    "number": 2140,
    "title": "Save last checkpoint before validation",
    "created_at": "2020-05-16T07:38:27Z",
    "closed_at": "2020-05-27T16:39:57Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2140",
    "body": "## 🚀 Feature Request\r\nAt least a couple of times I started a training and the model crashed after an epoch during validation (because of OOM or just because I write shitty code). All of these times I wished that the epoch checkpoint was saved before validation, to restart training from the next epoch (and discarding that one validation run).\r\n\r\n### Pitch\r\n\r\nSave epoch checkpoint (`checkpoint_last.pt`) before validation. \r\nMaybe update it after validation with stats.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2140/comments",
    "author": "villmow",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-05-27T16:39:57Z",
        "body": "Fixed in 145bc9de1278414812b2aef837be9ca0e9c1aebc"
      }
    ]
  },
  {
    "number": 2117,
    "title": "Adding more dataset source for mBART",
    "created_at": "2020-05-11T03:29:59Z",
    "closed_at": "2022-04-18T03:21:17Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2117",
    "body": "## 🚀 Feature Request\r\nIs it possible to add the dataset source for mBART? \r\n### Motivation\r\n\r\nCurrently, the repo contains data link to EN_RO. However, other languages in the paper are not trivial to obtain with exact setup. This makes the reproduction of the paper results hard, as the dataset may not be in same size or with same content. \r\n\r\n\r\n### Pitch\r\n\r\nThe link of dataset author used in the paper. If there is any special process, like split, I would hope it is also clarified in the repo.\r\n\r\nThanks,\r\nGuoyin",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2117/comments",
    "author": "guoyinwang",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T17:04:59Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:46Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 2095,
    "title": "BART: How to use BART for the question and answering ?",
    "created_at": "2020-05-06T09:03:17Z",
    "closed_at": "2022-04-18T03:21:10Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/2095",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2095/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T03:20:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1912,
    "title": "Have Criterions define default checkpoint metric",
    "created_at": "2020-03-25T22:12:35Z",
    "closed_at": "2022-04-18T02:21:13Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1912",
    "body": "The criterion can define what the default checkpoint metric is, and whether to maximize or minimize.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1912/comments",
    "author": "myleott",
    "comments": [
      {
        "user": "villmow",
        "created_at": "2020-03-30T21:37:47Z",
        "body": "Shouldn't the criterion define a default metric, which can be overridden by `--maximize-best-checkpoint-metric`?  For example if I want to maximize valid-bleu..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T18:04:59Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-18T02:20:44Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1828,
    "title": "Allow different scorers during (validation) inference",
    "created_at": "2020-03-12T14:05:57Z",
    "closed_at": "2022-04-28T22:21:58Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1828",
    "body": "## 🚀 Feature Request\r\n\r\nDuring inference one might like to score with different metrics (bleu, rouge, precision/recall, gleu, ...).\r\nDefine a common Scorer interface and allow implementing different scorers e.g. `register_eval_scorer()`.\r\n\r\n### Motivation\r\n\r\nOn some datasets I need to score my translation results with ROUGE and Precision/Recall as this is the common practice.\r\n\r\n### Pitch\r\n\r\n1. Define an `register_eval_scorer` decorator and a protocol that scorers need to follow:\r\n   1.1. A `score(refs, hypos)` method that computes a summable score for a batch and returns that as a dictionary which will be included in `logging_outputs`\r\n   1.2 A `reduce_metrics(logging_outputs, criterion)` that reduces the logging outputs to a single score for the whole validation set and logs this as a scalar or derived scalar to `metrics`.\r\n2. In translation:\r\n   2.1 Remove the BLEU specific arguments from `translation`.\r\n   2.2 Leave and rename inference specific arguments in translation (e.g. `--eval-scorer-generation-args`, `--eval-scorer-remove-bpe`, ...).\r\n   2.3 Generation should only be triggered when an evaluation scorer is set.\r\n   2.4  The references and hypos (the output of the generator) are then passed to a scorer (or multiple scorers) and let the scorer handle the logging of the metrics.\r\n\r\nI'm not sure if this could be integrated in `fairseq-generate`, but it seems possible to me.\r\n\r\n### Alternatives\r\n\r\nAn alternative would be to generate with fairseq-generate, parse the output and score it yourself. This is hard to do during validation.\r\n\r\n### Additional context\r\n\r\nI implemented this for myself using the following code on Translation. This is just a modified version of `--eval-bleu`.\r\n\r\nIt passes the hypos and references to an `eval_scorer`, which implements a `score(hypos, refs)` method and returns the scores as a dictionary which can be included in `logging_outputs`. The task then calls `reduce_metrics(logging_outputs, criterion)` on that scorer, which should process the logging_outputs and log scalars to metrics.\r\n\r\nNote, that this only supports one scorer at a time, while it would be possible to have multiple scorers process the translations. However, I don't know how the `registry` handles multiple arguments.\r\n\r\nModified translation_task.py\r\n```python\r\nfrom eval_scorer import build_eval_scorer, add_eval_scoring_args\r\n\r\nclass EvaluatingTranslationTask(TranslationTask):\r\n    @staticmethod\r\n    def add_args(parser):\r\n        \"\"\"Add task-specific arguments to the parser.\"\"\"\r\n        TranslationTask.add_args(parser)\r\n        add_eval_scoring_args(parser)\r\n\r\n    def build_model(self, args):\r\n        self.eval_scorer = build_eval_scorer(args)\r\n        if self.eval_scorer is not None:\r\n            assert getattr(args, 'eval_scorer_detok', None) is not None, (\r\n                '--eval-scorer-detok is required if using --eval-scorer; '\r\n                'try --eval-scorer-detok=moses (or --eval-scorer-detok=space '\r\n                'to disable detokenization, e.g., when using sentencepiece)'\r\n            )\r\n            detok_args = json.loads(getattr(args, 'eval_scorer_detok_args', '{}') or '{}')\r\n            self.tokenizer = encoders.build_tokenizer(Namespace(\r\n                tokenizer=getattr(args, 'eval_scorer_detok', None),\r\n                **detok_args\r\n            ))\r\n\r\n            gen_args = json.loads(getattr(args, 'eval_scorer_args', '{}') or '{}')\r\n            self.sequence_generator = self.build_generator(Namespace(**gen_args))\r\n\r\n        return super().build_model(args)\r\n\r\n    def valid_step(self, sample, model, criterion):\r\n        loss, sample_size, logging_output = super().valid_step(sample, model, criterion)\r\n\r\n        if self.eval_scorer is not None:\r\n            hyps, refs = self._inference(self.sequence_generator, sample, model)\r\n\r\n            if self.args.eval_scorer_print_samples:\r\n                logger.info('example hypothesis: ' + hyps[0])\r\n                logger.info('example reference: ' + refs[0])\r\n\r\n            scorer_logging_outputs = self.eval_scorer.score(hyps, refs)\r\n            logging_output = {**logging_output, **scorer_logging_outputs}\r\n\r\n        return loss, sample_size, logging_output\r\n\r\n    def reduce_metrics(self, logging_outputs, criterion):\r\n        \"\"\"\r\n        added smoothing bleu arguments to sacrebleu\r\n        \"\"\"\r\n\r\n        FairseqTask.reduce_metrics(self, logging_outputs, criterion)\r\n\r\n        if self.eval_scorer:\r\n            self.eval_scorer.reduce_metrics(logging_outputs, criterion)\r\n\r\n    def _inference(self, generator, sample, model):\r\n        \"\"\"\r\n        return references and hypos instead of immediate scoring\r\n        \"\"\"\r\n        def decode(toks, escape_unk=False):\r\n            s = self.tgt_dict.string(\r\n                toks.int().cpu(),\r\n                self.args.eval_scorer_remove_bpe,\r\n                escape_unk=escape_unk,\r\n            )\r\n            if self.tokenizer:\r\n                s = self.tokenizer.decode(s)\r\n            return s\r\n\r\n        gen_out = self.inference_step(generator, [model], sample, None)\r\n        hyps, refs = [], []\r\n        for i in range(len(gen_out)):\r\n            hypo = decode(gen_out[i][0]['tokens'])\r\n            ref = decode(\r\n                utils.strip_pad(sample['target'][i], self.tgt_dict.pad()),\r\n                escape_unk=True,  # don't count <unk> as matches to the hypo\r\n            )\r\n            if self.args.eval_scorer_lowercase:\r\n                hypo = hypo.lower()\r\n                ref = ref.lower()\r\n            refs.append(ref)\r\n            hyps.append(hypo)\r\n\r\n        return hyps, refs\r\n```\r\n\r\nand the corresponding `eval_scorer.py`:\r\n\r\n```python\r\nfrom abc import abstractmethod\r\nimport logging\r\nfrom typing import *\r\n\r\nfrom fairseq import registry, metrics\r\nfrom fairseq.utils import item\r\nimport numpy as np\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nbuild_eval_scorer, register_eval_scorer, EVAL_SCORER_REGISTRY = registry.setup_registry(\r\n    '--eval-scorer',\r\n    default=None,\r\n)\r\n\r\ndef add_eval_scoring_args(parser):\r\n    parser.add_argument('--eval-scorer-generation-args', type=str, metavar='JSON',\r\n                        help='generation args for validation inference, '\r\n                             'e.g., \\'{\"beam\": 4, \"lenpen\": 0.6}\\'')\r\n    parser.add_argument('--eval-scorer-detok', type=str, default=\"space\",\r\n                        help='detokenizer before computing any scores (e.g., \"moses\"); '\r\n                             'required if using --eval-bleu; use \"space\" to '\r\n                             'disable detokenization; see fairseq.data.encoders '\r\n                             'for other options')\r\n    parser.add_argument('--eval-scorer-detok-args', type=str, metavar='JSON',\r\n                        help='args for building the tokenizer, if needed')\r\n    parser.add_argument('--eval-scorer-remove-bpe', nargs='?', const='@@ ', default=None,\r\n                        help='remove BPE before computing any scores')\r\n    parser.add_argument('--eval-scorer-print-samples', action='store_true', default=False,\r\n                        help='print sample generations during validation')\r\n    parser.add_argument('--eval-scorer-lowercase', action='store_true', default=True,\r\n                            help='lowercase outputs/references before computing any score')\r\n\r\n\r\nclass GenerationScorer:\r\n    def __init__(self, args):\r\n        self.args = args\r\n\r\n    @abstractmethod\r\n    def score(self, references, hypos) -> Dict[str, Any]:\r\n        \"\"\" score references and hypos and output something that fits in logging_outputs\"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def reduce_metrics(self, logging_outputs, criterion):\r\n        \"\"\"reduce multiple score dicts into a single metric\"\"\"\r\n        pass\r\n\r\n\r\n@register_eval_scorer(\"eval-bleu\")\r\nclass BleuGenerationScorer(GenerationScorer):\r\n    @staticmethod\r\n    def add_args(parser):\r\n        # options for reporting BLEU during validation\r\n        parser.add_argument('--eval-scorer-bleu-tokenized', action='store_true', default=True,\r\n                            help='if setting, we compute tokenized BLEU instead of sacrebleu')\r\n        parser.add_argument('--eval-scorer-bleu-smooth', choices=['exp', 'floor', 'add-k', 'none'], default='exp',\r\n                            help='smoothing method: exponential decay (default), floor (increment zero counts), add-k (increment num/denom by k for n>1), or none')\r\n        parser.add_argument('--eval-scorer-bleu-smooth-value', type=float, default=0.,\r\n                            help='The value to pass to the smoothing technique, when relevant. Default: %(default)s.')\r\n\r\n    def __init__(self, args):\r\n        super().__init__(args)\r\n        self.eval_bleu_order = 4\r\n        try:\r\n            import sacrebleu\r\n        except ImportError:\r\n            raise ImportError(\"Please install sacrebleu with pip install sacrebleu.\")\r\n\r\n    def score(self, refs: Iterable[str], hypos: Iterable[str]) -> Dict:\r\n        import sacrebleu\r\n        tokenize = sacrebleu.DEFAULT_TOKENIZER if not self.args.eval_scorer_bleu_tokenized else 'none'\r\n        bleu = sacrebleu.corpus_bleu(\r\n            hypos,\r\n            [refs],\r\n            tokenize=tokenize,\r\n        )\r\n\r\n        # create logging output in here\r\n        assert len(bleu.counts) == self.eval_bleu_order\r\n        bleu_logging_output = {\r\n            '_bleu_sys_len': bleu.sys_len,\r\n            '_bleu_ref_len': bleu.ref_len,\r\n        }\r\n        # we split counts into separate entries so that they can be\r\n        # summed efficiently across workers using fast-stat-sync\r\n        for i in range(self.eval_bleu_order):\r\n            bleu_logging_output['_bleu_counts_' + str(i)] = bleu.counts[i]\r\n            bleu_logging_output['_bleu_totals_' + str(i)] = bleu.totals[i]\r\n\r\n        return bleu_logging_output\r\n\r\n    def reduce_metrics(self, logging_outputs, criterion):\r\n        def sum_logs(key):\r\n            return sum(log.get(key, 0) for log in logging_outputs)\r\n\r\n        counts, totals = [], []\r\n        for i in range(self.eval_bleu_order):\r\n            counts.append(sum_logs('_bleu_counts_' + str(i)))\r\n            totals.append(sum_logs('_bleu_totals_' + str(i)))\r\n\r\n        if max(totals) > 0:\r\n            # log counts as numpy arrays -- log_scalar will sum them correctly\r\n            metrics.log_scalar('_bleu_counts', np.array(counts))\r\n            metrics.log_scalar('_bleu_totals', np.array(totals))\r\n            metrics.log_scalar('_bleu_sys_len', sum_logs('_bleu_sys_len'))\r\n            metrics.log_scalar('_bleu_ref_len', sum_logs('_bleu_ref_len'))\r\n\r\n            def compute_bleu(meters):\r\n                import inspect\r\n                import sacrebleu\r\n                fn_sig = inspect.getfullargspec(sacrebleu.compute_bleu)[0]\r\n                if 'smooth_method' in fn_sig:\r\n                    smooth = {'smooth_method': self.args.eval_scorer_bleu_smooth}\r\n                else:\r\n                    smooth = {'smooth': self.args.eval_scorer_bleu_smooth}\r\n\r\n                bleu = sacrebleu.compute_bleu(\r\n                    correct=meters['_bleu_counts'].sum,\r\n                    total=meters['_bleu_totals'].sum,\r\n                    sys_len=meters['_bleu_sys_len'].sum,\r\n                    ref_len=meters['_bleu_ref_len'].sum,\r\n                    smooth_value=self.args.eval_scorer_bleu_smooth_value,\r\n                    **smooth\r\n                )\r\n                return round(bleu.score, 2)\r\n\r\n            metrics.log_derived('bleu', compute_bleu)\r\n\r\n\r\n@register_eval_scorer(\"eval-precision-recall\")\r\nclass PrecisionRecallGenerationScorer(GenerationScorer):\r\n\r\n    @staticmethod\r\n    def add_args(parser):\r\n        pass\r\n\r\n    def __init__(self, args):\r\n        super().__init__(args)\r\n\r\n\r\n    @staticmethod\r\n    def sentence_score(\r\n            reference: Iterable[Any], hypothesis: Iterable[Any]\r\n    ) -> Tuple[int, int, int, int]:\r\n\r\n        tp, fp, tn, fn = 0, 0, 0, 0\r\n\r\n        reference_set = set(reference)\r\n        hypothesis_set = set(hypothesis)\r\n\r\n        for token in hypothesis:\r\n            if token in reference_set:\r\n                tp += 1\r\n            else:\r\n                fp += 1\r\n\r\n        for token in reference:\r\n            if token not in hypothesis_set:\r\n                fn += 1\r\n\r\n        return tp, fp, tn, fn\r\n    \r\n    @staticmethod\r\n    def corpus_micro_score(\r\n            refs: Iterable[Iterable[Any]], hypos: Iterable[Iterable[Any]]\r\n    ) -> Tuple[int, int, int, int]:\r\n        tp, fp, tn, fn = 0, 0, 0, 0\r\n        for ref, hypo in zip(refs, hypos):\r\n            sample_tp, sample_fp, sample_tn, sample_fn = PrecisionRecallGenerationScorer.sentence_score(ref, hypo)\r\n            tp += sample_tp\r\n            fp += sample_fp\r\n            tn += sample_tn\r\n            fn += sample_fn\r\n        return tp, fp, tn, fn\r\n\r\n    def score(self, refs, hypos) -> Dict:\r\n\r\n        assert len(refs) == len(hypos)\r\n        total = len(refs)\r\n        # tokenize words FIXME maybe use some tokenizer\r\n        refs = (ref.split() for ref in refs)\r\n        hypos = (hypo.split() for hypo in hypos)\r\n\r\n        tp, fp, tn, fn = self.corpus_micro_score(refs, hypos)\r\n\r\n        precision_logging_output = {\r\n            '_pr_tp': tp,\r\n            '_pr_fp': fp,\r\n            '_pr_tn': tn,\r\n            '_pr_fn': fn,\r\n            '_pr_total': total\r\n        }\r\n\r\n        return precision_logging_output\r\n\r\n    def reduce_metrics(self, logging_outputs, criterion):\r\n        def sum_logs(key):\r\n            return sum(log.get(key, 0) for log in logging_outputs)\r\n\r\n        total = sum_logs('_pr_total')\r\n        if total:\r\n            metrics.log_scalar('_pr_tp', sum_logs('_pr_tp'))\r\n            metrics.log_scalar('_pr_fp', sum_logs('_pr_fp'))\r\n            metrics.log_scalar('_pr_fn', sum_logs('_pr_fn'))\r\n\r\n            def compute_precision(meters):\r\n                _precision = precision(\r\n                    tp=meters['_pr_tp'].sum,\r\n                    fp=meters['_pr_fp'].sum,\r\n                )\r\n                return round(item(_precision * 100), 2)\r\n\r\n            def compute_recall(meters):\r\n                _recall = recall(\r\n                    tp=meters['_pr_tp'].sum,\r\n                    fn=meters['_pr_fn'].sum,\r\n                )\r\n                return round(item(_recall * 100), 2)\r\n\r\n            def compute_f_measure(meters):\r\n                _f_measure = f_measure(\r\n                    tp=meters['_pr_tp'].sum,\r\n                    fp=meters['_pr_fp'].sum,\r\n                    fn=meters['_pr_fn'].sum,\r\n                )\r\n\r\n                return round(item(_f_measure * 100), 2)\r\n\r\n            metrics.log_derived('recall', compute_recall)\r\n            metrics.log_derived('precision', compute_precision)\r\n            metrics.log_derived('f-1', compute_f_measure)\r\n\r\n\r\ndef precision(tp: int, fp: int) -> float:\r\n    return tp / (tp + fp) if (tp + fp) > 0 else 0.\r\n\r\n\r\ndef recall(tp: int, fn: int) -> float:\r\n    if (tp + fn) == 0:\r\n        return 0.\r\n    return float(tp) / (tp + fn)\r\n\r\n\r\ndef f_measure(tp: int, fp: int, fn: int, beta=1) -> float:\r\n    f_precision = precision(tp, fp)\r\n    f_recall = recall(tp, fn)\r\n    if f_precision == 0 or recall == 0:\r\n        return 0.\r\n    else:\r\n        return (1 + beta**2) * (f_precision * f_recall) / ((beta**2 * f_precision) + f_recall)\r\n```\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1828/comments",
    "author": "villmow",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-03-13T19:14:46Z",
        "body": "Great idea! Would you mind submitting a PR with this?"
      },
      {
        "user": "villmow",
        "created_at": "2020-03-14T09:05:03Z",
        "body": "Sure :)"
      },
      {
        "user": "AlanHassen",
        "created_at": "2020-03-16T20:47:51Z",
        "body": "`\r\n    def sentence_score():\r\n\r\n        tp, fp, tn, fn = 0, 0, 0, 0\r\n\r\n        reference_set = set(reference)\r\n        hypothesis_set = set(hypothesis)\r\n\r\n        for token in hypothesis:\r\n            if token in reference_set:\r\n                tp += 1\r\n            else:\r\n                fp += 1\r\n`\r\n\r\nI'm rather unsure if we should calculate the tp based on a set operation. Isn't the position the token has also relevant? In the end, it's a true positive, if our model can predict the correct token for a given position."
      },
      {
        "user": "villmow",
        "created_at": "2020-03-17T07:48:56Z",
        "body": "I think this depends on the definition of the scoring. I compute the overlap of unigrams (each word) between the predictions and references, which is the same as ROUGE-1. This may not be a very good metric for longer sequences (and maybe for sequences at all), but it is used in my task.\r\n\r\nIn this case a `true positive` would be any word from the *hypothesis* that appears in the *reference* regardless of the position. I use sets only to check the membership in either reference or hypothesis, as member lookups in sets are O(1) instead of O(n) for lists. \r\n\r\nI'm not even sure if this Scorer is wanted, as it basically is a stripped down ROUGE-1 implementation. I just left it here for completeness. One could implement a full ROUGE scorer, with ROUGE-1, ROUGE-2, ROUGE-L instead.\r\n\r\nBtw., I have prepared the Pull Request just need to run the translation example to see if everything works, but I'm currently running out of GPUs. So it might take a little. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:46Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:29Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1747,
    "title": "Loading data from two data loaders with two training data iterators ",
    "created_at": "2020-02-25T22:51:19Z",
    "closed_at": "2020-02-26T15:47:54Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1747",
    "body": "### Motivation\r\n\r\nMultitaks learning and Semi-Supervised Learning is getting so popular. So it is nice to have a feature that can load data from two uneven datasets in the sample time.\r\n\r\n### Pitch\r\n\r\nIn the training loop, right now we get data from a group iterator. I want to get data from two group iterators in multi GPU training. Right now Multi-GPU setting is failing. \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1747/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-02-26T15:47:53Z",
        "body": "Duplicate of #1746 :)"
      }
    ]
  },
  {
    "number": 1648,
    "title": "Refactor training code and metrics to use ignite",
    "created_at": "2020-01-27T00:06:49Z",
    "closed_at": "2022-04-28T00:22:08Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1648",
    "body": "## 🚀 Feature Request\r\n\r\nCustom training, logging, and checkpointing code should be replaced with ignite.\r\n\r\n### Motivation\r\n\r\nBecause ignite is the official Pytorch training library and has support for metrics, it seems like  some of fairseq's code reinvents the wheel. It would be nice if ignite could fill the gaps instead.\r\n\r\n### Pitch\r\n\r\nMaintaining messy training, checkpointing, and metrics logic can introduce bugs -- ignite can do this for you.\r\n\r\n### Alternatives\r\n\r\nKeep the code as-is.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1648/comments",
    "author": "erip",
    "comments": [
      {
        "user": "myleott",
        "created_at": "2020-01-27T14:37:32Z",
        "body": "This would be interesting to explore. Do you have much experience with ignite and/or would you want to take a stab at it?\r\n\r\nTwo potential concerns:\r\n1) events and hooks often become unwieldy and make the code less readable, especially as complexity grows. How do we manage this in a sensible way?\r\n2) will we be able to port the various speed optimizations in our current Trainer into ignite? For example, apex FP16 is actually slower than fairseq's implementation due to not collapsing the parameters into a single buffer. Fairseq also has an alternative DDP implementation (\"no_c10d\" or LegacyDistributedDataParallel), which is necessary for a lot of cases where the pytorch DDP doesn't work properly; can we port that over as well?\r\n\r\nAnother alternative is pytorch-lightning, which already mirrors FairseqTask in many ways. Concern 2 above still applies.\r\n\r\nWhat do you think?"
      },
      {
        "user": "erip",
        "created_at": "2020-01-27T21:30:11Z",
        "body": "The readability question is tough to answer because it depends on how the logic is encapsulated, etc. I suspect that as the notional refactor begins, we can keep an eye on how it looks as a smoketest. \n\nI almost wonder if the two unique capabilities afforded by fairseq that you mentioned could be abstracted into their open libraries? I don't think ignite forces you into any particular AMP or DDP."
      },
      {
        "user": "erip",
        "created_at": "2020-01-27T21:37:06Z",
        "body": "I guess I didn't quite address this explicitly, but I think the `Trainer` would encapsulate this change in its entirety. I'm happy to try to work on this with full disclosure that I've only toyed with ignite. :smile:"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:38Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1638,
    "title": "Can number of layers be specified for pretraining Roberta ?",
    "created_at": "2020-01-21T06:22:28Z",
    "closed_at": "2020-01-21T14:08:59Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1638",
    "body": "## 🚀 Feature Request\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1638/comments",
    "author": "parry2403",
    "comments": [
      {
        "user": "ngoyal2707",
        "created_at": "2020-01-21T14:08:59Z",
        "body": "You can pass `--encoder-layers` param with training command to specify number of layers."
      }
    ]
  },
  {
    "number": 1597,
    "title": "Thoughts on implementation of Levenshtein Transformer",
    "created_at": "2020-01-07T05:42:01Z",
    "closed_at": "2022-04-28T00:22:06Z",
    "labels": [
      "enhancement",
      "help wanted",
      "needs triage",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1597",
    "body": "## 🚀 Feature Request\r\n\r\n1. **Improve noise injection**\r\nSwitch between random delete, random mask, random insert and do nothing based on probability\r\n\r\n~2. **Better define CPU and GPU tasks**\r\nAllocate all data processing tasks strictly to CPU and reserve GPU time for forward passes through transformer layers.~\r\n\r\n3. **`<mask>` token instead of `<unk>`**\r\nUse `<mask>` token in insert subtask instead of `<unk>`.\r\n\r\n### Motivation\r\n\r\n1. **Improve noise injection**\r\nDuring inference time, all of these tasks are directly utilized. It is sensible to perform training using all of them as well.\r\n\r\n~2. **Better define CPU and GPU tasks**\r\nCurrent GPU utilization during inference time is low. Optimizing the GPU tasks can potentially lead to even more speed wise improvements.~\r\n\r\n3. **`<mask>` token instead of `<unk>`**\r\nIn typical translation tasks, `<unk>` tokens are sparingly used (usually in about 0.1% of all sentences).\r\nUsing `<unk>` in the insertion stage would lead to a huge amount of data imbalance (Model would likely generalize that `<unk>` is placeholder but not unknown token).\r\n\r\n### Pitch\r\n\r\n1. **Improve noise injection**\r\nA probability can be used to determine the type of noise to inject.\r\n\r\n~2. **Better define CPU and GPU tasks**\r\nWe can take advantage of the small sizes of model input and output (only token ids are necessary, encoder outputs can be stored on GPU memory).\r\nKnowing that forward passes of transformer layers are highly efficient and CPU-to-GPU I/O costs are low, data processing can be done purely on the CPU side so GPU time can be better allocated for transformer layer forward passes.\r\nInference performance can be improved by performing inference on multiple batches simultaneously in a multi-threaded manner (on the same python process).\r\nThis is not a perfect solution. Though throughput will improve, latency would likely suffer.~\r\n\r\n3. **`<mask>` token instead of `<unk>`**\r\nUse `<mask>` token instead of `<unk>`.\r\n\r\n### Alternatives\r\n\r\n*Reserved for suggestions.\r\n\r\n### Additional context\r\n\r\n*Reserved for additional remarks.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1597/comments",
    "author": "mingruimingrui",
    "comments": [
      {
        "user": "mingruimingrui",
        "created_at": "2020-01-13T08:36:31Z",
        "body": "I did a little bit of benchmarking and is glad to say that `LevenshteinTransformerModel.forward_decoder` is rather GPU efficient.\r\nThe reason I bought up point 2 was because of an observed 15% GPU utilization using `fairseq-generate`.\r\nThe likely reason for the low utilization is likely due to the auxiliary actions like verbosity and edit history keeping."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:42Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:36Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 1579,
    "title": "Weights for RoBERTa finetuned on SQuAD",
    "created_at": "2020-01-04T20:14:30Z",
    "closed_at": "2022-04-28T22:22:03Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/1579",
    "body": "## 🚀 Feature Request\r\nRelease model weights for RoBERTa finetuned on SQuAD.\r\n\r\n### Motivation\r\n\r\nSQuAD is one of the most popular datasets for evaluating NLU models. Models trained on SQuAD can be useful for various real-world tasks.\r\n\r\n### Pitch\r\n\r\nCould you please release pre-trained model weights for RoBERTa (base and large) finetuned on SQuAD?\r\n\r\n### Alternatives\r\n\r\nProvide scripts for finetuning RoBERTa on SQuAD ( #933 ).\r\n\r\n### Additional context\r\n\r\nNA\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/1579/comments",
    "author": "OlegPlatonov",
    "comments": [
      {
        "user": "phosseini",
        "created_at": "2020-03-21T00:19:31Z",
        "body": "Any update?!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T19:20:40Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T22:21:34Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 953,
    "title": "Any implementation integration for pointer network to the  translation model?",
    "created_at": "2019-08-02T04:34:30Z",
    "closed_at": "2022-04-27T23:22:06Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/953",
    "body": "Pointer network is extremely important for the translation/summary tasks, which can copy words from source sequence directly to boost the model performance hugely.\r\nI hope there can be a built-in implementation of the module to use in fairseq, which is a very important module in openNMT-py package.\r\nAnd I think some guideline examples using RoBerTa as transformer encoder for the transformer model is very necessary for testing and researching the RoBerTa in generation task.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/953/comments",
    "author": "zhangzhenyu13",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T21:20:39Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:35Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 863,
    "title": "Error occurs when Learning-rate goes to zero",
    "created_at": "2019-07-06T14:00:10Z",
    "closed_at": "2022-04-27T23:22:08Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/863",
    "body": "Error occurs when Learning-rate goes to zero. See the trace below:\r\n\r\n| epoch 523:   6%| | 1/16 [00:00<00:03,  4.24it/s, loss=1.355, nll_loss=0.432, ppl=1.35, wps=1406, ups=0, wpb=2931.000, bsz=512.000, num_updates=8353, lr=0, gnorm=0.030, clip=0.000, oom=0.000, wall=9258, train_wall=1648]Traceback (most recent call last):\r\n  File \"/usr/lib/python3.6/pdb.py\", line 1667, in main\r\n    pdb._runscript(mainpyfile)\r\n  File \"/usr/lib/python3.6/pdb.py\", line 1548, in _runscript\r\n    self.run(statement)\r\n  File \"/usr/lib/python3.6/bdb.py\", line 434, in run\r\n    exec(cmd, globals, locals)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/vin/fairseq/train.py\", line 306, in <module>\r\n    cli_main()\r\n  File \"/home/vin/fairseq/train.py\", line 302, in cli_main\r\n    main(args)\r\n  File \"/home/vin/fairseq/train.py\", line 78, in main\r\n    while lr > args.min_lr and epoch_itr.epoch < max_epoch and trainer.get_num_updates() < max_update:\r\n  File \"/home/vin/fairseq/train.py\", line 121, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/vin/fairseq/fairseq/trainer.py\", line 347, in train_step\r\n    self.optimizer.step()\r\n  File \"/home/vin/fairseq/fairseq/optim/fairseq_optimizer.py\", line 91, in step\r\n    self.optimizer.step(closure)\r\n  File \"/home/vin/fairseq/fairseq/optim/nag.py\", line 70, in step\r\n    lr_correct = lr / lr_old\r\nZeroDivisionError: float division by zero\r\nUncaught exception. Entering post mortem debugging\r\nRunning 'cont' or 'step' will restart the program\r\n> /home/vin/fairseq/fairseq/optim/nag.py(70)step()\r\n-> lr_correct = lr / lr_old\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/863/comments",
    "author": "vineetk1",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2019-08-02T21:05:35Z",
        "body": "Can you provide the command that was used to reproduce this result?  "
      },
      {
        "user": "vineetk1",
        "created_at": "2019-08-03T00:08:00Z",
        "body": "CUDA_VISIBLE_DEVICES=0 python3 train.py --task dialog_task data-bin/dialog/task1 --arch lstm_dialog --save-dir checkpoints/dialog/task1 --max-tokens 200 --required-batch-size-multiple 1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer nag --lr-scheduler fixed --force-anneal 200 --lr 0.25 --clip-norm 0.1\r\n\r\nLet me know if you need more information"
      },
      {
        "user": "vineetk1",
        "created_at": "2019-08-30T14:25:26Z",
        "body": "What is the status of this issue?"
      },
      {
        "user": "lematt1991",
        "created_at": "2019-08-30T14:34:32Z",
        "body": "I think the easiest way around this is to set `--min-lr` to some small non-zero value.  This way training will stop once the learning rate gets too small and will avoid the divide by zero."
      },
      {
        "user": "vineetk1",
        "created_at": "2019-08-31T21:13:52Z",
        "body": "That is what I have been doing. I am assuming that sometime in future this issue will be resolved."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T21:20:36Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:39Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 805,
    "title": "BacktranslationDataset doesn't work with multiple workers",
    "created_at": "2019-06-13T15:13:42Z",
    "closed_at": "2022-04-27T23:22:24Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/805",
    "body": "if num_workers > 0, there will be an error:\r\n`AttributeError: Can't pickle local object 'SemisupervisedTranslationTask.build_model.<locals>.backtranslate_fn'`",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/805/comments",
    "author": "hxbai",
    "comments": [
      {
        "user": "lematt1991",
        "created_at": "2019-06-14T14:58:00Z",
        "body": "Yeah, I'm not sure `BacktranslationDataset` was ever intended to be run with multiple workers since it relies on models that are on the GPU.   cc @pipibjc "
      },
      {
        "user": "pipibjc",
        "created_at": "2019-06-14T16:04:22Z",
        "body": "Thanks for reporting! I will need to find some workaround of it. For now we can only use one worker if we want to use `BacktranslationDataset`"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:52Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-27T23:21:54Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      },
      {
        "user": "mnoukhov",
        "created_at": "2022-08-08T17:32:36Z",
        "body": "This is still an issue"
      }
    ]
  },
  {
    "number": 596,
    "title": "Compatible data parallelism for model parallelism",
    "created_at": "2019-03-25T01:53:21Z",
    "closed_at": "2022-04-28T00:21:59Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/596",
    "body": "I built a large model that does not fit in one GPU, so I distributed it to several devices. Currently, my code works fine when I set `--distributed-world-size 1`. Since I want to utilize the remaining GPUs to perform data parallelism for acceleration, I would like to know how to modify the raw fairseq code in order to make model parallelism compatible with data parallelism.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/596/comments",
    "author": "lyy1994",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:50Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:29Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  },
  {
    "number": 565,
    "title": "Proposal to remove dictionary from model abstract class signatures in Encoder and Decoder Models",
    "created_at": "2019-03-09T06:09:46Z",
    "closed_at": "2022-04-28T00:22:00Z",
    "labels": [
      "enhancement",
      "help wanted",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/fairseq/issues/565",
    "body": "I found that we are building embedding now in build_model static method instead of the constructor for all classes except for fconv (which would be trivial to refactor out). \r\nThis makes `dictionary` property unnecessary for encoders and decoders. I checked access to `dictionary` property of encoders and decoders throughout the codebase and found that it was unused.\r\n\r\nI propose that we remove it from constructors of the abstract classes of `FairseqIncrementalDecoder`, `FairseqEncoder`, `FairseqDecoder` and concrete models. \r\n\r\nThis would help easy adoption of encoders and decoders of fairseq into other libraries.\r\n\r\nAnd as a sidenote, if we can do something about using global caches in `utils.make_position` etc. It would also help as other libraries might use DataParallel instead of multi processing.",
    "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/565/comments",
    "author": "sai-prasanna",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-04-17T20:20:49Z",
        "body": "This issue has been automatically marked as stale. **If this issue is still affecting you, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize it yet. If you have any new additional information, please include it with your comment!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-28T00:21:30Z",
        "body": "Closing this issue after a prolonged period of inactivity. If this issue is still present in the latest release, please create a new issue with up-to-date information. Thank you!\n"
      }
    ]
  }
]