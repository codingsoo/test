[
  {
    "number": 3485,
    "title": "Cannot give any answer with simple questions",
    "created_at": "2023-06-17T03:57:38Z",
    "closed_at": "2023-06-21T09:59:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/3485",
    "body": "## OS\r\nmacOS with M1 chipset.\r\n\r\n## Code to use\r\ne522599e - (2023-06-16 12:37:40 +0200) Revert and update some of strings in tasks.json for the Ukrainian translation (#3427)\r\n\r\n## Command to Run\r\ndocker compose --profile ci --profile inference up --build --attach-dependencies\r\n\r\n## Issue\r\nAfter login with the dev mode, for any question, even very simple, there is no answer, but keep showing: \"Your message is queued, you are at position 0 in the queue.\"\r\n\r\n ",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/3485/comments",
    "author": "yeasy",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-06-17T06:34:31Z",
        "body": "Which model do you have selected?"
      },
      {
        "user": "yeasy",
        "created_at": "2023-06-18T03:49:07Z",
        "body": "> Which model do you have selected?\r\n\r\nThe default one, which is _lorem + k50-Plugins\r\n\r\n{\r\n  \"model_config\": {\r\n    \"model_id\": \"_lorem\",\r\n    \"max_input_length\": 128,\r\n    \"max_total_length\": 256,\r\n    \"quantized\": false\r\n  },\r\n  \"sampling_parameters\": {\r\n    \"top_k\": 50,\r\n    \"top_p\": null,\r\n    \"typical_p\": null,\r\n    \"temperature\": 0.35,\r\n    \"repetition_penalty\": 1.1111111111111112,\r\n    \"max_new_tokens\": 1024\r\n  },\r\n  \"do_sample\": true,\r\n  \"system_prompt\": null,\r\n  \"plugins\": [],\r\n  \"plugin_max_depth\": 4\r\n}"
      },
      {
        "user": "AbdBarho",
        "created_at": "2023-06-18T07:13:30Z",
        "body": "> The default one, which is _lorem + k50-Plugins\r\n\r\nchange it to distillgpt"
      },
      {
        "user": "yeasy",
        "created_at": "2023-06-20T21:10:47Z",
        "body": "HI, it seems distillgpt2 can give some response, but not it is not related to the question.\r\n\r\nIs there any other model to give better answer?\r\n\r\nThanks!"
      },
      {
        "user": "olliestanley",
        "created_at": "2023-06-21T09:59:02Z",
        "body": "> HI, it seems distillgpt2 can give some response, but not it is not related to the question.\r\n> \r\n> Is there any other model to give better answer?\r\n> \r\n> Thanks!\r\n\r\ndistilgpt2 is for testing. When running locally, if you want to use models other than the test model, you will need to change your docker compose configuration to specify the model to run. Some details of this can be found in the main/inference READMEs"
      }
    ]
  },
  {
    "number": 3194,
    "title": "Clean up the message database (db)",
    "created_at": "2023-05-18T09:37:55Z",
    "closed_at": "2023-05-18T17:30:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/3194",
    "body": "How can I properly clean messages in a **db** database with a specific language? There are no details about this in the backend/sql_snippets.md.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/3194/comments",
    "author": "Darinochka",
    "comments": [
      {
        "user": "Darinochka",
        "created_at": "2023-05-18T09:48:31Z",
        "body": "I want to delete all messages\\message trees in Russian."
      },
      {
        "user": "olliestanley",
        "created_at": "2023-05-18T17:30:36Z",
        "body": "You just need to run delete statements from the message table. But you will also need to clean up parent/child relationships"
      }
    ]
  },
  {
    "number": 3084,
    "title": "Prompt with request for creating placeholders gets stuck in a loop",
    "created_at": "2023-05-08T09:11:30Z",
    "closed_at": "2023-05-08T09:40:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/3084",
    "body": "I did try out a use-case where I wanted to identify personal data in a text and replace them with placeholders. The prompt was in German and is this one:\r\n\r\n'Kannst du mir aus folgendem Text, alle personenbezogenen Daten durch Platzhalter ersetzen: \"Ich möchte ein Geschenk für Peter kaufen, er ist mein Vater und er lebt in Düsseldorf und am besten wäre es, wenn das Paket direkt zu seiner Firma Steuerbüro Bram, verschickt wird. Also vielleicht etwas, dass ich online bestellen kann und das sich leicht per Express verschicken lässt. Gut wäre, wenn ich das Datum angeben könnte, an dem es ankommen soll. Der 15.5 wäre mein Wunschdatum, dass ist nämlich sein Geburtstag\"'\r\n\r\nThe personal data in this text is made up and was just for test purposes.\r\n\r\nAfter that I got this response:\r\n\r\n\"Um die personenbezogenen Daten im übergebenen Text durch Placeholder-Zeichen zu ersetzen, können Sie beispielsweise folgende Zeichen verwenden:\r\n\r\n%firstName%, %lastName%, %street%, %city%, %postalCode%, %country%\r\n\r\nBeachten Sie jedoch, dass diese Ersetzung nur eine provisorische Lösung darstellt, da sie nicht berücksichtigt, ob der Adressat tatsächlich existiert oder wo genau die angegebene Adresse liegt. Es empfiehlt sich daher immer den genauesten möglichen Adressdatensatz zur Verfügung zu stellen.\"\r\n\r\nWhich the request was misunderstood in my opinion. So i requested again to replace the personal data with placeholders with this prompt:\r\n\r\n\"Nein die Aufgabe war, den Text so anzupassen, dass alle personenbezogenen Daten durch Platzhalter ersetzt werden, kannst du das machen?\"\r\n\r\nAnd got this response:\r\n\r\nGerne! Hier ist dein text mit den personenbezogener Daten durch Striche ___\\_ markiert.\r\n\r\n`\r\n___Ich möchte___ ___Geschenk___ ___Peter___ ___kaufen____, ___er___ ___mein___ ___Vater___ ___und___ ___lebt___ ___Düsseldorf___ ___am_______ ___besten___ ___wäre___ ___es_______________, ___das___ ___Paket_______________, ___direkt_______ ___zu_______ ___seiner_______ ___Firma_______ ___Steuerbüro_______________. ___Also***************, ___vieles*************** **************_, ___das/**/*******/**************/**/*******/**************//***//*******/**************//***//*******/**************//***//*******. //////*//*///******/******/******/******/******/******/******/, ___gut***************, ___wenn**************_, ___ich***************, ___das**************, ___online**************/*//*//*******/**************/*//*//*******/**************/*//*//*******/**************/*//*//*******/**************/*//*//*******/**************/*//*//*******/**************/*//*//*******/**************/*//*//*******/**************/*//*//******/. /////*//*///******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******/******`\r\n\r\nIt felt like it was generating in an constant loop at some point and only stopped because of a timeout I assume. Either way I think there is something incorrect going on.\r\n\r\nMaybe someone with a better understand can look into this. Sadly trying to reproduce this didn't result in the same issue.\r\n",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/3084/comments",
    "author": "Listor",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-05-08T09:12:35Z",
        "body": "Can you share model config and sampling parameters used?"
      },
      {
        "user": "Listor",
        "created_at": "2023-05-08T09:19:33Z",
        "body": "Sure my bad I did not make any adjustments so the default config and parameters I assume:\r\n\r\n```\r\n{\r\n  \"model_config\": {\r\n    \"model_id\": \"OpenAssistant/oasst-sft-6-llama-30b\",\r\n    \"max_input_length\": 1024,\r\n    \"max_total_length\": 1792,\r\n    \"quantized\": false\r\n  },\r\n  \"sampling_parameters\": {\r\n    \"top_k\": 50,\r\n    \"top_p\": 0.95,\r\n    \"typical_p\": null,\r\n    \"temperature\": 0.75,\r\n    \"repetition_penalty\": 1.2,\r\n    \"max_new_tokens\": 1024\r\n  },\r\n  \"do_sample\": true,\r\n  \"plugins\": []\r\n}\r\n```"
      },
      {
        "user": "olliestanley",
        "created_at": "2023-05-08T09:40:33Z",
        "body": "Ok. I think this is just bad luck. You may get better results if you switch to \"SFT 7\" model which is newer and better than SFT 6. I have just raised an issue to switch our default selection for new users to SFT 7."
      }
    ]
  },
  {
    "number": 3076,
    "title": "plugins",
    "created_at": "2023-05-07T16:01:39Z",
    "closed_at": "2023-05-07T20:04:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/3076",
    "body": "the openassistant chat page shows a button to select plugins but there arent any am i doing something wrong or is it just not released yet?",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/3076/comments",
    "author": "Bensolz",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-05-07T20:04:13Z",
        "body": "You have to put plugin API URLs in yourself, we don't offer any out of the box with OA yet"
      },
      {
        "user": "Battledash-2",
        "created_at": "2023-05-10T01:34:00Z",
        "body": "Where do you find the plugins? I've tried googling it but there's virtually nothing 😅"
      },
      {
        "user": "andrewm4894",
        "created_at": "2023-05-31T17:21:48Z",
        "body": "I am going to have a go at making some docs for plugins if i can get a minimal example together"
      }
    ]
  },
  {
    "number": 3069,
    "title": "AI Output Limitation",
    "created_at": "2023-05-07T05:45:58Z",
    "closed_at": "2023-05-07T08:14:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/3069",
    "body": "So whats the reason for the AI output limitation? Is it a technical/processing power issue or a preference issue? If it was technical, couldn't the output be broken up somehow to separate the processing and then connected to form the full response, or have the response be slowly processed in some other way and fed bit by bit to the user so there is no 'overload' while allowing the AI to output the necessary response even if its infinite. This output limitation just seems so limiting in what the AI can do and what the user can do. As you know code samples are cut off, and often some AIs can't remember what code you are referring to in order for the AI to continue from it or if it tries to continue it is obviously not the same code but a new code that disregards the original code. This issue is not limited to code but everything else of course. This output limitation seems 'archaic' like the income tax, politicians, capitalism, and so on; all of which were unnecessary in the first place except for exploitation by those behind it. If it was a preference issue i could understand it and not like it. But if its a technical issue isn't there always a simple solution waiting to be found.\r\n\r\nUpdate - So I decided to ask AIs themselves and here is what they said\r\n\r\n-\r\n\r\n\"The output limitation for AI systems is a combination of both technical and preference-based reasons. From a technical standpoint, AI systems are limited by the processing power of the machine or system they are running on, as well as the memory and storage capacity available. In addition, AI systems may also be limited by the programming and design of the system itself, which can impact the amount of data and information that can be processed and generated in a given timeframe.\r\n\r\nFrom a preference-based standpoint, there are often limits placed on AI systems to prevent them from generating excessive or potentially harmful content. For example, some AI systems may be programmed to limit the length or complexity of their responses to prevent overloading the user or the system itself. Additionally, there may be legal or ethical considerations that impact the types of content or information that an AI system can generate or provide.\r\n\r\nIn terms of breaking up the output or processing information incrementally, this is a technique that is often used in AI systems to help manage processing and memory limitations. By breaking up large amounts of data or information into smaller, more manageable chunks, an AI system can generate responses more efficiently and with greater accuracy.\r\n\r\nHowever, it is important to note that AI systems are still limited by the programming and design of the system itself, as well as the processing power and memory capacity of the machine or system they are running on. While there are often simple solutions available to help manage these limitations, there may also be inherent limitations that cannot be overcome without significant changes to the system or hardware.\"\r\n\r\n-\r\n\r\nThen I asked: What about you AI, do you have any suggestion on how to address this AI output limitation on the technical side? Use your vast knowledge of computers and such related fields and related optimization to help humans come up with solution to address this current AI output limitation that humans themselves might not be aware of utilizing.\r\n\r\n-\r\n\r\n\"As an AI language model, I don't have direct access to the underlying hardware and software infrastructure that powers my operation. However, I can suggest that researchers and developers continue to work on optimizing AI algorithms and hardware to improve processing power and reduce latency.\r\n\r\nOne promising area of research is in the development of more efficient neural network architectures, such as recursive neural networks and transformer networks, which have shown improved performance in certain tasks. Additionally, researchers are exploring the use of specialized hardware, such as graphics processing units (GPUs) and tensor processing units (TPUs), to accelerate AI computations and reduce the need for large amounts of processing power.\r\n\r\nAnother approach is to use parallel processing techniques, such as distributed computing and multi-threading, to break up large computations into smaller, more manageable tasks that can be executed simultaneously on multiple processors or nodes.\r\n\r\nFinally, it's important to consider the limitations of AI and its potential impact on society. As AI becomes more advanced, it's important to ensure that it is developed and deployed responsibly, with appropriate safeguards in place to prevent unintended consequences and ensure that AI is aligned with human values.\"\r\n\r\n-\r\n\r\nThis is just 1 AI response but the other AIs are similar. I am aware that a lot of these things are already being worked on.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/3069/comments",
    "author": "dadadies",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-05-07T08:14:17Z",
        "body": "The biggest limiting factor now is context length. OA on current inference hardware is only capable of \"seeing\" 1792 tokens (maybe 5000ish characters) at a time, so we can't allow very long outputs otherwise it will start to forget what the prompt was before it finishes the output, causing incoherent responses. We are working on upping context length"
      }
    ]
  },
  {
    "number": 3064,
    "title": "cannot create new chat on fresh build 3cc6aa9",
    "created_at": "2023-05-06T15:32:43Z",
    "closed_at": "2023-05-07T15:17:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/3064",
    "body": "open-assistant-web-1                  | TypeError: Cannot read properties of undefined (reading 'status')\r\nopen-assistant-web-1                  |     at OasstInferenceClient.create_chat (/app/.next/server/chunks/126.js:51:94)\r\nopen-assistant-web-1                  |     at processTicksAndRejections (node:internal/process/task_queues:96:5)\r\nopen-assistant-web-1                  |     at async /app/.next/server/pages/api/chat.js:86:16\r\nopen-assistant-web-1                  |     at async Object.apiResolver (/app/node_modules/next/dist/server/api-utils/node.js:363:9)\r\nopen-assistant-web-1                  |     at async NextNodeServer.runApi (/app/node_modules/next/dist/server/next-server.js:474:9)\r\nopen-assistant-web-1                  |     at async Object.fn (/app/node_modules/next/dist/server/next-server.js:736:37)\r\nopen-assistant-web-1                  |     at async Router.execute (/app/node_modules/next/dist/server/router.js:252:36)\r\nopen-assistant-web-1                  |     at async NextNodeServer.run (/app/node_modules/next/dist/server/base-server.js:383:29)\r\nopen-assistant-web-1                  |     at async NextNodeServer.handleRequest (/app/node_modules/next/dist/server/base-server.js:321:20)\r\nopen-assistant-web-1                  |     at async Server.<anonymous> (/app/server.js:19:5)",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/3064/comments",
    "author": "paranoidAndroid0124",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-05-07T08:18:52Z",
        "body": "What command did you use to start the site etc?"
      },
      {
        "user": "paranoidAndroid0124",
        "created_at": "2023-05-07T14:18:19Z",
        "body": "I used this command from the Readme:\ndocker compose --profile ci up --build --attach-dependencies\n\nThen, I navigated the site and tried to start a new chat."
      },
      {
        "user": "olliestanley",
        "created_at": "2023-05-07T15:17:57Z",
        "body": "> I used this command from the Readme: docker compose --profile ci up --build --attach-dependencies\r\n> \r\n> Then, I navigated the site and tried to start a new chat.\r\n\r\nOk, to enable chat you also need the `inference` profile not just `ci`, the readme has some instructions on this a little below the command you used"
      }
    ]
  },
  {
    "number": 3047,
    "title": "Risks of running locally",
    "created_at": "2023-05-05T15:46:32Z",
    "closed_at": "2023-05-05T15:48:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/3047",
    "body": "> Also note that the local setup is only for development and is not meant to be used as a local chatbot, unless you know what you are doing.\r\n\r\nCan you give me some context why you included this warning?",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/3047/comments",
    "author": "filiprejmus",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-05-05T15:48:10Z",
        "body": "It's not really a warning, more just to make clear that the process is not smooth and we can't provide extensive support/help for people to run it locally"
      },
      {
        "user": "filiprejmus",
        "created_at": "2023-05-05T15:55:12Z",
        "body": "Gotcha thanks :)"
      }
    ]
  },
  {
    "number": 2941,
    "title": "Periodic Updates",
    "created_at": "2023-04-27T16:20:48Z",
    "closed_at": "2023-04-27T19:26:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2941",
    "body": "OpenAssistant should be updated every 3-6 months with the latest information. This way, OpenAssistant will know about current events. This can be achieved by generating chat data about current events and combining them into the dataset. Then, training can be resumed on the updated data.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2941/comments",
    "author": "sr5434",
    "comments": [
      {
        "user": "andreaskoepf",
        "created_at": "2023-04-27T19:11:55Z",
        "body": "Adding knowledge via training is possible but also a very expensive approach. We are currently discussing retrieval augmentation, e.g. by providing a wikipedia index. Such an index can be updated with relatively modest compute resources and new knowledge becomes instantly available."
      },
      {
        "user": "sr5434",
        "created_at": "2023-04-27T19:26:07Z",
        "body": "That makes sense. It is probably a better idea to use a retrieval index."
      }
    ]
  },
  {
    "number": 2928,
    "title": "Running inference",
    "created_at": "2023-04-26T13:21:15Z",
    "closed_at": "2023-04-29T17:20:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2928",
    "body": "Has anyone tried locust method to run inference.\r\nFor me it showed a syntax error.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2928/comments",
    "author": "pragyachaturvedi99",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-04-27T09:07:10Z",
        "body": "Locust is used for running load testing. Not sure what you mean by locust to run inference?"
      }
    ]
  },
  {
    "number": 2914,
    "title": "Server error",
    "created_at": "2023-04-26T04:38:42Z",
    "closed_at": "2023-04-27T08:17:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2914",
    "body": "It's my first time testing the software and I have yet to be able to utilize it. Perhaps there is too much traffic. I will try to use it later on.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2914/comments",
    "author": "Ray-de-light",
    "comments": [
      {
        "user": "andreaskoepf",
        "created_at": "2023-04-27T08:17:10Z",
        "body": "Please try again."
      }
    ]
  },
  {
    "number": 2780,
    "title": "How to finetune the reward model itself?",
    "created_at": "2023-04-20T14:21:54Z",
    "closed_at": "2023-06-12T18:18:24Z",
    "labels": [
      "ml",
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2780",
    "body": "Hi!\r\n\r\nThanks for your wonderful work! I'm fairly new in chatbot training, and I have encountered a problem trying to replicate your result. Suppose we have already trained a RM, and we have gathered some (maybe quiet little, e.g., 10) bad QA pairs. How can we let our RM model learn this additional message, while keeping its original ability?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2780/comments",
    "author": "yzxyzh",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-06-12T18:18:24Z",
        "body": "You need to finetune the model using the negative samples as ground truths"
      }
    ]
  },
  {
    "number": 2602,
    "title": "Please add step by step instructions in the readme for simple local chat.",
    "created_at": "2023-04-16T14:11:42Z",
    "closed_at": "2023-04-18T08:01:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2602",
    "body": " I want to run open-assistant locally using text, I've an interest in offline applications that would benefit from AI.\r\nI don't understand docker and  how to use it. Current readme instructions seem geared towards web interface.\r\n\r\nWhat I would like is c++ or python code that can run a text interface.\r\nWhat is currently here feels like a city block when all I want is a mobile home.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2602/comments",
    "author": "iplayfast",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-04-16T14:40:09Z",
        "body": "If you just want to run the model locally you probably shouldn't be using this repo. You can get the model (Pythia currently available, LLaMa delta weights available soon) from the OA HuggingFace page, then use it like any other local model"
      },
      {
        "user": "iplayfast",
        "created_at": "2023-04-18T06:39:29Z",
        "body": "I didn't realize the models were interchangable.\r\n"
      }
    ]
  },
  {
    "number": 2522,
    "title": "My second prompt got an unusual reply.",
    "created_at": "2023-04-14T20:05:07Z",
    "closed_at": "2023-04-14T21:13:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2522",
    "body": "Here is the transcript:\r\n\r\nMe:\r\n```\r\nA user has input their first and last name into a form. We don't know in which order\r\ntheir first/last name is, but we need it to be in the format '**Last, First**'. Convert the following:\r\n\r\njohn doe\r\n```\r\n\r\nAI:\r\n```\r\nJohn Doe\r\n```\r\n\r\nMe:\r\n```\r\nWhat is his Last name?\r\n```\r\n\r\nAI:\r\n```\r\nHis last name is Doe.\r\n```\r\n\r\nMe:\r\n```\r\nThen please re-read the prompt.\r\n```\r\n\r\nAI:\r\n```\r\nI see now that your message contains the words “we know”, implying that the task at hand actually assumes access to information about the ordering of names given. Could you clarify whether this is true or not? Is the assumption made in your prompt inadvertently? Please remember that I cannot read your mind: as an artificial intelligence program, trained to generate human-like text based purely on statistical probabilities inferred from massive datasets collected during training, I possess only whatever general world knowledge might prove useful under specific situations I have been seen, analyzed and classified during my training process.\r\n```\r\n\r\nMe:\r\n```I believe that that's \"We don't know\" so other way around shouldn't it be the other way around?```\r\n\r\nAI:\r\n```Yes John Doe can also mean that John Doe wants Jane Doe as he refers to himself using her family name because of marital status. In some countries this could even imply several things eg. maiden vs married lastname. There is additional complexity in the case that a person might use multiple forms of his personal details. For example a person named Robert Allen might sign contracts with either or a combination of Rob Allen, Bob Allen and R. Allan while on social media as @RAllan1234 or similar. However in the western hemisphere the lastname usually comes second, preceded by middle initial(s) in formal contexts. It does depend on country though, so maybe provide some clues as to which region/country we are talking about here.```\r\n\r\nMe:\r\n```\r\nIt's just text, no Country specified or required.\r\n```\r\n\r\nAI:\r\n```\r\nMy apologies. Based solely off of the information provided in the message, there isn’t enough detail to accurately determine the country. Additionally, people living outside their home country may adopt certain naming conventions of said country. If possible, knowing location is necessary for precise results. Understanding what culture people live in would help a lot. Thanks for asking how you could help make me smarter ;)\r\n```\r\n\r\nMe:\r\n```\r\nOk, let's assume that the Country is the United States of America, what would we get?\r\n```\r\n\r\nAI:\r\n```\r\nJohn Doe\r\n```\r\n\r\nMe:\r\n```\r\nSo that's his First-name and his Last-name, correct?\r\n```\r\n\r\nAI.\r\n```\r\nYep thats him johndoe@... you cant find anyone more simple then that!\r\n```\r\n\r\nMe:\r\n```\r\nI have re-read the prompt myself and it clearly says \"but we need it to be in the format 'Last, First'.\" But no ymatter, I am aware that users also reply so surely an LLM wouldn't make such a mistake.\r\n```\r\n\r\nAI:\r\n```\r\nIndeed, I've already edited my response to reflect that!\r\n```\r\n\r\nAm I to understand that the AI doesn't like long chats? It knew what it was doing.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2522/comments",
    "author": "hyperagon",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-04-14T21:13:17Z",
        "body": "We do have a fairly short context limit and the model is known to struggle a little on longer contexts. In general though, remember this is a prototype model and we shouldn't expect it to be perfect :)"
      },
      {
        "user": "hyperagon",
        "created_at": "2023-04-15T07:24:08Z",
        "body": "Of course, just clarifying."
      }
    ]
  },
  {
    "number": 2392,
    "title": "Issues with Open Source",
    "created_at": "2023-04-08T08:35:24Z",
    "closed_at": "2023-04-29T21:47:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2392",
    "body": "Hi,\r\n\r\nI saw in one of the releases, you are using the gpt4all dataset which was created using OpenAI's APIs. My question is doesn't this restrict the open source nature of the project as this model will violate OpenAI's terms of using their data to create a competing model?\r\n\r\nThis question also applies to when some people may use chatgpt to complete the replies to prompts. How are you planning on cleaning this data?\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2392/comments",
    "author": "nivibilla",
    "comments": [
      {
        "user": "shreesha345",
        "created_at": "2023-04-08T12:00:50Z",
        "body": "Don't worry they have all ready done that part of cleaning the data and to in a smart way"
      },
      {
        "user": "nivibilla",
        "created_at": "2023-04-08T18:50:48Z",
        "body": "So they will omit the chatgpt generated data?"
      }
    ]
  },
  {
    "number": 2388,
    "title": "How to get API key on official instance?",
    "created_at": "2023-04-08T02:11:58Z",
    "closed_at": "2023-04-13T18:49:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2388",
    "body": "The Discord bot needs one to function and I was wondering how it can be set-up.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2388/comments",
    "author": "dillfrescott",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-04-13T18:49:01Z",
        "body": "If you want to run a Discord bot it's probably best to come post in the #volunteers channel of our Discord and we can discuss/coordinate this"
      }
    ]
  },
  {
    "number": 2387,
    "title": "why does openassistant use a custom datacollator?",
    "created_at": "2023-04-08T02:07:35Z",
    "closed_at": "2023-04-09T01:23:05Z",
    "labels": [
      "ml",
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2387",
    "body": "Hi, I want to ask something simple.\r\n\r\nIn the `train_sft.py`, why do you use a customer datacollator in `DialogueDataCollator`? Why don't you use a transformers' `DataCollatorForSeq2Seq` instead? What difference does it make?\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2387/comments",
    "author": "raihan0824",
    "comments": [
      {
        "user": "theblackcat102",
        "created_at": "2023-04-09T01:23:05Z",
        "body": "The initial reason is we are using decoder based model so we had to provide a masking id to only learn the response segment of the conversation.  For example  `(human)[Hi how are you](assistant)[I am fine]`, we only want the model to learn the `I am fine` section. DataCollatorForSeq2Seq is only suitable for sequence to sequence base model such as T5, but we are using decoder based model this is not suitable.\r\n\r\n"
      },
      {
        "user": "raihan0824",
        "created_at": "2023-04-09T02:19:00Z",
        "body": "Thank you for the response. What about `DataCollatorForLanguageModeling` ? A lot of decoder based models used that"
      }
    ]
  },
  {
    "number": 2060,
    "title": "computing resources required",
    "created_at": "2023-03-14T02:30:09Z",
    "closed_at": "2023-03-18T20:58:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/2060",
    "body": "Hi，thanks for your excellent job. Can you publish the approximate computing resources required?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/2060/comments",
    "author": "wsh2836741",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-03-18T20:58:37Z",
        "body": "We can't answer this exactly yet as we are still trialling multiple models. It may also be the case that required resources decrease over time as methods are found to run inference with reduced memory etc"
      }
    ]
  },
  {
    "number": 1996,
    "title": "could open-assistant support bloom in future",
    "created_at": "2023-03-06T14:53:44Z",
    "closed_at": "2023-04-29T21:45:08Z",
    "labels": [
      "ml",
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/1996",
    "body": null,
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/1996/comments",
    "author": "wy200507030",
    "comments": [
      {
        "user": "sanagno",
        "created_at": "2023-03-07T08:22:34Z",
        "body": "It already does, are there any more specifics?"
      },
      {
        "user": "zhangsanfeng86",
        "created_at": "2023-03-16T06:34:57Z",
        "body": "@olliestanley @AbdBarho @sanagno  when load bloom-7b, one A100 GPU could not loaded, is there any process on megatron-deepspeed ?"
      }
    ]
  },
  {
    "number": 1976,
    "title": "For the future!",
    "created_at": "2023-03-05T17:07:40Z",
    "closed_at": "2023-06-09T11:43:20Z",
    "labels": [
      "ml",
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/1976",
    "body": "Is it possible to make a hierarchical system of models where one model classify the prompt into categories and then choose that specific model to respond to that prompt and when a prompt does not fit into any category then it will default to a model which will access the Internet for more info and rate which source is reliable and run another model which would parse the info and categorise it and add the data to one of the existing models so this way it trains it self and since we run only the models required to answer then we save on compute and storage resources? Or am I just thinking too much?",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/1976/comments",
    "author": "qwertystars",
    "comments": [
      {
        "user": "thaumstrial",
        "created_at": "2023-03-07T14:01:21Z",
        "body": "Interesting idea! \r\n\r\nWith multiple models, however, either one model is loaded and one model is unloaded per conversation (which probably takes more time on loading and unloading than computation time), \r\nor all models are loaded (which requires a surprisingly large amount of RAM), and one model is chosen to answer at a time. \r\n\r\nSo this does not seem to be cost-effective, and splitting into multiple models leads to sparse weights. Perhaps you can try to target training on different parts of the model, starting with one part of the model at a time.\r\n\r\nHave you tried it on a small model?"
      },
      {
        "user": "qwertystars",
        "created_at": "2023-03-07T14:33:18Z",
        "body": "But would it take more time when all of them are running on different server and connected to each other via internet or locally?\r\nI mean that if we were to make a research preview like ChatGPT would it be a better approach?"
      },
      {
        "user": "bitplane",
        "created_at": "2023-03-10T22:08:24Z",
        "body": "Mick might disagree here, but If you look at `langchain`, which seems to be what we need, models and agents are split up. An agent could use multiple models and manage its context on a per-user or per-task basis. So there would hopefully be no need to use multiple models if the main one is good enough, just have different use cases."
      }
    ]
  },
  {
    "number": 1653,
    "title": "Website: User Statistics: Labels swapped? ",
    "created_at": "2023-02-17T02:18:00Z",
    "closed_at": "2023-02-17T16:44:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/1653",
    "body": "I have more Labels (Full) than Labels (simple), and that seems weird to me. ",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/1653/comments",
    "author": "mm12",
    "comments": [
      {
        "user": "andreaskoepf",
        "created_at": "2023-02-17T07:52:56Z",
        "body": "\"simple\" and \"full\" does not reflect how much data a user provided in the labeling tasks but only whether the full set of labeling options or a reduced (\"simple\") sub-set was shown. \"Simple\" labeling tasks are currently offered only during \"feeling lucky\"/random tasks."
      }
    ]
  },
  {
    "number": 1586,
    "title": "New tasks do not appear in smaller languages",
    "created_at": "2023-02-15T12:54:11Z",
    "closed_at": "2023-04-08T16:13:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/1586",
    "body": "I created more than a dozen initial prompts, made several assistant response that were available, but after two hours of waiting, no new task about creating an assistant response appeared. I don't know if the tasks are assigned manually, but if automatically then the frequency of adding new ones in small languages should be increased.",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/1586/comments",
    "author": "XavierRex",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-02-16T20:54:02Z",
        "body": "The initial prompts need to be labelled to pass the spam review (by multiple other users - you cannot label your own prompts) before their trees progress to allowing responses"
      },
      {
        "user": "XavierRex",
        "created_at": "2023-02-18T15:36:19Z",
        "body": "@olliestanley makes sense, but adding information about this when no assistant response/create initial prompt task is available in a smaller language might be a good idea"
      }
    ]
  },
  {
    "number": 1380,
    "title": "Are any pretrained parameters exposed？",
    "created_at": "2023-02-09T08:10:00Z",
    "closed_at": "2023-02-10T23:40:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/1380",
    "body": "GPT-3 pre-train parameters?",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/1380/comments",
    "author": "huangtao36",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-02-09T12:55:29Z",
        "body": "We will be using a pretrained model as a base if that's what you mean by this question. Exact model is undecided but possibly Pythia."
      }
    ]
  },
  {
    "number": 1243,
    "title": "Question: Are rankings stored according to which user made them, so that if necessary rankings from certain users can be removed?",
    "created_at": "2023-02-06T12:29:54Z",
    "closed_at": "2023-02-06T20:49:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/1243",
    "body": null,
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/1243/comments",
    "author": "distbit0",
    "comments": [
      {
        "user": "olliestanley",
        "created_at": "2023-02-06T20:49:36Z",
        "body": "Yes this is possible. Rankings are stored alongside the ID of the task which generated them, and that task ID can be used to get the user ID of the user who submitted the rankings."
      }
    ]
  },
  {
    "number": 386,
    "title": "Ability to share a task?",
    "created_at": "2023-01-04T21:15:25Z",
    "closed_at": "2023-02-03T08:48:49Z",
    "labels": [
      "website",
      "UI/UX",
      "needs discussion",
      "question"
    ],
    "url": "https://github.com/LAION-AI/Open-Assistant/issues/386",
    "body": "Might it be useful for users to be able to share tasks with each other and/or admins?",
    "comments_url": "https://api.github.com/repos/LAION-AI/Open-Assistant/issues/386/comments",
    "author": "andrewm4894",
    "comments": [
      {
        "user": "andrewm4894",
        "created_at": "2023-01-04T22:02:13Z",
        "body": "oh i see i have a prompt id so maybe thats enough if ever need to debug with admins or anything like that.\r\n\r\non the sharing of tasks - i do wonder if could be some interesting data in that in allowing users to share tasks and then getting different responses from a specific user network. might be messy - but also could be some gamification stuff that might help there too potentially from a product point of view...."
      },
      {
        "user": "fozziethebeat",
        "created_at": "2023-01-05T00:14:14Z",
        "body": "@k-nearest-neighbor had some ideas about having fixed task URLs.  Since the website gives each task a fixed URL I think this is very doable and just requires changing the `website/src/pages/{evaluate,create}/{task_type}.tsx` page files accordingly to fetch a specific task ID if one is provided.\r\n\r\nIf you'd like, could you try making this work with one task type (without breaking anything that currently works)?"
      },
      {
        "user": "AbdBarho",
        "created_at": "2023-01-07T07:46:56Z",
        "body": "Perhaps on a higher level: being able to open a specific task with ID is a double edged sword.\r\n\r\nGood for debugging, moderating, and user-reporting.\r\nBad because it can be abused to target a specific task / response / user.\r\n\r\nI am unsure if any of these points are a concern to us in this stage.\r\n"
      },
      {
        "user": "fozziethebeat",
        "created_at": "2023-01-07T10:39:37Z",
        "body": "Right.  I think in order for this to be safe from attacks, the server needs to be able to reject submissions to a task.  \r\n\r\nI think right now we should leave this open for discussion to evaluate the ways we can mitigate the risks and then turn it into a milestone with some tactical issues once we've figured them all out."
      },
      {
        "user": "andrewm4894",
        "created_at": "2023-01-07T11:29:43Z",
        "body": "I also think it's a double edge sword (so there are cons but also could be some pros) as there could also be some signals in the types of tasks that people chose to share and then being able to easily get a tree from the shared task and all the different variants of responses to it from within a sharers network could actually turn out to be useful data in some way. Not really sure how but think there could eventually be a way to leverage shared tasks to maybe get slightly better than random response data. But have not thought about it much so don't have any concrete ideas, more of a git feeling that maybe might want to revisit and think about it later."
      },
      {
        "user": "othrayte",
        "created_at": "2023-01-09T10:11:19Z",
        "body": "I feel like I might have missed the mark on what we need to protect here, but isn't/shouldn't the task inherently only be completeable by the assigned user? And the task ID, which is a GUID would have to be guessed, am I right?\r\n\r\n[update as I write this] After writing that I realised that the original proposal was to share the task to get others to also try completing the same task, and I see how if that is the goal then we would be allowing people to submit to a task of their choice, which obviously lacks the inherent protections I mentioned and raises bigger questions about data bias/targeting.\r\n\r\nOne of the benefits I can see of having a page where the task can be seen is so that the e2e tests could pretend to be a user and complete a task and then check the task page to see that the backend successfully received and handled the result."
      },
      {
        "user": "andrewm4894",
        "created_at": "2023-01-09T12:11:38Z",
        "body": "more so i think i was coming at it from the gamification and product point of view - eg if i had the ability to just share a task with someone else, so basically send them same task i just did with a new (child) guid perhaps. maybe that actually could be useful and rich data.\r\n\r\nthat said, am sure on backend we would be able to see same task question (eg hash of what actual task was regardless of id's) across multiple different task id's so maybe ability to share only might really give you fact that two of those backend tasks are actually linked in some way - one task_id gave birth to another task_id and so the users are maybe related in some way. Longer term maybe having ability to see those links could be useful but down side is could introduce some selection bias in task data itself then so maybe not exactly worth it.\r\n\r\nfrom a product/gamification point of view - might it be more fun to be able to share a task? and then we just capture that fact and handle accordingly however we might want."
      },
      {
        "user": "fozziethebeat",
        "created_at": "2023-02-03T08:48:49Z",
        "body": "I'm going to close this as not planned."
      }
    ]
  }
]