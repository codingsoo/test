[
  {
    "number": 3712,
    "title": "[BUG] Bad Auth when trying to run a Chatflow",
    "created_at": "2024-12-15T23:07:24Z",
    "closed_at": "2025-01-26T20:50:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/3712",
    "body": "So i recently spin up a docker container in the cloud running Flowise\r\n\r\nI built my first flow and and trying to access it via API and am getting this error:\r\n\"statusCode\": 500, \"success\": false, \"message\": \"Error: predictionsServices.buildChatflow - bad auth : authentication failed\",\r\n\r\nAll things ive done\r\n\r\nMade sure ports 443 and 80 are open on the ubuntu server running docker\r\n\r\nAllowed the IP I'm trying to access the API on through the firewall to the server\r\n\r\nTried creating an API key and adding that to the header for authorization\r\n\r\nRemoved App level login\r\n\r\nDoes anyone know what might be causing this??",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/3712/comments",
    "author": "frickgav",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-12-18T01:16:56Z",
        "body": "I dont recall us having this specific error message: `bad auth : authentication failed`, I'd suspect it might be the underlying model or component that you were using is having the wrong credential"
      },
      {
        "user": "frickgav",
        "created_at": "2024-12-18T05:03:51Z",
        "body": "\r\n> I dont recall us having this specific error message: `bad auth : authentication failed`, I'd suspect it might be the underlying model or component that you were using is having the wrong credential\r\n\r\nYeah so it was actually a MongoDB Error. "
      }
    ]
  },
  {
    "number": 3613,
    "title": "[BUG] ChatOllama with function is missing",
    "created_at": "2024-11-30T20:20:32Z",
    "closed_at": "2024-12-12T02:26:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/3613",
    "body": "@HenryHengZJ \r\n\r\n**Describe the bug**\r\nI installed flowise with Docker using docker-compose.yml: \r\n```\r\nPress ENTER or type command to continue\r\nversion: '3.1'\r\n\r\nservices:\r\n  flowise:\r\n    image: flowiseai/flowise:latest\r\n    restart: always\r\n    environment:\r\n      - PORT=${PORT}\r\n      - FLOWISE_USERNAME=${FLOWISE_USERNAME}\r\n      - FLOWISE_PASSWORD=${FLOWISE_PASSWORD}\r\n    ports:\r\n      - '${PORT}:${PORT}'\r\n    volumes:\r\n      - ~/.flowise:/root/.flowise\r\n    entrypoint: /bin/sh -c \"sleep 3; flowise start\"\r\n```\r\n\r\n**To Reproduce**\r\n1) Execute `docker compose up -d`\r\n2) Point a browser (Safari) to `localhost:3000`\r\n3) Search for Ollama in both agents and chat nodes, and observe that Chat Ollama with Functions is not present. \r\n\r\nHow do I fix this issue? Without using Docker, I can use Flowise, but the Marketplace is a blank page. On the other hand, Ollama Chat with functions is present. \r\n\r\n\r\n**Expected behavior**\r\nI expect the same nodes to be available whether I use Docker or not to install Flowise. Perhaps the Docker file should be updated. \r\n\r\n**Setup**\r\n-   Installation [e.g. docker, `npx flowise start`, `pnpm start`]\r\n-   Flowise Version [e.g. 1.2.11]\r\n-   OS: macOS with Sequoia on M2. \r\n-   Browser: Safari\r\n\r\nI checked that without using Docker, ChatOllama with Functions is also not present. \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/3613/comments",
    "author": "erlebach",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-12-04T19:50:55Z",
        "body": "We've deprecated and removed ChatOllama Function, you can just directly use ChatOllama, its now compatible with functions calling"
      }
    ]
  },
  {
    "number": 3389,
    "title": "[BUG] Message stopped after start the program on local host",
    "created_at": "2024-10-21T01:20:26Z",
    "closed_at": "2024-12-12T02:12:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/3389",
    "body": "**Describe the bug**\r\nMessage stopped after start the program on local host. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/3389/comments",
    "author": "Julieyu7747",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-10-21T15:51:26Z",
        "body": "Can you list the steps how you install flowise? and steps to reproduce the issue"
      },
      {
        "user": "kryptologyst",
        "created_at": "2025-02-01T04:51:20Z",
        "body": "Had the \"Stopped Message\" error, until I realized my API key I used was not the current one. Hope that helps."
      }
    ]
  },
  {
    "number": 3383,
    "title": "[BUG] Conversational Retrieval QA Chain",
    "created_at": "2024-10-19T20:09:12Z",
    "closed_at": "2024-12-12T02:18:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/3383",
    "body": "**Describe the bug**\r\nI have a fully working RAG with Conversational Retrieval QA Chain and using Document Store (Vector) I have also tried direct in the flow with splitters and PDF document. It workes fine with Conversational Retrieval QA Chain with the additional parameters at first. When I ask something outside the PDF which is loaded then I get sorry answer that it can't find it but after some hours or day after when I check exact same thing Conversational Retrieval QA Chain doesn't seems to know anything in additional parameters as the model answers direct outside the document which is loaded. I seems that the additional parameters not working as it should. And the FAISS is used and the faiss.index is in it´s place so it works.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. After creating the document store which works after test I go to step 2.\r\n2. Chatflows\r\n3. Add the Conversational Retrieval QA Chain, document store (vector) ollama llm buffer memory and the chat works normally and everything from the document is working.\r\n4. There is no error but some time after the additional parameters in Conversational Retrieval QA Chain is not getting affected at all.\r\n\r\n-   Installation Docker Portainer\r\n-   Flowise Version 2.1.2\r\n-   OS: Docker Linux\r\n-   Browser all\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/3383/comments",
    "author": "doggi87",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-10-21T23:09:36Z",
        "body": "what do you mean by `additional parameters` ? and does it only happens with faiss? because faiss index file will get replaced entirely everytime you have a new upsert, it does not append, it replaces"
      }
    ]
  },
  {
    "number": 3067,
    "title": "[BUG] MODULE_NOT_FOUND : node_modules/openai/helpers/zod.js === since updating to 2.0.5",
    "created_at": "2024-08-23T16:26:53Z",
    "closed_at": "2024-11-08T20:28:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/3067",
    "body": "**Describe the bug**\r\nError on build time and run time : \r\n\r\n[MODULE_NOT_FOUND] ModuleLoadError Plugin: flowise: [MODULE_NOT_FOUND] require failed to load <project-path>/packages/server/dist/commands/start.js Cannot find module <project-path>/node_modules/.pnpm/@langchain+openai@0.2.7_encoding@0.1.13_langchain@0.2.17_4oakmayaajwtwwxwds3rutg7xa_/node_modules/openai/helpers/zod.js\r\n\r\nmodule: @oclif/core@1.26.2\r\ntask: toCached\r\nplugin: flowise\r\nroot: <project-path>/packages/server\r\n\r\nMore specifically I think this occurred after the commit number d5153c384076d889ed6b70a0d1db6b51a2d472b2 but I haven't rolled back commits yet to pinpoint the exact commit. Any help would be really appreciated, thanksss",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/3067/comments",
    "author": "Sarahkunk",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-08-27T17:30:04Z",
        "body": "is a clean install and fresh build works? \r\n```\r\npnpm i\r\npnpm clean\r\npnpm build-force\r\n```"
      },
      {
        "user": "sagar-jani",
        "created_at": "2024-10-11T04:43:37Z",
        "body": "I am getting the same error with pnpm and docker both."
      }
    ]
  },
  {
    "number": 2919,
    "title": "[Question] Obtain Chatlfow ID",
    "created_at": "2024-07-31T19:38:52Z",
    "closed_at": "2024-08-18T22:21:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2919",
    "body": "**Describe the feature you'd like**\r\nHi, i'm not sure if this is already an existing feature or if I should suggest it as a possible enhancement (I looked through the GitHub issues but couldn't find anything). **I'm trying to obtain the ID of the flows (the ID that appears in the URL when I generate a new flow). Where could I find or modify this ID?**\r\n\r\nThanks in advance.\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2919/comments",
    "author": "Bandikyu",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-08-01T15:27:31Z",
        "body": "this ID is not modifiable, in what scenario you trying to obtain the ID? through API?"
      }
    ]
  },
  {
    "number": 2874,
    "title": "[BUG] Expected property name or '}' in JSON at position 1",
    "created_at": "2024-07-24T16:44:30Z",
    "closed_at": "2025-01-26T20:42:20Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2874",
    "body": "**Describe the bug**\r\nWhen running a chatflow using ollamaembeddings and Redis VectorStore i am receiving this error :\r\n\r\nSyntaxError: Expected property name or '}' in JSON at position 1\r\n    at JSON.parse (<anonymous>)\r\n    at similaritySearchVectorWithScore (~\\Flowise\\packages\\components\\dist\\nodes\\vectorstores\\Redis\\Redis.js:276:44)\r\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\r\n    at async vectorStore.similaritySearchVectorWithScore (~\\Flowise\\packages\\components\\dist\\nodes\\vectorstores\\Redis\\Redis.js:214:20)\r\n    at async RedisVectorStore.similaritySearch (~\\Flowise\\node_modules\\.pnpm\\@langchain+core@0.2.14_langchain@0.2.3_@aws-sdk+client-s3@3.529.1_@aws-sdk+credential-provide_iy4pkl2ze2vpuxlmnivkyhhz5q\\node_modules\\@langchain\\core\\dist\\vectorstores.cjs:108:25)\r\n\r\n\r\n**To Reproduce**\r\nBuild a chatflow with \"Recursive Character Text Splitter\", Read from a folder, pass the document to a Redis vector store\r\nUse OllamaEmbeddings and use ChatOllama and Convesational Retrieval QA Chain\r\n\r\n**Expected behavior**\r\nThe pexected behavior is after upserting the files, when communicating with the Chatflow i want to receive results.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2874/comments",
    "author": "yyoussef11",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-07-25T23:49:14Z",
        "body": "does it works with other embeddings? or does ollama embedding work with other vector store"
      },
      {
        "user": "yyoussef11",
        "created_at": "2024-07-26T06:20:10Z",
        "body": "@HenryHengZJ actually when i changed the \"top k\" value of the Redis retriever from 1000 to 100, it didn't fail.\r\nNoting that the same \"top k\" value is used with another embedded data, and it worked.\r\n\r\nCould it be the amount of embedded data was not enough? \r\n\r\nto answer your questions: Ollama Embeddings worked with In Memory vector store"
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2024-08-18T20:55:36Z",
        "body": "I am guessing there might be some sort of rate limit from Redis? or from the embeddings side"
      }
    ]
  },
  {
    "number": 2787,
    "title": "[FEATURE] web scraper with custom tool agent ",
    "created_at": "2024-07-10T08:24:01Z",
    "closed_at": "2024-12-12T02:30:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2787",
    "body": "i have a web scraper and its working fine and i have create more than custom tools and i have connected them to custom tool agent \r\nthe question is can i used them with the same bot ?",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2787/comments",
    "author": "yahya-shalash",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-07-12T16:39:32Z",
        "body": "what do you mean by same bot? you can have a tool agent that connected to multiple custom tools"
      },
      {
        "user": "yahya-shalash",
        "created_at": "2024-07-16T10:09:13Z",
        "body": "> what do you mean by same bot? you can have a tool agent that connected to multiple custom tools\r\n\r\ni need in the same flow that im used custom tool i want to add a web scrapper \r\n"
      }
    ]
  },
  {
    "number": 2668,
    "title": "[BUG]Unexpected sync check result: window.synccheck={retcode:\"1101\",selector:\"0\"}",
    "created_at": "2024-06-19T03:53:43Z",
    "closed_at": "2024-11-08T20:23:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2668",
    "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Flow**\r\nIf applicable, add exported flow in order to help replicating the problem.\r\n\r\n**Setup**\r\n\r\n-   Installation [e.g. docker, `npx flowise start`, `pnpm start`]\r\n-   Flowise Version [e.g. 1.2.11]\r\n-   OS: [e.g. macOS, Windows, Linux]\r\n-   Browser [e.g. chrome, safari]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2668/comments",
    "author": "Cuttuer",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-06-19T16:42:33Z",
        "body": "can you provide more info?"
      }
    ]
  },
  {
    "number": 2664,
    "title": "[BUG] Iframe Not Working",
    "created_at": "2024-06-18T08:51:13Z",
    "closed_at": "2024-12-12T02:30:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2664",
    "body": "**Describe the bug**\r\nin flowise 1.8.2 Chatbot Imbedding iFrame no longer works, the chat bot only work within the canvas only\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2664/comments",
    "author": "asrguru20",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-06-19T16:37:23Z",
        "body": "how are you using the iframe? can you give example code?"
      },
      {
        "user": "asrguru20",
        "created_at": "2024-06-20T19:05:16Z",
        "body": "\r\n> how are you using the iframe? can you give example code?\r\n\r\nHi Henry \r\nI have installed The Flowise 1.8.2 in my ubuntu 20.04 server by using Docker, \r\nI have created an agent and used the HTML imbedded iFrame in my WordPress website  for AI Tech support . and when I type or ask something it shows fetch error but when I downgraded to 1.8.1 it fixed \r\nI can attach screen shot or take a video if you want to show you."
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2024-12-12T02:30:56Z",
        "body": "Feel free to re-open if issue still persists"
      }
    ]
  },
  {
    "number": 2585,
    "title": "\"Embedding length mismatch\" on UI in all local chats [BUG]",
    "created_at": "2024-06-05T16:09:31Z",
    "closed_at": "2024-07-26T01:46:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2585",
    "body": "\"Embedding length mismatch\" on UI in all local chats [BUG] ? ",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2585/comments",
    "author": "kamal-rewayz",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-06-12T20:53:39Z",
        "body": "might be a wrong embedding dimension you were using?"
      }
    ]
  },
  {
    "number": 2487,
    "title": "Notion reader on large table times out after 30 seconds",
    "created_at": "2024-05-25T15:52:42Z",
    "closed_at": "2024-06-02T01:37:14Z",
    "labels": [
      "question",
      "setup"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2487",
    "body": "**Describe the bug**\r\nIf I use a notion database node to get KB info from a table and vectorize into pinecone and the data on the notion database is large, clicking on the upsert button after exactly 30 seconds it responds back with a HTTP 504 Gateway timeout error.\r\n\r\nSame table with Airtable doesnt have the same issue.\r\n\r\nUsing Docker deployed using cloudformation on AWS.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a chat flow with notion table, pinecone and chat gpt chat for Q&A. Choose a DB in notion that is large and contains lots of rows.\r\n2. Click on the upsert button\r\n3. After 30 seconds get a timeout 504 popup and see the gateway timeout error.\r\n\r\n**Expected behavior**\r\nThere shouldnt be any timeout and the data from the table should be vectorized into pinecone.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2487/comments",
    "author": "vahidkowsari",
    "comments": [
      {
        "user": "AyushmanChatterjee",
        "created_at": "2024-05-30T11:03:16Z",
        "body": "Hi,\r\n\r\nFacing the same issue using Apify Web Crawler or any other web crawler when the number of pages are a bit more. If the crawling and database upsert completes within approximately 30 seconds, then there is no such error."
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2024-05-31T22:32:34Z",
        "body": "Somehow I think it is related to setup on AWS, are you able to reproduce locally?"
      },
      {
        "user": "vahidkowsari",
        "created_at": "2024-06-01T15:45:54Z",
        "body": "Ah I think this is the culprit (in the AWS Cloudformation file)\r\n\r\n```\r\nPublicLoadBalancer:\r\n    Type: AWS::ElasticLoadBalancingV2::LoadBalancer\r\n    Properties:\r\n      Name: !Join [\"-\", [!Ref Stage, public-lb]]\r\n      Scheme: internet-facing\r\n      LoadBalancerAttributes:\r\n        - Key: idle_timeout.timeout_seconds\r\n          Value: \"30\"\r\n```"
      }
    ]
  },
  {
    "number": 2081,
    "title": "[question]Flowis 如何配置梯子通过中国可以访问呢openai接口",
    "created_at": "2024-04-02T08:19:19Z",
    "closed_at": "2024-05-05T12:06:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2081",
    "body": "1、Flowise 如何配置梯子通过中国可以访问呢openai接口\r\n\r\n2、Flowise 如何配置其他中国的大模型，比如文心一言，智谱等",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2081/comments",
    "author": "huangbz2007",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-04-03T09:26:00Z",
        "body": "目前还没有直接的方式，欢迎任何的pull requests!"
      },
      {
        "user": "crazygo",
        "created_at": "2024-04-07T02:00:12Z",
        "body": "You can set the base url in the LLM model"
      },
      {
        "user": "iceycc",
        "created_at": "2024-07-01T12:32:08Z",
        "body": "> You can set the base url in the LLM model\r\n\r\nHow does it work, and are there any examples?"
      }
    ]
  },
  {
    "number": 2014,
    "title": "Use Flowise for multiple tables",
    "created_at": "2024-03-21T15:29:29Z",
    "closed_at": "2024-05-05T12:02:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/2014",
    "body": "Is it possible to use this tool for multiple tables using the \"SQL Prompt\" template from the marketplace. \r\n\r\nI've been testing it out and as far as I've seen you can only use it for one table at a time.\r\n\r\nAny help is greatly appreciated.",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/2014/comments",
    "author": "BrianMoreno1994",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-03-25T09:06:26Z",
        "body": "You can try using SQL Database Chain as that will get all tables from specified database. Otherwise, you'll have to write custom JS code to get the multiple tables"
      }
    ]
  },
  {
    "number": 1721,
    "title": "[BUG] i cant see my saved flow after i enable database setting at the server/.env file.  ",
    "created_at": "2024-02-13T15:26:44Z",
    "closed_at": "2024-02-19T16:35:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/1721",
    "body": "**Describe the bug**\r\nif i set the database setting, i cant see my saved flow. if i disable it i can see again. is there any way flow transferring between local to database. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. save a flow\r\n2. Go to \"package/server/.env\"\r\n3. enable the database settings. \r\n```\r\nDATABASE_TYPE=postgres\r\nDATABASE_PORT=5432\r\nDATABASE_HOST=localhost\r\nDATABASE_NAME=flowise\r\nDATABASE_USER=postgres\r\nDATABASE_PASSWORD=***\r\n\r\n```\r\n5. restart flowise\r\n6. See error\r\n\r\n**Expected behavior**\r\nbe able to transfer local saved flow to database. or something like that.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Flow**\r\nIf applicable, add exported flow in order to help replicating the problem.\r\n\r\n**Setup**\r\n-   Installation [e.g. git into conda , `yarn install`, `yarn build`, `yarn dev` ]\r\n-   Flowise Version [e.g. 1.15]\r\n-   OS: [ Windows, ]\r\n-   Browser [e.g. brave, chrome]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/1721/comments",
    "author": "nerkasnimo",
    "comments": [
      {
        "user": "Jaredude",
        "created_at": "2024-02-13T20:21:56Z",
        "body": "Change your install to local. Then export the flow you have. Then change your env file. Restart flowise, and import the json file into your flow. You'll also have to setup your credentials. Hope that helps."
      },
      {
        "user": "chungyau97",
        "created_at": "2024-02-14T00:40:50Z",
        "body": "Hi @nerkasnimo,\r\n\r\nThe default database we used is SQLite. \r\nWhen you enable database configuration, you are creating another database, which results in not being able to see your saved chatflows."
      }
    ]
  },
  {
    "number": 1696,
    "title": "[BUG] WebSocket stops responding unexpectedly",
    "created_at": "2024-02-07T19:27:44Z",
    "closed_at": "2024-05-05T11:57:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/1696",
    "body": "**Describe the bug**\r\nThe websocket stops responding (incognito session fixes it).\r\n\r\nThe websocket times out after working successfully multiple times. The only way I have to restore connection is to open a new incognito session. Working on Chrome\r\n\r\nWebSocket connection to '<URL>' failed: WebSocket is closed before the connection is established.\r\nindex-e0da01c4.js:942 WebSocket connection to 'wss://HOST/socket.io/?EIO=4&transport=websocket' failed: WebSocket is closed before the connection is established.",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/1696/comments",
    "author": "DrBanjo11",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-02-10T14:19:28Z",
        "body": "Dont think its Flowise related, maybe a cache on browser that mess it up?"
      },
      {
        "user": "DrBanjo11",
        "created_at": "2024-02-10T14:57:20Z",
        "body": "I think it has to do with connection disposal, how often does Flowise closes the connection if it does not receive the ping?"
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2024-02-19T16:41:09Z",
        "body": "> I think it has to do with connection disposal, how often does Flowise closes the connection if it does not receive the ping?\r\n\r\nthat is the default socketIO config, in Flowise we didnt specify any configuration for timeout"
      }
    ]
  },
  {
    "number": 1484,
    "title": "Request Timed Out [BUG]",
    "created_at": "2024-01-05T04:52:35Z",
    "closed_at": "2024-01-16T23:16:19Z",
    "labels": [
      "question",
      "setup"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/1484",
    "body": "**Describe the bug**\r\nChat box instantly responds with \"Request Timed Out\"\r\n\r\n**To Reproduce**\r\n1. Installed Flowise using the Docker instructions from Flowise Github page.  \r\n2. Loaded the Translator flow from the Marketplace tab\r\n3. Loaded the OpenAI credentials key name and value in credentials and set in ChatOpenAI node\r\n4. Saved Flow\r\n5. Started the chat window\r\n6. Entered text to translate in chat window\r\n7. Got \"Request Timed Out\"\r\n8. Browser debugger console says :\r\n```\r\n:3000/api/v1/internal-prediction/c302....89cd:1 \r\nFailed to load resource: the server responded with a status of 500 (Internal Server Error)\r\n```\r\n\r\n**Expected behavior**\r\nText translated to French (default set in flow)\r\n\r\n**Flow**\r\nStandard Language Translator flow from Marketplace\r\n\r\n**Setup**\r\n-   Installation: Docker\r\n-   Flowise Version: 1.4.8\r\n-   OS: Windows 11\r\n-   Browser: Bug occurring in both Chrome and Brave\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/1484/comments",
    "author": "Digital-Thor",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-01-14T15:07:47Z",
        "body": "Might be related to your local proxy setup?"
      },
      {
        "user": "Digital-Thor",
        "created_at": "2024-01-16T23:16:11Z",
        "body": "> Might be related to your local proxy setup?\r\n\r\nThanks.  Possibly.  I loaded the Dev version in Node and things are working now."
      },
      {
        "user": "Digital-Thor",
        "created_at": "2024-01-16T23:16:46Z",
        "body": "Got Dev version working instead."
      }
    ]
  },
  {
    "number": 1449,
    "title": "[BUG] PG Vector Store Fetch Failed",
    "created_at": "2023-12-30T01:15:22Z",
    "closed_at": "2024-06-19T16:46:56Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/1449",
    "body": "**Describe the bug**\r\ntried to store a document which was almost 80 MB, after increasing the size for files which we can upload at index.ts.\r\nVector store was Postgres\r\n\r\n**To Reproduce**\r\nSimply try to upload a large doc 80MB+ in a postgres DB, locally hosted\r\n\r\n**Expected behavior**\r\nShould work easily as ir does with small docs\r\n\r\n**error below**\r\n\r\nError: TypeError: fetch failed\r\n    at Postgres_VectorStores.upsert (/root/Flowise/packages/components/dist/nodes/vectorstores/Postgres/Postgres.js:58:27)\r\n    at async buildLangchain (/root/Flowise/packages/server/dist/utils/index.js:273:17)\r\n    at async App.upsertVector (/root/Flowise/packages/server/dist/index.js:1299:13)\r\n    at async /root/Flowise/packages/server/dist/index.js:958:13\r\n2023-12-30 00:43:41 [ERROR]: [server]: Error: Error: TypeError: fetch failed\r\n\r\n\r\nAny advise ?\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/1449/comments",
    "author": "sidhellman",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2024-01-04T23:29:53Z",
        "body": "have u tried uploading a smaller file, if that works? `fetch failed` often means its failing to reach the Postgres instance"
      },
      {
        "user": "sidhellman",
        "created_at": "2024-01-04T23:35:07Z",
        "body": "Yes, on smaller files, works perfectly fine. "
      },
      {
        "user": "fejdraus",
        "created_at": "2024-01-11T23:33:44Z",
        "body": "In another reference I wrote a solution to this problem - you need to use \"node-fetch\" instead of \"fetch\", since fetch doesn't timeout correctly. For such changes you need to write to the LangChain library repository, since this is their code."
      }
    ]
  },
  {
    "number": 1121,
    "title": "[QUESTION] \"version\" of the nodes",
    "created_at": "2023-10-24T02:06:40Z",
    "closed_at": "2023-12-07T19:09:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/1121",
    "body": "I saw there is \"version\" in the definition of  `node`, some of them is `1.0` and others is `2.0`, what these versions do there?\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/1121/comments",
    "author": "vincent-pli",
    "comments": [
      {
        "user": "vincent-pli",
        "created_at": "2023-10-24T02:40:11Z",
        "body": "@HenryHengZJ "
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2023-10-25T14:06:09Z",
        "body": "when there is an update on the node input/output parameters, we update the version. This will allow UI to pick it up and notify user that a newer version is available"
      },
      {
        "user": "vincent-pli",
        "created_at": "2023-10-29T23:41:17Z",
        "body": "Just for notification?\r\nIf user use latest version to `build` a original version node, could it backward compatibility？ "
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2023-12-07T19:09:00Z",
        "body": "Yep just for notification, its not backward compatibility unfortunately"
      }
    ]
  },
  {
    "number": 1015,
    "title": "Loading private model from huggingface hub",
    "created_at": "2023-10-07T18:04:51Z",
    "closed_at": "2024-06-19T16:44:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/1015",
    "body": "I have created a basic flow which connects the LLMChain with huggingface hub and a basic prompt. The flow works fine with any public open source models like falcon-7b.\r\nHowever, When I am trying to load my fine-tuned private model from the huggingface hub, it gives me error when I am asking questions:\r\n**Task not found for this model**\r\n\r\nWhat would be the reason for this output?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/1015/comments",
    "author": "murtuzamdahod",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2023-10-09T19:16:03Z",
        "body": "if you just try to use HuggingFace Inference API outside of flowise with your model does it work?"
      },
      {
        "user": "murtuzamdahod",
        "created_at": "2023-10-10T06:31:03Z",
        "body": "Yes, The model works fine when loaded with huggingface library in Python."
      }
    ]
  },
  {
    "number": 360,
    "title": "[BUG] JSON parse error in Prompt Values when used in LLMChain",
    "created_at": "2023-06-18T02:29:28Z",
    "closed_at": "2024-07-26T01:36:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/360",
    "body": "**Describe the bug**\r\nI am using a LLMChain like the Prompt Chaining marketplace template. For the first prompt, I am asking GPT3 to return a JSON object in the response. I am passing the Output Prediction as a Prompt Value for the second prompt to use as a variable in the input text. When I do that, it returns a JSON parse error trying to parse the Prompt Values of the second prompt.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Start with the Prompt Chaining template.\r\n2. Add Open API key to Model inputs\r\n3. Change the first prompt Template to this \"You are an AI who performs one task based on the following objective: {objective}.\r\nRespond with how you would complete this task as a JSON object with your answer as an \"answer\" key:\"\r\n4. Change the second prompt Prompt Values to this so it will populate the result key with the output of the first: \r\n{\r\n  \"objective\": \"{{question}}\",\r\n  \"result\": \"{{llmChain_2.data.instance}}\"\r\n}\r\n5. Run the chat and ask it any question: What is a bug?\r\n6. It will respond with a syntax error like this - SyntaxError: Unexpected token A in JSON at position 49 \r\n\r\n**Expected behavior**\r\nI expect to use the first output as a string to input in the second prompt. The only way to do this is to add it as a Prompt Value.\r\n\r\n**Setup**\r\n-   Installation docker\r\n-   Flowise Version 1.2.12\r\n-   OS: macOS\r\n-   Browser Chrome\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/360/comments",
    "author": "kevjett",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2023-06-19T18:13:29Z",
        "body": "hey @kevjett do you mind to export the flow? \r\n\r\nThis is likely due to incorrect format of JSON prompt values"
      },
      {
        "user": "kevjett",
        "created_at": "2023-06-19T23:07:42Z",
        "body": "Here's the json data for the flow.\r\n\r\n```json\r\n{\"nodes\":[{\"width\":300,\"height\":526,\"id\":\"openAI_2\",\"position\":{\"x\":608.8115177605811,\"y\":-15.28075425600099},\"type\":\"customNode\",\"data\":{\"id\":\"openAI_2\",\"label\":\"OpenAI\",\"name\":\"openAI\",\"type\":\"OpenAI\",\"baseClasses\":[\"OpenAI\",\"BaseLLM\",\"BaseLanguageModel\",\"BaseLangChain\"],\"category\":\"LLMs\",\"description\":\"Wrapper around OpenAI large language models\",\"inputParams\":[{\"label\":\"OpenAI Api Key\",\"name\":\"openAIApiKey\",\"type\":\"password\",\"id\":\"openAI_2-input-openAIApiKey-password\"},{\"label\":\"Model Name\",\"name\":\"modelName\",\"type\":\"options\",\"options\":[{\"label\":\"text-davinci-003\",\"name\":\"text-davinci-003\"},{\"label\":\"text-davinci-002\",\"name\":\"text-davinci-002\"},{\"label\":\"text-curie-001\",\"name\":\"text-curie-001\"},{\"label\":\"text-babbage-001\",\"name\":\"text-babbage-001\"}],\"default\":\"text-davinci-003\",\"optional\":true,\"id\":\"openAI_2-input-modelName-options\"},{\"label\":\"Temperature\",\"name\":\"temperature\",\"type\":\"number\",\"default\":0.7,\"optional\":true,\"id\":\"openAI_2-input-temperature-number\"},{\"label\":\"Max Tokens\",\"name\":\"maxTokens\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_2-input-maxTokens-number\"},{\"label\":\"Top Probability\",\"name\":\"topP\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_2-input-topP-number\"},{\"label\":\"Best Of\",\"name\":\"bestOf\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_2-input-bestOf-number\"},{\"label\":\"Frequency Penalty\",\"name\":\"frequencyPenalty\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_2-input-frequencyPenalty-number\"},{\"label\":\"Presence Penalty\",\"name\":\"presencePenalty\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_2-input-presencePenalty-number\"},{\"label\":\"Batch Size\",\"name\":\"batchSize\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_2-input-batchSize-number\"},{\"label\":\"Timeout\",\"name\":\"timeout\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_2-input-timeout-number\"},{\"label\":\"BasePath\",\"name\":\"basepath\",\"type\":\"string\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_2-input-basepath-string\"}],\"inputAnchors\":[],\"inputs\":{\"modelName\":\"text-davinci-003\",\"temperature\":0.7,\"maxTokens\":\"\",\"topP\":\"\",\"bestOf\":\"\",\"frequencyPenalty\":\"\",\"presencePenalty\":\"\",\"batchSize\":\"\",\"timeout\":\"\"},\"outputAnchors\":[{\"id\":\"openAI_2-output-openAI-OpenAI|BaseLLM|BaseLanguageModel|BaseLangChain\",\"name\":\"openAI\",\"label\":\"OpenAI\",\"type\":\"OpenAI | BaseLLM | BaseLanguageModel | BaseLangChain\"}],\"outputs\":{},\"selected\":false},\"positionAbsolute\":{\"x\":608.8115177605811,\"y\":-15.28075425600099},\"selected\":false,\"dragging\":false},{\"width\":300,\"height\":534,\"id\":\"promptTemplate_2\",\"position\":{\"x\":814.825552985309,\"y\":490.1669513592959},\"type\":\"customNode\",\"data\":{\"id\":\"promptTemplate_2\",\"label\":\"Prompt Template\",\"name\":\"promptTemplate\",\"type\":\"PromptTemplate\",\"baseClasses\":[\"PromptTemplate\",\"BaseStringPromptTemplate\",\"BasePromptTemplate\"],\"category\":\"Prompts\",\"description\":\"Schema to represent a basic prompt for an LLM\",\"inputParams\":[{\"label\":\"Template\",\"name\":\"template\",\"type\":\"string\",\"rows\":4,\"placeholder\":\"What is a good name for a company that makes {product}?\",\"id\":\"promptTemplate_2-input-template-string\"},{\"label\":\"Format Prompt Values\",\"name\":\"promptValues\",\"type\":\"string\",\"rows\":4,\"placeholder\":\"{\\n  \\\"input_language\\\": \\\"English\\\",\\n  \\\"output_language\\\": \\\"French\\\"\\n}\",\"optional\":true,\"acceptVariable\":true,\"list\":true,\"id\":\"promptTemplate_2-input-promptValues-string\"}],\"inputAnchors\":[],\"inputs\":{\"template\":\"You are an AI who performs one task based on the following objective: {objective}.\\nRespond with how you would complete this task in a JSON object with your answer as a \\\"answer\\\" key:\",\"promptValues\":\"{\\n  \\\"objective\\\": \\\"{{question}}\\\"\\n}\"},\"outputAnchors\":[{\"id\":\"promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate\",\"name\":\"promptTemplate\",\"label\":\"PromptTemplate\",\"type\":\"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate\"}],\"outputs\":{},\"selected\":false},\"selected\":false,\"positionAbsolute\":{\"x\":814.825552985309,\"y\":490.1669513592959},\"dragging\":false},{\"width\":300,\"height\":407,\"id\":\"llmChain_2\",\"position\":{\"x\":1225.2861408370582,\"y\":485.62403908243243},\"type\":\"customNode\",\"data\":{\"id\":\"llmChain_2\",\"label\":\"LLM Chain\",\"name\":\"llmChain\",\"type\":\"LLMChain\",\"baseClasses\":[\"LLMChain\",\"BaseChain\",\"BaseLangChain\"],\"category\":\"Chains\",\"description\":\"Chain to run queries against LLMs\",\"inputParams\":[{\"label\":\"Chain Name\",\"name\":\"chainName\",\"type\":\"string\",\"placeholder\":\"Name Your Chain\",\"optional\":true,\"id\":\"llmChain_2-input-chainName-string\"}],\"inputAnchors\":[{\"label\":\"Language Model\",\"name\":\"model\",\"type\":\"BaseLanguageModel\",\"id\":\"llmChain_2-input-model-BaseLanguageModel\"},{\"label\":\"Prompt\",\"name\":\"prompt\",\"type\":\"BasePromptTemplate\",\"id\":\"llmChain_2-input-prompt-BasePromptTemplate\"}],\"inputs\":{\"model\":\"{{openAI_2.data.instance}}\",\"prompt\":\"{{promptTemplate_2.data.instance}}\",\"chainName\":\"First Chain\"},\"outputAnchors\":[{\"name\":\"output\",\"label\":\"Output\",\"type\":\"options\",\"options\":[{\"id\":\"llmChain_2-output-llmChain-LLMChain|BaseChain|BaseLangChain\",\"name\":\"llmChain\",\"label\":\"LLM Chain\",\"type\":\"LLMChain | BaseChain | BaseLangChain\"},{\"id\":\"llmChain_2-output-outputPrediction-string\",\"name\":\"outputPrediction\",\"label\":\"Output Prediction\",\"type\":\"string\"}],\"default\":\"llmChain\"}],\"outputs\":{\"output\":\"outputPrediction\"},\"selected\":false},\"selected\":false,\"dragging\":false,\"positionAbsolute\":{\"x\":1225.2861408370582,\"y\":485.62403908243243}},{\"width\":300,\"height\":534,\"id\":\"promptTemplate_3\",\"position\":{\"x\":1589.206555911206,\"y\":460.23470154201766},\"type\":\"customNode\",\"data\":{\"id\":\"promptTemplate_3\",\"label\":\"Prompt Template\",\"name\":\"promptTemplate\",\"type\":\"PromptTemplate\",\"baseClasses\":[\"PromptTemplate\",\"BaseStringPromptTemplate\",\"BasePromptTemplate\"],\"category\":\"Prompts\",\"description\":\"Schema to represent a basic prompt for an LLM\",\"inputParams\":[{\"label\":\"Template\",\"name\":\"template\",\"type\":\"string\",\"rows\":4,\"placeholder\":\"What is a good name for a company that makes {product}?\",\"id\":\"promptTemplate_3-input-template-string\"},{\"label\":\"Format Prompt Values\",\"name\":\"promptValues\",\"type\":\"string\",\"rows\":4,\"placeholder\":\"{\\n  \\\"input_language\\\": \\\"English\\\",\\n  \\\"output_language\\\": \\\"French\\\"\\n}\",\"optional\":true,\"acceptVariable\":true,\"list\":true,\"id\":\"promptTemplate_3-input-promptValues-string\"}],\"inputAnchors\":[],\"inputs\":{\"template\":\"You are a task creation AI that uses the result of an execution agent to create new tasks with the following objective: {objective}.\\nThe last completed task has the result: {result}.\\nBased on the result, create new tasks to be completed by the AI system that do not overlap with result.\\nReturn the tasks as an array.\",\"promptValues\":\"{\\n  \\\"objective\\\": \\\"{{question}}\\\",\\n  \\\"result\\\": \\\"{{llmChain_2.data.instance}}\\\"\\n}\"},\"outputAnchors\":[{\"id\":\"promptTemplate_3-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate\",\"name\":\"promptTemplate\",\"label\":\"PromptTemplate\",\"type\":\"PromptTemplate | BaseStringPromptTemplate | BasePromptTemplate\"}],\"outputs\":{},\"selected\":false},\"selected\":false,\"positionAbsolute\":{\"x\":1589.206555911206,\"y\":460.23470154201766},\"dragging\":false},{\"width\":300,\"height\":526,\"id\":\"openAI_3\",\"position\":{\"x\":1225.2861408370586,\"y\":-62.7856517905272},\"type\":\"customNode\",\"data\":{\"id\":\"openAI_3\",\"label\":\"OpenAI\",\"name\":\"openAI\",\"type\":\"OpenAI\",\"baseClasses\":[\"OpenAI\",\"BaseLLM\",\"BaseLanguageModel\",\"BaseLangChain\"],\"category\":\"LLMs\",\"description\":\"Wrapper around OpenAI large language models\",\"inputParams\":[{\"label\":\"OpenAI Api Key\",\"name\":\"openAIApiKey\",\"type\":\"password\",\"id\":\"openAI_3-input-openAIApiKey-password\"},{\"label\":\"Model Name\",\"name\":\"modelName\",\"type\":\"options\",\"options\":[{\"label\":\"text-davinci-003\",\"name\":\"text-davinci-003\"},{\"label\":\"text-davinci-002\",\"name\":\"text-davinci-002\"},{\"label\":\"text-curie-001\",\"name\":\"text-curie-001\"},{\"label\":\"text-babbage-001\",\"name\":\"text-babbage-001\"}],\"default\":\"text-davinci-003\",\"optional\":true,\"id\":\"openAI_3-input-modelName-options\"},{\"label\":\"Temperature\",\"name\":\"temperature\",\"type\":\"number\",\"default\":0.7,\"optional\":true,\"id\":\"openAI_3-input-temperature-number\"},{\"label\":\"Max Tokens\",\"name\":\"maxTokens\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_3-input-maxTokens-number\"},{\"label\":\"Top Probability\",\"name\":\"topP\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_3-input-topP-number\"},{\"label\":\"Best Of\",\"name\":\"bestOf\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_3-input-bestOf-number\"},{\"label\":\"Frequency Penalty\",\"name\":\"frequencyPenalty\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_3-input-frequencyPenalty-number\"},{\"label\":\"Presence Penalty\",\"name\":\"presencePenalty\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_3-input-presencePenalty-number\"},{\"label\":\"Batch Size\",\"name\":\"batchSize\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_3-input-batchSize-number\"},{\"label\":\"Timeout\",\"name\":\"timeout\",\"type\":\"number\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_3-input-timeout-number\"},{\"label\":\"BasePath\",\"name\":\"basepath\",\"type\":\"string\",\"optional\":true,\"additionalParams\":true,\"id\":\"openAI_3-input-basepath-string\"}],\"inputAnchors\":[],\"inputs\":{\"modelName\":\"text-davinci-003\",\"temperature\":0.7,\"maxTokens\":\"\",\"topP\":\"\",\"bestOf\":\"\",\"frequencyPenalty\":\"\",\"presencePenalty\":\"\",\"batchSize\":\"\",\"timeout\":\"\"},\"outputAnchors\":[{\"id\":\"openAI_3-output-openAI-OpenAI|BaseLLM|BaseLanguageModel|BaseLangChain\",\"name\":\"openAI\",\"label\":\"OpenAI\",\"type\":\"OpenAI | BaseLLM | BaseLanguageModel | BaseLangChain\"}],\"outputs\":{},\"selected\":false},\"positionAbsolute\":{\"x\":1225.2861408370586,\"y\":-62.7856517905272},\"selected\":false,\"dragging\":false},{\"width\":300,\"height\":407,\"id\":\"llmChain_3\",\"position\":{\"x\":1972.2671768945252,\"y\":142.73435419451476},\"type\":\"customNode\",\"data\":{\"id\":\"llmChain_3\",\"label\":\"LLM Chain\",\"name\":\"llmChain\",\"type\":\"LLMChain\",\"baseClasses\":[\"LLMChain\",\"BaseChain\",\"BaseLangChain\"],\"category\":\"Chains\",\"description\":\"Chain to run queries against LLMs\",\"inputParams\":[{\"label\":\"Chain Name\",\"name\":\"chainName\",\"type\":\"string\",\"placeholder\":\"Name Your Chain\",\"optional\":true,\"id\":\"llmChain_3-input-chainName-string\"}],\"inputAnchors\":[{\"label\":\"Language Model\",\"name\":\"model\",\"type\":\"BaseLanguageModel\",\"id\":\"llmChain_3-input-model-BaseLanguageModel\"},{\"label\":\"Prompt\",\"name\":\"prompt\",\"type\":\"BasePromptTemplate\",\"id\":\"llmChain_3-input-prompt-BasePromptTemplate\"}],\"inputs\":{\"model\":\"{{openAI_3.data.instance}}\",\"prompt\":\"{{promptTemplate_3.data.instance}}\",\"chainName\":\"LastChain\"},\"outputAnchors\":[{\"name\":\"output\",\"label\":\"Output\",\"type\":\"options\",\"options\":[{\"id\":\"llmChain_3-output-llmChain-LLMChain|BaseChain|BaseLangChain\",\"name\":\"llmChain\",\"label\":\"LLM Chain\",\"type\":\"LLMChain | BaseChain | BaseLangChain\"},{\"id\":\"llmChain_3-output-outputPrediction-string\",\"name\":\"outputPrediction\",\"label\":\"Output Prediction\",\"type\":\"string\"}],\"default\":\"llmChain\"}],\"outputs\":{\"output\":\"llmChain\"},\"selected\":false},\"selected\":false,\"dragging\":false,\"positionAbsolute\":{\"x\":1972.2671768945252,\"y\":142.73435419451476}}],\"edges\":[{\"source\":\"llmChain_2\",\"sourceHandle\":\"llmChain_2-output-outputPrediction-string\",\"target\":\"promptTemplate_3\",\"targetHandle\":\"promptTemplate_3-input-promptValues-string\",\"type\":\"buttonedge\",\"id\":\"llmChain_2-llmChain_2-output-outputPrediction-string-promptTemplate_3-promptTemplate_3-input-promptValues-string\",\"data\":{\"label\":\"\"}},{\"source\":\"openAI_2\",\"sourceHandle\":\"openAI_2-output-openAI-OpenAI|BaseLLM|BaseLanguageModel|BaseLangChain\",\"target\":\"llmChain_2\",\"targetHandle\":\"llmChain_2-input-model-BaseLanguageModel\",\"type\":\"buttonedge\",\"id\":\"openAI_2-openAI_2-output-openAI-OpenAI|BaseLLM|BaseLanguageModel|BaseLangChain-llmChain_2-llmChain_2-input-model-BaseLanguageModel\",\"data\":{\"label\":\"\"}},{\"source\":\"promptTemplate_2\",\"sourceHandle\":\"promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate\",\"target\":\"llmChain_2\",\"targetHandle\":\"llmChain_2-input-prompt-BasePromptTemplate\",\"type\":\"buttonedge\",\"id\":\"promptTemplate_2-promptTemplate_2-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate-llmChain_2-llmChain_2-input-prompt-BasePromptTemplate\",\"data\":{\"label\":\"\"}},{\"source\":\"openAI_3\",\"sourceHandle\":\"openAI_3-output-openAI-OpenAI|BaseLLM|BaseLanguageModel|BaseLangChain\",\"target\":\"llmChain_3\",\"targetHandle\":\"llmChain_3-input-model-BaseLanguageModel\",\"type\":\"buttonedge\",\"id\":\"openAI_3-openAI_3-output-openAI-OpenAI|BaseLLM|BaseLanguageModel|BaseLangChain-llmChain_3-llmChain_3-input-model-BaseLanguageModel\",\"data\":{\"label\":\"\"}},{\"source\":\"promptTemplate_3\",\"sourceHandle\":\"promptTemplate_3-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate\",\"target\":\"llmChain_3\",\"targetHandle\":\"llmChain_3-input-prompt-BasePromptTemplate\",\"type\":\"buttonedge\",\"id\":\"promptTemplate_3-promptTemplate_3-output-promptTemplate-PromptTemplate|BaseStringPromptTemplate|BasePromptTemplate-llmChain_3-llmChain_3-input-prompt-BasePromptTemplate\",\"data\":{\"label\":\"\"}}]}\r\n```\r\n"
      },
      {
        "user": "dimknaf",
        "created_at": "2023-06-20T20:57:08Z",
        "body": "{{llmChain_2.data.instance}}, this was not working for me either.\r\nI turned it to {{llmChain_0.data.instance}} and worked. "
      },
      {
        "user": "kevjett",
        "created_at": "2023-06-20T22:00:34Z",
        "body": "Changing that variable did not work. Did you check your debug logs? For me, it didn't pass the result of the first chain into the second chain. It only passed that variable name into the prompt, not the value. See here in the debug info for the second prompt.\r\n\r\n`[llm/start] [1:chain:LLMChain > 2:llm:OpenAI] Entering LLM run with input:\r\n {\r\n  \"prompts\": [\r\n    \"You are a task creation AI that uses the result of an execution agent to create new tasks with the following objective: Howtochangecaroil?.\\nThe last completed task has the result: {{llmChain_0.data.instance}}.\\nBased on the result, create new tasks to be completed by the AI system that do not overlap with result.\\nReturn the tasks as an array.\"\r\n  ]\r\n}`\r\n\r\n\r\nThe result of the first prompt is a json formatted string which should have been passed into the second prompt."
      },
      {
        "user": "dimknaf",
        "created_at": "2023-06-20T22:33:14Z",
        "body": "Where can I find the debug info?\r\n"
      },
      {
        "user": "kevjett",
        "created_at": "2023-06-21T02:28:54Z",
        "body": "I'm using docker so it's in my docker desktop app under the logs for the container. If you pass DEBUG=true as an environment variable, it will show all debug info in the container logs."
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2023-06-21T19:53:46Z",
        "body": "> 5\\. What is a bug\r\n\r\nI just tried it there it was because how you asked the chain to return as JSON and thats why its doesn't parse correctly.\r\n\r\nInstead of this:\r\n`You are an AI who performs one task based on the following objective: {objective}.\r\nRespond with how you would complete this task in a JSON object with your answer as a \"answer\" key:`\r\n\r\nI'd remove the JSON part like this:\r\n`You are an AI who performs one task based on the following objective: {objective}.\r\nRespond with how you would complete this task:`\r\n\r\nand this works for me"
      },
      {
        "user": "kevjett",
        "created_at": "2023-06-21T22:37:06Z",
        "body": "Removing the JSON and having it only return text has always worked fine. But my use case is to ask GPT to evaluated a dataset, then return a complex JSON object with specific keys. Then I will take that response and feed it back into a second chain with further instructions to refine the JSON again. It would best with simpler instructions with two chains than trying to do it all in one prompt. My only issue is how the code is written to do a JSON.parse on the 2nd chain’s Prompt Variables. The quotes are causing the main issue. Since it’s passed directly from the 1st chain without escaping any of the text, it treats the first quote as a closing quote in the Prompt Variable json data. I think it needs to escape the quotes to fix the issue. \r\n\r\nI haven’t tried it yet but I think the same problem would happen if you ask GTP to return a famous quote that’s surrounded by quotes. "
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2024-07-26T01:36:40Z",
        "body": "Closing for now, feel free to reopen if issue persists"
      },
      {
        "user": "mehrajagdish",
        "created_at": "2024-07-27T16:31:25Z",
        "body": "Using this in prompt template\r\n\r\nOutput your script in this JSON format:\r\n\r\n{\r\n  \"greeting\": \"Hello {name}, how are you ?\"\r\n}\r\n\r\nif i simply write a name instead of {name} it works , but doesnt work this way "
      }
    ]
  },
  {
    "number": 144,
    "title": "[FEATURE] Keeping older messages in the chat and the initial message as reference",
    "created_at": "2023-05-21T20:21:09Z",
    "closed_at": "2024-03-25T09:44:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/144",
    "body": "**Describe the feature you'd like**\r\nHaving older messages in the chat, and the initial prompt may only be sent at the top of the messages for reference, so that the OPEN-AI API may respond accordingly, as I see right now that it just replaces the text with placeholder which is not viable for each time a user is sending a message, it should only be keeping the context for the first time\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/144/comments",
    "author": "kdhaka94",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2023-05-23T20:21:49Z",
        "body": "hey im not sure if im following. \r\n\r\nmind clarifying more with logs and screenshots?"
      }
    ]
  },
  {
    "number": 139,
    "title": "Save and run of bot from editor causes the save password dialog",
    "created_at": "2023-05-20T21:02:20Z",
    "closed_at": "2024-03-25T09:44:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/139",
    "body": "**Describe the bug**\r\nAfter creating a new flow, save and execute cause the browser save password dialog to pop up.\r\n\r\n**To Reproduce**\r\nCreate a new flow.\r\nClick save.\r\n\r\n**Expected behavior**\r\nlocalhost:3000 in editor mode should just save the flow.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/139/comments",
    "author": "nothans",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2023-05-23T20:19:43Z",
        "body": "do you have a screenshot of which dialog you meant?"
      }
    ]
  },
  {
    "number": 77,
    "title": "Hoax.JS.ExtMsg.a",
    "created_at": "2023-05-08T12:51:12Z",
    "closed_at": "2023-07-05T21:09:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/77",
    "body": "Event: Object deleted\r\nUser: \r\nUser type: Initiator\r\nApplication name: node.exe\r\nApplication path: C:\\Program Files\\nodejs\r\nComponent: File Anti-Virus\r\nResult description: Deleted\r\nType: Software that may cause harm\r\nName: Hoax.JS.ExtMsg.a\r\nPrecision: Exactly\r\nThreat level: Medium\r\nObject type: File\r\nObject name: _postinstall.js\r\nObject path: C:\\Users\\xxx\\AppData\\Roaming\\npm\\node_modules\\flowise\\node_modules\\es5-ext\r\nMD5 of an object: 7DE8D84BD9ECC1D0904048956C94817B",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/77/comments",
    "author": "neoOpus",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2023-05-23T22:39:24Z",
        "body": "hey sorry but Im not following the error here\r\n\r\nwhats the steps to replicate, your OS? Kindly follow the template so we can help to identify the issue"
      }
    ]
  },
  {
    "number": 261,
    "title": "[API input configuration]",
    "created_at": "2023-06-05T18:18:49Z",
    "closed_at": "2023-11-29T11:43:39Z",
    "labels": [
      "enhancement",
      "good first issue",
      "help wanted"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/261",
    "body": "**Describe the feature you'd like**\r\nConsider updating the documentation for API input configuration using a context manager for input files to upload\r\n\r\nProposed code:\r\n\r\n```python\r\ndata = {\r\n    \"chunkSize\": 1000,\r\n    \"chunkOverlap\": 200,\r\n    \"metadata\": file_path,\r\n    \"question\": \"what is Pierre mentioning about Paris\"\r\n}\r\n\r\ndef query(data):\r\n    # Use a context manager to handle the file\r\n    with open(file_path, 'rb') as f:\r\n        # use form data to upload files\r\n        form_data = {\r\n            \"files\": (file_name, f)\r\n        }\r\n        response = requests.post(API_URL, files=form_data, data=data)\r\n    return response.json()\r\n\r\noutput = query(data)\r\noutput\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/261/comments",
    "author": "cnndabbler",
    "comments": [
      {
        "user": "govind-kumarr",
        "created_at": "2023-06-09T07:02:42Z",
        "body": "@HenryHengZJ  what needs to be done here? I can do this if you give me some hints."
      }
    ]
  },
  {
    "number": 154,
    "title": "[FEATURE]Can you add milvus to project?",
    "created_at": "2023-05-23T07:14:01Z",
    "closed_at": "2023-08-24T00:35:41Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/FlowiseAI/Flowise/issues/154",
    "body": "Langchainjs support milvus.\r\nCan you add milvus to Flowise?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FlowiseAI/Flowise/issues/154/comments",
    "author": "lhkg1988",
    "comments": [
      {
        "user": "HenryHengZJ",
        "created_at": "2023-05-23T11:24:05Z",
        "body": "certainly! work in progress :)"
      },
      {
        "user": "hujghc",
        "created_at": "2023-06-13T07:26:44Z",
        "body": "Is there any new progress? "
      },
      {
        "user": "lhkg1988",
        "created_at": "2023-06-14T01:56:55Z",
        "body": "This branch feature/Milvus, but problem <out of memory>"
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2023-06-18T21:09:51Z",
        "body": "I tried working on it on this branch `feature/Milvus`, but keep getting heap memory issue. Appreciate if anyone can chime in"
      },
      {
        "user": "shanghaikid",
        "created_at": "2023-07-10T03:08:20Z",
        "body": "> I tried working on it on this branch `feature/Milvus`, but keep getting heap memory issue. Appreciate if anyone can chime in\r\n\r\nWhat's the error ? "
      },
      {
        "user": "shanghaikid",
        "created_at": "2023-07-11T14:29:34Z",
        "body": "> I tried working on it on this branch `feature/Milvus`, but keep getting heap memory issue. Appreciate if anyone can chime in\r\n\r\nSeems like the GRPC client can not be reused after the vector store class initialized , it keeps retrying to connect until hitting the memory limit.  Don't know why... "
      },
      {
        "user": "shanghaikid",
        "created_at": "2023-07-13T01:47:06Z",
        "body": "I understand the problem, here\r\n\r\n```javascript\r\n        this.app.get('/api/v1/nodes', (req: Request, res: Response) => {\r\n            const returnData = []\r\n            for (const nodeName in this.nodesPool.componentNodes) {\r\n                const clonedNode = cloneDeep(this.nodesPool.componentNodes[nodeName])\r\n                returnData.push(clonedNode)\r\n            }\r\n            return res.json(returnData)\r\n        })\r\n```\r\nYou deep clone the node, milvus node sdk uses grpc to communicate with the milvus server, when it is initialized, it builds the GRPC channel to the server, but after you clone the object, the GRPC channel is not allowed to be reused, so later requests can not be finished, and the grpc node package will retry to connect,  again and again, then we hit the memory limit. \r\n\r\nIs there a way that we can avoid this cloneDeep?  "
      },
      {
        "user": "shanghaikid",
        "created_at": "2023-07-13T01:47:24Z",
        "body": "@HenryHengZJ "
      },
      {
        "user": "shanghaikid",
        "created_at": "2023-07-13T02:50:42Z",
        "body": "```javascript\r\n// in processPrediction\r\n const nodeToExecute = reactFlowNodes.find((node: IReactFlowNode) => node.id === endingNodeId)\r\n                    if (!nodeToExecute) return res.status(404).send(`Node ${endingNodeId} not found`)\r\n\r\n                    const reactFlowNodeData: INodeData = resolveVariables(nodeToExecute.data, reactFlowNodes, incomingInput.question)\r\n                    nodeToExecuteData = reactFlowNodeData\r\n\r\n                    const startingNodes = nodes.filter((nd) => startingNodeIds.includes(nd.id))\r\n                    this.chatflowPool.add(chatflowid, nodeToExecuteData, startingNodes, incomingInput?.overrideConfig)\r\n\r\n```\r\n\r\n```javascript\r\n\r\nexport const resolveVariables = (reactFlowNodeData: INodeData, reactFlowNodes: IReactFlowNode[], question: string): INodeData => {\r\n    let flowNodeData = cloneDeep(reactFlowNodeData)\r\n    if (reactFlowNodeData.instance && isVectorStoreFaiss(reactFlowNodeData)) {\r\n        // omit and merge because cloneDeep of instance gives \"Illegal invocation\" Exception\r\n        const flowNodeDataWithoutInstance = cloneDeep(omit(reactFlowNodeData, ['instance']))\r\n        flowNodeData = merge(flowNodeDataWithoutInstance, { instance: reactFlowNodeData.instance })\r\n    }\r\n    const types = 'inputs'\r\n\r\n    const getParamValues = (paramsObj: ICommonObject) => {\r\n        for (const key in paramsObj) {\r\n            const paramValue: string = paramsObj[key]\r\n            if (Array.isArray(paramValue)) {\r\n                const resolvedInstances = []\r\n                for (const param of paramValue) {\r\n                    const resolvedInstance = getVariableValue(param, reactFlowNodes, question)\r\n                    resolvedInstances.push(resolvedInstance)\r\n                }\r\n                paramsObj[key] = resolvedInstances\r\n            } else {\r\n                const isAcceptVariable = reactFlowNodeData.inputParams.find((param) => param.name === key)?.acceptVariable ?? false\r\n                const resolvedInstance = getVariableValue(paramValue, reactFlowNodes, question, isAcceptVariable)\r\n                paramsObj[key] = resolvedInstance\r\n            }\r\n        }\r\n    }\r\n\r\n    const paramsObj = flowNodeData[types] ?? {}\r\n\r\n    getParamValues(paramsObj)\r\n\r\n    return flowNodeData\r\n}\r\n\r\n```\r\n\r\nlooks like here"
      },
      {
        "user": "HenryHengZJ",
        "created_at": "2023-08-24T00:35:41Z",
        "body": "Milvus is now supported on Flowise! Closing as #794 merged"
      }
    ]
  }
]